{
    "RECORDS": [
        {
            "_id": "62e60f3bd76274f8a4026e10",
            "all_context": "{ \"import\" : \"\", \"file\" : \"\", \"class\" : \"\" }",
            "code": "def dehydrate_timedelta(value):\n    \"\"\" Dehydrator for `timedelta` values.\n\n    :param value:\n    :type value: timedelta\n    :return:\n    \"\"\"\n    months = 0\n    days = value.days\n    seconds = value.seconds\n    nanoseconds = 1000 * value.microseconds\n    return Structure(b\"E\", months, days, seconds, nanoseconds)\n",
            "dependency": "",
            "docstring": "Dehydrator for `timedelta` values.\n\n:param value:\n:type value: timedelta\n:return:",
            "end_lineno": "207",
            "file_content": "# Copyright (c) \"Neo4j\"\n# Neo4j Sweden AB [https://neo4j.com]\n#\n# This file is part of Neo4j.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom datetime import (\n    datetime,\n    time,\n    timedelta,\n)\n\nfrom ....time import (\n    Date,\n    DateTime,\n    Duration,\n    Time,\n)\nfrom ...packstream import Structure\n\n\ndef get_date_unix_epoch():\n    return Date(1970, 1, 1)\n\n\ndef get_date_unix_epoch_ordinal():\n    return get_date_unix_epoch().to_ordinal()\n\n\ndef get_datetime_unix_epoch_utc():\n    from pytz import utc\n    return DateTime(1970, 1, 1, 0, 0, 0, utc)\n\n\ndef hydrate_date(days):\n    \"\"\" Hydrator for `Date` values.\n\n    :param days:\n    :return: Date\n    \"\"\"\n    return Date.from_ordinal(get_date_unix_epoch_ordinal() + days)\n\n\ndef dehydrate_date(value):\n    \"\"\" Dehydrator for `date` values.\n\n    :param value:\n    :type value: Date\n    :return:\n    \"\"\"\n    return Structure(b\"D\", value.toordinal() - get_date_unix_epoch().toordinal())\n\n\ndef hydrate_time(nanoseconds, tz=None):\n    \"\"\" Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds:\n    :param tz:\n    :return: Time\n    \"\"\"\n    from pytz import FixedOffset\n    seconds, nanoseconds = map(int, divmod(nanoseconds, 1000000000))\n    minutes, seconds = map(int, divmod(seconds, 60))\n    hours, minutes = map(int, divmod(minutes, 60))\n    t = Time(hours, minutes, seconds, nanoseconds)\n    if tz is None:\n        return t\n    tz_offset_minutes, tz_offset_seconds = divmod(tz, 60)\n    zone = FixedOffset(tz_offset_minutes)\n    return zone.localize(t)\n\n\ndef dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :return:\n    \"\"\"\n    if isinstance(value, Time):\n        nanoseconds = value.ticks\n    elif isinstance(value, time):\n        nanoseconds = (3600000000000 * value.hour + 60000000000 * value.minute +\n                       1000000000 * value.second + 1000 * value.microsecond)\n    else:\n        raise TypeError(\"Value must be a neo4j.time.Time or a datetime.time\")\n    if value.tzinfo:\n        return Structure(b\"T\", nanoseconds,\n                         int(value.tzinfo.utcoffset(value).total_seconds()))\n    else:\n        return Structure(b\"t\", nanoseconds)\n\n\ndef hydrate_datetime(seconds, nanoseconds, tz=None):\n    \"\"\" Hydrator for `DateTime` and `LocalDateTime` values.\n\n    :param seconds:\n    :param nanoseconds:\n    :param tz:\n    :return: datetime\n    \"\"\"\n    from pytz import (\n        FixedOffset,\n        timezone,\n    )\n    minutes, seconds = map(int, divmod(seconds, 60))\n    hours, minutes = map(int, divmod(minutes, 60))\n    days, hours = map(int, divmod(hours, 24))\n    t = DateTime.combine(\n        Date.from_ordinal(get_date_unix_epoch_ordinal() + days),\n        Time(hours, minutes, seconds, nanoseconds)\n    )\n    if tz is None:\n        return t\n    if isinstance(tz, int):\n        tz_offset_minutes, tz_offset_seconds = divmod(tz, 60)\n        zone = FixedOffset(tz_offset_minutes)\n    else:\n        zone = timezone(tz)\n    return zone.localize(t)\n\n\ndef dehydrate_datetime(value):\n    \"\"\" Dehydrator for `datetime` values.\n\n    :param value:\n    :type value: datetime or DateTime\n    :return:\n    \"\"\"\n\n    def seconds_and_nanoseconds(dt):\n        if isinstance(dt, datetime):\n            dt = DateTime.from_native(dt)\n        zone_epoch = DateTime(1970, 1, 1, tzinfo=dt.tzinfo)\n        dt_clock_time = dt.to_clock_time()\n        zone_epoch_clock_time = zone_epoch.to_clock_time()\n        t = dt_clock_time - zone_epoch_clock_time\n        return t.seconds, t.nanoseconds\n\n    tz = value.tzinfo\n    if tz is None:\n        # without time zone\n        from pytz import utc\n        value = utc.localize(value)\n        seconds, nanoseconds = seconds_and_nanoseconds(value)\n        return Structure(b\"d\", seconds, nanoseconds)\n    elif hasattr(tz, \"zone\") and tz.zone and isinstance(tz.zone, str):\n        # with named pytz time zone\n        seconds, nanoseconds = seconds_and_nanoseconds(value)\n        return Structure(b\"f\", seconds, nanoseconds, tz.zone)\n    elif hasattr(tz, \"key\") and tz.key and isinstance(tz.key, str):\n        # with named zoneinfo (Python 3.9+) time zone\n        seconds, nanoseconds = seconds_and_nanoseconds(value)\n        return Structure(b\"f\", seconds, nanoseconds, tz.key)\n    else:\n        # with time offset\n        seconds, nanoseconds = seconds_and_nanoseconds(value)\n        return Structure(b\"F\", seconds, nanoseconds,\n                         int(tz.utcoffset(value).total_seconds()))\n\n\ndef hydrate_duration(months, days, seconds, nanoseconds):\n    \"\"\" Hydrator for `Duration` values.\n\n    :param months:\n    :param days:\n    :param seconds:\n    :param nanoseconds:\n    :return: `duration` namedtuple\n    \"\"\"\n    return Duration(months=months, days=days, seconds=seconds, nanoseconds=nanoseconds)\n\n\ndef dehydrate_duration(value):\n    \"\"\" Dehydrator for `duration` values.\n\n    :param value:\n    :type value: Duration\n    :return:\n    \"\"\"\n    return Structure(b\"E\", value.months, value.days, value.seconds, value.nanoseconds)\n\n\ndef dehydrate_timedelta(value):\n    \"\"\" Dehydrator for `timedelta` values.\n\n    :param value:\n    :type value: timedelta\n    :return:\n    \"\"\"\n    months = 0\n    days = value.days\n    seconds = value.seconds\n    nanoseconds = 1000 * value.microseconds\n    return Structure(b\"E\", months, days, seconds, nanoseconds)\n",
            "file_path": "neo4j/_codec/hydration/v1/temporal.py",
            "human_label": "Use the value in timedelta to generate the Structure class.",
            "level": "project_runnable",
            "lineno": "196",
            "name": "dehydrate_timedelta",
            "oracle_context": "{ \"apis\" : \"[]\", \"classes\" : \"['Structure']\", \"vars\" : \"['days', 'seconds', 'microseconds']\" }",
            "package": "temporal",
            "project": "neo4j/neo4j-python-driver",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62e60f37d76274f8a4026dfd",
            "all_context": "{ \"import\" : \"\", \"file\" : \"\", \"class\" : \"\" }",
            "code": "def dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :return:\n    \"\"\"\n    if isinstance(value, Time):\n        nanoseconds = value.ticks\n    elif isinstance(value, time):\n        nanoseconds = (3600000000000 * value.hour + 60000000000 * value.minute +\n                       1000000000 * value.second + 1000 * value.microsecond)\n    else:\n        raise TypeError(\"Value must be a neo4j.time.Time or a datetime.time\")\n    if value.tzinfo:\n        return Structure(b\"T\", nanoseconds,\n                         int(value.tzinfo.utcoffset(value).total_seconds()))\n    else:\n        return Structure(b\"t\", nanoseconds)\n",
            "dependency": "",
            "docstring": "Dehydrator for `time` values.\n\n:param value:\n:type value: Time\n:return:",
            "end_lineno": "103",
            "file_content": "# Copyright (c) \"Neo4j\"\n# Neo4j Sweden AB [https://neo4j.com]\n#\n# This file is part of Neo4j.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom datetime import (\n    datetime,\n    time,\n    timedelta,\n)\n\nfrom ....time import (\n    Date,\n    DateTime,\n    Duration,\n    Time,\n)\nfrom ...packstream import Structure\n\n\ndef get_date_unix_epoch():\n    return Date(1970, 1, 1)\n\n\ndef get_date_unix_epoch_ordinal():\n    return get_date_unix_epoch().to_ordinal()\n\n\ndef get_datetime_unix_epoch_utc():\n    from pytz import utc\n    return DateTime(1970, 1, 1, 0, 0, 0, utc)\n\n\ndef hydrate_date(days):\n    \"\"\" Hydrator for `Date` values.\n\n    :param days:\n    :return: Date\n    \"\"\"\n    return Date.from_ordinal(get_date_unix_epoch_ordinal() + days)\n\n\ndef dehydrate_date(value):\n    \"\"\" Dehydrator for `date` values.\n\n    :param value:\n    :type value: Date\n    :return:\n    \"\"\"\n    return Structure(b\"D\", value.toordinal() - get_date_unix_epoch().toordinal())\n\n\ndef hydrate_time(nanoseconds, tz=None):\n    \"\"\" Hydrator for `Time` and `LocalTime` values.\n\n    :param nanoseconds:\n    :param tz:\n    :return: Time\n    \"\"\"\n    from pytz import FixedOffset\n    seconds, nanoseconds = map(int, divmod(nanoseconds, 1000000000))\n    minutes, seconds = map(int, divmod(seconds, 60))\n    hours, minutes = map(int, divmod(minutes, 60))\n    t = Time(hours, minutes, seconds, nanoseconds)\n    if tz is None:\n        return t\n    tz_offset_minutes, tz_offset_seconds = divmod(tz, 60)\n    zone = FixedOffset(tz_offset_minutes)\n    return zone.localize(t)\n\n\ndef dehydrate_time(value):\n    \"\"\" Dehydrator for `time` values.\n\n    :param value:\n    :type value: Time\n    :return:\n    \"\"\"\n    if isinstance(value, Time):\n        nanoseconds = value.ticks\n    elif isinstance(value, time):\n        nanoseconds = (3600000000000 * value.hour + 60000000000 * value.minute +\n                       1000000000 * value.second + 1000 * value.microsecond)\n    else:\n        raise TypeError(\"Value must be a neo4j.time.Time or a datetime.time\")\n    if value.tzinfo:\n        return Structure(b\"T\", nanoseconds,\n                         int(value.tzinfo.utcoffset(value).total_seconds()))\n    else:\n        return Structure(b\"t\", nanoseconds)\n\n\ndef hydrate_datetime(seconds, nanoseconds, tz=None):\n    \"\"\" Hydrator for `DateTime` and `LocalDateTime` values.\n\n    :param seconds:\n    :param nanoseconds:\n    :param tz:\n    :return: datetime\n    \"\"\"\n    from pytz import (\n        FixedOffset,\n        timezone,\n    )\n    minutes, seconds = map(int, divmod(seconds, 60))\n    hours, minutes = map(int, divmod(minutes, 60))\n    days, hours = map(int, divmod(hours, 24))\n    t = DateTime.combine(\n        Date.from_ordinal(get_date_unix_epoch_ordinal() + days),\n        Time(hours, minutes, seconds, nanoseconds)\n    )\n    if tz is None:\n        return t\n    if isinstance(tz, int):\n        tz_offset_minutes, tz_offset_seconds = divmod(tz, 60)\n        zone = FixedOffset(tz_offset_minutes)\n    else:\n        zone = timezone(tz)\n    return zone.localize(t)\n\n\ndef dehydrate_datetime(value):\n    \"\"\" Dehydrator for `datetime` values.\n\n    :param value:\n    :type value: datetime or DateTime\n    :return:\n    \"\"\"\n\n    def seconds_and_nanoseconds(dt):\n        if isinstance(dt, datetime):\n            dt = DateTime.from_native(dt)\n        zone_epoch = DateTime(1970, 1, 1, tzinfo=dt.tzinfo)\n        dt_clock_time = dt.to_clock_time()\n        zone_epoch_clock_time = zone_epoch.to_clock_time()\n        t = dt_clock_time - zone_epoch_clock_time\n        return t.seconds, t.nanoseconds\n\n    tz = value.tzinfo\n    if tz is None:\n        # without time zone\n        from pytz import utc\n        value = utc.localize(value)\n        seconds, nanoseconds = seconds_and_nanoseconds(value)\n        return Structure(b\"d\", seconds, nanoseconds)\n    elif hasattr(tz, \"zone\") and tz.zone and isinstance(tz.zone, str):\n        # with named pytz time zone\n        seconds, nanoseconds = seconds_and_nanoseconds(value)\n        return Structure(b\"f\", seconds, nanoseconds, tz.zone)\n    elif hasattr(tz, \"key\") and tz.key and isinstance(tz.key, str):\n        # with named zoneinfo (Python 3.9+) time zone\n        seconds, nanoseconds = seconds_and_nanoseconds(value)\n        return Structure(b\"f\", seconds, nanoseconds, tz.key)\n    else:\n        # with time offset\n        seconds, nanoseconds = seconds_and_nanoseconds(value)\n        return Structure(b\"F\", seconds, nanoseconds,\n                         int(tz.utcoffset(value).total_seconds()))\n\n\ndef hydrate_duration(months, days, seconds, nanoseconds):\n    \"\"\" Hydrator for `Duration` values.\n\n    :param months:\n    :param days:\n    :param seconds:\n    :param nanoseconds:\n    :return: `duration` namedtuple\n    \"\"\"\n    return Duration(months=months, days=days, seconds=seconds, nanoseconds=nanoseconds)\n\n\ndef dehydrate_duration(value):\n    \"\"\" Dehydrator for `duration` values.\n\n    :param value:\n    :type value: Duration\n    :return:\n    \"\"\"\n    return Structure(b\"E\", value.months, value.days, value.seconds, value.nanoseconds)\n\n\ndef dehydrate_timedelta(value):\n    \"\"\" Dehydrator for `timedelta` values.\n\n    :param value:\n    :type value: timedelta\n    :return:\n    \"\"\"\n    months = 0\n    days = value.days\n    seconds = value.seconds\n    nanoseconds = 1000 * value.microseconds\n    return Structure(b\"E\", months, days, seconds, nanoseconds)\n",
            "file_path": "neo4j/_codec/hydration/v1/temporal.py",
            "human_label": "Use ticks in the Time class to generate the Structure class.",
            "level": "project_runnable",
            "lineno": "85",
            "name": "dehydrate_time",
            "oracle_context": "{ \"apis\" : \"['total_seconds', 'isinstance', 'int', 'utcoffset']\", \"classes\" : \"['Time', 'TypeError', 'Structure', 'time']\", \"vars\" : \"['second', 'microsecond', 'minute', 'ticks', 'hour', 'tzinfo']\" }",
            "package": "temporal",
            "project": "neo4j/neo4j-python-driver",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62e60da4d76274f8a4026cf1",
            "all_context": "{ \"import\" : \"abc operator functools collections functools \", \"file\" : \"\", \"class\" : \"self.__getslice__(self,start,stop) ; self.__class__ ; self.__str__(self) ; self._super_getitem_single(self,index) ; self.items ; self.value(self,key,default) ; self.__eq__ ; self.keys(self) ; self.__getitem__(self,key) ; self.index(self,key) ; self.index ; self.items(self) ; self.__repr__(self) ; self._broken_record_error ; self._broken_record_error(self,index) ; self.data(self) ; self.__new__(cls,iterable) ; self.__eq__(self,other) ; self.__iter__(self) ; self.get(self,key,default) ; self.__hash__(self) ; self.__ne__(self,other) ; self.__keys ; self.__repr__ ; self._super_getitem_single ; self.values(self) ; \" }",
            "code": "    def values(self, *keys):\n        \"\"\" Return the values of the record, optionally filtering to\n        include only certain values by index or key.\n\n        :param keys: indexes or keys of the items to include; if none\n                     are provided, all values will be included\n        :return: list of values\n        :rtype: list\n        \"\"\"\n        if keys:\n            d = []\n            for key in keys:\n                try:\n                    i = self.index(key)\n                except KeyError:\n                    d.append(None)\n                else:\n                    d.append(self[i])\n            return d\n        return list(self)\n",
            "dependency": "",
            "docstring": "Return the values of the record, optionally filtering to\ninclude only certain values by index or key.\n\n:param keys: indexes or keys of the items to include; if none\n             are provided, all values will be included\n:return: list of values\n:rtype: list",
            "end_lineno": "210",
            "file_content": "# Copyright (c) \"Neo4j\"\n# Neo4j Sweden AB [https://neo4j.com]\n#\n# This file is part of Neo4j.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom abc import (\n    ABCMeta,\n    abstractmethod,\n)\nfrom collections.abc import (\n    Mapping,\n    Sequence,\n    Set,\n)\nfrom functools import reduce\nfrom operator import xor as xor_operator\n\nfrom ._codec.hydration import BrokenHydrationObject\nfrom ._conf import iter_items\nfrom ._meta import deprecated\nfrom .exceptions import BrokenRecordError\nfrom .graph import (\n    Node,\n    Path,\n    Relationship,\n)\n\n\nclass Record(tuple, Mapping):\n    \"\"\" A :class:`.Record` is an immutable ordered collection of key-value\n    pairs. It is generally closer to a :py:class:`namedtuple` than to a\n    :py:class:`OrderedDict` in as much as iteration of the collection will\n    yield values rather than keys.\n    \"\"\"\n\n    __keys = None\n\n    def __new__(cls, iterable=()):\n        keys = []\n        values = []\n        for key, value in iter_items(iterable):\n            keys.append(key)\n            values.append(value)\n        inst = tuple.__new__(cls, values)\n        inst.__keys = tuple(keys)\n        return inst\n\n    def _broken_record_error(self, index):\n        return BrokenRecordError(\n            f\"Record contains broken data at {index} ('{self.__keys[index]}')\"\n        )\n\n    def _super_getitem_single(self, index):\n        value = super().__getitem__(index)\n        if isinstance(value, BrokenHydrationObject):\n            raise self._broken_record_error(index) from value.error\n        return value\n\n    def __repr__(self):\n        return \"<%s %s>\" % (\n            self.__class__.__name__,\n            \" \".join(\"%s=%r\" % (field, value)\n                     for field, value in zip(self.__keys, super().__iter__()))\n        )\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __eq__(self, other):\n        \"\"\" In order to be flexible regarding comparison, the equality rules\n        for a record permit comparison with any other Sequence or Mapping.\n\n        :param other:\n        :return:\n        \"\"\"\n        compare_as_sequence = isinstance(other, Sequence)\n        compare_as_mapping = isinstance(other, Mapping)\n        if compare_as_sequence and compare_as_mapping:\n            return list(self) == list(other) and dict(self) == dict(other)\n        elif compare_as_sequence:\n            return list(self) == list(other)\n        elif compare_as_mapping:\n            return dict(self) == dict(other)\n        else:\n            return False\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __hash__(self):\n        return reduce(xor_operator, map(hash, self.items()))\n\n    def __iter__(self):\n        for i, v in enumerate(super().__iter__()):\n            if isinstance(v, BrokenHydrationObject):\n                raise self._broken_record_error(i) from v.error\n            yield v\n\n    def __getitem__(self, key):\n        if isinstance(key, slice):\n            keys = self.__keys[key]\n            values = super().__getitem__(key)\n            return self.__class__(zip(keys, values))\n        try:\n            index = self.index(key)\n        except IndexError:\n            return None\n        else:\n            return self._super_getitem_single(index)\n\n    # TODO: 6.0 - remove\n    @deprecated(\"This method is deprecated and will be removed in the future.\")\n    def __getslice__(self, start, stop):\n        key = slice(start, stop)\n        keys = self.__keys[key]\n        values = tuple(self)[key]\n        return self.__class__(zip(keys, values))\n\n    def get(self, key, default=None):\n        \"\"\" Obtain a value from the record by key, returning a default\n        value if the key does not exist.\n\n        :param key: a key\n        :param default: default value\n        :return: a value\n        \"\"\"\n        try:\n            index = self.__keys.index(str(key))\n        except ValueError:\n            return default\n        if 0 <= index < len(self):\n            return self._super_getitem_single(index)\n        else:\n            return default\n\n    def index(self, key):\n        \"\"\" Return the index of the given item.\n\n        :param key: a key\n        :return: index\n        :rtype: int\n        \"\"\"\n        if isinstance(key, int):\n            if 0 <= key < len(self.__keys):\n                return key\n            raise IndexError(key)\n        elif isinstance(key, str):\n            try:\n                return self.__keys.index(key)\n            except ValueError:\n                raise KeyError(key)\n        else:\n            raise TypeError(key)\n\n    def value(self, key=0, default=None):\n        \"\"\" Obtain a single value from the record by index or key. If no\n        index or key is specified, the first value is returned. If the\n        specified item does not exist, the default value is returned.\n\n        :param key: an index or key\n        :param default: default value\n        :return: a single value\n        \"\"\"\n        try:\n            index = self.index(key)\n        except (IndexError, KeyError):\n            return default\n        else:\n            return self[index]\n\n    def keys(self):\n        \"\"\" Return the keys of the record.\n\n        :return: list of key names\n        \"\"\"\n        return list(self.__keys)\n\n    def values(self, *keys):\n        \"\"\" Return the values of the record, optionally filtering to\n        include only certain values by index or key.\n\n        :param keys: indexes or keys of the items to include; if none\n                     are provided, all values will be included\n        :return: list of values\n        :rtype: list\n        \"\"\"\n        if keys:\n            d = []\n            for key in keys:\n                try:\n                    i = self.index(key)\n                except KeyError:\n                    d.append(None)\n                else:\n                    d.append(self[i])\n            return d\n        return list(self)\n\n    def items(self, *keys):\n        \"\"\" Return the fields of the record as a list of key and value tuples\n\n        :return: a list of value tuples\n        :rtype: list\n        \"\"\"\n        if keys:\n            d = []\n            for key in keys:\n                try:\n                    i = self.index(key)\n                except KeyError:\n                    d.append((key, None))\n                else:\n                    d.append((self.__keys[i], self[i]))\n            return d\n        return list((self.__keys[i], self._super_getitem_single(i))\n                    for i in range(len(self)))\n\n    def data(self, *keys):\n        \"\"\" Return the keys and values of this record as a dictionary,\n        optionally including only certain values by index or key. Keys\n        provided in the items that are not in the record will be\n        inserted with a value of :const:`None`; indexes provided\n        that are out of bounds will trigger an :exc:`IndexError`.\n\n        :param keys: indexes or keys of the items to include; if none\n                      are provided, all values will be included\n        :return: dictionary of values, keyed by field name\n        :raises: :exc:`IndexError` if an out-of-bounds index is specified\n        \"\"\"\n        return RecordExporter().transform(dict(self.items(*keys)))\n\n\nclass DataTransformer(metaclass=ABCMeta):\n    \"\"\" Abstract base class for transforming data from one form into\n    another.\n    \"\"\"\n\n    @abstractmethod\n    def transform(self, x):\n        \"\"\" Transform a value, or collection of values.\n\n        :param x: input value\n        :return: output value\n        \"\"\"\n\n\nclass RecordExporter(DataTransformer):\n    \"\"\" Transformer class used by the :meth:`.Record.data` method.\n    \"\"\"\n\n    def transform(self, x):\n        if isinstance(x, Node):\n            return self.transform(dict(x))\n        elif isinstance(x, Relationship):\n            return (self.transform(dict(x.start_node)),\n                    x.__class__.__name__,\n                    self.transform(dict(x.end_node)))\n        elif isinstance(x, Path):\n            path = [self.transform(x.start_node)]\n            for i, relationship in enumerate(x.relationships):\n                path.append(self.transform(relationship.__class__.__name__))\n                path.append(self.transform(x.nodes[i + 1]))\n            return path\n        elif isinstance(x, str):\n            return x\n        elif isinstance(x, Sequence):\n            t = type(x)\n            return t(map(self.transform, x))\n        elif isinstance(x, Set):\n            t = type(x)\n            return t(map(self.transform, x))\n        elif isinstance(x, Mapping):\n            t = type(x)\n            return t((k, self.transform(v)) for k, v in x.items())\n        else:\n            return x\n\n\nclass RecordTableRowExporter(DataTransformer):\n    \"\"\"Transformer class used by the :meth:`.Result.to_df` method.\"\"\"\n\n    def transform(self, x):\n        assert isinstance(x, Mapping)\n        t = type(x)\n        return t(item\n                 for k, v in x.items()\n                 for item in self._transform(\n                     v, prefix=k.replace(\"\\\\\", \"\\\\\\\\\").replace(\".\", \"\\\\.\")\n                 ).items())\n\n    def _transform(self, x, prefix):\n        if isinstance(x, Node):\n            res = {\n                \"%s().element_id\" % prefix: x.element_id,\n                \"%s().labels\" % prefix: x.labels,\n            }\n            res.update((\"%s().prop.%s\" % (prefix, k), v) for k, v in x.items())\n            return res\n        elif isinstance(x, Relationship):\n            res = {\n                \"%s->.element_id\" % prefix: x.element_id,\n                \"%s->.start.element_id\" % prefix: x.start_node.element_id,\n                \"%s->.end.element_id\" % prefix: x.end_node.element_id,\n                \"%s->.type\" % prefix: x.__class__.__name__,\n            }\n            res.update((\"%s->.prop.%s\" % (prefix, k), v) for k, v in x.items())\n            return res\n        elif isinstance(x, Path) or isinstance(x, str):\n            return {prefix: x}\n        elif isinstance(x, Sequence):\n            return dict(\n                item\n                for i, v in enumerate(x)\n                for item in self._transform(\n                    v, prefix=\"%s[].%i\" % (prefix, i)\n                ).items()\n            )\n        elif isinstance(x, Mapping):\n            t = type(x)\n            return t(\n                item\n                for k, v in x.items()\n                for item in self._transform(\n                    v, prefix=\"%s{}.%s\" % (prefix, k.replace(\"\\\\\", \"\\\\\\\\\")\n                                                    .replace(\".\", \"\\\\.\"))\n                ).items()\n            )\n        else:\n            return {prefix: x}\n",
            "file_path": "neo4j/_data.py",
            "human_label": "Returns the key filtered by self.index in the form of a list.",
            "level": "class_runnable",
            "lineno": "191",
            "name": "values",
            "oracle_context": "{ \"apis\" : \"['list', 'index', 'append', 'keys']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }",
            "package": "_data",
            "project": "neo4j/neo4j-python-driver",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62e60b10d76274f8a4026ccd",
            "all_context": "{ \"import\" : \"abc operator functools collections functools \", \"file\" : \"\", \"class\" : \"self.__getslice__(self,start,stop) ; self.__class__ ; self.__str__(self) ; self._super_getitem_single(self,index) ; self.items ; self.value(self,key,default) ; self.__eq__ ; self.keys(self) ; self.__getitem__(self,key) ; self.index(self,key) ; self.index ; self.items(self) ; self.__repr__(self) ; self._broken_record_error ; self._broken_record_error(self,index) ; self.data(self) ; self.__new__(cls,iterable) ; self.__eq__(self,other) ; self.__iter__(self) ; self.get(self,key,default) ; self.__hash__(self) ; self.__ne__(self,other) ; self.__keys ; self.__repr__ ; self._super_getitem_single ; self.values(self) ; \" }",
            "code": "    def data(self, *keys):\n        \"\"\" Return the keys and values of this record as a dictionary,\n        optionally including only certain values by index or key. Keys\n        provided in the items that are not in the record will be\n        inserted with a value of :const:`None`; indexes provided\n        that are out of bounds will trigger an :exc:`IndexError`.\n\n        :param keys: indexes or keys of the items to include; if none\n                      are provided, all values will be included\n        :return: dictionary of values, keyed by field name\n        :raises: :exc:`IndexError` if an out-of-bounds index is specified\n        \"\"\"\n        return RecordExporter().transform(dict(self.items(*keys)))\n",
            "dependency": "",
            "docstring": "Return the keys and values of this record as a dictionary,\noptionally including only certain values by index or key. Keys\nprovided in the items that are not in the record will be\ninserted with a value of :const:`None`; indexes provided\nthat are out of bounds will trigger an :exc:`IndexError`.\n\n:param keys: indexes or keys of the items to include; if none\n              are provided, all values will be included\n:return: dictionary of values, keyed by field name\n:raises: :exc:`IndexError` if an out-of-bounds index is specified",
            "end_lineno": "243",
            "file_content": "# Copyright (c) \"Neo4j\"\n# Neo4j Sweden AB [https://neo4j.com]\n#\n# This file is part of Neo4j.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom abc import (\n    ABCMeta,\n    abstractmethod,\n)\nfrom collections.abc import (\n    Mapping,\n    Sequence,\n    Set,\n)\nfrom functools import reduce\nfrom operator import xor as xor_operator\n\nfrom ._codec.hydration import BrokenHydrationObject\nfrom ._conf import iter_items\nfrom ._meta import deprecated\nfrom .exceptions import BrokenRecordError\nfrom .graph import (\n    Node,\n    Path,\n    Relationship,\n)\n\n\nclass Record(tuple, Mapping):\n    \"\"\" A :class:`.Record` is an immutable ordered collection of key-value\n    pairs. It is generally closer to a :py:class:`namedtuple` than to a\n    :py:class:`OrderedDict` in as much as iteration of the collection will\n    yield values rather than keys.\n    \"\"\"\n\n    __keys = None\n\n    def __new__(cls, iterable=()):\n        keys = []\n        values = []\n        for key, value in iter_items(iterable):\n            keys.append(key)\n            values.append(value)\n        inst = tuple.__new__(cls, values)\n        inst.__keys = tuple(keys)\n        return inst\n\n    def _broken_record_error(self, index):\n        return BrokenRecordError(\n            f\"Record contains broken data at {index} ('{self.__keys[index]}')\"\n        )\n\n    def _super_getitem_single(self, index):\n        value = super().__getitem__(index)\n        if isinstance(value, BrokenHydrationObject):\n            raise self._broken_record_error(index) from value.error\n        return value\n\n    def __repr__(self):\n        return \"<%s %s>\" % (\n            self.__class__.__name__,\n            \" \".join(\"%s=%r\" % (field, value)\n                     for field, value in zip(self.__keys, super().__iter__()))\n        )\n\n    def __str__(self):\n        return self.__repr__()\n\n    def __eq__(self, other):\n        \"\"\" In order to be flexible regarding comparison, the equality rules\n        for a record permit comparison with any other Sequence or Mapping.\n\n        :param other:\n        :return:\n        \"\"\"\n        compare_as_sequence = isinstance(other, Sequence)\n        compare_as_mapping = isinstance(other, Mapping)\n        if compare_as_sequence and compare_as_mapping:\n            return list(self) == list(other) and dict(self) == dict(other)\n        elif compare_as_sequence:\n            return list(self) == list(other)\n        elif compare_as_mapping:\n            return dict(self) == dict(other)\n        else:\n            return False\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __hash__(self):\n        return reduce(xor_operator, map(hash, self.items()))\n\n    def __iter__(self):\n        for i, v in enumerate(super().__iter__()):\n            if isinstance(v, BrokenHydrationObject):\n                raise self._broken_record_error(i) from v.error\n            yield v\n\n    def __getitem__(self, key):\n        if isinstance(key, slice):\n            keys = self.__keys[key]\n            values = super().__getitem__(key)\n            return self.__class__(zip(keys, values))\n        try:\n            index = self.index(key)\n        except IndexError:\n            return None\n        else:\n            return self._super_getitem_single(index)\n\n    # TODO: 6.0 - remove\n    @deprecated(\"This method is deprecated and will be removed in the future.\")\n    def __getslice__(self, start, stop):\n        key = slice(start, stop)\n        keys = self.__keys[key]\n        values = tuple(self)[key]\n        return self.__class__(zip(keys, values))\n\n    def get(self, key, default=None):\n        \"\"\" Obtain a value from the record by key, returning a default\n        value if the key does not exist.\n\n        :param key: a key\n        :param default: default value\n        :return: a value\n        \"\"\"\n        try:\n            index = self.__keys.index(str(key))\n        except ValueError:\n            return default\n        if 0 <= index < len(self):\n            return self._super_getitem_single(index)\n        else:\n            return default\n\n    def index(self, key):\n        \"\"\" Return the index of the given item.\n\n        :param key: a key\n        :return: index\n        :rtype: int\n        \"\"\"\n        if isinstance(key, int):\n            if 0 <= key < len(self.__keys):\n                return key\n            raise IndexError(key)\n        elif isinstance(key, str):\n            try:\n                return self.__keys.index(key)\n            except ValueError:\n                raise KeyError(key)\n        else:\n            raise TypeError(key)\n\n    def value(self, key=0, default=None):\n        \"\"\" Obtain a single value from the record by index or key. If no\n        index or key is specified, the first value is returned. If the\n        specified item does not exist, the default value is returned.\n\n        :param key: an index or key\n        :param default: default value\n        :return: a single value\n        \"\"\"\n        try:\n            index = self.index(key)\n        except (IndexError, KeyError):\n            return default\n        else:\n            return self[index]\n\n    def keys(self):\n        \"\"\" Return the keys of the record.\n\n        :return: list of key names\n        \"\"\"\n        return list(self.__keys)\n\n    def values(self, *keys):\n        \"\"\" Return the values of the record, optionally filtering to\n        include only certain values by index or key.\n\n        :param keys: indexes or keys of the items to include; if none\n                     are provided, all values will be included\n        :return: list of values\n        :rtype: list\n        \"\"\"\n        if keys:\n            d = []\n            for key in keys:\n                try:\n                    i = self.index(key)\n                except KeyError:\n                    d.append(None)\n                else:\n                    d.append(self[i])\n            return d\n        return list(self)\n\n    def items(self, *keys):\n        \"\"\" Return the fields of the record as a list of key and value tuples\n\n        :return: a list of value tuples\n        :rtype: list\n        \"\"\"\n        if keys:\n            d = []\n            for key in keys:\n                try:\n                    i = self.index(key)\n                except KeyError:\n                    d.append((key, None))\n                else:\n                    d.append((self.__keys[i], self[i]))\n            return d\n        return list((self.__keys[i], self._super_getitem_single(i))\n                    for i in range(len(self)))\n\n    def data(self, *keys):\n        \"\"\" Return the keys and values of this record as a dictionary,\n        optionally including only certain values by index or key. Keys\n        provided in the items that are not in the record will be\n        inserted with a value of :const:`None`; indexes provided\n        that are out of bounds will trigger an :exc:`IndexError`.\n\n        :param keys: indexes or keys of the items to include; if none\n                      are provided, all values will be included\n        :return: dictionary of values, keyed by field name\n        :raises: :exc:`IndexError` if an out-of-bounds index is specified\n        \"\"\"\n        return RecordExporter().transform(dict(self.items(*keys)))\n\n\nclass DataTransformer(metaclass=ABCMeta):\n    \"\"\" Abstract base class for transforming data from one form into\n    another.\n    \"\"\"\n\n    @abstractmethod\n    def transform(self, x):\n        \"\"\" Transform a value, or collection of values.\n\n        :param x: input value\n        :return: output value\n        \"\"\"\n\n\nclass RecordExporter(DataTransformer):\n    \"\"\" Transformer class used by the :meth:`.Record.data` method.\n    \"\"\"\n\n    def transform(self, x):\n        if isinstance(x, Node):\n            return self.transform(dict(x))\n        elif isinstance(x, Relationship):\n            return (self.transform(dict(x.start_node)),\n                    x.__class__.__name__,\n                    self.transform(dict(x.end_node)))\n        elif isinstance(x, Path):\n            path = [self.transform(x.start_node)]\n            for i, relationship in enumerate(x.relationships):\n                path.append(self.transform(relationship.__class__.__name__))\n                path.append(self.transform(x.nodes[i + 1]))\n            return path\n        elif isinstance(x, str):\n            return x\n        elif isinstance(x, Sequence):\n            t = type(x)\n            return t(map(self.transform, x))\n        elif isinstance(x, Set):\n            t = type(x)\n            return t(map(self.transform, x))\n        elif isinstance(x, Mapping):\n            t = type(x)\n            return t((k, self.transform(v)) for k, v in x.items())\n        else:\n            return x\n\n\nclass RecordTableRowExporter(DataTransformer):\n    \"\"\"Transformer class used by the :meth:`.Result.to_df` method.\"\"\"\n\n    def transform(self, x):\n        assert isinstance(x, Mapping)\n        t = type(x)\n        return t(item\n                 for k, v in x.items()\n                 for item in self._transform(\n                     v, prefix=k.replace(\"\\\\\", \"\\\\\\\\\").replace(\".\", \"\\\\.\")\n                 ).items())\n\n    def _transform(self, x, prefix):\n        if isinstance(x, Node):\n            res = {\n                \"%s().element_id\" % prefix: x.element_id,\n                \"%s().labels\" % prefix: x.labels,\n            }\n            res.update((\"%s().prop.%s\" % (prefix, k), v) for k, v in x.items())\n            return res\n        elif isinstance(x, Relationship):\n            res = {\n                \"%s->.element_id\" % prefix: x.element_id,\n                \"%s->.start.element_id\" % prefix: x.start_node.element_id,\n                \"%s->.end.element_id\" % prefix: x.end_node.element_id,\n                \"%s->.type\" % prefix: x.__class__.__name__,\n            }\n            res.update((\"%s->.prop.%s\" % (prefix, k), v) for k, v in x.items())\n            return res\n        elif isinstance(x, Path) or isinstance(x, str):\n            return {prefix: x}\n        elif isinstance(x, Sequence):\n            return dict(\n                item\n                for i, v in enumerate(x)\n                for item in self._transform(\n                    v, prefix=\"%s[].%i\" % (prefix, i)\n                ).items()\n            )\n        elif isinstance(x, Mapping):\n            t = type(x)\n            return t(\n                item\n                for k, v in x.items()\n                for item in self._transform(\n                    v, prefix=\"%s{}.%s\" % (prefix, k.replace(\"\\\\\", \"\\\\\\\\\")\n                                                    .replace(\".\", \"\\\\.\"))\n                ).items()\n            )\n        else:\n            return {prefix: x}\n",
            "file_path": "neo4j/_data.py",
            "human_label": "Returns the keys processed by the transform method of the RecordExporter class.",
            "level": "file_runnable",
            "lineno": "231",
            "name": "data",
            "oracle_context": "{ \"apis\" : \"['dict', 'keys', 'items', 'transform']\", \"classes\" : \"['RecordExporter']\", \"vars\" : \"['RecordExporter']\" }",
            "package": "_data",
            "project": "neo4j/neo4j-python-driver",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62e6087ad76274f8a4026bf2",
            "all_context": "{ \"import\" : \"logging enum ssl logging enum ssl api \", \"file\" : \"\", \"class\" : \"self.run(self,query,parameters,mode,bookmarks,metadata,timeout,db,imp_user,dehydration_hooks,hydration_hooks) ; self.PROTOCOL_VERSION ; self.encrypted(self) ; self._append ; self.run ; self.discard(self,n,qid,dehydration_hooks,hydration_hooks) ; self.pull ; self.fetch_all ; self.rollback(self,dehydration_hooks,hydration_hooks) ; self.pull(self,n,qid,dehydration_hooks,hydration_hooks) ; self.get_base_headers(self) ; self.__init__(self) ; self.goodbye(self,dehydration_hooks,hydration_hooks) ; self.der_encoded_server_certificate(self) ; self.begin(self,mode,bookmarks,metadata,timeout,db,imp_user,dehydration_hooks,hydration_hooks) ; self.user_agent ; self.commit(self,dehydration_hooks,hydration_hooks) ; self.send_all ; self.auth_dict ; self.unresolved_address ; self._on_server_state_change(self,old_state,new_state) ; self.socket ; self.pool ; self._on_server_state_change ; self.get_base_headers ; self.routing_context ; self.is_reset(self) ; self.responses ; self.local_port ; self.server_info ; self._server_state_manager ; \" }",
            "code": "    def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n        # Just ignore n and qid, it is not supported in the Bolt 3 Protocol.\n        log.debug(\"[#%04X]  C: DISCARD_ALL\", self.local_port)\n        self._append(b\"\\x2F\", (),\n                     Response(self, \"discard\", hydration_hooks, **handlers),\n                     dehydration_hooks=dehydration_hooks)\n",
            "dependency": "",
            "docstring": "Appends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object",
            "end_lineno": "252",
            "file_content": "# Copyright (c) \"Neo4j\"\n# Neo4j Sweden AB [https://neo4j.com]\n#\n# This file is part of Neo4j.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom enum import Enum\nfrom logging import getLogger\nfrom ssl import SSLSocket\n\nfrom ..._async_compat.util import AsyncUtil\nfrom ..._exceptions import (\n    BoltError,\n    BoltProtocolError,\n)\nfrom ...api import (\n    READ_ACCESS,\n    Version,\n)\nfrom ...exceptions import (\n    ConfigurationError,\n    DatabaseUnavailable,\n    DriverError,\n    ForbiddenOnReadOnlyDatabase,\n    Neo4jError,\n    NotALeader,\n    ServiceUnavailable,\n)\nfrom ._bolt import AsyncBolt\nfrom ._common import (\n    check_supported_server_product,\n    CommitResponse,\n    InitResponse,\n    Response,\n)\n\n\nlog = getLogger(\"neo4j\")\n\n\nclass ServerStates(Enum):\n    CONNECTED = \"CONNECTED\"\n    READY = \"READY\"\n    STREAMING = \"STREAMING\"\n    TX_READY_OR_TX_STREAMING = \"TX_READY||TX_STREAMING\"\n    FAILED = \"FAILED\"\n\n\nclass ServerStateManager:\n    _STATE_TRANSITIONS = {\n        ServerStates.CONNECTED: {\n            \"hello\": ServerStates.READY,\n        },\n        ServerStates.READY: {\n            \"run\": ServerStates.STREAMING,\n            \"begin\": ServerStates.TX_READY_OR_TX_STREAMING,\n        },\n        ServerStates.STREAMING: {\n            \"pull\": ServerStates.READY,\n            \"discard\": ServerStates.READY,\n            \"reset\": ServerStates.READY,\n        },\n        ServerStates.TX_READY_OR_TX_STREAMING: {\n            \"commit\": ServerStates.READY,\n            \"rollback\": ServerStates.READY,\n            \"reset\": ServerStates.READY,\n        },\n        ServerStates.FAILED: {\n            \"reset\": ServerStates.READY,\n        }\n    }\n\n    def __init__(self, init_state, on_change=None):\n        self.state = init_state\n        self._on_change = on_change\n\n    def transition(self, message, metadata):\n        if metadata.get(\"has_more\"):\n            return\n        state_before = self.state\n        self.state = self._STATE_TRANSITIONS\\\n            .get(self.state, {})\\\n            .get(message, self.state)\n        if state_before != self.state and callable(self._on_change):\n            self._on_change(state_before, self.state)\n\n\nclass AsyncBolt3(AsyncBolt):\n    \"\"\" Protocol handler for Bolt 3.\n\n    This is supported by Neo4j versions 3.5, 4.0, 4.1, 4.2, 4.3, and 4.4.\n    \"\"\"\n\n    PROTOCOL_VERSION = Version(3, 0)\n\n    supports_multiple_results = False\n\n    supports_multiple_databases = False\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._server_state_manager = ServerStateManager(\n            ServerStates.CONNECTED, on_change=self._on_server_state_change\n        )\n\n    def _on_server_state_change(self, old_state, new_state):\n        log.debug(\"[#%04X]  State: %s > %s\", self.local_port,\n                  old_state.name, new_state.name)\n\n    @property\n    def is_reset(self):\n        # We can't be sure of the server's state if there are still pending\n        # responses. Unless the last message we sent was RESET. In that case\n        # the server state will always be READY when we're done.\n        if (self.responses and self.responses[-1]\n                and self.responses[-1].message == \"reset\"):\n            return True\n        return self._server_state_manager.state == ServerStates.READY\n\n    @property\n    def encrypted(self):\n        return isinstance(self.socket, SSLSocket)\n\n    @property\n    def der_encoded_server_certificate(self):\n        return self.socket.getpeercert(binary_form=True)\n\n    def get_base_headers(self):\n        return {\n            \"user_agent\": self.user_agent,\n        }\n\n    async def hello(self, dehydration_hooks=None, hydration_hooks=None):\n        headers = self.get_base_headers()\n        headers.update(self.auth_dict)\n        logged_headers = dict(headers)\n        if \"credentials\" in logged_headers:\n            logged_headers[\"credentials\"] = \"*******\"\n        log.debug(\"[#%04X]  C: HELLO %r\", self.local_port, logged_headers)\n        self._append(b\"\\x01\", (headers,),\n                     response=InitResponse(self, \"hello\", hydration_hooks,\n                                           on_success=self.server_info.update),\n                     dehydration_hooks=dehydration_hooks)\n        await self.send_all()\n        await self.fetch_all()\n        check_supported_server_product(self.server_info.agent)\n\n    async def route(\n        self, database=None, imp_user=None, bookmarks=None,\n        dehydration_hooks=None, hydration_hooks=None\n    ):\n        if database is not None:\n            raise ConfigurationError(\n                \"Database name parameter for selecting database is not \"\n                \"supported in Bolt Protocol {!r}. Database name {!r}. \"\n                \"Server Agent {!r}\".format(\n                    self.PROTOCOL_VERSION, database, self.server_info.agent\n                )\n            )\n        if imp_user is not None:\n            raise ConfigurationError(\n                \"Impersonation is not supported in Bolt Protocol {!r}. \"\n                \"Trying to impersonate {!r}.\".format(\n                    self.PROTOCOL_VERSION, imp_user\n                )\n            )\n\n        metadata = {}\n        records = []\n\n        # Ignoring database and bookmarks because there is no multi-db support.\n        # The bookmarks are only relevant for making sure a previously created\n        # db exists before querying a routing table for it.\n        self.run(\n            \"CALL dbms.cluster.routing.getRoutingTable($context)\",  # This is an internal procedure call. Only available if the Neo4j 3.5 is setup with clustering.\n            {\"context\": self.routing_context},\n            mode=\"r\",                                               # Bolt Protocol Version(3, 0) supports mode=\"r\"\n            dehydration_hooks=dehydration_hooks,\n            hydration_hooks=hydration_hooks,\n            on_success=metadata.update\n        )\n        self.pull(dehydration_hooks = None, hydration_hooks = None,\n                  on_success=metadata.update, on_records=records.extend)\n        await self.send_all()\n        await self.fetch_all()\n        routing_info = [dict(zip(metadata.get(\"fields\", ()), values)) for values in records]\n        return routing_info\n\n    def run(self, query, parameters=None, mode=None, bookmarks=None,\n            metadata=None, timeout=None, db=None, imp_user=None,\n            dehydration_hooks=None, hydration_hooks=None, **handlers):\n        if db is not None:\n            raise ConfigurationError(\n                \"Database name parameter for selecting database is not \"\n                \"supported in Bolt Protocol {!r}. Database name {!r}.\".format(\n                    self.PROTOCOL_VERSION, db\n                )\n            )\n        if imp_user is not None:\n            raise ConfigurationError(\n                \"Impersonation is not supported in Bolt Protocol {!r}. \"\n                \"Trying to impersonate {!r}.\".format(\n                    self.PROTOCOL_VERSION, imp_user\n                )\n            )\n        if not parameters:\n            parameters = {}\n        extra = {}\n        if mode in (READ_ACCESS, \"r\"):\n            extra[\"mode\"] = \"r\"  # It will default to mode \"w\" if nothing is specified\n        if bookmarks:\n            try:\n                extra[\"bookmarks\"] = list(bookmarks)\n            except TypeError:\n                raise TypeError(\"Bookmarks must be provided within an iterable\")\n        if metadata:\n            try:\n                extra[\"tx_metadata\"] = dict(metadata)\n            except TypeError:\n                raise TypeError(\"Metadata must be coercible to a dict\")\n        if timeout is not None:\n            try:\n                extra[\"tx_timeout\"] = int(1000 * float(timeout))\n            except TypeError:\n                raise TypeError(\"Timeout must be specified as a number of seconds\")\n            if extra[\"tx_timeout\"] < 0:\n                raise ValueError(\"Timeout must be a positive number or 0.\")\n        fields = (query, parameters, extra)\n        log.debug(\"[#%04X]  C: RUN %s\", self.local_port, \" \".join(map(repr, fields)))\n        self._append(b\"\\x10\", fields,\n                     Response(self, \"run\", hydration_hooks, **handlers),\n                     dehydration_hooks=dehydration_hooks)\n\n    def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n        # Just ignore n and qid, it is not supported in the Bolt 3 Protocol.\n        log.debug(\"[#%04X]  C: DISCARD_ALL\", self.local_port)\n        self._append(b\"\\x2F\", (),\n                     Response(self, \"discard\", hydration_hooks, **handlers),\n                     dehydration_hooks=dehydration_hooks)\n\n    def pull(self, n=-1, qid=-1, dehydration_hooks=None, hydration_hooks=None,\n             **handlers):\n        # Just ignore n and qid, it is not supported in the Bolt 3 Protocol.\n        log.debug(\"[#%04X]  C: PULL_ALL\", self.local_port)\n        self._append(b\"\\x3F\", (),\n                     Response(self, \"pull\", hydration_hooks, **handlers),\n                     dehydration_hooks=dehydration_hooks)\n\n    def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n        if db is not None:\n            raise ConfigurationError(\n                \"Database name parameter for selecting database is not \"\n                \"supported in Bolt Protocol {!r}. Database name {!r}.\".format(\n                    self.PROTOCOL_VERSION, db\n                )\n            )\n        if imp_user is not None:\n            raise ConfigurationError(\n                \"Impersonation is not supported in Bolt Protocol {!r}. \"\n                \"Trying to impersonate {!r}.\".format(\n                    self.PROTOCOL_VERSION, imp_user\n                )\n            )\n        extra = {}\n        if mode in (READ_ACCESS, \"r\"):\n            extra[\"mode\"] = \"r\"  # It will default to mode \"w\" if nothing is specified\n        if bookmarks:\n            try:\n                extra[\"bookmarks\"] = list(bookmarks)\n            except TypeError:\n                raise TypeError(\"Bookmarks must be provided within an iterable\")\n        if metadata:\n            try:\n                extra[\"tx_metadata\"] = dict(metadata)\n            except TypeError:\n                raise TypeError(\"Metadata must be coercible to a dict\")\n        if timeout is not None:\n            try:\n                extra[\"tx_timeout\"] = int(1000 * float(timeout))\n            except TypeError:\n                raise TypeError(\"Timeout must be specified as a number of seconds\")\n            if extra[\"tx_timeout\"] < 0:\n                raise ValueError(\"Timeout must be a positive number or 0.\")\n        log.debug(\"[#%04X]  C: BEGIN %r\", self.local_port, extra)\n        self._append(b\"\\x11\", (extra,),\n                     Response(self, \"begin\", hydration_hooks, **handlers),\n                     dehydration_hooks=dehydration_hooks)\n\n    def commit(self, dehydration_hooks=None, hydration_hooks=None, **handlers):\n        log.debug(\"[#%04X]  C: COMMIT\", self.local_port)\n        self._append(b\"\\x12\", (),\n                     CommitResponse(self, \"commit\", hydration_hooks,\n                                    **handlers),\n                     dehydration_hooks=dehydration_hooks)\n\n    def rollback(self, dehydration_hooks=None, hydration_hooks=None,\n                 **handlers):\n        log.debug(\"[#%04X]  C: ROLLBACK\", self.local_port)\n        self._append(b\"\\x13\", (),\n                     Response(self, \"rollback\", hydration_hooks, **handlers),\n                     dehydration_hooks=dehydration_hooks)\n\n    async def reset(self, dehydration_hooks=None, hydration_hooks=None):\n        \"\"\" Add a RESET message to the outgoing queue, send\n        it and consume all remaining messages.\n        \"\"\"\n\n        def fail(metadata):\n            raise BoltProtocolError(\"RESET failed %r\" % metadata, address=self.unresolved_address)\n\n        log.debug(\"[#%04X]  C: RESET\", self.local_port)\n        self._append(b\"\\x0F\",\n                     response=Response(self, \"reset\", hydration_hooks,\n                                       on_failure=fail),\n                     dehydration_hooks=dehydration_hooks)\n        await self.send_all()\n        await self.fetch_all()\n\n    def goodbye(self, dehydration_hooks=None, hydration_hooks=None):\n        log.debug(\"[#%04X]  C: GOODBYE\", self.local_port)\n        self._append(b\"\\x02\", (), dehydration_hooks=dehydration_hooks)\n\n    async def _process_message(self, tag, fields):\n        \"\"\" Process at most one message from the server, if available.\n\n        :return: 2-tuple of number of detail messages and number of summary\n                 messages fetched\n        \"\"\"\n        details = []\n        summary_signature = summary_metadata = None\n        if tag == b\"\\x71\":  # RECORD\n            details = fields\n        elif fields:\n            summary_signature = tag\n            summary_metadata = fields[0]\n        else:\n            summary_signature = tag\n\n        if details:\n            log.debug(\"[#%04X]  S: RECORD * %d\", self.local_port, len(details))  # Do not log any data\n            await self.responses[0].on_records(details)\n\n        if summary_signature is None:\n            return len(details), 0\n\n        response = self.responses.popleft()\n        response.complete = True\n        if summary_signature == b\"\\x70\":\n            log.debug(\"[#%04X]  S: SUCCESS %r\", self.local_port, summary_metadata)\n            self._server_state_manager.transition(response.message,\n                                                  summary_metadata)\n            await response.on_success(summary_metadata or {})\n        elif summary_signature == b\"\\x7E\":\n            log.debug(\"[#%04X]  S: IGNORED\", self.local_port)\n            await response.on_ignored(summary_metadata or {})\n        elif summary_signature == b\"\\x7F\":\n            log.debug(\"[#%04X]  S: FAILURE %r\", self.local_port, summary_metadata)\n            self._server_state_manager.state = ServerStates.FAILED\n            try:\n                await response.on_failure(summary_metadata or {})\n            except (ServiceUnavailable, DatabaseUnavailable):\n                if self.pool:\n                    await self.pool.deactivate(address=self.unresolved_address)\n                raise\n            except (NotALeader, ForbiddenOnReadOnlyDatabase):\n                if self.pool:\n                    self.pool.on_write_failure(address=self.unresolved_address)\n                raise\n            except Neo4jError as e:\n                if self.pool and e.invalidates_all_connections():\n                    await self.pool.mark_all_stale()\n                raise\n        else:\n            raise BoltProtocolError(\"Unexpected response message with signature %02X\" % summary_signature, address=self.unresolved_address)\n\n        return len(details), 1\n",
            "file_path": "neo4j/_async/io/_bolt3.py",
            "human_label": "Appends a DISCARD message to the output queue.\n\n:param n: number of records to discard, default = -1 (ALL)\n:param qid: query ID to discard for, default = -1 (last query)\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object",
            "level": "project_runnable",
            "lineno": "246",
            "name": "discard",
            "oracle_context": "{ \"apis\" : \"['debug', '_append']\", \"classes\" : \"['Response']\", \"vars\" : \"['local_port', 'log']\" }",
            "package": "_bolt3",
            "project": "neo4j/neo4j-python-driver",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62e60879d76274f8a4026bec",
            "all_context": "{ \"import\" : \"logging enum ssl logging enum ssl api \", \"file\" : \"\", \"class\" : \"self.run(self,query,parameters,mode,bookmarks,metadata,timeout,db,imp_user,dehydration_hooks,hydration_hooks) ; self.PROTOCOL_VERSION ; self.encrypted(self) ; self._append ; self.run ; self.discard(self,n,qid,dehydration_hooks,hydration_hooks) ; self.pull ; self.fetch_all ; self.rollback(self,dehydration_hooks,hydration_hooks) ; self.pull(self,n,qid,dehydration_hooks,hydration_hooks) ; self.get_base_headers(self) ; self.__init__(self) ; self.goodbye(self,dehydration_hooks,hydration_hooks) ; self.der_encoded_server_certificate(self) ; self.begin(self,mode,bookmarks,metadata,timeout,db,imp_user,dehydration_hooks,hydration_hooks) ; self.user_agent ; self.commit(self,dehydration_hooks,hydration_hooks) ; self.send_all ; self.auth_dict ; self.unresolved_address ; self._on_server_state_change(self,old_state,new_state) ; self.socket ; self.pool ; self._on_server_state_change ; self.get_base_headers ; self.routing_context ; self.is_reset(self) ; self.responses ; self.local_port ; self.server_info ; self._server_state_manager ; \" }",
            "code": "    def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n        if db is not None:\n            raise ConfigurationError(\n                \"Database name parameter for selecting database is not \"\n                \"supported in Bolt Protocol {!r}. Database name {!r}.\".format(\n                    self.PROTOCOL_VERSION, db\n                )\n            )\n        if imp_user is not None:\n            raise ConfigurationError(\n                \"Impersonation is not supported in Bolt Protocol {!r}. \"\n                \"Trying to impersonate {!r}.\".format(\n                    self.PROTOCOL_VERSION, imp_user\n                )\n            )\n        extra = {}\n        if mode in (READ_ACCESS, \"r\"):\n            extra[\"mode\"] = \"r\"  # It will default to mode \"w\" if nothing is specified\n        if bookmarks:\n            try:\n                extra[\"bookmarks\"] = list(bookmarks)\n            except TypeError:\n                raise TypeError(\"Bookmarks must be provided within an iterable\")\n        if metadata:\n            try:\n                extra[\"tx_metadata\"] = dict(metadata)\n            except TypeError:\n                raise TypeError(\"Metadata must be coercible to a dict\")\n        if timeout is not None:\n            try:\n                extra[\"tx_timeout\"] = int(1000 * float(timeout))\n            except TypeError:\n                raise TypeError(\"Timeout must be specified as a number of seconds\")\n            if extra[\"tx_timeout\"] < 0:\n                raise ValueError(\"Timeout must be a positive number or 0.\")\n        log.debug(\"[#%04X]  C: BEGIN %r\", self.local_port, extra)\n        self._append(b\"\\x11\", (extra,),\n                     Response(self, \"begin\", hydration_hooks, **handlers),\n                     dehydration_hooks=dehydration_hooks)\n",
            "dependency": "",
            "docstring": "Appends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object",
            "end_lineno": "302",
            "file_content": "# Copyright (c) \"Neo4j\"\n# Neo4j Sweden AB [https://neo4j.com]\n#\n# This file is part of Neo4j.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nfrom enum import Enum\nfrom logging import getLogger\nfrom ssl import SSLSocket\n\nfrom ..._async_compat.util import AsyncUtil\nfrom ..._exceptions import (\n    BoltError,\n    BoltProtocolError,\n)\nfrom ...api import (\n    READ_ACCESS,\n    Version,\n)\nfrom ...exceptions import (\n    ConfigurationError,\n    DatabaseUnavailable,\n    DriverError,\n    ForbiddenOnReadOnlyDatabase,\n    Neo4jError,\n    NotALeader,\n    ServiceUnavailable,\n)\nfrom ._bolt import AsyncBolt\nfrom ._common import (\n    check_supported_server_product,\n    CommitResponse,\n    InitResponse,\n    Response,\n)\n\n\nlog = getLogger(\"neo4j\")\n\n\nclass ServerStates(Enum):\n    CONNECTED = \"CONNECTED\"\n    READY = \"READY\"\n    STREAMING = \"STREAMING\"\n    TX_READY_OR_TX_STREAMING = \"TX_READY||TX_STREAMING\"\n    FAILED = \"FAILED\"\n\n\nclass ServerStateManager:\n    _STATE_TRANSITIONS = {\n        ServerStates.CONNECTED: {\n            \"hello\": ServerStates.READY,\n        },\n        ServerStates.READY: {\n            \"run\": ServerStates.STREAMING,\n            \"begin\": ServerStates.TX_READY_OR_TX_STREAMING,\n        },\n        ServerStates.STREAMING: {\n            \"pull\": ServerStates.READY,\n            \"discard\": ServerStates.READY,\n            \"reset\": ServerStates.READY,\n        },\n        ServerStates.TX_READY_OR_TX_STREAMING: {\n            \"commit\": ServerStates.READY,\n            \"rollback\": ServerStates.READY,\n            \"reset\": ServerStates.READY,\n        },\n        ServerStates.FAILED: {\n            \"reset\": ServerStates.READY,\n        }\n    }\n\n    def __init__(self, init_state, on_change=None):\n        self.state = init_state\n        self._on_change = on_change\n\n    def transition(self, message, metadata):\n        if metadata.get(\"has_more\"):\n            return\n        state_before = self.state\n        self.state = self._STATE_TRANSITIONS\\\n            .get(self.state, {})\\\n            .get(message, self.state)\n        if state_before != self.state and callable(self._on_change):\n            self._on_change(state_before, self.state)\n\n\nclass AsyncBolt3(AsyncBolt):\n    \"\"\" Protocol handler for Bolt 3.\n\n    This is supported by Neo4j versions 3.5, 4.0, 4.1, 4.2, 4.3, and 4.4.\n    \"\"\"\n\n    PROTOCOL_VERSION = Version(3, 0)\n\n    supports_multiple_results = False\n\n    supports_multiple_databases = False\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self._server_state_manager = ServerStateManager(\n            ServerStates.CONNECTED, on_change=self._on_server_state_change\n        )\n\n    def _on_server_state_change(self, old_state, new_state):\n        log.debug(\"[#%04X]  State: %s > %s\", self.local_port,\n                  old_state.name, new_state.name)\n\n    @property\n    def is_reset(self):\n        # We can't be sure of the server's state if there are still pending\n        # responses. Unless the last message we sent was RESET. In that case\n        # the server state will always be READY when we're done.\n        if (self.responses and self.responses[-1]\n                and self.responses[-1].message == \"reset\"):\n            return True\n        return self._server_state_manager.state == ServerStates.READY\n\n    @property\n    def encrypted(self):\n        return isinstance(self.socket, SSLSocket)\n\n    @property\n    def der_encoded_server_certificate(self):\n        return self.socket.getpeercert(binary_form=True)\n\n    def get_base_headers(self):\n        return {\n            \"user_agent\": self.user_agent,\n        }\n\n    async def hello(self, dehydration_hooks=None, hydration_hooks=None):\n        headers = self.get_base_headers()\n        headers.update(self.auth_dict)\n        logged_headers = dict(headers)\n        if \"credentials\" in logged_headers:\n            logged_headers[\"credentials\"] = \"*******\"\n        log.debug(\"[#%04X]  C: HELLO %r\", self.local_port, logged_headers)\n        self._append(b\"\\x01\", (headers,),\n                     response=InitResponse(self, \"hello\", hydration_hooks,\n                                           on_success=self.server_info.update),\n                     dehydration_hooks=dehydration_hooks)\n        await self.send_all()\n        await self.fetch_all()\n        check_supported_server_product(self.server_info.agent)\n\n    async def route(\n        self, database=None, imp_user=None, bookmarks=None,\n        dehydration_hooks=None, hydration_hooks=None\n    ):\n        if database is not None:\n            raise ConfigurationError(\n                \"Database name parameter for selecting database is not \"\n                \"supported in Bolt Protocol {!r}. Database name {!r}. \"\n                \"Server Agent {!r}\".format(\n                    self.PROTOCOL_VERSION, database, self.server_info.agent\n                )\n            )\n        if imp_user is not None:\n            raise ConfigurationError(\n                \"Impersonation is not supported in Bolt Protocol {!r}. \"\n                \"Trying to impersonate {!r}.\".format(\n                    self.PROTOCOL_VERSION, imp_user\n                )\n            )\n\n        metadata = {}\n        records = []\n\n        # Ignoring database and bookmarks because there is no multi-db support.\n        # The bookmarks are only relevant for making sure a previously created\n        # db exists before querying a routing table for it.\n        self.run(\n            \"CALL dbms.cluster.routing.getRoutingTable($context)\",  # This is an internal procedure call. Only available if the Neo4j 3.5 is setup with clustering.\n            {\"context\": self.routing_context},\n            mode=\"r\",                                               # Bolt Protocol Version(3, 0) supports mode=\"r\"\n            dehydration_hooks=dehydration_hooks,\n            hydration_hooks=hydration_hooks,\n            on_success=metadata.update\n        )\n        self.pull(dehydration_hooks = None, hydration_hooks = None,\n                  on_success=metadata.update, on_records=records.extend)\n        await self.send_all()\n        await self.fetch_all()\n        routing_info = [dict(zip(metadata.get(\"fields\", ()), values)) for values in records]\n        return routing_info\n\n    def run(self, query, parameters=None, mode=None, bookmarks=None,\n            metadata=None, timeout=None, db=None, imp_user=None,\n            dehydration_hooks=None, hydration_hooks=None, **handlers):\n        if db is not None:\n            raise ConfigurationError(\n                \"Database name parameter for selecting database is not \"\n                \"supported in Bolt Protocol {!r}. Database name {!r}.\".format(\n                    self.PROTOCOL_VERSION, db\n                )\n            )\n        if imp_user is not None:\n            raise ConfigurationError(\n                \"Impersonation is not supported in Bolt Protocol {!r}. \"\n                \"Trying to impersonate {!r}.\".format(\n                    self.PROTOCOL_VERSION, imp_user\n                )\n            )\n        if not parameters:\n            parameters = {}\n        extra = {}\n        if mode in (READ_ACCESS, \"r\"):\n            extra[\"mode\"] = \"r\"  # It will default to mode \"w\" if nothing is specified\n        if bookmarks:\n            try:\n                extra[\"bookmarks\"] = list(bookmarks)\n            except TypeError:\n                raise TypeError(\"Bookmarks must be provided within an iterable\")\n        if metadata:\n            try:\n                extra[\"tx_metadata\"] = dict(metadata)\n            except TypeError:\n                raise TypeError(\"Metadata must be coercible to a dict\")\n        if timeout is not None:\n            try:\n                extra[\"tx_timeout\"] = int(1000 * float(timeout))\n            except TypeError:\n                raise TypeError(\"Timeout must be specified as a number of seconds\")\n            if extra[\"tx_timeout\"] < 0:\n                raise ValueError(\"Timeout must be a positive number or 0.\")\n        fields = (query, parameters, extra)\n        log.debug(\"[#%04X]  C: RUN %s\", self.local_port, \" \".join(map(repr, fields)))\n        self._append(b\"\\x10\", fields,\n                     Response(self, \"run\", hydration_hooks, **handlers),\n                     dehydration_hooks=dehydration_hooks)\n\n    def discard(self, n=-1, qid=-1, dehydration_hooks=None,\n                hydration_hooks=None, **handlers):\n        # Just ignore n and qid, it is not supported in the Bolt 3 Protocol.\n        log.debug(\"[#%04X]  C: DISCARD_ALL\", self.local_port)\n        self._append(b\"\\x2F\", (),\n                     Response(self, \"discard\", hydration_hooks, **handlers),\n                     dehydration_hooks=dehydration_hooks)\n\n    def pull(self, n=-1, qid=-1, dehydration_hooks=None, hydration_hooks=None,\n             **handlers):\n        # Just ignore n and qid, it is not supported in the Bolt 3 Protocol.\n        log.debug(\"[#%04X]  C: PULL_ALL\", self.local_port)\n        self._append(b\"\\x3F\", (),\n                     Response(self, \"pull\", hydration_hooks, **handlers),\n                     dehydration_hooks=dehydration_hooks)\n\n    def begin(self, mode=None, bookmarks=None, metadata=None, timeout=None,\n              db=None, imp_user=None, dehydration_hooks=None,\n              hydration_hooks=None, **handlers):\n        if db is not None:\n            raise ConfigurationError(\n                \"Database name parameter for selecting database is not \"\n                \"supported in Bolt Protocol {!r}. Database name {!r}.\".format(\n                    self.PROTOCOL_VERSION, db\n                )\n            )\n        if imp_user is not None:\n            raise ConfigurationError(\n                \"Impersonation is not supported in Bolt Protocol {!r}. \"\n                \"Trying to impersonate {!r}.\".format(\n                    self.PROTOCOL_VERSION, imp_user\n                )\n            )\n        extra = {}\n        if mode in (READ_ACCESS, \"r\"):\n            extra[\"mode\"] = \"r\"  # It will default to mode \"w\" if nothing is specified\n        if bookmarks:\n            try:\n                extra[\"bookmarks\"] = list(bookmarks)\n            except TypeError:\n                raise TypeError(\"Bookmarks must be provided within an iterable\")\n        if metadata:\n            try:\n                extra[\"tx_metadata\"] = dict(metadata)\n            except TypeError:\n                raise TypeError(\"Metadata must be coercible to a dict\")\n        if timeout is not None:\n            try:\n                extra[\"tx_timeout\"] = int(1000 * float(timeout))\n            except TypeError:\n                raise TypeError(\"Timeout must be specified as a number of seconds\")\n            if extra[\"tx_timeout\"] < 0:\n                raise ValueError(\"Timeout must be a positive number or 0.\")\n        log.debug(\"[#%04X]  C: BEGIN %r\", self.local_port, extra)\n        self._append(b\"\\x11\", (extra,),\n                     Response(self, \"begin\", hydration_hooks, **handlers),\n                     dehydration_hooks=dehydration_hooks)\n\n    def commit(self, dehydration_hooks=None, hydration_hooks=None, **handlers):\n        log.debug(\"[#%04X]  C: COMMIT\", self.local_port)\n        self._append(b\"\\x12\", (),\n                     CommitResponse(self, \"commit\", hydration_hooks,\n                                    **handlers),\n                     dehydration_hooks=dehydration_hooks)\n\n    def rollback(self, dehydration_hooks=None, hydration_hooks=None,\n                 **handlers):\n        log.debug(\"[#%04X]  C: ROLLBACK\", self.local_port)\n        self._append(b\"\\x13\", (),\n                     Response(self, \"rollback\", hydration_hooks, **handlers),\n                     dehydration_hooks=dehydration_hooks)\n\n    async def reset(self, dehydration_hooks=None, hydration_hooks=None):\n        \"\"\" Add a RESET message to the outgoing queue, send\n        it and consume all remaining messages.\n        \"\"\"\n\n        def fail(metadata):\n            raise BoltProtocolError(\"RESET failed %r\" % metadata, address=self.unresolved_address)\n\n        log.debug(\"[#%04X]  C: RESET\", self.local_port)\n        self._append(b\"\\x0F\",\n                     response=Response(self, \"reset\", hydration_hooks,\n                                       on_failure=fail),\n                     dehydration_hooks=dehydration_hooks)\n        await self.send_all()\n        await self.fetch_all()\n\n    def goodbye(self, dehydration_hooks=None, hydration_hooks=None):\n        log.debug(\"[#%04X]  C: GOODBYE\", self.local_port)\n        self._append(b\"\\x02\", (), dehydration_hooks=dehydration_hooks)\n\n    async def _process_message(self, tag, fields):\n        \"\"\" Process at most one message from the server, if available.\n\n        :return: 2-tuple of number of detail messages and number of summary\n                 messages fetched\n        \"\"\"\n        details = []\n        summary_signature = summary_metadata = None\n        if tag == b\"\\x71\":  # RECORD\n            details = fields\n        elif fields:\n            summary_signature = tag\n            summary_metadata = fields[0]\n        else:\n            summary_signature = tag\n\n        if details:\n            log.debug(\"[#%04X]  S: RECORD * %d\", self.local_port, len(details))  # Do not log any data\n            await self.responses[0].on_records(details)\n\n        if summary_signature is None:\n            return len(details), 0\n\n        response = self.responses.popleft()\n        response.complete = True\n        if summary_signature == b\"\\x70\":\n            log.debug(\"[#%04X]  S: SUCCESS %r\", self.local_port, summary_metadata)\n            self._server_state_manager.transition(response.message,\n                                                  summary_metadata)\n            await response.on_success(summary_metadata or {})\n        elif summary_signature == b\"\\x7E\":\n            log.debug(\"[#%04X]  S: IGNORED\", self.local_port)\n            await response.on_ignored(summary_metadata or {})\n        elif summary_signature == b\"\\x7F\":\n            log.debug(\"[#%04X]  S: FAILURE %r\", self.local_port, summary_metadata)\n            self._server_state_manager.state = ServerStates.FAILED\n            try:\n                await response.on_failure(summary_metadata or {})\n            except (ServiceUnavailable, DatabaseUnavailable):\n                if self.pool:\n                    await self.pool.deactivate(address=self.unresolved_address)\n                raise\n            except (NotALeader, ForbiddenOnReadOnlyDatabase):\n                if self.pool:\n                    self.pool.on_write_failure(address=self.unresolved_address)\n                raise\n            except Neo4jError as e:\n                if self.pool and e.invalidates_all_connections():\n                    await self.pool.mark_all_stale()\n                raise\n        else:\n            raise BoltProtocolError(\"Unexpected response message with signature %02X\" % summary_signature, address=self.unresolved_address)\n\n        return len(details), 1\n",
            "file_path": "neo4j/_async/io/_bolt3.py",
            "human_label": "Appends a BEGIN message to the output queue.\n\n:param mode: access mode for routing - \"READ\" or \"WRITE\" (default)\n:param bookmarks: iterable of bookmark values after which this transaction should begin\n:param metadata: custom metadata dictionary to attach to the transaction\n:param timeout: timeout for transaction execution (seconds)\n:param db: name of the database against which to begin the transaction\n    Requires Bolt 4.0+.\n:param imp_user: the user to impersonate\n    Requires Bolt 4.4+\n:param dehydration_hooks:\n    Hooks to dehydrate types (dict from type (class) to dehydration\n    function). Dehydration functions receive the value and returns an\n    object of type understood by packstream.\n:param hydration_hooks:\n    Hooks to hydrate types (mapping from type (class) to\n    dehydration function). Dehydration functions receive the value of\n    type understood by packstream and are free to return anything.\n:param handlers: handler functions passed into the returned Response object\n:return: Response object",
            "level": "project_runnable",
            "lineno": "262",
            "name": "begin",
            "oracle_context": "{ \"apis\" : \"['debug', 'dict', 'format', 'list', 'int', 'float', '_append']\", \"classes\" : \"['ValueError', 'TypeError', 'ConfigurationError', 'Response', 'READ_ACCESS']\", \"vars\" : \"['Str', 'PROTOCOL_VERSION', 'local_port', 'log']\" }",
            "package": "_bolt3",
            "project": "neo4j/neo4j-python-driver",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62e5dc9ed76274f8a4026b5b",
            "all_context": "{ \"import\" : \"warnings sys functools tracemalloc asyncio functools asyncio \", \"file\" : \"package ; version ; get_user_agent() ; deprecation_warn(message,stack_level) ; deprecated(message) ; experimental_warn(message,stack_level) ; experimental(message) ; unclosed_resource_warn(obj) ; \", \"class\" : \"\" }",
            "code": "def deprecated(message):\n    \"\"\" Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n\n    \"\"\"\n    def decorator(f):\n        if asyncio.iscoroutinefunction(f):\n            @wraps(f)\n            async def inner(*args, **kwargs):\n                deprecation_warn(message, stack_level=2)\n                return await f(*args, **kwargs)\n\n            return inner\n        else:\n            @wraps(f)\n            def inner(*args, **kwargs):\n                deprecation_warn(message, stack_level=2)\n                return f(*args, **kwargs)\n\n            return inner\n\n    return decorator\n",
            "dependency": "",
            "docstring": "Decorator for deprecating functions and methods.\n\n::\n\n    @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n    def foo(x):\n        pass",
            "end_lineno": "72",
            "file_content": "# Copyright (c) \"Neo4j\"\n# Neo4j Sweden AB [https://neo4j.com]\n#\n# This file is part of Neo4j.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\nimport asyncio\nfrom functools import wraps\nfrom warnings import warn\n\n\n# Can be automatically overridden in builds\npackage = \"neo4j\"\nversion = \"5.0.dev0\"\n\n\ndef get_user_agent():\n    \"\"\" Obtain the default user agent string sent to the server after\n    a successful handshake.\n    \"\"\"\n    from sys import (\n        platform,\n        version_info,\n    )\n    template = \"neo4j-python/{} Python/{}.{}.{}-{}-{} ({})\"\n    fields = (version,) + tuple(version_info) + (platform,)\n    return template.format(*fields)\n\n\ndef deprecation_warn(message, stack_level=1):\n    warn(message, category=DeprecationWarning, stacklevel=stack_level + 1)\n\n\ndef deprecated(message):\n    \"\"\" Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n\n    \"\"\"\n    def decorator(f):\n        if asyncio.iscoroutinefunction(f):\n            @wraps(f)\n            async def inner(*args, **kwargs):\n                deprecation_warn(message, stack_level=2)\n                return await f(*args, **kwargs)\n\n            return inner\n        else:\n            @wraps(f)\n            def inner(*args, **kwargs):\n                deprecation_warn(message, stack_level=2)\n                return f(*args, **kwargs)\n\n            return inner\n\n    return decorator\n\n\nclass ExperimentalWarning(Warning):\n    \"\"\" Base class for warnings about experimental features.\n    \"\"\"\n\n\ndef experimental_warn(message, stack_level=1):\n    warn(message, category=ExperimentalWarning, stacklevel=stack_level + 1)\n\n\ndef experimental(message):\n    \"\"\" Decorator for tagging experimental functions and methods.\n\n    ::\n\n        @experimental(\"'foo' is an experimental function and may be \"\n                      \"removed in a future release\")\n        def foo(x):\n            pass\n\n    \"\"\"\n    def decorator(f):\n        if asyncio.iscoroutinefunction(f):\n            @wraps(f)\n            async def inner(*args, **kwargs):\n                experimental_warn(message, stack_level=2)\n                return await f(*args, **kwargs)\n\n            return inner\n        else:\n            @wraps(f)\n            def inner(*args, **kwargs):\n                experimental_warn(message, stack_level=2)\n                return f(*args, **kwargs)\n\n            return inner\n\n    return decorator\n\n\ndef unclosed_resource_warn(obj):\n    import tracemalloc\n    from warnings import warn\n    msg = f\"Unclosed {obj!r}.\"\n    trace = tracemalloc.get_object_traceback(obj)\n    if trace:\n        msg += \"\\nObject allocated at (most recent call last):\\n\"\n        msg += \"\\n\".join(trace.format())\n    else:\n        msg += \"\\nEnable tracemalloc to get the object allocation traceback.\"\n    warn(msg, ResourceWarning, stacklevel=2, source=obj)\n",
            "file_path": "neo4j/_meta.py",
            "human_label": "Return a decorator function for deprecating functions and methods.",
            "level": "file_runnable",
            "lineno": "46",
            "name": "deprecated",
            "oracle_context": "{ \"apis\" : \"['wraps', 'iscoroutinefunction', 'deprecation_warn', 'f']\", \"classes\" : \"['wraps', 'asyncio']\", \"vars\" : \"[]\" }",
            "package": "_meta",
            "project": "neo4j/neo4j-python-driver",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62e4fbda85ea986430890405",
            "all_context": "{ \"import\" : \"\", \"file\" : \"\", \"class\" : \"\" }",
            "code": "def xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    cmd_fn = cmd_output_p if color else cmd_output_b\n    retcode = 0\n    stdout = b''\n\n    try:\n        cmd = parse_shebang.normalize_cmd(cmd)\n    except parse_shebang.ExecutableNotFoundError as e:\n        return e.to_output()[:2]\n\n    # on windows, batch files have a separate length limit than windows itself\n    if (\n            sys.platform == 'win32' and\n            cmd[0].lower().endswith(('.bat', '.cmd'))\n    ):  # pragma: win32 cover\n        # this is implementation details but the command gets translated into\n        # full/path/to/cmd.exe /c *cmd\n        cmd_exe = parse_shebang.find_executable('cmd.exe')\n        # 1024 is additionally subtracted to give headroom for further\n        # expansion inside the batch file\n        _max_length = 8192 - len(cmd_exe) - len(' /c ') - 1024\n\n    partitions = partition(cmd, varargs, target_concurrency, _max_length)\n\n    def run_cmd_partition(\n            run_cmd: tuple[str, ...],\n    ) -> tuple[int, bytes, bytes | None]:\n        return cmd_fn(\n            *run_cmd, retcode=None, stderr=subprocess.STDOUT, **kwargs,\n        )\n\n    threads = min(len(partitions), target_concurrency)\n    with _thread_mapper(threads) as thread_map:\n        results = thread_map(run_cmd_partition, partitions)\n\n        for proc_retcode, proc_out, _ in results:\n            retcode = max(retcode, proc_retcode)\n            stdout += proc_out\n\n    return retcode, stdout\n",
            "dependency": "",
            "docstring": "A simplified implementation of xargs.\n\ncolor: Make a pty if on a platform that supports it\ntarget_concurrency: Target number of partitions to run concurrently",
            "end_lineno": "168",
            "file_content": "from __future__ import annotations\n\nimport concurrent.futures\nimport contextlib\nimport math\nimport os\nimport subprocess\nimport sys\nfrom typing import Any\nfrom typing import Callable\nfrom typing import Generator\nfrom typing import Iterable\nfrom typing import MutableMapping\nfrom typing import Sequence\nfrom typing import TypeVar\n\nfrom pre_commit import parse_shebang\nfrom pre_commit.util import cmd_output_b\nfrom pre_commit.util import cmd_output_p\n\nTArg = TypeVar('TArg')\nTRet = TypeVar('TRet')\n\n\ndef _environ_size(_env: MutableMapping[str, str] | None = None) -> int:\n    environ = _env if _env is not None else getattr(os, 'environb', os.environ)\n    size = 8 * len(environ)  # number of pointers in `envp`\n    for k, v in environ.items():\n        size += len(k) + len(v) + 2  # c strings in `envp`\n    return size\n\n\ndef _get_platform_max_length() -> int:  # pragma: no cover (platform specific)\n    if os.name == 'posix':\n        maximum = os.sysconf('SC_ARG_MAX') - 2048 - _environ_size()\n        maximum = max(min(maximum, 2 ** 17), 2 ** 12)\n        return maximum\n    elif os.name == 'nt':\n        return 2 ** 15 - 2048  # UNICODE_STRING max - headroom\n    else:\n        # posix minimum\n        return 2 ** 12\n\n\ndef _command_length(*cmd: str) -> int:\n    full_cmd = ' '.join(cmd)\n\n    # win32 uses the amount of characters, more details at:\n    # https://github.com/pre-commit/pre-commit/pull/839\n    if sys.platform == 'win32':\n        return len(full_cmd.encode('utf-16le')) // 2\n    else:\n        return len(full_cmd.encode(sys.getfilesystemencoding()))\n\n\nclass ArgumentTooLongError(RuntimeError):\n    pass\n\n\ndef partition(\n        cmd: Sequence[str],\n        varargs: Sequence[str],\n        target_concurrency: int,\n        _max_length: int | None = None,\n) -> tuple[tuple[str, ...], ...]:\n    _max_length = _max_length or _get_platform_max_length()\n\n    # Generally, we try to partition evenly into at least `target_concurrency`\n    # partitions, but we don't want a bunch of tiny partitions.\n    max_args = max(4, math.ceil(len(varargs) / target_concurrency))\n\n    cmd = tuple(cmd)\n    ret = []\n\n    ret_cmd: list[str] = []\n    # Reversed so arguments are in order\n    varargs = list(reversed(varargs))\n\n    total_length = _command_length(*cmd) + 1\n    while varargs:\n        arg = varargs.pop()\n\n        arg_length = _command_length(arg) + 1\n        if (\n                total_length + arg_length <= _max_length and\n                len(ret_cmd) < max_args\n        ):\n            ret_cmd.append(arg)\n            total_length += arg_length\n        elif not ret_cmd:\n            raise ArgumentTooLongError(arg)\n        else:\n            # We've exceeded the length, yield a command\n            ret.append(cmd + tuple(ret_cmd))\n            ret_cmd = []\n            total_length = _command_length(*cmd) + 1\n            varargs.append(arg)\n\n    ret.append(cmd + tuple(ret_cmd))\n\n    return tuple(ret)\n\n\n@contextlib.contextmanager\ndef _thread_mapper(maxsize: int) -> Generator[\n    Callable[[Callable[[TArg], TRet], Iterable[TArg]], Iterable[TRet]],\n    None, None,\n]:\n    if maxsize == 1:\n        yield map\n    else:\n        with concurrent.futures.ThreadPoolExecutor(maxsize) as ex:\n            yield ex.map\n\n\ndef xargs(\n        cmd: tuple[str, ...],\n        varargs: Sequence[str],\n        *,\n        color: bool = False,\n        target_concurrency: int = 1,\n        _max_length: int = _get_platform_max_length(),\n        **kwargs: Any,\n) -> tuple[int, bytes]:\n    \"\"\"A simplified implementation of xargs.\n\n    color: Make a pty if on a platform that supports it\n    target_concurrency: Target number of partitions to run concurrently\n    \"\"\"\n    cmd_fn = cmd_output_p if color else cmd_output_b\n    retcode = 0\n    stdout = b''\n\n    try:\n        cmd = parse_shebang.normalize_cmd(cmd)\n    except parse_shebang.ExecutableNotFoundError as e:\n        return e.to_output()[:2]\n\n    # on windows, batch files have a separate length limit than windows itself\n    if (\n            sys.platform == 'win32' and\n            cmd[0].lower().endswith(('.bat', '.cmd'))\n    ):  # pragma: win32 cover\n        # this is implementation details but the command gets translated into\n        # full/path/to/cmd.exe /c *cmd\n        cmd_exe = parse_shebang.find_executable('cmd.exe')\n        # 1024 is additionally subtracted to give headroom for further\n        # expansion inside the batch file\n        _max_length = 8192 - len(cmd_exe) - len(' /c ') - 1024\n\n    partitions = partition(cmd, varargs, target_concurrency, _max_length)\n\n    def run_cmd_partition(\n            run_cmd: tuple[str, ...],\n    ) -> tuple[int, bytes, bytes | None]:\n        return cmd_fn(\n            *run_cmd, retcode=None, stderr=subprocess.STDOUT, **kwargs,\n        )\n\n    threads = min(len(partitions), target_concurrency)\n    with _thread_mapper(threads) as thread_map:\n        results = thread_map(run_cmd_partition, partitions)\n\n        for proc_retcode, proc_out, _ in results:\n            retcode = max(retcode, proc_retcode)\n            stdout += proc_out\n\n    return retcode, stdout\n",
            "file_path": "pre_commit/xargs.py",
            "human_label": "Simplified Implementation of Xargs in Linux",
            "level": "project_runnable",
            "lineno": "116",
            "name": "xargs",
            "oracle_context": "{ \"apis\" : \"['max', '_get_platform_max_length', 'find_executable', 'to_output', 'cmd_fn', 'endswith', 'normalize_cmd', 'len', '_thread_mapper', 'partition', 'thread_map', 'lower', 'min']\", \"classes\" : \"['subprocess', 'cmd_output_p', 'cmd_output_b', 'Sequence', 'Any', 'parse_shebang', 'sys']\", \"vars\" : \"['e', 'platform', 'STDOUT', 'ExecutableNotFoundError']\" }",
            "package": "xargs",
            "project": "pre-commit/pre-commit",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62e4fb4d85ea9864308902e7",
            "all_context": "{ \"import\" : \"os __future__ typing identify typing \", \"file\" : \"parse_filename(filename) ; find_executable(exe,_environ) ; normexe(orig) ; normalize_cmd(cmd) ; \", \"class\" : \"\" }",
            "code": "def normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Use PATH to determine the executable\n    exe = normexe(cmd[0])\n\n    # Figure out the shebang from the resulting command\n    cmd = parse_filename(exe) + (exe,) + cmd[1:]\n\n    # This could have given us back another bare executable\n    exe = normexe(cmd[0])\n\n    return (exe,) + cmd[1:]\n",
            "dependency": "",
            "docstring": "Fixes for the following issues on windows\n- https://bugs.python.org/issue8557\n- windows does not parse shebangs\n\nThis function also makes deep-path shebangs work just fine",
            "end_lineno": "81",
            "file_content": "from __future__ import annotations\n\nimport os.path\nfrom typing import Mapping\nfrom typing import NoReturn\n\nfrom identify.identify import parse_shebang_from_file\n\n\nclass ExecutableNotFoundError(OSError):\n    def to_output(self) -> tuple[int, bytes, None]:\n        return (1, self.args[0].encode(), None)\n\n\ndef parse_filename(filename: str) -> tuple[str, ...]:\n    if not os.path.exists(filename):\n        return ()\n    else:\n        return parse_shebang_from_file(filename)\n\n\ndef find_executable(\n        exe: str, _environ: Mapping[str, str] | None = None,\n) -> str | None:\n    exe = os.path.normpath(exe)\n    if os.sep in exe:\n        return exe\n\n    environ = _environ if _environ is not None else os.environ\n\n    if 'PATHEXT' in environ:\n        exts = environ['PATHEXT'].split(os.pathsep)\n        possible_exe_names = tuple(f'{exe}{ext}' for ext in exts) + (exe,)\n    else:\n        possible_exe_names = (exe,)\n\n    for path in environ.get('PATH', '').split(os.pathsep):\n        for possible_exe_name in possible_exe_names:\n            joined = os.path.join(path, possible_exe_name)\n            if os.path.isfile(joined) and os.access(joined, os.X_OK):\n                return joined\n    else:\n        return None\n\n\ndef normexe(orig: str) -> str:\n    def _error(msg: str) -> NoReturn:\n        raise ExecutableNotFoundError(f'Executable `{orig}` {msg}')\n\n    if os.sep not in orig and (not os.altsep or os.altsep not in orig):\n        exe = find_executable(orig)\n        if exe is None:\n            _error('not found')\n        return exe\n    elif os.path.isdir(orig):\n        _error('is a directory')\n    elif not os.path.isfile(orig):\n        _error('not found')\n    elif not os.access(orig, os.X_OK):  # pragma: win32 no cover\n        _error('is not executable')\n    else:\n        return orig\n\n\ndef normalize_cmd(cmd: tuple[str, ...]) -> tuple[str, ...]:\n    \"\"\"Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Use PATH to determine the executable\n    exe = normexe(cmd[0])\n\n    # Figure out the shebang from the resulting command\n    cmd = parse_filename(exe) + (exe,) + cmd[1:]\n\n    # This could have given us back another bare executable\n    exe = normexe(cmd[0])\n\n    return (exe,) + cmd[1:]\n",
            "file_path": "pre_commit/parse_shebang.py",
            "human_label": "Complement the full path to exe and return it in its original form",
            "level": "file_runnable",
            "lineno": "65",
            "name": "normalize_cmd",
            "oracle_context": "{ \"apis\" : \"['parse_filename', 'normexe']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }",
            "package": "parse_shebang",
            "project": "pre-commit/pre-commit",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b8d24048ba5a41d1c3f49f",
            "all_context": "{ \"import\" : \"\", \"file\" : \"\", \"class\" : \"\" }",
            "code": "def ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundTTLCache(ttl, timer), typed)\n    elif callable(maxsize):\n        return _cache(TTLCache(128, ttl, timer), typed)(maxsize)\n    else:\n        return _cache(TTLCache(maxsize, ttl, timer), typed)\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : true, \"public_lib\" : true, \"current_class\" : false, \"current_file\" : true, \"current_project\" : true, \"external\" : false }",
            "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.",
            "end_lineno": "176",
            "file_content": "\"\"\"`functools.lru_cache` compatible memoizing function decorators.\"\"\"\n\nimport collections\nimport functools\nimport math\nimport random\nimport time\n\ntry:\n    from threading import RLock\nexcept ImportError:  # pragma: no cover\n    from dummy_threading import RLock\n\nfrom . import keys\nfrom .fifo import FIFOCache\nfrom .lfu import LFUCache\nfrom .lru import LRUCache\nfrom .mru import MRUCache\nfrom .rr import RRCache\nfrom .ttl import TTLCache\n\n__all__ = ('lfu_cache', 'lru_cache', 'mru_cache', 'rr_cache', 'ttl_cache')\n\n\n_CacheInfo = collections.namedtuple('CacheInfo', [\n    'hits', 'misses', 'maxsize', 'currsize'\n])\n\n\nclass _UnboundCache(dict):\n\n    @property\n    def maxsize(self):\n        return None\n\n    @property\n    def currsize(self):\n        return len(self)\n\n\nclass _UnboundTTLCache(TTLCache):\n    def __init__(self, ttl, timer):\n        TTLCache.__init__(self, math.inf, ttl, timer)\n\n    @property\n    def maxsize(self):\n        return None\n\n\ndef _cache(cache, typed):\n    maxsize = cache.maxsize\n\n    def decorator(func):\n        key = keys.typedkey if typed else keys.hashkey\n        lock = RLock()\n        stats = [0, 0]\n\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            with lock:\n                try:\n                    v = cache[k]\n                    stats[0] += 1\n                    return v\n                except KeyError:\n                    stats[1] += 1\n            v = func(*args, **kwargs)\n            # in case of a race, prefer the item already in the cache\n            try:\n                with lock:\n                    return cache.setdefault(k, v)\n            except ValueError:\n                return v  # value too large\n\n        def cache_info():\n            with lock:\n                hits, misses = stats\n                maxsize = cache.maxsize\n                currsize = cache.currsize\n            return _CacheInfo(hits, misses, maxsize, currsize)\n\n        def cache_clear():\n            with lock:\n                try:\n                    cache.clear()\n                finally:\n                    stats[:] = [0, 0]\n\n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = lambda: {'maxsize': maxsize, 'typed': typed}\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    return decorator\n\n\ndef fifo_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a First In First Out (FIFO)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(FIFOCache(128), typed)(maxsize)\n    else:\n        return _cache(FIFOCache(maxsize), typed)\n\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(LFUCache(128), typed)(maxsize)\n    else:\n        return _cache(LFUCache(maxsize), typed)\n\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(LRUCache(128), typed)(maxsize)\n    else:\n        return _cache(LRUCache(maxsize), typed)\n\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(MRUCache(128), typed)(maxsize)\n    else:\n        return _cache(MRUCache(maxsize), typed)\n\n\ndef rr_cache(maxsize=128, choice=random.choice, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Random Replacement (RR)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(RRCache(128, choice), typed)(maxsize)\n    else:\n        return _cache(RRCache(maxsize, choice), typed)\n\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundTTLCache(ttl, timer), typed)\n    elif callable(maxsize):\n        return _cache(TTLCache(128, ttl, timer), typed)(maxsize)\n    else:\n        return _cache(TTLCache(maxsize, ttl, timer), typed)\n",
            "file_path": "cachetools/func.py",
            "human_label": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm with a per-item time-to-live (TTL) value.",
            "level": "project_runnable",
            "lineno": "166",
            "name": "ttl_cache",
            "oracle_context": "{ \"apis\" : \"['_cache', '_UnboundTTLCache', 'callable']\", \"classes\" : \"['time', 'TTLCache', '_UnboundTTLCache']\", \"vars\" : \"['monotonic']\" }",
            "package": "func",
            "project": "pexip/os-python-cachetools",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b8d23b48ba5a41d1c3f49a",
            "all_context": "{ \"import\" : \"\", \"file\" : \"\", \"class\" : \"\" }",
            "code": "def mru_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(MRUCache(128), typed)(maxsize)\n    else:\n        return _cache(MRUCache(maxsize), typed)\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : false, \"public_lib\" : true, \"current_class\" : false, \"current_file\" : true, \"current_project\" : true, \"external\" : false }",
            "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.",
            "end_lineno": "149",
            "file_content": "\"\"\"`functools.lru_cache` compatible memoizing function decorators.\"\"\"\n\nimport collections\nimport functools\nimport math\nimport random\nimport time\n\ntry:\n    from threading import RLock\nexcept ImportError:  # pragma: no cover\n    from dummy_threading import RLock\n\nfrom . import keys\nfrom .fifo import FIFOCache\nfrom .lfu import LFUCache\nfrom .lru import LRUCache\nfrom .mru import MRUCache\nfrom .rr import RRCache\nfrom .ttl import TTLCache\n\n__all__ = ('lfu_cache', 'lru_cache', 'mru_cache', 'rr_cache', 'ttl_cache')\n\n\n_CacheInfo = collections.namedtuple('CacheInfo', [\n    'hits', 'misses', 'maxsize', 'currsize'\n])\n\n\nclass _UnboundCache(dict):\n\n    @property\n    def maxsize(self):\n        return None\n\n    @property\n    def currsize(self):\n        return len(self)\n\n\nclass _UnboundTTLCache(TTLCache):\n    def __init__(self, ttl, timer):\n        TTLCache.__init__(self, math.inf, ttl, timer)\n\n    @property\n    def maxsize(self):\n        return None\n\n\ndef _cache(cache, typed):\n    maxsize = cache.maxsize\n\n    def decorator(func):\n        key = keys.typedkey if typed else keys.hashkey\n        lock = RLock()\n        stats = [0, 0]\n\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            with lock:\n                try:\n                    v = cache[k]\n                    stats[0] += 1\n                    return v\n                except KeyError:\n                    stats[1] += 1\n            v = func(*args, **kwargs)\n            # in case of a race, prefer the item already in the cache\n            try:\n                with lock:\n                    return cache.setdefault(k, v)\n            except ValueError:\n                return v  # value too large\n\n        def cache_info():\n            with lock:\n                hits, misses = stats\n                maxsize = cache.maxsize\n                currsize = cache.currsize\n            return _CacheInfo(hits, misses, maxsize, currsize)\n\n        def cache_clear():\n            with lock:\n                try:\n                    cache.clear()\n                finally:\n                    stats[:] = [0, 0]\n\n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = lambda: {'maxsize': maxsize, 'typed': typed}\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    return decorator\n\n\ndef fifo_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a First In First Out (FIFO)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(FIFOCache(128), typed)(maxsize)\n    else:\n        return _cache(FIFOCache(maxsize), typed)\n\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(LFUCache(128), typed)(maxsize)\n    else:\n        return _cache(LFUCache(maxsize), typed)\n\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(LRUCache(128), typed)(maxsize)\n    else:\n        return _cache(LRUCache(maxsize), typed)\n\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(MRUCache(128), typed)(maxsize)\n    else:\n        return _cache(MRUCache(maxsize), typed)\n\n\ndef rr_cache(maxsize=128, choice=random.choice, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Random Replacement (RR)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(RRCache(128, choice), typed)(maxsize)\n    else:\n        return _cache(RRCache(maxsize, choice), typed)\n\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundTTLCache(ttl, timer), typed)\n    elif callable(maxsize):\n        return _cache(TTLCache(128, ttl, timer), typed)(maxsize)\n    else:\n        return _cache(TTLCache(maxsize, ttl, timer), typed)\n",
            "file_path": "cachetools/func.py",
            "human_label": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Most Recently Used (MRU)\nalgorithm.",
            "level": "project_runnable",
            "lineno": "139",
            "name": "mru_cache",
            "oracle_context": "{ \"apis\" : \"['_cache', '_UnboundCache', 'callable']\", \"classes\" : \"['MRUCache', '_UnboundCache']\", \"vars\" : \"[]\" }",
            "package": "func",
            "project": "pexip/os-python-cachetools",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b8d23948ba5a41d1c3f498",
            "all_context": "{ \"import\" : \"\", \"file\" : \"\", \"class\" : \"\" }",
            "code": "def lru_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(LRUCache(128), typed)(maxsize)\n    else:\n        return _cache(LRUCache(maxsize), typed)\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : false, \"public_lib\" : true, \"current_class\" : false, \"current_file\" : true, \"current_project\" : true, \"external\" : false }",
            "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.",
            "end_lineno": "136",
            "file_content": "\"\"\"`functools.lru_cache` compatible memoizing function decorators.\"\"\"\n\nimport collections\nimport functools\nimport math\nimport random\nimport time\n\ntry:\n    from threading import RLock\nexcept ImportError:  # pragma: no cover\n    from dummy_threading import RLock\n\nfrom . import keys\nfrom .fifo import FIFOCache\nfrom .lfu import LFUCache\nfrom .lru import LRUCache\nfrom .mru import MRUCache\nfrom .rr import RRCache\nfrom .ttl import TTLCache\n\n__all__ = ('lfu_cache', 'lru_cache', 'mru_cache', 'rr_cache', 'ttl_cache')\n\n\n_CacheInfo = collections.namedtuple('CacheInfo', [\n    'hits', 'misses', 'maxsize', 'currsize'\n])\n\n\nclass _UnboundCache(dict):\n\n    @property\n    def maxsize(self):\n        return None\n\n    @property\n    def currsize(self):\n        return len(self)\n\n\nclass _UnboundTTLCache(TTLCache):\n    def __init__(self, ttl, timer):\n        TTLCache.__init__(self, math.inf, ttl, timer)\n\n    @property\n    def maxsize(self):\n        return None\n\n\ndef _cache(cache, typed):\n    maxsize = cache.maxsize\n\n    def decorator(func):\n        key = keys.typedkey if typed else keys.hashkey\n        lock = RLock()\n        stats = [0, 0]\n\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            with lock:\n                try:\n                    v = cache[k]\n                    stats[0] += 1\n                    return v\n                except KeyError:\n                    stats[1] += 1\n            v = func(*args, **kwargs)\n            # in case of a race, prefer the item already in the cache\n            try:\n                with lock:\n                    return cache.setdefault(k, v)\n            except ValueError:\n                return v  # value too large\n\n        def cache_info():\n            with lock:\n                hits, misses = stats\n                maxsize = cache.maxsize\n                currsize = cache.currsize\n            return _CacheInfo(hits, misses, maxsize, currsize)\n\n        def cache_clear():\n            with lock:\n                try:\n                    cache.clear()\n                finally:\n                    stats[:] = [0, 0]\n\n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = lambda: {'maxsize': maxsize, 'typed': typed}\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    return decorator\n\n\ndef fifo_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a First In First Out (FIFO)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(FIFOCache(128), typed)(maxsize)\n    else:\n        return _cache(FIFOCache(maxsize), typed)\n\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(LFUCache(128), typed)(maxsize)\n    else:\n        return _cache(LFUCache(maxsize), typed)\n\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(LRUCache(128), typed)(maxsize)\n    else:\n        return _cache(LRUCache(maxsize), typed)\n\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(MRUCache(128), typed)(maxsize)\n    else:\n        return _cache(MRUCache(maxsize), typed)\n\n\ndef rr_cache(maxsize=128, choice=random.choice, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Random Replacement (RR)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(RRCache(128, choice), typed)(maxsize)\n    else:\n        return _cache(RRCache(maxsize, choice), typed)\n\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundTTLCache(ttl, timer), typed)\n    elif callable(maxsize):\n        return _cache(TTLCache(128, ttl, timer), typed)(maxsize)\n    else:\n        return _cache(TTLCache(maxsize, ttl, timer), typed)\n",
            "file_path": "cachetools/func.py",
            "human_label": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Recently Used (LRU)\nalgorithm.",
            "level": "project_runnable",
            "lineno": "125",
            "name": "lru_cache",
            "oracle_context": "{ \"apis\" : \"['_cache', '_UnboundCache', 'callable']\", \"classes\" : \"['LRUCache', '_UnboundCache']\", \"vars\" : \"[]\" }",
            "package": "func",
            "project": "pexip/os-python-cachetools",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b8d23748ba5a41d1c3f496",
            "all_context": "{ \"import\" : \"\", \"file\" : \"\", \"class\" : \"\" }",
            "code": "def lfu_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(LFUCache(128), typed)(maxsize)\n    else:\n        return _cache(LFUCache(maxsize), typed)\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : false, \"current_file\" : true, \"current_project\" : true, \"external\" : false }",
            "docstring": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.",
            "end_lineno": "122",
            "file_content": "\"\"\"`functools.lru_cache` compatible memoizing function decorators.\"\"\"\n\nimport collections\nimport functools\nimport math\nimport random\nimport time\n\ntry:\n    from threading import RLock\nexcept ImportError:  # pragma: no cover\n    from dummy_threading import RLock\n\nfrom . import keys\nfrom .fifo import FIFOCache\nfrom .lfu import LFUCache\nfrom .lru import LRUCache\nfrom .mru import MRUCache\nfrom .rr import RRCache\nfrom .ttl import TTLCache\n\n__all__ = ('lfu_cache', 'lru_cache', 'mru_cache', 'rr_cache', 'ttl_cache')\n\n\n_CacheInfo = collections.namedtuple('CacheInfo', [\n    'hits', 'misses', 'maxsize', 'currsize'\n])\n\n\nclass _UnboundCache(dict):\n\n    @property\n    def maxsize(self):\n        return None\n\n    @property\n    def currsize(self):\n        return len(self)\n\n\nclass _UnboundTTLCache(TTLCache):\n    def __init__(self, ttl, timer):\n        TTLCache.__init__(self, math.inf, ttl, timer)\n\n    @property\n    def maxsize(self):\n        return None\n\n\ndef _cache(cache, typed):\n    maxsize = cache.maxsize\n\n    def decorator(func):\n        key = keys.typedkey if typed else keys.hashkey\n        lock = RLock()\n        stats = [0, 0]\n\n        def wrapper(*args, **kwargs):\n            k = key(*args, **kwargs)\n            with lock:\n                try:\n                    v = cache[k]\n                    stats[0] += 1\n                    return v\n                except KeyError:\n                    stats[1] += 1\n            v = func(*args, **kwargs)\n            # in case of a race, prefer the item already in the cache\n            try:\n                with lock:\n                    return cache.setdefault(k, v)\n            except ValueError:\n                return v  # value too large\n\n        def cache_info():\n            with lock:\n                hits, misses = stats\n                maxsize = cache.maxsize\n                currsize = cache.currsize\n            return _CacheInfo(hits, misses, maxsize, currsize)\n\n        def cache_clear():\n            with lock:\n                try:\n                    cache.clear()\n                finally:\n                    stats[:] = [0, 0]\n\n        wrapper.cache_info = cache_info\n        wrapper.cache_clear = cache_clear\n        wrapper.cache_parameters = lambda: {'maxsize': maxsize, 'typed': typed}\n        functools.update_wrapper(wrapper, func)\n        return wrapper\n    return decorator\n\n\ndef fifo_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a First In First Out (FIFO)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(FIFOCache(128), typed)(maxsize)\n    else:\n        return _cache(FIFOCache(maxsize), typed)\n\n\ndef lfu_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Frequently Used (LFU)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(LFUCache(128), typed)(maxsize)\n    else:\n        return _cache(LFUCache(maxsize), typed)\n\n\ndef lru_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(LRUCache(128), typed)(maxsize)\n    else:\n        return _cache(LRUCache(maxsize), typed)\n\n\ndef mru_cache(maxsize=128, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Most Recently Used (MRU)\n    algorithm.\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(MRUCache(128), typed)(maxsize)\n    else:\n        return _cache(MRUCache(maxsize), typed)\n\n\ndef rr_cache(maxsize=128, choice=random.choice, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Random Replacement (RR)\n    algorithm.\n\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundCache(), typed)\n    elif callable(maxsize):\n        return _cache(RRCache(128, choice), typed)(maxsize)\n    else:\n        return _cache(RRCache(maxsize, choice), typed)\n\n\ndef ttl_cache(maxsize=128, ttl=600, timer=time.monotonic, typed=False):\n    \"\"\"Decorator to wrap a function with a memoizing callable that saves\n    up to `maxsize` results based on a Least Recently Used (LRU)\n    algorithm with a per-item time-to-live (TTL) value.\n    \"\"\"\n    if maxsize is None:\n        return _cache(_UnboundTTLCache(ttl, timer), typed)\n    elif callable(maxsize):\n        return _cache(TTLCache(128, ttl, timer), typed)(maxsize)\n    else:\n        return _cache(TTLCache(maxsize, ttl, timer), typed)\n",
            "file_path": "cachetools/func.py",
            "human_label": "Decorator to wrap a function with a memoizing callable that saves\nup to `maxsize` results based on a Least Frequently Used (LFU)\nalgorithm.",
            "level": "project_runnable",
            "lineno": "111",
            "name": "lfu_cache",
            "oracle_context": "{ \"apis\" : \"['_cache', '_UnboundCache', 'callable']\", \"classes\" : \"['LFUCache', '_UnboundCache']\", \"vars\" : \"[]\" }",
            "package": "func",
            "project": "pexip/os-python-cachetools",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b8bbbfe0d34b282c181210",
            "all_context": "{ \"import\" : \"codecs os logging logging pysolbase \", \"file\" : \"logger ; \", \"class\" : \"self.get_file_size(file_name) ; self.is_file_exist(file_name) ; self.append_binary_to_file(file_name,bin_buf) ; self.append_text_to_file(file_name,text_buffer,encoding,overwrite) ; self.get_current_dir(cls) ; self.file_to_textbuffer(file_name,encoding) ; self.is_path_exist(path_name) ; self.is_dir_exist(dir_name) ; self.file_to_binary(file_name) ; \" }",
            "code": "    @staticmethod\n    def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n        \"\"\"\n        Write to the specified filename, the provided binary buffer\n        Create the file if required.\n        :param file_name:  File name.\n        :type file_name: str\n        :param text_buffer: Text buffer to write.\n        :type text_buffer: str\n        :param encoding: The encoding to use.\n        :type encoding: str\n        :param overwrite: If true, file is overwritten.\n        :type overwrite: bool\n        :return: The number of bytes written or lt 0 if error.\n        :rtype int\n        \"\"\"\n\n        # Go\n        rd = None\n        try:\n            # Open (text : open return a io.BufferedReader)\n            if not overwrite:\n                rd = codecs.open(file_name, \"a+\", encoding, \"strict\", -1)\n            else:\n                rd = codecs.open(file_name, \"w\", encoding, \"strict\", -1)\n\n            # Read everything\n            # CAUTION : 2.7 return None :(\n            return rd.write(text_buffer)\n        except IOError as e:\n            # Exception...\n            logger.warning(\"append_text_to_file : IOError, ex=%s\", SolBase.extostr(e))\n            return -1\n        except Exception as e:\n            logger.warning(\"append_text_to_file : Exception, ex=%s\", SolBase.extostr(e))\n            return -1\n        finally:\n            # Close if not None...\n            if rd:\n                rd.close()\n",
            "dependency": "",
            "docstring": "Write to the specified filename, the provided binary buffer\nCreate the file if required.\n:param file_name:  File name.\n:type file_name: str\n:param text_buffer: Text buffer to write.\n:type text_buffer: str\n:param encoding: The encoding to use.\n:type encoding: str\n:param overwrite: If true, file is overwritten.\n:type overwrite: bool\n:return: The number of bytes written or lt 0 if error.\n:rtype int",
            "end_lineno": "271",
            "file_content": "\"\"\"\n# -*- coding: utf-8 -*-\n# ===============================================================================\n#\n# Copyright (C) 2013/2017 Laurent Labatut / Laurent Champagnac\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA\n# ===============================================================================\n\"\"\"\n\n# Import\nimport logging\n\nimport os\nimport codecs\n\nfrom pysolbase.SolBase import SolBase\n\nlogger = logging.getLogger(__name__)\n\n\nclass FileUtility(object):\n    \"\"\"\n    File utility\n    \"\"\"\n\n    @staticmethod\n    def is_path_exist(path_name):\n        \"\"\"\n        Check if a path (file or dir) name exist.\n        :param path_name: Path name.\n        :type path_name text_type\n        :return: Return true (exist), false (do not exist, or invalid file name)\n        :rtype bool\n        \"\"\"\n\n        # Check\n        if path_name is None:\n            logger.warning(\"is_path_exist : file_name is None\")\n            return False\n        elif not isinstance(path_name, str):\n            logger.warning(\"is_path_exist : path_name not a text_type, className=%s\", SolBase.get_classname(path_name))\n            return False\n\n        # Go\n        return os.path.exists(path_name)\n\n    @staticmethod\n    def is_file_exist(file_name):\n        \"\"\"\n        Check if file name exist.\n        :param file_name: File name.\n        :type file_name: str\n        :return: Return true (exist), false (do not exist, or invalid file name)\n        :rtype bool\n        \"\"\"\n\n        # Check\n        if file_name is None:\n            logger.warning(\"is_file_exist : file_name is None\")\n            return False\n        elif not isinstance(file_name, str):\n            logger.warning(\"is_file_exist : file_name not a text_type, className=%s\", SolBase.get_classname(file_name))\n            return False\n\n        # Go\n        return os.path.isfile(file_name)\n\n    @staticmethod\n    def is_dir_exist(dir_name):\n        \"\"\"\n        Check if dir name exist.\n        :param dir_name: Directory name.\n        :type dir_name: str\n        :return: Return true (exist), false (do not exist, or invalid file name)\n        :rtype bool\n        \"\"\"\n\n        # Check\n        if dir_name is None:\n            logger.warning(\"is_dir_exist : file_name is None\")\n            return False\n        elif not isinstance(dir_name, str):\n            logger.warning(\"is_dir_exist : file_name not a text_type, className=%s\", SolBase.get_classname(dir_name))\n            return False\n\n        # Go\n        return os.path.isdir(dir_name)\n\n    @staticmethod\n    def get_file_size(file_name):\n        \"\"\"\n        Return a file size in bytes.\n        :param file_name: File name.\n        :type file_name: str\n        :return: An integer, gt-eq 0 if file exist, lt 0 if error.\n        :rtype int\n        \"\"\"\n        if not FileUtility.is_file_exist(file_name):\n            return -1\n        else:\n            return os.path.getsize(file_name)\n\n    @classmethod\n    def get_current_dir(cls):\n        \"\"\"\n        Return the current directory.\n        :return: A String\n        :rtype text_type\n        \"\"\"\n\n        return os.getcwd()\n\n    @staticmethod\n    def file_to_binary(file_name):\n        \"\"\"\n        Load a file toward a binary buffer.\n        :param file_name: File name.\n        :type file_name: str\n        :return: Return the binary buffer or None in case of error.\n        :rtype: bytes,None\n        \"\"\"\n\n        # Check\n        if not FileUtility.is_file_exist(file_name):\n            logger.warning(\"file_to_binary : file_name not exist, file_name=%s\", file_name)\n            return None\n\n        # Go\n        rd = None\n        try:\n            # Open (binary : open return a io.BufferedReader)\n            rd = open(file_name, \"rb\")\n\n            # Read everything\n            return rd.read()\n        except IOError as e:\n            # Exception...\n            logger.warning(\"IOError, ex=%s\", SolBase.extostr(e))\n            return None\n        except Exception as e:\n            logger.warning(\"Exception, ex=%s\", SolBase.extostr(e))\n            return None\n        finally:\n            # Close if not None...\n            if rd:\n                rd.close()\n\n    @staticmethod\n    def file_to_textbuffer(file_name, encoding):\n        \"\"\"\n        Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n        CAUTION : This will read the whole file IN MEMORY.\n        :param file_name: File name.\n        :type file_name: str\n        :param encoding: Encoding to use.\n        :type encoding: str\n        :return: A text buffer or None in case of error.\n        :rtype str\n        \"\"\"\n\n        # Check\n        if not FileUtility.is_file_exist(file_name):\n            logger.warning(\"file_to_textbuffer : file_name not exist, file_name=%s\", file_name)\n            return None\n\n        # Go\n        rd = None\n        try:\n            # Open (text : open return a io.BufferedReader)\n            rd = codecs.open(file_name, \"r\", encoding, \"strict\", -1)\n\n            # Read everything\n            return rd.read()\n        except IOError as e:\n            # Exception...\n            logger.warning(\"file_to_binary : IOError, ex=%s\", SolBase.extostr(e))\n            return None\n        except Exception as e:\n            logger.warning(\"file_to_binary : Exception, ex=%s\", SolBase.extostr(e))\n            return None\n        finally:\n            # Close if not None...\n            if rd:\n                rd.close()\n\n    @staticmethod\n    def append_binary_to_file(file_name, bin_buf):\n        \"\"\"\n        Write to the specified filename, the provided binary buffer.\n        Create the file if required.\n        :param file_name:  File name.\n        :type file_name: str\n        :param bin_buf: Binary buffer to write.\n        :type bin_buf: bytes\n        :return: The number of bytes written or lt 0 if error.\n        :rtype int\n        \"\"\"\n\n        # Go\n        rd = None\n        try:\n            # Open (text : open return a io.BufferedReader)\n            rd = open(file_name, \"ab+\")\n\n            # Read everything\n            return rd.write(bin_buf)\n        except IOError as e:\n            # Exception...\n            logger.warning(\"append_binary_to_file : IOError, ex=%s\", SolBase.extostr(e))\n            return -1\n        except Exception as e:\n            logger.warning(\"append_binary_to_file : Exception, ex=%s\", SolBase.extostr(e))\n            return -1\n        finally:\n            # Close if not None...\n            if rd:\n                rd.close()\n\n    @staticmethod\n    def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n        \"\"\"\n        Write to the specified filename, the provided binary buffer\n        Create the file if required.\n        :param file_name:  File name.\n        :type file_name: str\n        :param text_buffer: Text buffer to write.\n        :type text_buffer: str\n        :param encoding: The encoding to use.\n        :type encoding: str\n        :param overwrite: If true, file is overwritten.\n        :type overwrite: bool\n        :return: The number of bytes written or lt 0 if error.\n        :rtype int\n        \"\"\"\n\n        # Go\n        rd = None\n        try:\n            # Open (text : open return a io.BufferedReader)\n            if not overwrite:\n                rd = codecs.open(file_name, \"a+\", encoding, \"strict\", -1)\n            else:\n                rd = codecs.open(file_name, \"w\", encoding, \"strict\", -1)\n\n            # Read everything\n            # CAUTION : 2.7 return None :(\n            return rd.write(text_buffer)\n        except IOError as e:\n            # Exception...\n            logger.warning(\"append_text_to_file : IOError, ex=%s\", SolBase.extostr(e))\n            return -1\n        except Exception as e:\n            logger.warning(\"append_text_to_file : Exception, ex=%s\", SolBase.extostr(e))\n            return -1\n        finally:\n            # Close if not None...\n            if rd:\n                rd.close()\n",
            "file_path": "pysolbase/FileUtility.py",
            "human_label": "Writes the data in the text buffer to a file",
            "level": "file_runnable",
            "lineno": "232",
            "name": "append_text_to_file",
            "oracle_context": "{ \"apis\" : \"['warning', 'write', 'close', 'extostr', 'open']\", \"classes\" : \"['codecs', 'SolBase']\", \"vars\" : \"['logger']\" }",
            "package": "FileUtility",
            "project": "champax/pysolbase",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b8bbbfe0d34b282c18120f",
            "all_context": "{ \"import\" : \"codecs os logging logging pysolbase \", \"file\" : \"logger ; \", \"class\" : \"self.get_file_size(file_name) ; self.is_file_exist(file_name) ; self.append_binary_to_file(file_name,bin_buf) ; self.append_text_to_file(file_name,text_buffer,encoding,overwrite) ; self.get_current_dir(cls) ; self.file_to_textbuffer(file_name,encoding) ; self.is_path_exist(path_name) ; self.is_dir_exist(dir_name) ; self.file_to_binary(file_name) ; \" }",
            "code": "    @staticmethod\n    def file_to_textbuffer(file_name, encoding):\n        \"\"\"\n        Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n        CAUTION : This will read the whole file IN MEMORY.\n        :param file_name: File name.\n        :type file_name: str\n        :param encoding: Encoding to use.\n        :type encoding: str\n        :return: A text buffer or None in case of error.\n        :rtype str\n        \"\"\"\n\n        # Check\n        if not FileUtility.is_file_exist(file_name):\n            logger.warning(\"file_to_textbuffer : file_name not exist, file_name=%s\", file_name)\n            return None\n\n        # Go\n        rd = None\n        try:\n            # Open (text : open return a io.BufferedReader)\n            rd = codecs.open(file_name, \"r\", encoding, \"strict\", -1)\n\n            # Read everything\n            return rd.read()\n        except IOError as e:\n            # Exception...\n            logger.warning(\"file_to_binary : IOError, ex=%s\", SolBase.extostr(e))\n            return None\n        except Exception as e:\n            logger.warning(\"file_to_binary : Exception, ex=%s\", SolBase.extostr(e))\n            return None\n        finally:\n            # Close if not None...\n            if rd:\n                rd.close()\n",
            "dependency": "",
            "docstring": "Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\nCAUTION : This will read the whole file IN MEMORY.\n:param file_name: File name.\n:type file_name: str\n:param encoding: Encoding to use.\n:type encoding: str\n:return: A text buffer or None in case of error.\n:rtype str",
            "end_lineno": "197",
            "file_content": "\"\"\"\n# -*- coding: utf-8 -*-\n# ===============================================================================\n#\n# Copyright (C) 2013/2017 Laurent Labatut / Laurent Champagnac\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA\n# ===============================================================================\n\"\"\"\n\n# Import\nimport logging\n\nimport os\nimport codecs\n\nfrom pysolbase.SolBase import SolBase\n\nlogger = logging.getLogger(__name__)\n\n\nclass FileUtility(object):\n    \"\"\"\n    File utility\n    \"\"\"\n\n    @staticmethod\n    def is_path_exist(path_name):\n        \"\"\"\n        Check if a path (file or dir) name exist.\n        :param path_name: Path name.\n        :type path_name text_type\n        :return: Return true (exist), false (do not exist, or invalid file name)\n        :rtype bool\n        \"\"\"\n\n        # Check\n        if path_name is None:\n            logger.warning(\"is_path_exist : file_name is None\")\n            return False\n        elif not isinstance(path_name, str):\n            logger.warning(\"is_path_exist : path_name not a text_type, className=%s\", SolBase.get_classname(path_name))\n            return False\n\n        # Go\n        return os.path.exists(path_name)\n\n    @staticmethod\n    def is_file_exist(file_name):\n        \"\"\"\n        Check if file name exist.\n        :param file_name: File name.\n        :type file_name: str\n        :return: Return true (exist), false (do not exist, or invalid file name)\n        :rtype bool\n        \"\"\"\n\n        # Check\n        if file_name is None:\n            logger.warning(\"is_file_exist : file_name is None\")\n            return False\n        elif not isinstance(file_name, str):\n            logger.warning(\"is_file_exist : file_name not a text_type, className=%s\", SolBase.get_classname(file_name))\n            return False\n\n        # Go\n        return os.path.isfile(file_name)\n\n    @staticmethod\n    def is_dir_exist(dir_name):\n        \"\"\"\n        Check if dir name exist.\n        :param dir_name: Directory name.\n        :type dir_name: str\n        :return: Return true (exist), false (do not exist, or invalid file name)\n        :rtype bool\n        \"\"\"\n\n        # Check\n        if dir_name is None:\n            logger.warning(\"is_dir_exist : file_name is None\")\n            return False\n        elif not isinstance(dir_name, str):\n            logger.warning(\"is_dir_exist : file_name not a text_type, className=%s\", SolBase.get_classname(dir_name))\n            return False\n\n        # Go\n        return os.path.isdir(dir_name)\n\n    @staticmethod\n    def get_file_size(file_name):\n        \"\"\"\n        Return a file size in bytes.\n        :param file_name: File name.\n        :type file_name: str\n        :return: An integer, gt-eq 0 if file exist, lt 0 if error.\n        :rtype int\n        \"\"\"\n        if not FileUtility.is_file_exist(file_name):\n            return -1\n        else:\n            return os.path.getsize(file_name)\n\n    @classmethod\n    def get_current_dir(cls):\n        \"\"\"\n        Return the current directory.\n        :return: A String\n        :rtype text_type\n        \"\"\"\n\n        return os.getcwd()\n\n    @staticmethod\n    def file_to_binary(file_name):\n        \"\"\"\n        Load a file toward a binary buffer.\n        :param file_name: File name.\n        :type file_name: str\n        :return: Return the binary buffer or None in case of error.\n        :rtype: bytes,None\n        \"\"\"\n\n        # Check\n        if not FileUtility.is_file_exist(file_name):\n            logger.warning(\"file_to_binary : file_name not exist, file_name=%s\", file_name)\n            return None\n\n        # Go\n        rd = None\n        try:\n            # Open (binary : open return a io.BufferedReader)\n            rd = open(file_name, \"rb\")\n\n            # Read everything\n            return rd.read()\n        except IOError as e:\n            # Exception...\n            logger.warning(\"IOError, ex=%s\", SolBase.extostr(e))\n            return None\n        except Exception as e:\n            logger.warning(\"Exception, ex=%s\", SolBase.extostr(e))\n            return None\n        finally:\n            # Close if not None...\n            if rd:\n                rd.close()\n\n    @staticmethod\n    def file_to_textbuffer(file_name, encoding):\n        \"\"\"\n        Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n        CAUTION : This will read the whole file IN MEMORY.\n        :param file_name: File name.\n        :type file_name: str\n        :param encoding: Encoding to use.\n        :type encoding: str\n        :return: A text buffer or None in case of error.\n        :rtype str\n        \"\"\"\n\n        # Check\n        if not FileUtility.is_file_exist(file_name):\n            logger.warning(\"file_to_textbuffer : file_name not exist, file_name=%s\", file_name)\n            return None\n\n        # Go\n        rd = None\n        try:\n            # Open (text : open return a io.BufferedReader)\n            rd = codecs.open(file_name, \"r\", encoding, \"strict\", -1)\n\n            # Read everything\n            return rd.read()\n        except IOError as e:\n            # Exception...\n            logger.warning(\"file_to_binary : IOError, ex=%s\", SolBase.extostr(e))\n            return None\n        except Exception as e:\n            logger.warning(\"file_to_binary : Exception, ex=%s\", SolBase.extostr(e))\n            return None\n        finally:\n            # Close if not None...\n            if rd:\n                rd.close()\n\n    @staticmethod\n    def append_binary_to_file(file_name, bin_buf):\n        \"\"\"\n        Write to the specified filename, the provided binary buffer.\n        Create the file if required.\n        :param file_name:  File name.\n        :type file_name: str\n        :param bin_buf: Binary buffer to write.\n        :type bin_buf: bytes\n        :return: The number of bytes written or lt 0 if error.\n        :rtype int\n        \"\"\"\n\n        # Go\n        rd = None\n        try:\n            # Open (text : open return a io.BufferedReader)\n            rd = open(file_name, \"ab+\")\n\n            # Read everything\n            return rd.write(bin_buf)\n        except IOError as e:\n            # Exception...\n            logger.warning(\"append_binary_to_file : IOError, ex=%s\", SolBase.extostr(e))\n            return -1\n        except Exception as e:\n            logger.warning(\"append_binary_to_file : Exception, ex=%s\", SolBase.extostr(e))\n            return -1\n        finally:\n            # Close if not None...\n            if rd:\n                rd.close()\n\n    @staticmethod\n    def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n        \"\"\"\n        Write to the specified filename, the provided binary buffer\n        Create the file if required.\n        :param file_name:  File name.\n        :type file_name: str\n        :param text_buffer: Text buffer to write.\n        :type text_buffer: str\n        :param encoding: The encoding to use.\n        :type encoding: str\n        :param overwrite: If true, file is overwritten.\n        :type overwrite: bool\n        :return: The number of bytes written or lt 0 if error.\n        :rtype int\n        \"\"\"\n\n        # Go\n        rd = None\n        try:\n            # Open (text : open return a io.BufferedReader)\n            if not overwrite:\n                rd = codecs.open(file_name, \"a+\", encoding, \"strict\", -1)\n            else:\n                rd = codecs.open(file_name, \"w\", encoding, \"strict\", -1)\n\n            # Read everything\n            # CAUTION : 2.7 return None :(\n            return rd.write(text_buffer)\n        except IOError as e:\n            # Exception...\n            logger.warning(\"append_text_to_file : IOError, ex=%s\", SolBase.extostr(e))\n            return -1\n        except Exception as e:\n            logger.warning(\"append_text_to_file : Exception, ex=%s\", SolBase.extostr(e))\n            return -1\n        finally:\n            # Close if not None...\n            if rd:\n                rd.close()\n",
            "file_path": "pysolbase/FileUtility.py",
            "human_label": "Load a file toward a text buffer",
            "level": "file_runnable",
            "lineno": "161",
            "name": "file_to_textbuffer",
            "oracle_context": "{ \"apis\" : \"['warning', 'read', 'is_file_exist', 'close', 'extostr', 'open']\", \"classes\" : \"['codecs', 'SolBase']\", \"vars\" : \"['FileUtility', 'logger']\" }",
            "package": "FileUtility",
            "project": "champax/pysolbase",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b8bbbce0d34b282c18120d",
            "all_context": "{ \"import\" : \"codecs os logging logging pysolbase \", \"file\" : \"logger ; \", \"class\" : \"self.get_file_size(file_name) ; self.is_file_exist(file_name) ; self.append_binary_to_file(file_name,bin_buf) ; self.append_text_to_file(file_name,text_buffer,encoding,overwrite) ; self.get_current_dir(cls) ; self.file_to_textbuffer(file_name,encoding) ; self.is_path_exist(path_name) ; self.is_dir_exist(dir_name) ; self.file_to_binary(file_name) ; \" }",
            "code": "    @staticmethod\n    def is_file_exist(file_name):\n        \"\"\"\n        Check if file name exist.\n        :param file_name: File name.\n        :type file_name: str\n        :return: Return true (exist), false (do not exist, or invalid file name)\n        :rtype bool\n        \"\"\"\n\n        # Check\n        if file_name is None:\n            logger.warning(\"is_file_exist : file_name is None\")\n            return False\n        elif not isinstance(file_name, str):\n            logger.warning(\"is_file_exist : file_name not a text_type, className=%s\", SolBase.get_classname(file_name))\n            return False\n\n        # Go\n        return os.path.isfile(file_name)\n",
            "dependency": "",
            "docstring": "Check if file name exist.\n:param file_name: File name.\n:type file_name: str\n:return: Return true (exist), false (do not exist, or invalid file name)\n:rtype bool",
            "end_lineno": "79",
            "file_content": "\"\"\"\n# -*- coding: utf-8 -*-\n# ===============================================================================\n#\n# Copyright (C) 2013/2017 Laurent Labatut / Laurent Champagnac\n#\n# This program is free software; you can redistribute it and/or\n# modify it under the terms of the GNU General Public License\n# as published by the Free Software Foundation; either version 2\n# of the License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA\n# ===============================================================================\n\"\"\"\n\n# Import\nimport logging\n\nimport os\nimport codecs\n\nfrom pysolbase.SolBase import SolBase\n\nlogger = logging.getLogger(__name__)\n\n\nclass FileUtility(object):\n    \"\"\"\n    File utility\n    \"\"\"\n\n    @staticmethod\n    def is_path_exist(path_name):\n        \"\"\"\n        Check if a path (file or dir) name exist.\n        :param path_name: Path name.\n        :type path_name text_type\n        :return: Return true (exist), false (do not exist, or invalid file name)\n        :rtype bool\n        \"\"\"\n\n        # Check\n        if path_name is None:\n            logger.warning(\"is_path_exist : file_name is None\")\n            return False\n        elif not isinstance(path_name, str):\n            logger.warning(\"is_path_exist : path_name not a text_type, className=%s\", SolBase.get_classname(path_name))\n            return False\n\n        # Go\n        return os.path.exists(path_name)\n\n    @staticmethod\n    def is_file_exist(file_name):\n        \"\"\"\n        Check if file name exist.\n        :param file_name: File name.\n        :type file_name: str\n        :return: Return true (exist), false (do not exist, or invalid file name)\n        :rtype bool\n        \"\"\"\n\n        # Check\n        if file_name is None:\n            logger.warning(\"is_file_exist : file_name is None\")\n            return False\n        elif not isinstance(file_name, str):\n            logger.warning(\"is_file_exist : file_name not a text_type, className=%s\", SolBase.get_classname(file_name))\n            return False\n\n        # Go\n        return os.path.isfile(file_name)\n\n    @staticmethod\n    def is_dir_exist(dir_name):\n        \"\"\"\n        Check if dir name exist.\n        :param dir_name: Directory name.\n        :type dir_name: str\n        :return: Return true (exist), false (do not exist, or invalid file name)\n        :rtype bool\n        \"\"\"\n\n        # Check\n        if dir_name is None:\n            logger.warning(\"is_dir_exist : file_name is None\")\n            return False\n        elif not isinstance(dir_name, str):\n            logger.warning(\"is_dir_exist : file_name not a text_type, className=%s\", SolBase.get_classname(dir_name))\n            return False\n\n        # Go\n        return os.path.isdir(dir_name)\n\n    @staticmethod\n    def get_file_size(file_name):\n        \"\"\"\n        Return a file size in bytes.\n        :param file_name: File name.\n        :type file_name: str\n        :return: An integer, gt-eq 0 if file exist, lt 0 if error.\n        :rtype int\n        \"\"\"\n        if not FileUtility.is_file_exist(file_name):\n            return -1\n        else:\n            return os.path.getsize(file_name)\n\n    @classmethod\n    def get_current_dir(cls):\n        \"\"\"\n        Return the current directory.\n        :return: A String\n        :rtype text_type\n        \"\"\"\n\n        return os.getcwd()\n\n    @staticmethod\n    def file_to_binary(file_name):\n        \"\"\"\n        Load a file toward a binary buffer.\n        :param file_name: File name.\n        :type file_name: str\n        :return: Return the binary buffer or None in case of error.\n        :rtype: bytes,None\n        \"\"\"\n\n        # Check\n        if not FileUtility.is_file_exist(file_name):\n            logger.warning(\"file_to_binary : file_name not exist, file_name=%s\", file_name)\n            return None\n\n        # Go\n        rd = None\n        try:\n            # Open (binary : open return a io.BufferedReader)\n            rd = open(file_name, \"rb\")\n\n            # Read everything\n            return rd.read()\n        except IOError as e:\n            # Exception...\n            logger.warning(\"IOError, ex=%s\", SolBase.extostr(e))\n            return None\n        except Exception as e:\n            logger.warning(\"Exception, ex=%s\", SolBase.extostr(e))\n            return None\n        finally:\n            # Close if not None...\n            if rd:\n                rd.close()\n\n    @staticmethod\n    def file_to_textbuffer(file_name, encoding):\n        \"\"\"\n        Load a file toward a text buffer (UTF-8), using the specify encoding while reading.\n        CAUTION : This will read the whole file IN MEMORY.\n        :param file_name: File name.\n        :type file_name: str\n        :param encoding: Encoding to use.\n        :type encoding: str\n        :return: A text buffer or None in case of error.\n        :rtype str\n        \"\"\"\n\n        # Check\n        if not FileUtility.is_file_exist(file_name):\n            logger.warning(\"file_to_textbuffer : file_name not exist, file_name=%s\", file_name)\n            return None\n\n        # Go\n        rd = None\n        try:\n            # Open (text : open return a io.BufferedReader)\n            rd = codecs.open(file_name, \"r\", encoding, \"strict\", -1)\n\n            # Read everything\n            return rd.read()\n        except IOError as e:\n            # Exception...\n            logger.warning(\"file_to_binary : IOError, ex=%s\", SolBase.extostr(e))\n            return None\n        except Exception as e:\n            logger.warning(\"file_to_binary : Exception, ex=%s\", SolBase.extostr(e))\n            return None\n        finally:\n            # Close if not None...\n            if rd:\n                rd.close()\n\n    @staticmethod\n    def append_binary_to_file(file_name, bin_buf):\n        \"\"\"\n        Write to the specified filename, the provided binary buffer.\n        Create the file if required.\n        :param file_name:  File name.\n        :type file_name: str\n        :param bin_buf: Binary buffer to write.\n        :type bin_buf: bytes\n        :return: The number of bytes written or lt 0 if error.\n        :rtype int\n        \"\"\"\n\n        # Go\n        rd = None\n        try:\n            # Open (text : open return a io.BufferedReader)\n            rd = open(file_name, \"ab+\")\n\n            # Read everything\n            return rd.write(bin_buf)\n        except IOError as e:\n            # Exception...\n            logger.warning(\"append_binary_to_file : IOError, ex=%s\", SolBase.extostr(e))\n            return -1\n        except Exception as e:\n            logger.warning(\"append_binary_to_file : Exception, ex=%s\", SolBase.extostr(e))\n            return -1\n        finally:\n            # Close if not None...\n            if rd:\n                rd.close()\n\n    @staticmethod\n    def append_text_to_file(file_name, text_buffer, encoding, overwrite=False):\n        \"\"\"\n        Write to the specified filename, the provided binary buffer\n        Create the file if required.\n        :param file_name:  File name.\n        :type file_name: str\n        :param text_buffer: Text buffer to write.\n        :type text_buffer: str\n        :param encoding: The encoding to use.\n        :type encoding: str\n        :param overwrite: If true, file is overwritten.\n        :type overwrite: bool\n        :return: The number of bytes written or lt 0 if error.\n        :rtype int\n        \"\"\"\n\n        # Go\n        rd = None\n        try:\n            # Open (text : open return a io.BufferedReader)\n            if not overwrite:\n                rd = codecs.open(file_name, \"a+\", encoding, \"strict\", -1)\n            else:\n                rd = codecs.open(file_name, \"w\", encoding, \"strict\", -1)\n\n            # Read everything\n            # CAUTION : 2.7 return None :(\n            return rd.write(text_buffer)\n        except IOError as e:\n            # Exception...\n            logger.warning(\"append_text_to_file : IOError, ex=%s\", SolBase.extostr(e))\n            return -1\n        except Exception as e:\n            logger.warning(\"append_text_to_file : Exception, ex=%s\", SolBase.extostr(e))\n            return -1\n        finally:\n            # Close if not None...\n            if rd:\n                rd.close()\n",
            "file_path": "pysolbase/FileUtility.py",
            "human_label": "Check whether file_name is an existing file.",
            "level": "file_runnable",
            "lineno": "60",
            "name": "is_file_exist",
            "oracle_context": "{ \"apis\" : \"['isfile', 'isinstance', 'get_classname', 'warning']\", \"classes\" : \"['SolBase', 'os']\", \"vars\" : \"['logger', 'path']\" }",
            "package": "FileUtility",
            "project": "champax/pysolbase",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b8b3d5eb7e40a82d2d1110",
            "all_context": "{ \"import\" : \"os unittest sys types zope unittest \", \"file\" : \"PYPY ; PYPY2 ; _skip_under_py3k(test_method) ; _skip_under_py2(test_method) ; _c_optimizations_required() ; _c_optimizations_available() ; _c_optimizations_ignored() ; _should_attempt_c_optimizations() ; _use_c_impl(py_impl,name,globs) ; \", \"class\" : \"\" }",
            "code": "def _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    catch = () if _c_optimizations_required() else (ImportError,)\n    try:\n        from zope.interface import _zope_interface_coptimizations as c_opt\n        return c_opt\n    except catch: # pragma: no cover (only Jython doesn't build extensions)\n        return False\n",
            "dependency": "{ \"builtin\" : false, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Return the C optimization module, if available, otherwise\na false value.\n\nIf the optimizations are required but not available, this\nraises the ImportError.\n\nThis does not say whether they should be used or not.",
            "end_lineno": "96",
            "file_content": "##############################################################################\n#\n# Copyright (c) 2006 Zope Foundation and Contributors.\n# All Rights Reserved.\n#\n# This software is subject to the provisions of the Zope Public License,\n# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.\n# THIS SOFTWARE IS PROVIDED \"AS IS\" AND ANY AND ALL EXPRESS OR IMPLIED\n# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS\n# FOR A PARTICULAR PURPOSE.\n#\n##############################################################################\n\"\"\"\nSupport functions for dealing with differences in platforms, including Python\nversions and implementations.\n\nThis file should have no imports from the rest of zope.interface because it is\nused during early bootstrapping.\n\"\"\"\nimport os\nimport sys\nimport types\n\nif sys.version_info[0] < 3:\n\n    def _normalize_name(name):\n        if isinstance(name, basestring):\n            return unicode(name)\n        raise TypeError(\"name must be a regular or unicode string\")\n\n    CLASS_TYPES = (type, types.ClassType)\n    STRING_TYPES = (basestring,)\n\n    _BUILTINS = '__builtin__'\n\n    PYTHON3 = False\n    PYTHON2 = True\n\nelse:\n\n    def _normalize_name(name):\n        if isinstance(name, bytes):\n            name = str(name, 'ascii')\n        if isinstance(name, str):\n            return name\n        raise TypeError(\"name must be a string or ASCII-only bytes\")\n\n    CLASS_TYPES = (type,)\n    STRING_TYPES = (str,)\n\n    _BUILTINS = 'builtins'\n\n    PYTHON3 = True\n    PYTHON2 = False\n\nPYPY = hasattr(sys, 'pypy_version_info')\nPYPY2 = PYTHON2 and PYPY\n\ndef _skip_under_py3k(test_method):\n    import unittest\n    return unittest.skipIf(sys.version_info[0] >= 3, \"Only on Python 2\")(test_method)\n\n\ndef _skip_under_py2(test_method):\n    import unittest\n    return unittest.skipIf(sys.version_info[0] < 3, \"Only on Python 3\")(test_method)\n\n\ndef _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    pure_env = os.environ.get('PURE_PYTHON')\n    require_c = pure_env == \"0\"\n    return require_c\n\n\ndef _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    catch = () if _c_optimizations_required() else (ImportError,)\n    try:\n        from zope.interface import _zope_interface_coptimizations as c_opt\n        return c_opt\n    except catch: # pragma: no cover (only Jython doesn't build extensions)\n        return False\n\n\ndef _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    pure_env = os.environ.get('PURE_PYTHON')\n    return pure_env is not None and pure_env != \"0\"\n\n\ndef _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\n    This takes into account whether we're on PyPy and the value of the\n    ``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    is_pypy = hasattr(sys, 'pypy_version_info')\n\n    if _c_optimizations_required():\n        return True\n    if is_pypy:\n        return False\n    return not _c_optimizations_ignored()\n\n\ndef _use_c_impl(py_impl, name=None, globs=None):\n    \"\"\"\n    Decorator. Given an object implemented in Python, with a name like\n    ``Foo``, import the corresponding C implementation from\n    ``zope.interface._zope_interface_coptimizations`` with the name\n    ``Foo`` and use it instead.\n\n    If the ``PURE_PYTHON`` environment variable is set to any value\n    other than ``\"0\"``, or we're on PyPy, ignore the C implementation\n    and return the Python version. If the C implementation cannot be\n    imported, return the Python version. If ``PURE_PYTHON`` is set to\n    0, *require* the C implementation (let the ImportError propagate);\n    note that PyPy can import the C implementation in this case (and all\n    tests pass).\n\n    In all cases, the Python version is kept available. in the module\n    globals with the name ``FooPy`` and the name ``FooFallback`` (both\n    conventions have been used; the C implementation of some functions\n    looks for the ``Fallback`` version, as do some of the Sphinx\n    documents).\n\n    Example::\n\n        @_use_c_impl\n        class Foo(object):\n            ...\n    \"\"\"\n    name = name or py_impl.__name__\n    globs = globs or sys._getframe(1).f_globals\n\n    def find_impl():\n        if not _should_attempt_c_optimizations():\n            return py_impl\n\n        c_opt = _c_optimizations_available()\n        if not c_opt: # pragma: no cover (only Jython doesn't build extensions)\n            return py_impl\n\n        __traceback_info__ = c_opt\n        return getattr(c_opt, name)\n\n    c_impl = find_impl()\n    # Always make available by the FooPy name and FooFallback\n    # name (for testing and documentation)\n    globs[name + 'Py'] = py_impl\n    globs[name + 'Fallback'] = py_impl\n\n    return c_impl\n",
            "file_path": "src/zope/interface/_compat.py",
            "human_label": "If available, return the C optimization module, otherwise a false value.",
            "level": "file_runnable",
            "lineno": "81",
            "name": "_c_optimizations_available",
            "oracle_context": "{ \"apis\" : \"['_c_optimizations_required']\", \"classes\" : \"['c_opt']\", \"vars\" : \"[]\" }",
            "package": "_compat",
            "project": "pexip/os-zope",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b8b3d4eb7e40a82d2d110f",
            "all_context": "{ \"import\" : \"os unittest sys types zope unittest \", \"file\" : \"PYPY ; PYPY2 ; _skip_under_py3k(test_method) ; _skip_under_py2(test_method) ; _c_optimizations_required() ; _c_optimizations_available() ; _c_optimizations_ignored() ; _should_attempt_c_optimizations() ; _use_c_impl(py_impl,name,globs) ; \", \"class\" : \"\" }",
            "code": "def _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\n    This takes into account whether we're on PyPy and the value of the\n    ``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    is_pypy = hasattr(sys, 'pypy_version_info')\n\n    if _c_optimizations_required():\n        return True\n    if is_pypy:\n        return False\n    return not _c_optimizations_ignored()\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Return a true value if we should attempt to use the C optimizations.\n\nThis takes into account whether we're on PyPy and the value of the\n``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.",
            "end_lineno": "120",
            "file_content": "##############################################################################\n#\n# Copyright (c) 2006 Zope Foundation and Contributors.\n# All Rights Reserved.\n#\n# This software is subject to the provisions of the Zope Public License,\n# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.\n# THIS SOFTWARE IS PROVIDED \"AS IS\" AND ANY AND ALL EXPRESS OR IMPLIED\n# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS\n# FOR A PARTICULAR PURPOSE.\n#\n##############################################################################\n\"\"\"\nSupport functions for dealing with differences in platforms, including Python\nversions and implementations.\n\nThis file should have no imports from the rest of zope.interface because it is\nused during early bootstrapping.\n\"\"\"\nimport os\nimport sys\nimport types\n\nif sys.version_info[0] < 3:\n\n    def _normalize_name(name):\n        if isinstance(name, basestring):\n            return unicode(name)\n        raise TypeError(\"name must be a regular or unicode string\")\n\n    CLASS_TYPES = (type, types.ClassType)\n    STRING_TYPES = (basestring,)\n\n    _BUILTINS = '__builtin__'\n\n    PYTHON3 = False\n    PYTHON2 = True\n\nelse:\n\n    def _normalize_name(name):\n        if isinstance(name, bytes):\n            name = str(name, 'ascii')\n        if isinstance(name, str):\n            return name\n        raise TypeError(\"name must be a string or ASCII-only bytes\")\n\n    CLASS_TYPES = (type,)\n    STRING_TYPES = (str,)\n\n    _BUILTINS = 'builtins'\n\n    PYTHON3 = True\n    PYTHON2 = False\n\nPYPY = hasattr(sys, 'pypy_version_info')\nPYPY2 = PYTHON2 and PYPY\n\ndef _skip_under_py3k(test_method):\n    import unittest\n    return unittest.skipIf(sys.version_info[0] >= 3, \"Only on Python 2\")(test_method)\n\n\ndef _skip_under_py2(test_method):\n    import unittest\n    return unittest.skipIf(sys.version_info[0] < 3, \"Only on Python 3\")(test_method)\n\n\ndef _c_optimizations_required():\n    \"\"\"\n    Return a true value if the C optimizations are required.\n\n    This uses the ``PURE_PYTHON`` variable as documented in `_use_c_impl`.\n    \"\"\"\n    pure_env = os.environ.get('PURE_PYTHON')\n    require_c = pure_env == \"0\"\n    return require_c\n\n\ndef _c_optimizations_available():\n    \"\"\"\n    Return the C optimization module, if available, otherwise\n    a false value.\n\n    If the optimizations are required but not available, this\n    raises the ImportError.\n\n    This does not say whether they should be used or not.\n    \"\"\"\n    catch = () if _c_optimizations_required() else (ImportError,)\n    try:\n        from zope.interface import _zope_interface_coptimizations as c_opt\n        return c_opt\n    except catch: # pragma: no cover (only Jython doesn't build extensions)\n        return False\n\n\ndef _c_optimizations_ignored():\n    \"\"\"\n    The opposite of `_c_optimizations_required`.\n    \"\"\"\n    pure_env = os.environ.get('PURE_PYTHON')\n    return pure_env is not None and pure_env != \"0\"\n\n\ndef _should_attempt_c_optimizations():\n    \"\"\"\n    Return a true value if we should attempt to use the C optimizations.\n\n    This takes into account whether we're on PyPy and the value of the\n    ``PURE_PYTHON`` environment variable, as defined in `_use_c_impl`.\n    \"\"\"\n    is_pypy = hasattr(sys, 'pypy_version_info')\n\n    if _c_optimizations_required():\n        return True\n    if is_pypy:\n        return False\n    return not _c_optimizations_ignored()\n\n\ndef _use_c_impl(py_impl, name=None, globs=None):\n    \"\"\"\n    Decorator. Given an object implemented in Python, with a name like\n    ``Foo``, import the corresponding C implementation from\n    ``zope.interface._zope_interface_coptimizations`` with the name\n    ``Foo`` and use it instead.\n\n    If the ``PURE_PYTHON`` environment variable is set to any value\n    other than ``\"0\"``, or we're on PyPy, ignore the C implementation\n    and return the Python version. If the C implementation cannot be\n    imported, return the Python version. If ``PURE_PYTHON`` is set to\n    0, *require* the C implementation (let the ImportError propagate);\n    note that PyPy can import the C implementation in this case (and all\n    tests pass).\n\n    In all cases, the Python version is kept available. in the module\n    globals with the name ``FooPy`` and the name ``FooFallback`` (both\n    conventions have been used; the C implementation of some functions\n    looks for the ``Fallback`` version, as do some of the Sphinx\n    documents).\n\n    Example::\n\n        @_use_c_impl\n        class Foo(object):\n            ...\n    \"\"\"\n    name = name or py_impl.__name__\n    globs = globs or sys._getframe(1).f_globals\n\n    def find_impl():\n        if not _should_attempt_c_optimizations():\n            return py_impl\n\n        c_opt = _c_optimizations_available()\n        if not c_opt: # pragma: no cover (only Jython doesn't build extensions)\n            return py_impl\n\n        __traceback_info__ = c_opt\n        return getattr(c_opt, name)\n\n    c_impl = find_impl()\n    # Always make available by the FooPy name and FooFallback\n    # name (for testing and documentation)\n    globs[name + 'Py'] = py_impl\n    globs[name + 'Fallback'] = py_impl\n\n    return c_impl\n",
            "file_path": "src/zope/interface/_compat.py",
            "human_label": "Return a true value if we use the C optimizations.",
            "level": "file_runnable",
            "lineno": "107",
            "name": "_should_attempt_c_optimizations",
            "oracle_context": "{ \"apis\" : \"['_c_optimizations_required', 'hasattr', '_c_optimizations_ignored']\", \"classes\" : \"['sys']\", \"vars\" : \"[]\" }",
            "package": "_compat",
            "project": "pexip/os-zope",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b87b989a0c4fa8b80b35ee",
            "all_context": "{ \"import\" : \"copy lena  \", \"file\" : \"\", \"class\" : \"self.fill(self,value) ; self._hist ; self._make_bins ; self.compute(self) ; self.reset(self) ; self._cur_context ; self.edges ; self.__init__(self,edges,bins,make_bins,initial_value) ; self._initial_value ; self._initial_bins ; self.bins ; \" }",
            "code": "    def reset(self):\n        \"\"\"Reset the histogram.\n\n        Current context is reset to an empty dict.\n        Bins are reinitialized with the *initial_value*\n        or with *make_bins()* (depending on the initialization).\n        \"\"\"\n        if self._make_bins is not None:\n            self.bins = self._make_bins()\n        elif self._initial_bins is not None:\n            self.bins = copy.deepcopy(self._initial_bins)\n        else:\n            self.bins = hf.init_bins(self.edges, self._initial_value)\n\n        self._cur_context = {}\n",
            "dependency": "{ \"builtin\" : false, \"standard_lib\" : true, \"public_lib\" : true, \"current_class\" : true, \"current_file\" : false, \"current_project\" : false, \"external\" : false }",
            "docstring": "Reset the histogram.\n\nCurrent context is reset to an empty dict.\nBins are reinitialized with the *initial_value*\nor with *make_bins()* (depending on the initialization).",
            "end_lineno": "310",
            "file_content": "\"\"\"Histogram structure *histogram* and element *Histogram*.\"\"\"\nimport copy\n\nimport lena.context\nimport lena.core\nimport lena.flow\nimport lena.math\nfrom . import hist_functions as hf\n\n\nclass histogram():\n    \"\"\"A multidimensional histogram.\n\n    Arbitrary dimension, variable bin size and weights are supported.\n    Lower bin edge is included, upper edge is excluded.\n    Underflow and overflow values are skipped.\n    Bin content can be of arbitrary type,\n    which is defined during initialization.\n\n    Examples:\n\n    >>> # a two-dimensional histogram\n    >>> hist = histogram([[0, 1, 2], [0, 1, 2]])\n    >>> hist.fill([0, 1])\n    >>> hist.bins\n    [[0, 1], [0, 0]]\n    >>> values = [[0, 0], [1, 0], [1, 1]]\n    >>> # fill the histogram with values\n    >>> for v in values:\n    ...     hist.fill(v)\n    >>> hist.bins\n    [[1, 1], [1, 1]]\n    \"\"\"\n    # Note the differences from existing packages.\n    # Numpy 1.16 (numpy.histogram): all but the last\n    # (righthand-most) bin is half-open.\n    # This histogram class has bin limits as in ROOT\n    # (but without overflow and underflow).\n\n    # Numpy: the first element of the range must be less than or equal to the second.\n    # This histogram requires strictly increasing edges.\n    # https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html\n    # https://root.cern.ch/root/htmldoc/guides/users-guide/Histograms.html#bin-numbering\n\n    def __init__(self, edges, bins=None, initial_value=0):\n        \"\"\"*edges* is a sequence of one-dimensional arrays,\n        each containing strictly increasing bin edges.\n\n        Histogram's bins by default\n        are initialized with *initial_value*.\n        It can be any object that supports addition with *weight*\n        during *fill* (but that is not necessary\n        if you don't plan to fill the histogram).\n        If the *initial_value* is compound and requires special copying,\n        create initial bins yourself (see :func:`.init_bins`).\n\n        A histogram can be created from existing *bins* and *edges*.\n        In this case a simple check of the shape of *bins* is done\n        (raising :exc:`.LenaValueError` if failed).\n\n        **Attributes**\n\n        :attr:`edges` is a list of edges on each dimension.\n        Edges mark the borders of the bin.\n        Edges along each dimension are one-dimensional lists,\n        and the multidimensional bin is the result of all intersections\n        of one-dimensional edges.\n        For example, a 3-dimensional histogram has edges of the form\n        *[x_edges, y_edges, z_edges]*,\n        and the 0th bin has borders\n        *((x[0], x[1]), (y[0], y[1]), (z[0], z[1]))*.\n\n        Index in the edges is a tuple, where a given position corresponds\n        to a dimension, and the content at that position\n        to the bin along that dimension.\n        For example, index *(0, 1, 3)* corresponds to the bin\n        with lower edges *(x[0], y[1], z[3])*.\n\n        :attr:`bins` is a list of nested lists.\n        Same index as for edges can be used to get bin content:\n        bin at *(0, 1, 3)* can be obtained as *bins[0][1][3]*.\n        Most nested arrays correspond to highest\n        (further from x) coordinates.\n        For example, for a 3-dimensional histogram bins equal to\n        *[[[1, 1], [0, 0]], [[0, 0], [0, 0]]]*\n        mean that the only filled bins are those\n        where x and y indices are 0, and z index is 0 and 1.\n\n        :attr:`dim` is the dimension of a histogram\n        (length of its *edges* for a multidimensional histogram).\n\n        If subarrays of *edges* are not increasing\n        or if any of them has length less than 2,\n        :exc:`.LenaValueError` is raised.\n\n        .. admonition:: Programmer's note\n\n            one- and multidimensional histograms\n            have different *bins* and *edges* format.\n            To be unified, 1-dimensional edges should be\n            nested in a list (like *[[1, 2, 3]]*).\n            Instead, they are simply the x-edges list,\n            because it is more intuitive and one-dimensional histograms\n            are used more often.\n            To unify the interface for bins and edges in your code,\n            use :func:`.unify_1_md` function.\n        \"\"\"\n        # todo: allow creation of *edges* from tuples\n        # (without lena.math.mesh). Allow bin_size in this case.\n        hf.check_edges_increasing(edges)\n        self.edges = edges\n        self._scale = None\n\n        if hasattr(edges[0], \"__iter__\"):\n            self.dim = len(edges)\n        else:\n            self.dim = 1\n\n        # todo: add a kwarg no_check=False to disable bins testing\n        if bins is None:\n            self.bins = hf.init_bins(self.edges, initial_value)\n        else:\n            self.bins = bins\n            # We can't make scale for an arbitrary histogram,\n            # because it may contain compound values.\n            # self._scale = self.make_scale()\n            wrong_bins_error = lena.core.LenaValueError(\n                \"bins of incorrect shape given, {}\".format(bins)\n            )\n            if self.dim == 1:\n                if len(bins) != len(edges) - 1:\n                    raise wrong_bins_error\n            else:\n                if len(bins) != len(edges[0]) - 1:\n                    raise wrong_bins_error\n        if self.dim > 1:\n            self.ranges = [(axis[0], axis[-1]) for axis in edges]\n            self.nbins =  [len(axis) - 1 for axis in edges]\n        else:\n            self.ranges = [(edges[0], edges[-1])]\n            self.nbins = [len(edges)-1]\n\n    def __eq__(self, other):\n        \"\"\"Two histograms are equal, if and only if they have\n        equal bins and equal edges.\n\n        If *other* is not a :class:`.histogram`, return ``False``.\n\n        Note that floating numbers should be compared\n        approximately (using :func:`math.isclose`).\n        \"\"\"\n        if not isinstance(other, histogram):\n            # in Python comparison between different types is allowed\n            return False\n        return self.bins == other.bins and self.edges == other.edges\n\n    def fill(self, coord, weight=1):\n        \"\"\"Fill histogram at *coord* with the given *weight*.\n\n        Coordinates outside the histogram edges are ignored.\n        \"\"\"\n        indices = hf.get_bin_on_value(coord, self.edges)\n        subarr = self.bins\n        for ind in indices[:-1]:\n            # underflow\n            if ind < 0:\n                return\n            try:\n                subarr = subarr[ind]\n            # overflow\n            except IndexError:\n                return\n        ind = indices[-1]\n        # underflow\n        if ind < 0:\n            return\n\n        # fill\n        try:\n            subarr[ind] += weight\n        except IndexError:\n            return\n\n    def __repr__(self):\n        return \"histogram({}, bins={})\".format(self.edges, self.bins)\n\n    def scale(self, other=None, recompute=False):\n        \"\"\"Compute or set scale (integral of the histogram).\n\n        If *other* is ``None``, return scale of this histogram.\n        If its scale was not computed before,\n        it is computed and stored for subsequent use\n        (unless explicitly asked to *recompute*).\n        Note that after changing (filling) the histogram\n        one must explicitly recompute the scale\n        if it was computed before.\n\n        If a float *other* is provided, rescale self to *other*.\n\n        Histograms with scale equal to zero can't be rescaled.\n        :exc:`.LenaValueError` is raised if one tries to do that.\n        \"\"\"\n        # see graph.scale comments why this is called simply \"scale\"\n        # (not set_scale, get_scale, etc.)\n        if other is None:\n            # return scale\n            if self._scale is None or recompute:\n                self._scale = hf.integral(\n                    *hf.unify_1_md(self.bins, self.edges)\n                )\n            return self._scale\n        else:\n            # rescale from other\n            scale = self.scale()\n            if scale == 0:\n                raise lena.core.LenaValueError(\n                    \"can not rescale histogram with zero scale\"\n                )\n            self.bins = lena.math.md_map(lambda binc: binc*float(other) / scale,\n                                         self.bins)\n            self._scale = other\n            return None\n\n    def _update_context(self, context):\n        \"\"\"Update *context* with the properties of this histogram.\n\n        *context.histogram* is updated with \"dim\", \"nbins\"\n        and \"ranges\" with values for this histogram.\n        If this histogram has a computed scale, it is also added\n        to the context.\n\n        Called on \"destruction\" of the histogram structure (for example,\n        in :class:`.ToCSV`). See graph._update_context for more details.\n        \"\"\"\n\n        hist_context = {\n            \"dim\": self.dim,\n            \"nbins\": self.nbins,\n            \"ranges\": self.ranges\n        }\n\n        if self._scale is not None:\n            hist_context[\"scale\"] = self._scale\n\n        lena.context.update_recursively(context, {\"histogram\": hist_context})\n\n\nclass Histogram():\n    \"\"\"An element to produce histograms.\"\"\"\n\n    def __init__(self, edges, bins=None, make_bins=None, initial_value=0):\n        \"\"\"*edges*, *bins* and *initial_value* have the same meaning\n        as during creation of a :class:`histogram`.\n\n        *make_bins* is a function without arguments\n        that creates new bins\n        (it will be called during :meth:`__init__` and :meth:`reset`).\n        *initial_value* in this case is ignored, but bin check is made.\n        If both *bins* and *make_bins* are provided,\n        :exc:`.LenaTypeError` is raised.\n        \"\"\"\n        self._hist = histogram(edges, bins)\n\n        if make_bins is not None and bins is not None:\n            raise lena.core.LenaTypeError(\n                \"either initial bins or make_bins must be provided, \"\n                \"not both: {} and {}\".format(bins, make_bins)\n            )\n\n        # may be None\n        self._initial_bins = copy.deepcopy(bins)\n\n        # todo: bins, make_bins, initial_value look redundant\n        # and may be reconsidered when really using reset().\n        if make_bins:\n            bins = make_bins()\n        self._make_bins = make_bins\n\n        self._cur_context = {}\n\n    def fill(self, value):\n        \"\"\"Fill the histogram with *value*.\n\n        *value* can be a *(data, context)* pair. \n        Values outside the histogram edges are ignored.\n        \"\"\"\n        data, self._cur_context = lena.flow.get_data_context(value)\n        self._hist.fill(data)\n        # filling with weight is only allowed in histogram structure\n        # self._hist.fill(data, weight)\n\n    def compute(self):\n        \"\"\"Yield histogram with context.\"\"\"\n        yield (self._hist, self._cur_context)\n\n    def reset(self):\n        \"\"\"Reset the histogram.\n\n        Current context is reset to an empty dict.\n        Bins are reinitialized with the *initial_value*\n        or with *make_bins()* (depending on the initialization).\n        \"\"\"\n        if self._make_bins is not None:\n            self.bins = self._make_bins()\n        elif self._initial_bins is not None:\n            self.bins = copy.deepcopy(self._initial_bins)\n        else:\n            self.bins = hf.init_bins(self.edges, self._initial_value)\n\n        self._cur_context = {}\n",
            "file_path": "lena/structures/histogram.py",
            "human_label": "Current context is reset to an empty dict, bins of the class are reinitialized with the *initial_value* or with *make_bins()*.",
            "level": "class_runnable",
            "lineno": "296",
            "name": "reset",
            "oracle_context": "{ \"apis\" : \"['init_bins', '_make_bins', 'deepcopy']\", \"classes\" : \"['hf', 'copy']\", \"vars\" : \"['_initial_bins', 'bins', '_cur_context', 'edges', '_initial_value']\" }",
            "package": "histogram",
            "project": "ynikitenko/lena",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b87b859a0c4fa8b80b35d7",
            "all_context": "{ \"import\" : \"warnings operator functools re copy lena functools \", \"file\" : \"\", \"class\" : \"self._update ; self.request(self) ; self._cur_context ; self._context ; self._update(self) ; self.__init__(self,points,context,scale,sort) ; self.points ; self.scale(self,other) ; self.scale ; self.fill(self,value) ; self.points(self) ; self.__repr__(self) ; self._scale ; self._rescale_value ; self.__eq__(self,other) ; self.reset(self) ; self._sort ; self.to_csv(self,separator,header) ; self._init_context ; self._points ; self.dim ; \" }",
            "code": "    def to_csv(self, separator=\",\", header=None):\n        \"\"\".. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n              Iterables are converted to tables.\n\n        Convert graph's points to CSV.\n\n        *separator* delimits values, the default is comma.\n\n        *header*, if not ``None``, is the first string of the output\n        (new line is added automatically).\n\n        Since a graph can be multidimensional,\n        for each point first its coordinate is converted to string\n        (separated by *separator*), then each part of its value.\n\n        To convert :class:`Graph` to CSV inside a Lena sequence,\n        use :class:`lena.output.ToCSV`.\n        \"\"\"\n        if self._sort:\n            self._update()\n\n        def unpack_pt(pt):\n            coord = pt[0]\n            value = pt[1]\n            if isinstance(coord, tuple):\n                unpacked = list(coord)\n            else:\n                unpacked = [coord]\n            if isinstance(value, tuple):\n                unpacked += list(value)\n            else:\n                unpacked.append(value)\n            return unpacked\n\n        def pt_to_str(pt, separ):\n            return separ.join([str(val) for val in unpack_pt(pt)])\n\n        if header is not None:\n            # if one needs an empty header line, they may provide \"\"\n            lines = header + \"\\n\"\n        else:\n            lines = \"\"\n        lines += \"\\n\".join([pt_to_str(pt, separator) for pt in self.points])\n\n        return lines\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : true, \"current_file\" : false, \"current_project\" : false, \"external\" : false }",
            "docstring": ".. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.",
            "end_lineno": "596",
            "file_content": "\"\"\"A graph is a function at given coordinates.\"\"\"\nimport copy\nimport functools\nimport operator\nimport re\nimport warnings\n\nimport lena.core\nimport lena.context\nimport lena.flow\n\n\nclass graph():\n    \"\"\"Numeric arrays of equal size.\"\"\"\n\n    def __init__(self, coords, field_names=(\"x\", \"y\"), scale=None):\n        \"\"\"This structure generally corresponds\n        to the graph of a function\n        and represents arrays of coordinates and the function values\n        of arbitrary dimensions.\n\n        *coords* is a list of one-dimensional\n        coordinate and value sequences (usually lists).\n        There is little to no distinction between them,\n        and \"values\" can also be called \"coordinates\".\n\n        *field_names* provide the meaning of these arrays.\n        For example, a 3-dimensional graph could be distinguished\n        from a 2-dimensional graph with errors by its fields\n        (\"x\", \"y\", \"z\") versus (\"x\", \"y\", \"error_y\").\n        Field names don't affect drawing graphs:\n        for that :class:`~Variable`-s should be used.\n        Default field names,\n        provided for the most used 2-dimensional graphs,\n        are \"x\" and \"y\".\n\n        *field_names* can be a string separated by whitespace\n        and/or commas or a tuple of strings, such as (\"x\", \"y\").\n        *field_names* must have as many elements\n        as *coords* and each field name must be unique.\n        Otherwise field names are arbitrary.\n        Error fields must go after all other coordinates.\n        Name of a coordinate error is \"error\\\\_\"\n        appended by coordinate name. Further error details\n        are appended after '_'. They could be arbitrary depending\n        on the problem: \"low\", \"high\", \"low_90%_cl\", etc. Example:\n        (\"E\", \"time\", \"error_E_low\", \"error_time\").\n\n        *scale* of the graph is a kind of its norm. It could be\n        the integral of the function or its other property.\n        A scale of a normalised probability density\n        function would be one.\n        An initialized *scale* is required if one needs\n        to renormalise the graph in :meth:`scale`\n        (for example, to plot it with other graphs).\n\n        Coordinates of a function graph would usually be arrays\n        of increasing values, which is not required here.\n        Neither is it checked that coordinates indeed\n        contain one-dimensional numeric values.\n        However, non-standard graphs\n        will likely lead to errors during plotting\n        and will require more programmer's work and caution,\n        so use them only if you understand what you are doing.\n\n        A graph can be iterated yielding tuples of numbers\n        for each point.\n\n        **Attributes**\n\n        :attr:`coords` is a list \\\n            of one-dimensional lists of coordinates.\n\n        :attr:`field_names`\n\n        :attr:`dim` is the dimension of the graph,\n        that is of all its coordinates without errors.\n\n        In case of incorrect initialization arguments,\n        :exc:`~.LenaTypeError` or :exc:`~.LenaValueError` is raised.\n\n        .. versionadded:: 0.5\n        \"\"\"\n        if not coords:\n            raise lena.core.LenaValueError(\n                \"coords must be a non-empty sequence \"\n                \"of coordinate sequences\"\n            )\n\n        # require coords to be of the same size\n        pt_len = len(coords[0])\n        for arr in coords[1:]:\n            if len(arr) != pt_len:\n                raise lena.core.LenaValueError(\n                    \"coords must have subsequences of equal lengths\"\n                )\n\n        # Unicode (Python 2) field names would be just bad,\n        # so we don't check for it here.\n        if isinstance(field_names, str):\n            # split(', ') won't work.\n            # From https://stackoverflow.com/a/44785447/952234:\n            # \\s stands for whitespace.\n            field_names = tuple(re.findall(r'[^,\\s]+', field_names))\n        elif not isinstance(field_names, tuple):\n            # todo: why field_names are a tuple,\n            # while coords are a list?\n            # It might be non-Pythonic to require a tuple\n            # (to prohibit a list), but it's important\n            # for comparisons and uniformity\n            raise lena.core.LenaTypeError(\n                \"field_names must be a string or a tuple\"\n            )\n\n        if len(field_names) != len(coords):\n            raise lena.core.LenaValueError(\n                \"field_names must have must have the same size as coords\"\n            )\n\n        if len(set(field_names)) != len(field_names):\n            raise lena.core.LenaValueError(\n                \"field_names contains duplicates\"\n            )\n\n        self.coords = coords\n        self._scale = scale\n\n        # field_names are better than fields,\n        # because they are unambigous (as in namedtuple).\n        self.field_names = field_names\n\n        # decided to use \"error_x_low\" (like in ROOT).\n        # Other versions were x_error (looked better than x_err),\n        # but x_err_low looked much better than x_error_low).\n        try:\n            parsed_error_names = self._parse_error_names(field_names)\n        except lena.core.LenaValueError as err:\n            raise err\n            # in Python 3\n            # raise err from None\n        self._parsed_error_names = parsed_error_names\n\n        dim = len(field_names) - len(parsed_error_names)\n        self._coord_names = field_names[:dim]\n        self.dim = dim\n\n        # todo: add subsequences of coords as attributes\n        # with field names.\n        # In case if someone wants to create a graph of another function\n        # at the same coordinates.\n        # Should a) work when we rescale the graph\n        #        b) not interfere with other fields and methods\n\n        # Probably we won't add methods __del__(n), __add__(*coords),\n        # since it might change the scale.\n\n    def __eq__(self, other):\n        \"\"\"Two graphs are equal, if and only if they have\n        equal coordinates, field names and scales.\n\n        If *other* is not a :class:`.graph`, return ``False``.\n\n        Note that floating numbers should be compared\n        approximately (using :func:`math.isclose`).\n        Therefore this comparison may give false negatives.\n        \"\"\"\n        if not isinstance(other, graph):\n            # in Python comparison between different types is allowed\n            return False\n        return (self.coords == other.coords and self._scale == other._scale\n                and self.field_names == other.field_names)\n\n    def _get_err_indices(self, coord_name):\n        \"\"\"Get error indices corresponding to a coordinate.\"\"\"\n        err_indices = []\n        dim = self.dim\n        for ind, err in enumerate(self._parsed_error_names):\n            if err[1] == coord_name:\n                err_indices.append(ind+dim)\n        return err_indices\n\n    def __iter__(self):\n        \"\"\"Iterate graph coords one by one.\"\"\"\n        for val in zip(*self.coords):\n            yield val\n\n    def __repr__(self):\n        return \"\"\"graph({}, field_names={}, scale={})\"\"\".format(\n            self.coords, self.field_names, self._scale\n        )\n\n    def scale(self, other=None):\n        \"\"\"Get or set the scale of the graph.\n\n        If *other* is ``None``, return the scale of this graph.\n\n        If a numeric *other* is provided, rescale to that value.\n        If the graph has unknown or zero scale,\n        rescaling that will raise :exc:`~.LenaValueError`.\n\n        To get meaningful results, graph's fields are used.\n        Only the last coordinate is rescaled.\n        For example, if the graph has *x* and *y* coordinates,\n        then *y* will be rescaled, and for a 3-dimensional graph\n        *z* will be rescaled.\n        All errors are rescaled together with their coordinate.\n        \"\"\"\n        # this method is called scale() for uniformity with histograms\n        # And this looks really good: explicit for computations\n        # (not a subtle graph.scale, like a constant field (which is,\n        #  however, the case in graph - but not in other structures))\n        # and easy to remember (set_scale? rescale? change_scale_to?..)\n\n        # We modify the graph in place,\n        # because that would be redundant (not optimal)\n        # to create a new graph\n        # if we only want to change the scale of the existing one.\n\n        if other is None:\n            return self._scale\n\n        if not self._scale:\n            raise lena.core.LenaValueError(\n                \"can't rescale a graph with zero or unknown scale\"\n            )\n\n        last_coord_ind = self.dim - 1\n        last_coord_name = self.field_names[last_coord_ind]\n\n        last_coord_indices = ([last_coord_ind] +\n                self._get_err_indices(last_coord_name)\n        )\n\n        # In Python 2 3/2 is 1, so we want to be safe;\n        # the downside is that integer-valued graphs\n        # will become floating, but that is doubtfully an issue.\n        # Remove when/if dropping support for Python 2.\n        rescale = float(other) / self._scale\n\n        mul = operator.mul\n        partial = functools.partial\n\n        # a version with lambda is about 50% slower:\n        # timeit.timeit('[*map(lambda val: val*2, vals)]', \\\n        #     setup='vals = list(range(45)); from operator import mul; \\\n        #     from functools import partial')\n        # 3.159\n        # same setup for\n        # timeit.timeit('[*map(partial(mul, 2), vals)]',...):\n        # 2.075\n        # \n        # [*map(...)] is very slightly faster than list(map(...)),\n        # but it's unavailable in Python 2 (and anyway less readable).\n\n        # rescale arrays of values and errors\n        for ind, arr in enumerate(self.coords):\n            if ind in last_coord_indices:\n                # Python lists are faster than arrays,\n                # https://stackoverflow.com/a/62399645/952234\n                # (because each time taking a value from an array\n                #  creates a Python object)\n                self.coords[ind] = list(map(partial(mul, rescale),\n                                            arr))\n\n        self._scale = other\n\n        # as suggested in PEP 8\n        return None\n\n    def _parse_error_names(self, field_names):\n        # field_names is a parameter for easier testing,\n        # usually object's field_names are used.\n        errors = []\n\n        # collect all error fields and check that they are\n        # strictly after other fields\n        in_error_fields = False\n        # there is at least one field\n        last_coord_ind = 0\n        for ind, field in enumerate(field_names):\n            if field.startswith(\"error_\"):\n                in_error_fields = True\n                errors.append((field, ind))\n            else:\n                last_coord_ind = ind\n                if in_error_fields:\n                    raise lena.core.LenaValueError(\n                        \"errors must go after coordinate fields\"\n                    )\n\n        coords = set(field_names[:last_coord_ind+1])\n        parsed_errors = []\n\n        for err, ind in errors:\n            err_coords = []\n            for coord in coords:\n                err_main = err[6:]  # all after \"error_\"\n                if err_main == coord or err_main.startswith(coord + \"_\"):\n                    err_coords.append(coord)\n                    err_tail = err_main[len(coord)+1:]\n            if not err_coords:\n                raise lena.core.LenaValueError(\n                    \"no coordinate corresponding to {} given\".format(err)\n                )\n            elif len(err_coords) > 1:\n                raise lena.core.LenaValueError(\n                    \"ambiguous error \" + err +\\\n                    \" corresponding to several coordinates given\"\n                )\n            # \"error\" may be redundant, but it is explicit.\n            parsed_errors.append((\"error\", err_coords[0], err_tail, ind))\n\n        return parsed_errors\n\n    def _update_context(self, context):\n        \"\"\"Update *context* with the properties of this graph.\n\n        *context.error* is appended with indices of errors.\n        Example subcontext for a graph with fields \"E,t,error_E_low\":\n        {\"error\": {\"x_low\": {\"index\": 2}}}.\n        Note that error names are called \"x\", \"y\" and \"z\"\n        (this corresponds to first three coordinates,\n        if they are present), which allows to simplify plotting.\n        Existing values are not removed\n        from *context.value* and its subcontexts.\n\n        Called on \"destruction\" of the graph (for example,\n        in :class:`.ToCSV`). By destruction we mean conversion\n        to another structure (like text) in the flow.\n        The graph object is not really destroyed in this process.\n        \"\"\"\n        # this method is private, because we encourage users to yield\n        # graphs into the flow and process them with ToCSV element\n        # (not manually).\n\n        if not self._parsed_error_names:\n            # no error fields present\n            return\n\n        dim = self.dim\n\n        xyz_coord_names = self._coord_names[:3]\n        for name, coord_name in zip([\"x\", \"y\", \"z\"], xyz_coord_names):\n            for err in self._parsed_error_names:\n                if err[1] == coord_name:\n                    error_ind = err[3]\n                    if err[2]:\n                        # add error suffix\n                        error_name = name + \"_\" + err[2]\n                    else:\n                        error_name = name\n                    lena.context.update_recursively(\n                        context,\n                        \"error.{}.index\".format(error_name),\n                        # error can correspond both to variable and\n                        # value, so we put it outside value.\n                        # \"value.error.{}.index\".format(error_name),\n                        error_ind\n                    )\n\n\n# used in deprecated Graph\ndef _rescale_value(rescale, value):\n    return rescale * lena.flow.get_data(value)\n\n\nclass Graph(object):\n    \"\"\"\n    .. deprecated:: 0.5\n       use :class:`graph`.\n       This class may be used in the future,\n       but with a changed interface.\n\n    Function at given coordinates (arbitraty dimensions).\n\n    Graph points can be set during the initialization and\n    during :meth:`fill`. It can be rescaled (producing a new :class:`Graph`).\n    A point is a tuple of *(coordinate, value)*, where both *coordinate*\n    and *value* can be tuples of numbers.\n    *Coordinate* corresponds to a point in N-dimensional space,\n    while *value* is some function's value at this point\n    (the function can take a value in M-dimensional space).\n    Coordinate and value dimensions must be the same for all points.\n\n    One can get graph points as :attr:`Graph.points` attribute.\n    They will be sorted each time before return\n    if *sort* was set to ``True``.\n    An attempt to change points\n    (use :attr:`Graph.points` on the left of '=')\n    will raise Python's :exc:`AttributeError`.\n    \"\"\"\n\n    def __init__(self, points=None, context=None, scale=None, sort=True):\n        \"\"\"*points* is an array of *(coordinate, value)* tuples.\n\n        *context* is the same as the most recent context\n        during *fill*. Use it to provide a context\n        when initializing a :class:`Graph` from existing points.\n\n        *scale* sets the scale of the graph.\n        It is used during plotting if rescaling is needed.\n\n        Graph coordinates are sorted by default.\n        This is usually needed to plot graphs of functions.\n        If you need to keep the order of insertion, set *sort* to ``False``.\n\n        By default, sorting is done using standard Python\n        lists and functions. You can disable *sort* and provide your own\n        sorting container for *points*.\n        Some implementations are compared\n        `here <http://www.grantjenks.com/docs/sortedcontainers/performance.html>`_.\n        Note that a rescaled graph uses a default list.\n\n        Note that :class:`Graph` does not reduce data.\n        All filled values will be stored in it.\n        To reduce data, use histograms.\n        \"\"\"\n        warnings.warn(\"Graph is deprecated since Lena 0.5. Use graph.\",\n                      DeprecationWarning, stacklevel=2)\n\n        self._points = points if points is not None else []\n        # todo: add some sanity checks for points\n        self._scale = scale\n        self._init_context = {\"scale\": scale}\n        if context is None:\n            self._cur_context = {}\n        elif not isinstance(context, dict):\n            raise lena.core.LenaTypeError(\n                \"context must be a dict, {} provided\".format(context)\n            )\n        else:\n            self._cur_context = context\n        self._sort = sort\n\n        # todo: probably, scale from context is not needed.\n\n        ## probably this function is not needed.\n        ## it can't be copied, graphs won't be possible to compare.\n        # *rescale_value* is a function, which can be used to scale\n        # complex graph values.\n        # It must accept a rescale parameter and the value at a data point.\n        # By default, it is multiplication of rescale and the value\n        # (which must be a number).\n        # if rescale_value is None:\n        #     self._rescale_value = _rescale_value\n        self._rescale_value = _rescale_value\n        self._update()\n\n    def fill(self, value):\n        \"\"\"Fill the graph with *value*.\n\n        *Value* can be a *(data, context)* tuple.\n        *Data* part must be a *(coordinates, value)* pair,\n        where both coordinates and value are also tuples.\n        For example, *value* can contain the principal number\n        and its precision.\n        \"\"\"\n        point, self._cur_context = lena.flow.get_data_context(value)\n        # coords, val = point\n        self._points.append(point)\n\n    def request(self):\n        \"\"\"Yield graph with context.\n\n        If *sort* was initialized ``True``, graph points will be sorted.\n        \"\"\"\n        # If flow contained *scale* it the context, it is set now.\n        self._update()\n        yield (self, self._context)\n\n    # compute method shouldn't be in this class,\n    # because it is a pure FillRequest.\n    # def compute(self):\n    #     \"\"\"Yield graph with context (as in :meth:`request`),\n    #     and :meth:`reset`.\"\"\"\n    #     self._update()\n    #     yield (self, self._context)\n    #     self.reset()\n\n    @property\n    def points(self):\n        \"\"\"Get graph points (read only).\"\"\"\n        # sort points before giving them\n        self._update()\n        return self._points\n\n    def reset(self):\n        \"\"\"Reset points to an empty list\n        and current context to an empty dict.\n        \"\"\"\n        self._points = []\n        self._cur_context = {}\n\n    def __repr__(self):\n        self._update()\n        return (\"Graph(points={}, scale={}, sort={})\"\n                .format(self._points, self._scale, self._sort))\n\n    def scale(self, other=None):\n        \"\"\"Get or set the scale.\n\n        Graph's scale comes from an external source.\n        For example, if the graph was computed from a function,\n        this may be its integral passed via context during :meth:`fill`.\n        Once the scale is set, it is stored in the graph.\n        If one attempts to use scale which was not set,\n        :exc:`.LenaAttributeError` is raised.\n\n        If *other* is None, return the scale.\n\n        If a ``float`` *other* is provided, rescale to *other*.\n        A new graph with the scale equal to *other*\n        is returned, the original one remains unchanged.\n        Note that in this case its *points* will be a simple list\n        and new graph *sort* parameter will be ``True``.\n\n        Graphs with scale equal to zero can't be rescaled. \n        Attempts to do that raise :exc:`.LenaValueError`.\n        \"\"\"\n        if other is None:\n            # return scale\n            self._update()\n            if self._scale is None:\n                raise lena.core.LenaAttributeError(\n                    \"scale must be explicitly set before using that\"\n                )\n            return self._scale\n        else:\n            # rescale from other\n            scale = self.scale()\n            if scale == 0:\n                raise lena.core.LenaValueError(\n                    \"can't rescale graph with 0 scale\"\n                )\n\n            # new_init_context = copy.deepcopy(self._init_context)\n            # new_init_context.update({\"scale\": other})\n\n            rescale = float(other) / scale\n            new_points = []\n            for coord, val in self._points:\n                # probably not needed, because tuples are immutable:\n                # make a deep copy so that new values\n                # are completely independent from old ones.\n                new_points.append((coord, self._rescale_value(rescale, val)))\n            # todo: should it inherit context?\n            # Probably yes, but watch out scale.\n            new_graph = Graph(points=new_points, scale=other,\n                              sort=self._sort)\n            return new_graph\n\n    def to_csv(self, separator=\",\", header=None):\n        \"\"\".. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n              Iterables are converted to tables.\n\n        Convert graph's points to CSV.\n\n        *separator* delimits values, the default is comma.\n\n        *header*, if not ``None``, is the first string of the output\n        (new line is added automatically).\n\n        Since a graph can be multidimensional,\n        for each point first its coordinate is converted to string\n        (separated by *separator*), then each part of its value.\n\n        To convert :class:`Graph` to CSV inside a Lena sequence,\n        use :class:`lena.output.ToCSV`.\n        \"\"\"\n        if self._sort:\n            self._update()\n\n        def unpack_pt(pt):\n            coord = pt[0]\n            value = pt[1]\n            if isinstance(coord, tuple):\n                unpacked = list(coord)\n            else:\n                unpacked = [coord]\n            if isinstance(value, tuple):\n                unpacked += list(value)\n            else:\n                unpacked.append(value)\n            return unpacked\n\n        def pt_to_str(pt, separ):\n            return separ.join([str(val) for val in unpack_pt(pt)])\n\n        if header is not None:\n            # if one needs an empty header line, they may provide \"\"\n            lines = header + \"\\n\"\n        else:\n            lines = \"\"\n        lines += \"\\n\".join([pt_to_str(pt, separator) for pt in self.points])\n\n        return lines\n\n    #     *context* will be added to graph context.\n    #     If it contains \"scale\", :meth:`scale` method will be available.\n    #     Otherwise, if \"scale\" is contained in the context\n    #     during :meth:`fill`, it will be used.\n    #     In this case it is assumed that this scale\n    #     is same for all values (only the last filled context is checked).\n    #     Context from flow takes precedence over the initialized one.\n\n    def _update(self):\n        \"\"\"Sort points if needed, update context.\"\"\"\n        # todo: probably remove this context_scale?\n        context_scale = self._cur_context.get(\"scale\")\n        if context_scale is not None:\n            # this complex check is fine with rescale,\n            # because that returns a new graph (this scale unchanged).\n            if self._scale is not None and self._scale != context_scale:\n                raise lena.core.LenaRuntimeError(\n                    \"Initialization and context scale differ, \"\n                    \"{} and {} from context {}\"\n                    .format(self._scale, context_scale, self._cur_context)\n                )\n            self._scale = context_scale\n        if self._sort:\n            self._points = sorted(self._points)\n\n        self._context = copy.deepcopy(self._cur_context)\n        self._context.update(self._init_context)\n        # why this? Not *graph.scale*?\n        self._context.update({\"scale\": self._scale})\n        # self._context.update(lena.context.make_context(self, \"_scale\"))\n\n        # todo: make this check during fill. Probably initialize self._dim\n        # with kwarg dim. (dim of coordinates or values?)\n        if self._points:\n            # check points correctness\n            points = self._points\n            def coord_dim(coord):\n                if not hasattr(coord, \"__len__\"):\n                    return 1\n                return len(coord)\n            first_coord = points[0][0]\n            dim = coord_dim(first_coord)\n            same_dim = all(coord_dim(point[0]) == dim for point in points)\n            if not same_dim:\n                raise lena.core.LenaValueError(\n                    \"coordinates tuples must have same dimension, \"\n                    \"{} given\".format(points)\n                )\n            self.dim = dim\n            self._context[\"dim\"] = self.dim\n\n    def __eq__(self, other):\n        if not isinstance(other, Graph):\n            return False\n        if self.points != other.points:\n            return False\n        if self._scale is None and other._scale is None:\n            return True\n        try:\n            result = self.scale() == other.scale()\n        except lena.core.LenaAttributeError:\n            # one scale couldn't be computed\n            return False\n        else:\n            return result\n",
            "file_path": "lena/structures/graph.py",
            "human_label": ".. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n      Iterables are converted to tables.\n\nConvert graph's points to CSV.\n\n*separator* delimits values, the default is comma.\n\n*header*, if not ``None``, is the first string of the output\n(new line is added automatically).\n\nSince a graph can be multidimensional,\nfor each point first its coordinate is converted to string\n(separated by *separator*), then each part of its value.\n\nTo convert :class:`Graph` to CSV inside a Lena sequence,\nuse :class:`lena.output.ToCSV`.",
            "level": "class_runnable",
            "lineno": "552",
            "name": "to_csv",
            "oracle_context": "{ \"apis\" : \"['isinstance', 'append', 'join', '_update', 'list', 'str']\", \"classes\" : \"[]\", \"vars\" : \"['Str', 'points', 'separ', '_sort']\" }",
            "package": "graph",
            "project": "ynikitenko/lena",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b87b7e9a0c4fa8b80b35bc",
            "all_context": "{ \"import\" : \"warnings operator functools re copy lena functools \", \"file\" : \"\", \"class\" : \"self.__repr__(self) ; self._scale ; self.coords ; self.dim ; self._get_err_indices ; self._parse_error_names(self,field_names) ; self.__eq__(self,other) ; self._parse_error_names ; self.__iter__(self) ; self._update_context(self,context) ; self._parsed_error_names ; self._get_err_indices(self,coord_name) ; self.__init__(self,coords,field_names,scale) ; self._coord_names ; self.scale(self,other) ; self.field_names ; \" }",
            "code": "    def _update_context(self, context):\n        \"\"\"Update *context* with the properties of this graph.\n\n        *context.error* is appended with indices of errors.\n        Example subcontext for a graph with fields \"E,t,error_E_low\":\n        {\"error\": {\"x_low\": {\"index\": 2}}}.\n        Note that error names are called \"x\", \"y\" and \"z\"\n        (this corresponds to first three coordinates,\n        if they are present), which allows to simplify plotting.\n        Existing values are not removed\n        from *context.value* and its subcontexts.\n\n        Called on \"destruction\" of the graph (for example,\n        in :class:`.ToCSV`). By destruction we mean conversion\n        to another structure (like text) in the flow.\n        The graph object is not really destroyed in this process.\n        \"\"\"\n        # this method is private, because we encourage users to yield\n        # graphs into the flow and process them with ToCSV element\n        # (not manually).\n\n        if not self._parsed_error_names:\n            # no error fields present\n            return\n\n        dim = self.dim\n\n        xyz_coord_names = self._coord_names[:3]\n        for name, coord_name in zip([\"x\", \"y\", \"z\"], xyz_coord_names):\n            for err in self._parsed_error_names:\n                if err[1] == coord_name:\n                    error_ind = err[3]\n                    if err[2]:\n                        # add error suffix\n                        error_name = name + \"_\" + err[2]\n                    else:\n                        error_name = name\n                    lena.context.update_recursively(\n                        context,\n                        \"error.{}.index\".format(error_name),\n                        # error can correspond both to variable and\n                        # value, so we put it outside value.\n                        # \"value.error.{}.index\".format(error_name),\n                        error_ind\n                    )\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : true, \"current_file\" : false, \"current_project\" : false, \"external\" : false }",
            "docstring": "Update *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.",
            "end_lineno": "359",
            "file_content": "\"\"\"A graph is a function at given coordinates.\"\"\"\nimport copy\nimport functools\nimport operator\nimport re\nimport warnings\n\nimport lena.core\nimport lena.context\nimport lena.flow\n\n\nclass graph():\n    \"\"\"Numeric arrays of equal size.\"\"\"\n\n    def __init__(self, coords, field_names=(\"x\", \"y\"), scale=None):\n        \"\"\"This structure generally corresponds\n        to the graph of a function\n        and represents arrays of coordinates and the function values\n        of arbitrary dimensions.\n\n        *coords* is a list of one-dimensional\n        coordinate and value sequences (usually lists).\n        There is little to no distinction between them,\n        and \"values\" can also be called \"coordinates\".\n\n        *field_names* provide the meaning of these arrays.\n        For example, a 3-dimensional graph could be distinguished\n        from a 2-dimensional graph with errors by its fields\n        (\"x\", \"y\", \"z\") versus (\"x\", \"y\", \"error_y\").\n        Field names don't affect drawing graphs:\n        for that :class:`~Variable`-s should be used.\n        Default field names,\n        provided for the most used 2-dimensional graphs,\n        are \"x\" and \"y\".\n\n        *field_names* can be a string separated by whitespace\n        and/or commas or a tuple of strings, such as (\"x\", \"y\").\n        *field_names* must have as many elements\n        as *coords* and each field name must be unique.\n        Otherwise field names are arbitrary.\n        Error fields must go after all other coordinates.\n        Name of a coordinate error is \"error\\\\_\"\n        appended by coordinate name. Further error details\n        are appended after '_'. They could be arbitrary depending\n        on the problem: \"low\", \"high\", \"low_90%_cl\", etc. Example:\n        (\"E\", \"time\", \"error_E_low\", \"error_time\").\n\n        *scale* of the graph is a kind of its norm. It could be\n        the integral of the function or its other property.\n        A scale of a normalised probability density\n        function would be one.\n        An initialized *scale* is required if one needs\n        to renormalise the graph in :meth:`scale`\n        (for example, to plot it with other graphs).\n\n        Coordinates of a function graph would usually be arrays\n        of increasing values, which is not required here.\n        Neither is it checked that coordinates indeed\n        contain one-dimensional numeric values.\n        However, non-standard graphs\n        will likely lead to errors during plotting\n        and will require more programmer's work and caution,\n        so use them only if you understand what you are doing.\n\n        A graph can be iterated yielding tuples of numbers\n        for each point.\n\n        **Attributes**\n\n        :attr:`coords` is a list \\\n            of one-dimensional lists of coordinates.\n\n        :attr:`field_names`\n\n        :attr:`dim` is the dimension of the graph,\n        that is of all its coordinates without errors.\n\n        In case of incorrect initialization arguments,\n        :exc:`~.LenaTypeError` or :exc:`~.LenaValueError` is raised.\n\n        .. versionadded:: 0.5\n        \"\"\"\n        if not coords:\n            raise lena.core.LenaValueError(\n                \"coords must be a non-empty sequence \"\n                \"of coordinate sequences\"\n            )\n\n        # require coords to be of the same size\n        pt_len = len(coords[0])\n        for arr in coords[1:]:\n            if len(arr) != pt_len:\n                raise lena.core.LenaValueError(\n                    \"coords must have subsequences of equal lengths\"\n                )\n\n        # Unicode (Python 2) field names would be just bad,\n        # so we don't check for it here.\n        if isinstance(field_names, str):\n            # split(', ') won't work.\n            # From https://stackoverflow.com/a/44785447/952234:\n            # \\s stands for whitespace.\n            field_names = tuple(re.findall(r'[^,\\s]+', field_names))\n        elif not isinstance(field_names, tuple):\n            # todo: why field_names are a tuple,\n            # while coords are a list?\n            # It might be non-Pythonic to require a tuple\n            # (to prohibit a list), but it's important\n            # for comparisons and uniformity\n            raise lena.core.LenaTypeError(\n                \"field_names must be a string or a tuple\"\n            )\n\n        if len(field_names) != len(coords):\n            raise lena.core.LenaValueError(\n                \"field_names must have must have the same size as coords\"\n            )\n\n        if len(set(field_names)) != len(field_names):\n            raise lena.core.LenaValueError(\n                \"field_names contains duplicates\"\n            )\n\n        self.coords = coords\n        self._scale = scale\n\n        # field_names are better than fields,\n        # because they are unambigous (as in namedtuple).\n        self.field_names = field_names\n\n        # decided to use \"error_x_low\" (like in ROOT).\n        # Other versions were x_error (looked better than x_err),\n        # but x_err_low looked much better than x_error_low).\n        try:\n            parsed_error_names = self._parse_error_names(field_names)\n        except lena.core.LenaValueError as err:\n            raise err\n            # in Python 3\n            # raise err from None\n        self._parsed_error_names = parsed_error_names\n\n        dim = len(field_names) - len(parsed_error_names)\n        self._coord_names = field_names[:dim]\n        self.dim = dim\n\n        # todo: add subsequences of coords as attributes\n        # with field names.\n        # In case if someone wants to create a graph of another function\n        # at the same coordinates.\n        # Should a) work when we rescale the graph\n        #        b) not interfere with other fields and methods\n\n        # Probably we won't add methods __del__(n), __add__(*coords),\n        # since it might change the scale.\n\n    def __eq__(self, other):\n        \"\"\"Two graphs are equal, if and only if they have\n        equal coordinates, field names and scales.\n\n        If *other* is not a :class:`.graph`, return ``False``.\n\n        Note that floating numbers should be compared\n        approximately (using :func:`math.isclose`).\n        Therefore this comparison may give false negatives.\n        \"\"\"\n        if not isinstance(other, graph):\n            # in Python comparison between different types is allowed\n            return False\n        return (self.coords == other.coords and self._scale == other._scale\n                and self.field_names == other.field_names)\n\n    def _get_err_indices(self, coord_name):\n        \"\"\"Get error indices corresponding to a coordinate.\"\"\"\n        err_indices = []\n        dim = self.dim\n        for ind, err in enumerate(self._parsed_error_names):\n            if err[1] == coord_name:\n                err_indices.append(ind+dim)\n        return err_indices\n\n    def __iter__(self):\n        \"\"\"Iterate graph coords one by one.\"\"\"\n        for val in zip(*self.coords):\n            yield val\n\n    def __repr__(self):\n        return \"\"\"graph({}, field_names={}, scale={})\"\"\".format(\n            self.coords, self.field_names, self._scale\n        )\n\n    def scale(self, other=None):\n        \"\"\"Get or set the scale of the graph.\n\n        If *other* is ``None``, return the scale of this graph.\n\n        If a numeric *other* is provided, rescale to that value.\n        If the graph has unknown or zero scale,\n        rescaling that will raise :exc:`~.LenaValueError`.\n\n        To get meaningful results, graph's fields are used.\n        Only the last coordinate is rescaled.\n        For example, if the graph has *x* and *y* coordinates,\n        then *y* will be rescaled, and for a 3-dimensional graph\n        *z* will be rescaled.\n        All errors are rescaled together with their coordinate.\n        \"\"\"\n        # this method is called scale() for uniformity with histograms\n        # And this looks really good: explicit for computations\n        # (not a subtle graph.scale, like a constant field (which is,\n        #  however, the case in graph - but not in other structures))\n        # and easy to remember (set_scale? rescale? change_scale_to?..)\n\n        # We modify the graph in place,\n        # because that would be redundant (not optimal)\n        # to create a new graph\n        # if we only want to change the scale of the existing one.\n\n        if other is None:\n            return self._scale\n\n        if not self._scale:\n            raise lena.core.LenaValueError(\n                \"can't rescale a graph with zero or unknown scale\"\n            )\n\n        last_coord_ind = self.dim - 1\n        last_coord_name = self.field_names[last_coord_ind]\n\n        last_coord_indices = ([last_coord_ind] +\n                self._get_err_indices(last_coord_name)\n        )\n\n        # In Python 2 3/2 is 1, so we want to be safe;\n        # the downside is that integer-valued graphs\n        # will become floating, but that is doubtfully an issue.\n        # Remove when/if dropping support for Python 2.\n        rescale = float(other) / self._scale\n\n        mul = operator.mul\n        partial = functools.partial\n\n        # a version with lambda is about 50% slower:\n        # timeit.timeit('[*map(lambda val: val*2, vals)]', \\\n        #     setup='vals = list(range(45)); from operator import mul; \\\n        #     from functools import partial')\n        # 3.159\n        # same setup for\n        # timeit.timeit('[*map(partial(mul, 2), vals)]',...):\n        # 2.075\n        # \n        # [*map(...)] is very slightly faster than list(map(...)),\n        # but it's unavailable in Python 2 (and anyway less readable).\n\n        # rescale arrays of values and errors\n        for ind, arr in enumerate(self.coords):\n            if ind in last_coord_indices:\n                # Python lists are faster than arrays,\n                # https://stackoverflow.com/a/62399645/952234\n                # (because each time taking a value from an array\n                #  creates a Python object)\n                self.coords[ind] = list(map(partial(mul, rescale),\n                                            arr))\n\n        self._scale = other\n\n        # as suggested in PEP 8\n        return None\n\n    def _parse_error_names(self, field_names):\n        # field_names is a parameter for easier testing,\n        # usually object's field_names are used.\n        errors = []\n\n        # collect all error fields and check that they are\n        # strictly after other fields\n        in_error_fields = False\n        # there is at least one field\n        last_coord_ind = 0\n        for ind, field in enumerate(field_names):\n            if field.startswith(\"error_\"):\n                in_error_fields = True\n                errors.append((field, ind))\n            else:\n                last_coord_ind = ind\n                if in_error_fields:\n                    raise lena.core.LenaValueError(\n                        \"errors must go after coordinate fields\"\n                    )\n\n        coords = set(field_names[:last_coord_ind+1])\n        parsed_errors = []\n\n        for err, ind in errors:\n            err_coords = []\n            for coord in coords:\n                err_main = err[6:]  # all after \"error_\"\n                if err_main == coord or err_main.startswith(coord + \"_\"):\n                    err_coords.append(coord)\n                    err_tail = err_main[len(coord)+1:]\n            if not err_coords:\n                raise lena.core.LenaValueError(\n                    \"no coordinate corresponding to {} given\".format(err)\n                )\n            elif len(err_coords) > 1:\n                raise lena.core.LenaValueError(\n                    \"ambiguous error \" + err +\\\n                    \" corresponding to several coordinates given\"\n                )\n            # \"error\" may be redundant, but it is explicit.\n            parsed_errors.append((\"error\", err_coords[0], err_tail, ind))\n\n        return parsed_errors\n\n    def _update_context(self, context):\n        \"\"\"Update *context* with the properties of this graph.\n\n        *context.error* is appended with indices of errors.\n        Example subcontext for a graph with fields \"E,t,error_E_low\":\n        {\"error\": {\"x_low\": {\"index\": 2}}}.\n        Note that error names are called \"x\", \"y\" and \"z\"\n        (this corresponds to first three coordinates,\n        if they are present), which allows to simplify plotting.\n        Existing values are not removed\n        from *context.value* and its subcontexts.\n\n        Called on \"destruction\" of the graph (for example,\n        in :class:`.ToCSV`). By destruction we mean conversion\n        to another structure (like text) in the flow.\n        The graph object is not really destroyed in this process.\n        \"\"\"\n        # this method is private, because we encourage users to yield\n        # graphs into the flow and process them with ToCSV element\n        # (not manually).\n\n        if not self._parsed_error_names:\n            # no error fields present\n            return\n\n        dim = self.dim\n\n        xyz_coord_names = self._coord_names[:3]\n        for name, coord_name in zip([\"x\", \"y\", \"z\"], xyz_coord_names):\n            for err in self._parsed_error_names:\n                if err[1] == coord_name:\n                    error_ind = err[3]\n                    if err[2]:\n                        # add error suffix\n                        error_name = name + \"_\" + err[2]\n                    else:\n                        error_name = name\n                    lena.context.update_recursively(\n                        context,\n                        \"error.{}.index\".format(error_name),\n                        # error can correspond both to variable and\n                        # value, so we put it outside value.\n                        # \"value.error.{}.index\".format(error_name),\n                        error_ind\n                    )\n\n\n# used in deprecated Graph\ndef _rescale_value(rescale, value):\n    return rescale * lena.flow.get_data(value)\n\n\nclass Graph(object):\n    \"\"\"\n    .. deprecated:: 0.5\n       use :class:`graph`.\n       This class may be used in the future,\n       but with a changed interface.\n\n    Function at given coordinates (arbitraty dimensions).\n\n    Graph points can be set during the initialization and\n    during :meth:`fill`. It can be rescaled (producing a new :class:`Graph`).\n    A point is a tuple of *(coordinate, value)*, where both *coordinate*\n    and *value* can be tuples of numbers.\n    *Coordinate* corresponds to a point in N-dimensional space,\n    while *value* is some function's value at this point\n    (the function can take a value in M-dimensional space).\n    Coordinate and value dimensions must be the same for all points.\n\n    One can get graph points as :attr:`Graph.points` attribute.\n    They will be sorted each time before return\n    if *sort* was set to ``True``.\n    An attempt to change points\n    (use :attr:`Graph.points` on the left of '=')\n    will raise Python's :exc:`AttributeError`.\n    \"\"\"\n\n    def __init__(self, points=None, context=None, scale=None, sort=True):\n        \"\"\"*points* is an array of *(coordinate, value)* tuples.\n\n        *context* is the same as the most recent context\n        during *fill*. Use it to provide a context\n        when initializing a :class:`Graph` from existing points.\n\n        *scale* sets the scale of the graph.\n        It is used during plotting if rescaling is needed.\n\n        Graph coordinates are sorted by default.\n        This is usually needed to plot graphs of functions.\n        If you need to keep the order of insertion, set *sort* to ``False``.\n\n        By default, sorting is done using standard Python\n        lists and functions. You can disable *sort* and provide your own\n        sorting container for *points*.\n        Some implementations are compared\n        `here <http://www.grantjenks.com/docs/sortedcontainers/performance.html>`_.\n        Note that a rescaled graph uses a default list.\n\n        Note that :class:`Graph` does not reduce data.\n        All filled values will be stored in it.\n        To reduce data, use histograms.\n        \"\"\"\n        warnings.warn(\"Graph is deprecated since Lena 0.5. Use graph.\",\n                      DeprecationWarning, stacklevel=2)\n\n        self._points = points if points is not None else []\n        # todo: add some sanity checks for points\n        self._scale = scale\n        self._init_context = {\"scale\": scale}\n        if context is None:\n            self._cur_context = {}\n        elif not isinstance(context, dict):\n            raise lena.core.LenaTypeError(\n                \"context must be a dict, {} provided\".format(context)\n            )\n        else:\n            self._cur_context = context\n        self._sort = sort\n\n        # todo: probably, scale from context is not needed.\n\n        ## probably this function is not needed.\n        ## it can't be copied, graphs won't be possible to compare.\n        # *rescale_value* is a function, which can be used to scale\n        # complex graph values.\n        # It must accept a rescale parameter and the value at a data point.\n        # By default, it is multiplication of rescale and the value\n        # (which must be a number).\n        # if rescale_value is None:\n        #     self._rescale_value = _rescale_value\n        self._rescale_value = _rescale_value\n        self._update()\n\n    def fill(self, value):\n        \"\"\"Fill the graph with *value*.\n\n        *Value* can be a *(data, context)* tuple.\n        *Data* part must be a *(coordinates, value)* pair,\n        where both coordinates and value are also tuples.\n        For example, *value* can contain the principal number\n        and its precision.\n        \"\"\"\n        point, self._cur_context = lena.flow.get_data_context(value)\n        # coords, val = point\n        self._points.append(point)\n\n    def request(self):\n        \"\"\"Yield graph with context.\n\n        If *sort* was initialized ``True``, graph points will be sorted.\n        \"\"\"\n        # If flow contained *scale* it the context, it is set now.\n        self._update()\n        yield (self, self._context)\n\n    # compute method shouldn't be in this class,\n    # because it is a pure FillRequest.\n    # def compute(self):\n    #     \"\"\"Yield graph with context (as in :meth:`request`),\n    #     and :meth:`reset`.\"\"\"\n    #     self._update()\n    #     yield (self, self._context)\n    #     self.reset()\n\n    @property\n    def points(self):\n        \"\"\"Get graph points (read only).\"\"\"\n        # sort points before giving them\n        self._update()\n        return self._points\n\n    def reset(self):\n        \"\"\"Reset points to an empty list\n        and current context to an empty dict.\n        \"\"\"\n        self._points = []\n        self._cur_context = {}\n\n    def __repr__(self):\n        self._update()\n        return (\"Graph(points={}, scale={}, sort={})\"\n                .format(self._points, self._scale, self._sort))\n\n    def scale(self, other=None):\n        \"\"\"Get or set the scale.\n\n        Graph's scale comes from an external source.\n        For example, if the graph was computed from a function,\n        this may be its integral passed via context during :meth:`fill`.\n        Once the scale is set, it is stored in the graph.\n        If one attempts to use scale which was not set,\n        :exc:`.LenaAttributeError` is raised.\n\n        If *other* is None, return the scale.\n\n        If a ``float`` *other* is provided, rescale to *other*.\n        A new graph with the scale equal to *other*\n        is returned, the original one remains unchanged.\n        Note that in this case its *points* will be a simple list\n        and new graph *sort* parameter will be ``True``.\n\n        Graphs with scale equal to zero can't be rescaled. \n        Attempts to do that raise :exc:`.LenaValueError`.\n        \"\"\"\n        if other is None:\n            # return scale\n            self._update()\n            if self._scale is None:\n                raise lena.core.LenaAttributeError(\n                    \"scale must be explicitly set before using that\"\n                )\n            return self._scale\n        else:\n            # rescale from other\n            scale = self.scale()\n            if scale == 0:\n                raise lena.core.LenaValueError(\n                    \"can't rescale graph with 0 scale\"\n                )\n\n            # new_init_context = copy.deepcopy(self._init_context)\n            # new_init_context.update({\"scale\": other})\n\n            rescale = float(other) / scale\n            new_points = []\n            for coord, val in self._points:\n                # probably not needed, because tuples are immutable:\n                # make a deep copy so that new values\n                # are completely independent from old ones.\n                new_points.append((coord, self._rescale_value(rescale, val)))\n            # todo: should it inherit context?\n            # Probably yes, but watch out scale.\n            new_graph = Graph(points=new_points, scale=other,\n                              sort=self._sort)\n            return new_graph\n\n    def to_csv(self, separator=\",\", header=None):\n        \"\"\".. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n              Iterables are converted to tables.\n\n        Convert graph's points to CSV.\n\n        *separator* delimits values, the default is comma.\n\n        *header*, if not ``None``, is the first string of the output\n        (new line is added automatically).\n\n        Since a graph can be multidimensional,\n        for each point first its coordinate is converted to string\n        (separated by *separator*), then each part of its value.\n\n        To convert :class:`Graph` to CSV inside a Lena sequence,\n        use :class:`lena.output.ToCSV`.\n        \"\"\"\n        if self._sort:\n            self._update()\n\n        def unpack_pt(pt):\n            coord = pt[0]\n            value = pt[1]\n            if isinstance(coord, tuple):\n                unpacked = list(coord)\n            else:\n                unpacked = [coord]\n            if isinstance(value, tuple):\n                unpacked += list(value)\n            else:\n                unpacked.append(value)\n            return unpacked\n\n        def pt_to_str(pt, separ):\n            return separ.join([str(val) for val in unpack_pt(pt)])\n\n        if header is not None:\n            # if one needs an empty header line, they may provide \"\"\n            lines = header + \"\\n\"\n        else:\n            lines = \"\"\n        lines += \"\\n\".join([pt_to_str(pt, separator) for pt in self.points])\n\n        return lines\n\n    #     *context* will be added to graph context.\n    #     If it contains \"scale\", :meth:`scale` method will be available.\n    #     Otherwise, if \"scale\" is contained in the context\n    #     during :meth:`fill`, it will be used.\n    #     In this case it is assumed that this scale\n    #     is same for all values (only the last filled context is checked).\n    #     Context from flow takes precedence over the initialized one.\n\n    def _update(self):\n        \"\"\"Sort points if needed, update context.\"\"\"\n        # todo: probably remove this context_scale?\n        context_scale = self._cur_context.get(\"scale\")\n        if context_scale is not None:\n            # this complex check is fine with rescale,\n            # because that returns a new graph (this scale unchanged).\n            if self._scale is not None and self._scale != context_scale:\n                raise lena.core.LenaRuntimeError(\n                    \"Initialization and context scale differ, \"\n                    \"{} and {} from context {}\"\n                    .format(self._scale, context_scale, self._cur_context)\n                )\n            self._scale = context_scale\n        if self._sort:\n            self._points = sorted(self._points)\n\n        self._context = copy.deepcopy(self._cur_context)\n        self._context.update(self._init_context)\n        # why this? Not *graph.scale*?\n        self._context.update({\"scale\": self._scale})\n        # self._context.update(lena.context.make_context(self, \"_scale\"))\n\n        # todo: make this check during fill. Probably initialize self._dim\n        # with kwarg dim. (dim of coordinates or values?)\n        if self._points:\n            # check points correctness\n            points = self._points\n            def coord_dim(coord):\n                if not hasattr(coord, \"__len__\"):\n                    return 1\n                return len(coord)\n            first_coord = points[0][0]\n            dim = coord_dim(first_coord)\n            same_dim = all(coord_dim(point[0]) == dim for point in points)\n            if not same_dim:\n                raise lena.core.LenaValueError(\n                    \"coordinates tuples must have same dimension, \"\n                    \"{} given\".format(points)\n                )\n            self.dim = dim\n            self._context[\"dim\"] = self.dim\n\n    def __eq__(self, other):\n        if not isinstance(other, Graph):\n            return False\n        if self.points != other.points:\n            return False\n        if self._scale is None and other._scale is None:\n            return True\n        try:\n            result = self.scale() == other.scale()\n        except lena.core.LenaAttributeError:\n            # one scale couldn't be computed\n            return False\n        else:\n            return result\n",
            "file_path": "lena/structures/graph.py",
            "human_label": "Update *context* with the properties of this graph.\n\n*context.error* is appended with indices of errors.\nExample subcontext for a graph with fields \"E,t,error_E_low\":\n{\"error\": {\"x_low\": {\"index\": 2}}}.\nNote that error names are called \"x\", \"y\" and \"z\"\n(this corresponds to first three coordinates,\nif they are present), which allows to simplify plotting.\nExisting values are not removed\nfrom *context.value* and its subcontexts.\n\nCalled on \"destruction\" of the graph (for example,\nin :class:`.ToCSV`). By destruction we mean conversion\nto another structure (like text) in the flow.\nThe graph object is not really destroyed in this process.",
            "level": "class_runnable",
            "lineno": "315",
            "name": "_update_context",
            "oracle_context": "{ \"apis\" : \"['zip', 'format', 'update_recursively']\", \"classes\" : \"[]\", \"vars\" : \"['Str', '_parsed_error_names', 'context', 'dim', 'lena', '_coord_names']\" }",
            "package": "graph",
            "project": "ynikitenko/lena",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b87b4f9a0c4fa8b80b3580",
            "all_context": "{ \"import\" : \"itertools operator sys functools collections re copy lena functools \", \"file\" : \"cell_to_string(cell_edges,var_context,coord_names,coord_fmt,coord_join,reverse) ; _check_edges_increasing_1d(arr) ; check_edges_increasing(edges) ; get_bin_edges(index,edges) ; get_bin_on_index(index,bins) ; get_bin_on_value_1d(val,arr) ; get_bin_on_value(arg,edges) ; get_example_bin(struct) ; hist_to_graph(hist,make_value,get_coordinate,field_names,scale) ; init_bins(edges,value,deepcopy) ; integral(bins,edges) ; iter_bins(bins) ; iter_bins_with_edges(bins,edges) ; iter_cells(hist,ranges,coord_ranges) ; make_hist_context(hist,context) ; unify_1_md(bins,edges) ; \", \"class\" : \"\" }",
            "code": "def integral(bins, edges):\n    \"\"\"Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    total = 0\n    for ind, bin_content in iter_bins(bins):\n        bin_lengths = [\n            edges[coord][i+1] - edges[coord][i]\n            for coord, i in enumerate(ind)\n        ]\n        # product\n        vol = _reduce(operator.mul, bin_lengths, 1)\n        cell_integral = vol * bin_content\n        total += cell_integral\n    return total\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : true, \"public_lib\" : true, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Compute integral (scale for a histogram).\n\n*bins* contain values, and *edges* form the mesh\nfor the integration.\nTheir format is defined in :class:`.histogram` description.",
            "end_lineno": "455",
            "file_content": "\"\"\"Functions for histograms.\n\nThese functions are used for low-level work\nwith histograms and their contents.\nThey are not needed for normal usage.\n\"\"\"\nimport collections\nimport copy\nimport itertools\nimport operator\nimport re\nimport sys\nif sys.version_info.major == 3:\n    from functools import reduce as _reduce\nelse:\n    _reduce = reduce\n\nimport lena.core\nfrom .graph import graph as _graph\n\n\nclass HistCell(collections.namedtuple(\"HistCell\", (\"edges, bin, index\"))):\n    \"\"\"A namedtuple with fields *edges, bin, index*.\"\"\"\n    # from Aaron Hall's answer https://stackoverflow.com/a/28568351/952234\n    __slots__ = ()\n\n\ndef cell_to_string(\n        cell_edges, var_context=None, coord_names=None,\n        coord_fmt=\"{}_lte_{}_lt_{}\", coord_join=\"_\", reverse=False):\n    \"\"\"Transform cell edges into a string.\n\n    *cell_edges* is a tuple of pairs *(lower bound, upper bound)*\n    for each coordinate.\n\n    *coord_names* is a list of coordinates names.\n\n    *coord_fmt* is a string,\n    which defines how to format individual coordinates.\n\n    *coord_join* is a string, which joins coordinate pairs.\n\n    If *reverse* is True, coordinates are joined in reverse order.\n    \"\"\"\n    # todo: do we really need var_context?\n    # todo: even if so, why isn't that a {}? Is that dangerous?\n    if coord_names is None:\n        if var_context is None:\n            coord_names = [\n                \"coord{}\".format(ind) for ind in range(len(cell_edges))\n            ]\n        else:\n            if \"combine\" in var_context:\n                coord_names = [var[\"name\"]\n                               for var in var_context[\"combine\"]]\n            else:\n                coord_names = [var_context[\"name\"]]\n    if len(cell_edges) != len(coord_names):\n        raise lena.core.LenaValueError(\n            \"coord_names must have same length as cell_edges, \"\n            \"{} and {} given\".format(coord_names, cell_edges)\n        )\n    coord_strings = [coord_fmt.format(edge[0], coord_names[ind], edge[1])\n                     for (ind, edge) in enumerate(cell_edges)]\n    if reverse:\n        coord_strings = reversed(coord_strings)\n    coord_str = coord_join.join(coord_strings)\n    return coord_str\n\n\ndef _check_edges_increasing_1d(arr):\n    if len(arr) <= 1:\n        raise lena.core.LenaValueError(\"size of edges should be more than one,\"\n                                       \" {} provided\".format(arr))\n    increasing = (tup[0] < tup[1] for tup in zip(arr, arr[1:]))\n    if not all(increasing):\n        raise lena.core.LenaValueError(\n            \"expected strictly increasing values, \"\n            \"{} provided\".format(arr)\n        )\n\n\ndef check_edges_increasing(edges):\n    \"\"\"Assure that multidimensional *edges* are increasing.\n\n    If length of *edges* or its subarray is less than 2\n    or if some subarray of *edges*\n    contains not strictly increasing values,\n    :exc:`.LenaValueError` is raised.\n    \"\"\"\n    if not len(edges):\n        raise lena.core.LenaValueError(\"edges must be non-empty\")\n    elif not hasattr(edges[0], '__iter__'):\n        _check_edges_increasing_1d(edges)\n        return\n    for arr in edges:\n        if len(arr) <= 1:\n            raise lena.core.LenaValueError(\n                \"size of edges should be more than one. \"\n                \"{} provided\".format(arr)\n            )\n        _check_edges_increasing_1d(arr)\n\n\ndef get_bin_edges(index, edges):\n    \"\"\"Return edges of the bin for the given *edges* of a histogram.\n\n    In one-dimensional case *index* must be an integer and a tuple\n    of *(x_low_edge, x_high_edge)* for that bin is returned.\n\n    In a multidimensional case *index* is a container of numeric indices\n    in each dimension.\n    A list of bin edges in each dimension is returned.\"\"\"\n    # todo: maybe give up this 1- and multidimensional unification\n    # and write separate functions for each case.\n    if not hasattr(edges[0], '__iter__'):\n        # 1-dimensional edges\n        if hasattr(index, '__iter__'):\n            index = index[0]\n        return (edges[index], edges[index+1])\n    # multidimensional edges\n    return [(edges[coord][i], edges[coord][i+1])\n            for coord, i in enumerate(index)]\n\n\ndef get_bin_on_index(index, bins):\n    \"\"\"Return bin corresponding to multidimensional *index*.\n\n    *index* can be a number or a list/tuple.\n    If *index* length is less than dimension of *bins*,\n    a subarray of *bins* is returned.\n\n    In case of an index error, :exc:`.LenaIndexError` is raised.\n\n    Example:\n\n    >>> from lena.structures import histogram, get_bin_on_index\n    >>> hist = histogram([0, 1], [0])\n    >>> get_bin_on_index(0, hist.bins)\n    0\n    >>> get_bin_on_index((0, 1), [[0, 1], [0, 0]])\n    1\n    >>> get_bin_on_index(0, [[0, 1], [0, 0]])\n    [0, 1]\n    \"\"\"\n    if not isinstance(index, (list, tuple)):\n        index = [index]\n    subarr = bins\n    for ind in index:\n        try:\n            subarr = subarr[ind]\n        except IndexError:\n            raise lena.core.LenaIndexError(\n                \"bad index: {}, bins = {}\".format(index, bins)\n            )\n    return subarr\n\n\ndef get_bin_on_value_1d(val, arr):\n    \"\"\"Return index for value in one-dimensional array.\n\n    *arr* must contain strictly increasing values\n    (not necessarily equidistant),\n    it is not checked.\n\n    \"Linear binary search\" is used,\n    that is our array search by default assumes\n    the array to be split on equidistant steps.\n\n    Example:\n\n    >>> from lena.structures import get_bin_on_value_1d\n    >>> arr = [0, 1, 4, 5, 7, 10]\n    >>> get_bin_on_value_1d(0, arr)\n    0\n    >>> get_bin_on_value_1d(4.5, arr)\n    2\n    >>> # upper range is excluded\n    >>> get_bin_on_value_1d(10, arr)\n    5\n    >>> # underflow\n    >>> get_bin_on_value_1d(-10, arr)\n    -1\n    \"\"\"\n    # may also use numpy.searchsorted\n    # https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.searchsorted.html\n    ind_min = 0\n    ind_max = len(arr) - 1\n    while True:\n        if ind_max - ind_min <= 1:\n            # lower bound is close\n            if val < arr[ind_min]:\n                return ind_min - 1\n            # upper bound is open\n            elif val >= arr[ind_max]:\n                return ind_max\n            else:\n                return ind_min\n        if val == arr[ind_min]:\n            return ind_min\n        if val < arr[ind_min]:\n            return ind_min - 1\n        elif val >= arr[ind_max]:\n            return ind_max\n        else:\n            shift = int(\n                (ind_max - ind_min) * (\n                    float(val - arr[ind_min]) / (arr[ind_max] - arr[ind_min])\n                ))\n            ind_guess = ind_min + shift\n\n            if ind_min == ind_guess:\n                ind_min += 1\n                continue\n            # ind_max is always more that ind_guess,\n            # because val < arr[ind_max] (see the formula for shift).\n            # This branch is not needed and can't be tested.\n            # But for the sake of numerical inaccuracies, let us keep this\n            # so that we never get into an infinite loop.\n            elif ind_max == ind_guess:\n                ind_max -= 1\n                continue\n\n            if val < arr[ind_guess]:\n                ind_max = ind_guess\n            else:\n                ind_min = ind_guess\n\n\ndef get_bin_on_value(arg, edges):\n    \"\"\"Get the bin index for *arg* in a multidimensional array *edges*.\n\n    *arg* is a 1-dimensional array of numbers\n    (or a number for 1-dimensional *edges*),\n    and corresponds to a point in N-dimensional space.\n\n    *edges* is an array of N-1 dimensional arrays (lists or tuples) of numbers.\n    Each 1-dimensional subarray consists of increasing numbers.\n\n    *arg* and *edges* must have the same length\n    (otherwise :exc:`.LenaValueError` is raised).\n    *arg* and *edges* must be iterable and support *len()*.\n\n    Return list of indices in *edges* corresponding to *arg*.\n\n    If any coordinate is out of its corresponding edge range,\n    its index will be ``-1`` for underflow\n    or ``len(edge)-1`` for overflow.\n\n    Examples:\n\n    >>> from lena.structures import get_bin_on_value\n    >>> edges = [[1, 2, 3], [1, 3.5]]\n    >>> get_bin_on_value((1.5, 2), edges)\n    [0, 0]\n    >>> get_bin_on_value((1.5, 0), edges)\n    [0, -1]\n    >>> # the upper edge is excluded\n    >>> get_bin_on_value((3, 2), edges)\n    [2, 0]\n    >>> # one-dimensional edges\n    >>> edges = [1, 2, 3]\n    >>> get_bin_on_value(2, edges)\n    [1]\n    \"\"\"\n    # arg is a one-dimensional index\n    if not isinstance(arg, (tuple, list)):\n        return [get_bin_on_value_1d(arg, edges)]\n    # arg is a multidimensional index\n    if len(arg) != len(edges):\n        raise lena.core.LenaValueError(\n            \"argument should have same dimension as edges. \"\n            \"arg = {}, edges = {}\".format(arg, edges)\n        )\n    indices = []\n    for ind, array in enumerate(edges):\n        cur_bin = get_bin_on_value_1d(arg[ind], array)\n        indices.append(cur_bin)\n    return indices\n\n\ndef get_example_bin(struct):\n    \"\"\"Return bin with zero index on each axis of the histogram bins.\n\n    For example, if the histogram is two-dimensional, return hist[0][0].\n\n    *struct* can be a :class:`.histogram`\n    or an array of bins.\n    \"\"\"\n    if isinstance(struct, lena.structures.histogram):\n        return lena.structures.get_bin_on_index([0] * struct.dim, struct.bins)\n    else:\n        bins = struct\n        while isinstance(bins, list):\n            bins = bins[0]\n        return bins\n\n\ndef hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n    By default it is bin content.\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars.\n    For example, to create a graph with errors\n    from a histogram where bins contain\n    a named tuple with fields *mean*, *mean_error* and a context\n    one could use\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n    *get_coordinate* defines what the coordinate\n    of a graph point created from a histogram bin will be.\n    It can be \"left\" (default), \"right\" and \"middle\".\n\n    *field_names* set field names of the graph. Their number\n    must be the same as the dimension of the result.\n    For a *make_value* above they would be\n    *(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n    *scale* becomes the graph's scale (unknown by default).\n    If it is ``True``, it uses the histogram scale.\n\n    *hist* must contain only numeric bins (without context)\n    or *make_value* must remove context when creating a numeric graph.\n\n    Return the resulting graph.\n    \"\"\"\n    ## Could have allowed get_coordinate to be callable\n    # (for generality), but 1) first find a use case,\n    # 2) histogram bins could be adjusted in the first place.\n    # -- don't understand 2.\n    if get_coordinate == \"left\":\n        get_coord = lambda edges: tuple(coord[0] for coord in edges)\n    elif get_coordinate == \"right\":\n        get_coord = lambda edges: tuple(coord[1] for coord in edges)\n    # *middle* between the two edges, not the *center* of the bin\n    # as a whole (because the graph corresponds to a point)\n    elif get_coordinate == \"middle\":\n        get_coord = lambda edges: tuple(0.5*(coord[0] + coord[1])\n                                        for coord in edges)\n    else:\n        raise lena.core.LenaValueError(\n            'get_coordinate must be one of \"left\", \"right\" or \"middle\"; '\n            '\"{}\" provided'.format(get_coordinate)\n        )\n\n    # todo: make_value may be bad design.\n    # Maybe allow to change the graph in the sequence.\n    # However, make_value allows not to recreate a graph\n    # or its coordinates (if that is not needed).\n\n    if isinstance(field_names, str):\n        # copied from graph.__init__\n        field_names = tuple(re.findall(r'[^,\\s]+', field_names))\n    elif not isinstance(field_names, tuple):\n        raise lena.core.LenaTypeError(\n            \"field_names must be a string or a tuple\"\n        )\n    coords = [[] for _ in field_names]\n\n    chain = itertools.chain\n\n    if scale is True:\n        scale = hist.scale()\n\n    for value, edges in iter_bins_with_edges(hist.bins, hist.edges):\n        coord = get_coord(edges)\n\n        # Since we never use contexts here, it will be optimal\n        # to ignore them completely (remove them elsewhere).\n        # bin_value = lena.flow.get_data(value)\n        bin_value = value\n\n        if make_value is None:\n            graph_value = bin_value\n        else:\n            graph_value = make_value(bin_value)\n\n        # for iteration below\n        if not hasattr(graph_value, \"__iter__\"):\n            graph_value = (graph_value,)\n\n        # add each coordinate to respective array\n        for arr, coord_ in zip(coords, chain(coord, graph_value)):\n            arr.append(coord_)\n\n    return _graph(coords, field_names=field_names, scale=scale)\n\n\ndef init_bins(edges, value=0, deepcopy=False):\n    \"\"\"Initialize cells of the form *edges* with the given *value*.\n\n    Return bins filled with copies of *value*.\n\n    *Value* must be copyable, usual numbers will suit.\n    If the value is mutable, use *deepcopy =* ``True``\n    (or the content of cells will be identical).\n\n    Examples:\n\n    >>> edges = [[0, 1], [0, 1]]\n    >>> # one cell\n    >>> init_bins(edges)\n    [[0]]\n    >>> # no need to use floats,\n    >>> # because integers will automatically be cast to floats\n    >>> # when used together\n    >>> init_bins(edges, 0.0)\n    [[0.0]]\n    >>> init_bins([[0, 1, 2], [0, 1, 2]])\n    [[0, 0], [0, 0]]\n    >>> init_bins([0, 1, 2])\n    [0, 0]\n    \"\"\"\n    nbins = len(edges) - 1\n    if not isinstance(edges[0], (list, tuple)):\n        # edges is one-dimensional\n        if deepcopy:\n            return [copy.deepcopy(value) for _ in range(nbins)]\n        else:\n            return [value] * nbins\n    for ind, arr in enumerate(edges):\n        if ind == nbins:\n            if deepcopy:\n                return [copy.deepcopy(value) for _ in range(len(arr)-1)]\n            else:\n                return list([value] * (len(arr)-1))\n        bins = []\n        for _ in range(len(arr)-1):\n            bins.append(init_bins(edges[ind+1:], value, deepcopy))\n        return bins\n\n\ndef integral(bins, edges):\n    \"\"\"Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    total = 0\n    for ind, bin_content in iter_bins(bins):\n        bin_lengths = [\n            edges[coord][i+1] - edges[coord][i]\n            for coord, i in enumerate(ind)\n        ]\n        # product\n        vol = _reduce(operator.mul, bin_lengths, 1)\n        cell_integral = vol * bin_content\n        total += cell_integral\n    return total\n\n\ndef iter_bins(bins):\n    \"\"\"Iterate on *bins*. Yield *(index, bin content)*.\n\n    Edges with higher index are iterated first\n    (that is z, then y, then x for a 3-dimensional histogram).\n    \"\"\"\n    # if not isinstance(bins, (list, tuple)):\n    if not hasattr(bins, '__iter__'):\n        # cell\n        yield ((), bins)\n    else:\n        for ind, _ in enumerate(bins):\n            for sub_ind, val in iter_bins(bins[ind]):\n                yield (((ind,) + sub_ind), val)\n\n\ndef iter_bins_with_edges(bins, edges):\n    \"\"\"Generate *(bin content, bin edges)* pairs.\n\n    Bin edges is a tuple, such that\n    its item at index i is *(lower bound, upper bound)*\n    of the bin at i-th coordinate.\n\n    Examples:\n\n    >>> from lena.math import mesh\n    >>> list(iter_bins_with_edges([0, 1, 2], edges=mesh((0, 3), 3)))\n    [(0, ((0, 1.0),)), (1, ((1.0, 2.0),)), (2, ((2.0, 3),))]\n    >>>\n    >>> # 2-dimensional histogram\n    >>> list(iter_bins_with_edges(\n    ...     bins=[[2]], edges=mesh(((0, 1), (0, 1)), (1, 1))\n    ... ))\n    [(2, ((0, 1), (0, 1)))]\n\n    .. versionadded:: 0.5\n       made public.\n    \"\"\"\n    # todo: only a list or also a tuple, an array?\n    if not isinstance(edges[0], list):\n        edges = [edges]\n    bins_sizes = [len(edge)-1 for edge in edges]\n    indices = [list(range(nbins)) for nbins in bins_sizes]\n    for index in itertools.product(*indices):\n        bin_ = lena.structures.get_bin_on_index(index, bins)\n        edges_low = []\n        edges_high = []\n        for var, var_ind in enumerate(index):\n            edges_low.append(edges[var][var_ind])\n            edges_high.append(edges[var][var_ind+1])\n        yield (bin_, tuple(zip(edges_low, edges_high)))\n\n\ndef iter_cells(hist, ranges=None, coord_ranges=None):\n    \"\"\"Iterate cells of a histogram *hist*, possibly in a subrange.\n\n    For each bin, yield a :class:`HistCell`\n    containing *bin edges, bin content* and *bin index*.\n    The order of iteration is the same as for :func:`iter_bins`.\n\n    *ranges* are the ranges of bin indices to be used\n    for each coordinate\n    (the lower value is included, the upper value is excluded).\n\n    *coord_ranges* set real coordinate ranges based on histogram edges.\n    Obviously, they can be not exactly bin edges.\n    If one of the ranges for the given coordinate\n    is outside the histogram edges,\n    then only existing histogram edges within the range are selected.\n    If the coordinate range is completely outside histogram edges,\n    nothing is yielded.\n    If a lower or upper *coord_range*\n    falls within a bin, this bin is yielded.\n    Note that if a coordinate range falls on a bin edge,\n    the number of generated bins can be unstable\n    because of limited float precision.\n\n    *ranges* and *coord_ranges* are tuples of tuples of limits\n    in corresponding dimensions. \n    For one-dimensional histogram it must be a tuple \n    containing a tuple, for example\n    *((None, None),)*.\n\n    ``None`` as an upper or lower *range* means no limit\n    (*((None, None),)* is equivalent to *((0, len(bins)),)*\n    for a 1-dimensional histogram).\n\n    If a *range* index is lower than 0 or higher than possible index,\n    :exc:`.LenaValueError` is raised.\n    If both *coord_ranges* and *ranges* are provided,\n    :exc:`.LenaTypeError` is raised.\n    \"\"\"\n    # for bin_ind, bin_ in iter_bins(hist.bins):\n    #     yield HistCell(get_bin_edges(bin_ind, hist.edges), bin_, bin_ind)\n    # if bins and edges are calculated each time, save the result now\n    bins, edges = hist.bins, hist.edges\n    # todo: hist.edges must be same\n    # for 1- and multidimensional histograms.\n    if hist.dim == 1:\n        edges = (edges,)\n\n    if coord_ranges is not None:\n        if ranges is not None:\n            raise lena.core.LenaTypeError(\n                \"only ranges or coord_ranges can be provided, not both\"\n            )\n        ranges = []\n        if not isinstance(coord_ranges[0], (tuple, list)):\n            coord_ranges = (coord_ranges, )\n        for coord, coord_range in enumerate(coord_ranges):\n            # todo: (dis?)allow None as an infinite range.\n            # todo: raise or transpose unordered coordinates?\n            # todo: change the order of function arguments.\n            lower_bin_ind = get_bin_on_value_1d(coord_range[0], edges[coord])\n            if lower_bin_ind == -1:\n                 lower_bin_ind = 0\n            upper_bin_ind = get_bin_on_value_1d(coord_range[1], edges[coord])\n            max_ind = len(edges[coord])\n            if upper_bin_ind == max_ind:\n                 upper_bin_ind -= 1\n            if lower_bin_ind >= max_ind or upper_bin_ind <= 0:\n                 # histogram edges are outside the range.\n                 return\n            ranges.append((lower_bin_ind, upper_bin_ind))\n\n    if not ranges:\n        ranges = ((None, None),) * hist.dim\n\n    real_ind_ranges = []\n    for coord, coord_range in enumerate(ranges):\n        low, up = coord_range\n        if low is None:\n            low = 0\n        else:\n            # negative indices should not be supported\n            if low < 0:\n                raise lena.core.LenaValueError(\n                    \"low must be not less than 0 if provided\"\n                )\n        max_ind = len(edges[coord]) - 1\n        if up is None:\n            up = max_ind\n        else:\n            # huge indices should not be supported as well.\n            if up > max_ind:\n                raise lena.core.LenaValueError(\n                    \"up must not be greater than len(edges)-1, if provided\"\n                )\n        real_ind_ranges.append(list(range(low, up)))\n\n    indices = list(itertools.product(*real_ind_ranges))\n    for ind in indices:\n        yield HistCell(get_bin_edges(ind, edges),\n                       get_bin_on_index(ind, bins),\n                       ind)\n\n\ndef make_hist_context(hist, context):\n    \"\"\"Update a deep copy of *context* with the context\n    of a :class:`.histogram` *hist*.\n\n    .. deprecated:: 0.5\n       histogram context is updated automatically\n       during conversion in :class:`~.output.ToCSV`.\n       Use histogram._update_context explicitly if needed.\n    \"\"\"\n    # absolutely unnecessary.\n    context = copy.deepcopy(context)\n\n    hist_context = {\n        \"histogram\": {\n            \"dim\": hist.dim,\n            \"nbins\": hist.nbins,\n            \"ranges\": hist.ranges\n        }\n    }\n    context.update(hist_context)\n    # just bad.\n    return context\n\n\ndef unify_1_md(bins, edges):\n    \"\"\"Unify 1- and multidimensional bins and edges.\n\n    Return a tuple of *(bins, edges)*.  \n    Bins and multidimensional *edges* return unchanged,\n    while one-dimensional *edges* are inserted into a list.\n    \"\"\"\n    if hasattr(edges[0], '__iter__'):\n    # if isinstance(edges[0], (list, tuple)):\n        return (bins, edges)\n    else:\n        return (bins, [edges])\n",
            "file_path": "lena/structures/hist_functions.py",
            "human_label": "Calculate the area of the overall graph.",
            "level": "file_runnable",
            "lineno": "438",
            "name": "integral",
            "oracle_context": "{ \"apis\" : \"['_reduce', 'enumerate', 'iter_bins']\", \"classes\" : \"['_reduce', 'operator']\", \"vars\" : \"['mul']\" }",
            "package": "hist_functions",
            "project": "ynikitenko/lena",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b87b199a0c4fa8b80b354e",
            "all_context": "{ \"import\" : \" \", \"file\" : \"is_fill_compute_el(obj) ; is_fill_compute_seq(seq) ; is_fill_request_el(obj) ; is_fill_request_seq(seq) ; is_run_el(obj) ; is_source(seq) ; \", \"class\" : \"\" }",
            "code": "def is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if is_source(seq):\n        return False\n    is_fcseq = False\n    if hasattr(seq, \"__iter__\"):\n        is_fcseq = any(map(is_fill_request_el, seq))\n    if is_fill_request_el(seq):\n        is_fcseq = True\n    return is_fcseq\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Test whether *seq* can be converted to a FillRequestSeq.\n\nTrue only if it is a FillRequest element\nor contains at least one such,\nand it is not a Source sequence.",
            "end_lineno": "54",
            "file_content": "\"\"\"Check whether a sequence can be converted to a Lena Sequence.\"\"\"\n# otherwise import errors arise\n# from . import source\n\n\ndef is_fill_compute_el(obj):\n    \"\"\"Object contains executable methods 'fill' and 'compute'.\"\"\"\n    return (hasattr(obj, \"fill\")\n            and hasattr(obj, \"compute\")\n            and callable(obj.fill)\n            and callable(obj.compute))\n\n\ndef is_fill_compute_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillComputeSeq.\n\n    True only if it is a FillCompute element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if is_source(seq):\n        return False\n    is_fcseq = False\n    try:\n        is_fcseq = any(map(is_fill_compute_el, seq))\n    except TypeError:\n        # seq is non-iterable\n        pass\n    if is_fill_compute_el(seq):\n        is_fcseq = True\n    return is_fcseq\n\n\ndef is_fill_request_el(obj):\n    \"\"\"Object contains executable methods 'fill' and 'request'.\"\"\"\n    return hasattr(obj, \"fill\") and hasattr(obj, \"request\") \\\n            and callable(obj.fill) and callable(obj.request)\n\n\ndef is_fill_request_seq(seq):\n    \"\"\"Test whether *seq* can be converted to a FillRequestSeq.\n\n    True only if it is a FillRequest element\n    or contains at least one such,\n    and it is not a Source sequence.\n    \"\"\"\n    if is_source(seq):\n        return False\n    is_fcseq = False\n    if hasattr(seq, \"__iter__\"):\n        is_fcseq = any(map(is_fill_request_el, seq))\n    if is_fill_request_el(seq):\n        is_fcseq = True\n    return is_fcseq\n\n\ndef is_run_el(obj):\n    \"\"\"Object contains executable method 'run'.\"\"\"\n    return hasattr(obj, \"run\") and callable(obj.run)\n\n\ndef is_source(seq):\n    \"\"\"Sequence is a Source, if and only if its type is Source.\"\"\"\n    # Otherwise lambdas would be counted as Source,\n    # but they must be converted to Sequences.\n    # Moreover: this makes Source elements explicit and visible in code.\n    from . import source\n    return isinstance(seq, source.Source)\n",
            "file_path": "lena/core/check_sequence_type.py",
            "human_label": "Check whether seq can be converted to FillRequestSeq and bool is returned.",
            "level": "file_runnable",
            "lineno": "40",
            "name": "is_fill_request_seq",
            "oracle_context": "{ \"apis\" : \"['is_fill_request_el', 'map', 'any', 'is_source', 'hasattr']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }",
            "package": "check_sequence_type",
            "project": "ynikitenko/lena",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b87af09a0c4fa8b80b34f1",
            "all_context": "{ \"import\" : \"copy lena  \", \"file\" : \"\", \"class\" : \"self.__repr__(self) ; self._scale ; self.nbins ; self.dim ; self.__eq__(self,other) ; self._update_context(self,context) ; self.scale(self,other,recompute) ; self.__init__(self,edges,bins,initial_value) ; self.edges ; self.fill(self,coord,weight) ; self.ranges ; self.scale ; self.bins ; \" }",
            "code": "    def fill(self, coord, weight=1):\n        \"\"\"Fill histogram at *coord* with the given *weight*.\n\n        Coordinates outside the histogram edges are ignored.\n        \"\"\"\n        indices = hf.get_bin_on_value(coord, self.edges)\n        subarr = self.bins\n        for ind in indices[:-1]:\n            # underflow\n            if ind < 0:\n                return\n            try:\n                subarr = subarr[ind]\n            # overflow\n            except IndexError:\n                return\n        ind = indices[-1]\n        # underflow\n        if ind < 0:\n            return\n\n        # fill\n        try:\n            subarr[ind] += weight\n        except IndexError:\n            return\n",
            "dependency": "{ \"builtin\" : false, \"standard_lib\" : false, \"public_lib\" : true, \"current_class\" : true, \"current_file\" : false, \"current_project\" : false, \"external\" : false }",
            "docstring": "Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.",
            "end_lineno": "182",
            "file_content": "\"\"\"Histogram structure *histogram* and element *Histogram*.\"\"\"\nimport copy\n\nimport lena.context\nimport lena.core\nimport lena.flow\nimport lena.math\nfrom . import hist_functions as hf\n\n\nclass histogram():\n    \"\"\"A multidimensional histogram.\n\n    Arbitrary dimension, variable bin size and weights are supported.\n    Lower bin edge is included, upper edge is excluded.\n    Underflow and overflow values are skipped.\n    Bin content can be of arbitrary type,\n    which is defined during initialization.\n\n    Examples:\n\n    >>> # a two-dimensional histogram\n    >>> hist = histogram([[0, 1, 2], [0, 1, 2]])\n    >>> hist.fill([0, 1])\n    >>> hist.bins\n    [[0, 1], [0, 0]]\n    >>> values = [[0, 0], [1, 0], [1, 1]]\n    >>> # fill the histogram with values\n    >>> for v in values:\n    ...     hist.fill(v)\n    >>> hist.bins\n    [[1, 1], [1, 1]]\n    \"\"\"\n    # Note the differences from existing packages.\n    # Numpy 1.16 (numpy.histogram): all but the last\n    # (righthand-most) bin is half-open.\n    # This histogram class has bin limits as in ROOT\n    # (but without overflow and underflow).\n\n    # Numpy: the first element of the range must be less than or equal to the second.\n    # This histogram requires strictly increasing edges.\n    # https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html\n    # https://root.cern.ch/root/htmldoc/guides/users-guide/Histograms.html#bin-numbering\n\n    def __init__(self, edges, bins=None, initial_value=0):\n        \"\"\"*edges* is a sequence of one-dimensional arrays,\n        each containing strictly increasing bin edges.\n\n        Histogram's bins by default\n        are initialized with *initial_value*.\n        It can be any object that supports addition with *weight*\n        during *fill* (but that is not necessary\n        if you don't plan to fill the histogram).\n        If the *initial_value* is compound and requires special copying,\n        create initial bins yourself (see :func:`.init_bins`).\n\n        A histogram can be created from existing *bins* and *edges*.\n        In this case a simple check of the shape of *bins* is done\n        (raising :exc:`.LenaValueError` if failed).\n\n        **Attributes**\n\n        :attr:`edges` is a list of edges on each dimension.\n        Edges mark the borders of the bin.\n        Edges along each dimension are one-dimensional lists,\n        and the multidimensional bin is the result of all intersections\n        of one-dimensional edges.\n        For example, a 3-dimensional histogram has edges of the form\n        *[x_edges, y_edges, z_edges]*,\n        and the 0th bin has borders\n        *((x[0], x[1]), (y[0], y[1]), (z[0], z[1]))*.\n\n        Index in the edges is a tuple, where a given position corresponds\n        to a dimension, and the content at that position\n        to the bin along that dimension.\n        For example, index *(0, 1, 3)* corresponds to the bin\n        with lower edges *(x[0], y[1], z[3])*.\n\n        :attr:`bins` is a list of nested lists.\n        Same index as for edges can be used to get bin content:\n        bin at *(0, 1, 3)* can be obtained as *bins[0][1][3]*.\n        Most nested arrays correspond to highest\n        (further from x) coordinates.\n        For example, for a 3-dimensional histogram bins equal to\n        *[[[1, 1], [0, 0]], [[0, 0], [0, 0]]]*\n        mean that the only filled bins are those\n        where x and y indices are 0, and z index is 0 and 1.\n\n        :attr:`dim` is the dimension of a histogram\n        (length of its *edges* for a multidimensional histogram).\n\n        If subarrays of *edges* are not increasing\n        or if any of them has length less than 2,\n        :exc:`.LenaValueError` is raised.\n\n        .. admonition:: Programmer's note\n\n            one- and multidimensional histograms\n            have different *bins* and *edges* format.\n            To be unified, 1-dimensional edges should be\n            nested in a list (like *[[1, 2, 3]]*).\n            Instead, they are simply the x-edges list,\n            because it is more intuitive and one-dimensional histograms\n            are used more often.\n            To unify the interface for bins and edges in your code,\n            use :func:`.unify_1_md` function.\n        \"\"\"\n        # todo: allow creation of *edges* from tuples\n        # (without lena.math.mesh). Allow bin_size in this case.\n        hf.check_edges_increasing(edges)\n        self.edges = edges\n        self._scale = None\n\n        if hasattr(edges[0], \"__iter__\"):\n            self.dim = len(edges)\n        else:\n            self.dim = 1\n\n        # todo: add a kwarg no_check=False to disable bins testing\n        if bins is None:\n            self.bins = hf.init_bins(self.edges, initial_value)\n        else:\n            self.bins = bins\n            # We can't make scale for an arbitrary histogram,\n            # because it may contain compound values.\n            # self._scale = self.make_scale()\n            wrong_bins_error = lena.core.LenaValueError(\n                \"bins of incorrect shape given, {}\".format(bins)\n            )\n            if self.dim == 1:\n                if len(bins) != len(edges) - 1:\n                    raise wrong_bins_error\n            else:\n                if len(bins) != len(edges[0]) - 1:\n                    raise wrong_bins_error\n        if self.dim > 1:\n            self.ranges = [(axis[0], axis[-1]) for axis in edges]\n            self.nbins =  [len(axis) - 1 for axis in edges]\n        else:\n            self.ranges = [(edges[0], edges[-1])]\n            self.nbins = [len(edges)-1]\n\n    def __eq__(self, other):\n        \"\"\"Two histograms are equal, if and only if they have\n        equal bins and equal edges.\n\n        If *other* is not a :class:`.histogram`, return ``False``.\n\n        Note that floating numbers should be compared\n        approximately (using :func:`math.isclose`).\n        \"\"\"\n        if not isinstance(other, histogram):\n            # in Python comparison between different types is allowed\n            return False\n        return self.bins == other.bins and self.edges == other.edges\n\n    def fill(self, coord, weight=1):\n        \"\"\"Fill histogram at *coord* with the given *weight*.\n\n        Coordinates outside the histogram edges are ignored.\n        \"\"\"\n        indices = hf.get_bin_on_value(coord, self.edges)\n        subarr = self.bins\n        for ind in indices[:-1]:\n            # underflow\n            if ind < 0:\n                return\n            try:\n                subarr = subarr[ind]\n            # overflow\n            except IndexError:\n                return\n        ind = indices[-1]\n        # underflow\n        if ind < 0:\n            return\n\n        # fill\n        try:\n            subarr[ind] += weight\n        except IndexError:\n            return\n\n    def __repr__(self):\n        return \"histogram({}, bins={})\".format(self.edges, self.bins)\n\n    def scale(self, other=None, recompute=False):\n        \"\"\"Compute or set scale (integral of the histogram).\n\n        If *other* is ``None``, return scale of this histogram.\n        If its scale was not computed before,\n        it is computed and stored for subsequent use\n        (unless explicitly asked to *recompute*).\n        Note that after changing (filling) the histogram\n        one must explicitly recompute the scale\n        if it was computed before.\n\n        If a float *other* is provided, rescale self to *other*.\n\n        Histograms with scale equal to zero can't be rescaled.\n        :exc:`.LenaValueError` is raised if one tries to do that.\n        \"\"\"\n        # see graph.scale comments why this is called simply \"scale\"\n        # (not set_scale, get_scale, etc.)\n        if other is None:\n            # return scale\n            if self._scale is None or recompute:\n                self._scale = hf.integral(\n                    *hf.unify_1_md(self.bins, self.edges)\n                )\n            return self._scale\n        else:\n            # rescale from other\n            scale = self.scale()\n            if scale == 0:\n                raise lena.core.LenaValueError(\n                    \"can not rescale histogram with zero scale\"\n                )\n            self.bins = lena.math.md_map(lambda binc: binc*float(other) / scale,\n                                         self.bins)\n            self._scale = other\n            return None\n\n    def _update_context(self, context):\n        \"\"\"Update *context* with the properties of this histogram.\n\n        *context.histogram* is updated with \"dim\", \"nbins\"\n        and \"ranges\" with values for this histogram.\n        If this histogram has a computed scale, it is also added\n        to the context.\n\n        Called on \"destruction\" of the histogram structure (for example,\n        in :class:`.ToCSV`). See graph._update_context for more details.\n        \"\"\"\n\n        hist_context = {\n            \"dim\": self.dim,\n            \"nbins\": self.nbins,\n            \"ranges\": self.ranges\n        }\n\n        if self._scale is not None:\n            hist_context[\"scale\"] = self._scale\n\n        lena.context.update_recursively(context, {\"histogram\": hist_context})\n\n\nclass Histogram():\n    \"\"\"An element to produce histograms.\"\"\"\n\n    def __init__(self, edges, bins=None, make_bins=None, initial_value=0):\n        \"\"\"*edges*, *bins* and *initial_value* have the same meaning\n        as during creation of a :class:`histogram`.\n\n        *make_bins* is a function without arguments\n        that creates new bins\n        (it will be called during :meth:`__init__` and :meth:`reset`).\n        *initial_value* in this case is ignored, but bin check is made.\n        If both *bins* and *make_bins* are provided,\n        :exc:`.LenaTypeError` is raised.\n        \"\"\"\n        self._hist = histogram(edges, bins)\n\n        if make_bins is not None and bins is not None:\n            raise lena.core.LenaTypeError(\n                \"either initial bins or make_bins must be provided, \"\n                \"not both: {} and {}\".format(bins, make_bins)\n            )\n\n        # may be None\n        self._initial_bins = copy.deepcopy(bins)\n\n        # todo: bins, make_bins, initial_value look redundant\n        # and may be reconsidered when really using reset().\n        if make_bins:\n            bins = make_bins()\n        self._make_bins = make_bins\n\n        self._cur_context = {}\n\n    def fill(self, value):\n        \"\"\"Fill the histogram with *value*.\n\n        *value* can be a *(data, context)* pair. \n        Values outside the histogram edges are ignored.\n        \"\"\"\n        data, self._cur_context = lena.flow.get_data_context(value)\n        self._hist.fill(data)\n        # filling with weight is only allowed in histogram structure\n        # self._hist.fill(data, weight)\n\n    def compute(self):\n        \"\"\"Yield histogram with context.\"\"\"\n        yield (self._hist, self._cur_context)\n\n    def reset(self):\n        \"\"\"Reset the histogram.\n\n        Current context is reset to an empty dict.\n        Bins are reinitialized with the *initial_value*\n        or with *make_bins()* (depending on the initialization).\n        \"\"\"\n        if self._make_bins is not None:\n            self.bins = self._make_bins()\n        elif self._initial_bins is not None:\n            self.bins = copy.deepcopy(self._initial_bins)\n        else:\n            self.bins = hf.init_bins(self.edges, self._initial_value)\n\n        self._cur_context = {}\n",
            "file_path": "lena/structures/histogram.py",
            "human_label": "Fill histogram at *coord* with the given *weight*.\n\nCoordinates outside the histogram edges are ignored.",
            "level": "class_runnable",
            "lineno": "157",
            "name": "fill",
            "oracle_context": "{ \"apis\" : \"['get_bin_on_value']\", \"classes\" : \"['hf']\", \"vars\" : \"['edges', 'bins']\" }",
            "package": "histogram",
            "project": "ynikitenko/lena",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b86aa3b4d922cb0e688d36",
            "all_context": "{ \"import\" : \"dataclasses enum typing re datetime  marshmallow dataclasses enum typing serializable datetime \", \"file\" : \"_label_key_pattern ; _label_value_pattern ; _label_key_regex ; _label_value_regex ; _get_labels_regex() ; validate_key(key) ; validate_value(value) ; _validate_labels(labels) ; _resource_name_pattern ; _resource_name_regex ; _get_resource_name_regex() ; _validate_resource_name(name) ; _validate_resource_namespace(namespace) ; resource_ref(resource) ; \", \"class\" : \"\" }",
            "code": "def _validate_labels(labels):\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    errors = []\n    for key, value in labels.items():\n        try:\n            validate_key(key)\n        except (ValidationError, TypeError) as err:\n            errors.append({str(key): str(err)})\n\n        try:\n            validate_value(value)\n        except (ValidationError, TypeError) as err:\n            errors.append({str(value): str(err)})\n\n    if errors:\n        raise ValidationError(list(errors))\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : false, \"public_lib\" : true, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Check that keys and values in the given labels match against their corresponding\nregular expressions.\n\nArgs:\n    labels (dict): the different labels to validate.\n\nRaises:\n    ValidationError: if any of the keys and labels does not match their respective\n        regular expression. The error contains as message the list of all errors\n        which occurred in the labels. Each element of the list is a dictionary with\n        one key-value pair:\n        - key: the label key or label value for which an error occurred as string.\n        - value: the error message.\n\n        .. code:: python\n\n            # Example:\n            labels = {\n                \"key1\": \"valid\",\n                \"key2\": [\"invalid\"],\n                \"$$\": \"invalid\",\n                True: True,\n            }\n            try:\n                _validate_labels(labels)\n            except ValidationError as err:\n                assert err.messages == [\n                    {\"['invalid']\": 'expected string or bytes-like object'},\n                    {'$$': \"Label key '$$' does not match the regex [...]\"},\n                    {'True': 'expected string or bytes-like object'},\n                    {'True': 'expected string or bytes-like object'},\n                ]",
            "end_lineno": "163",
            "file_content": "import re\nfrom enum import Enum, IntEnum, auto\nfrom datetime import datetime\nfrom dataclasses import field\nfrom typing import List, Dict\n\nfrom marshmallow import ValidationError\n\nfrom . import persistent\nfrom .serializable import Serializable, ApiObject, PolymorphicContainer\n\n\nclass ResourceRef(Serializable):\n    api: str\n    namespace: str = field(default=None)\n    kind: str\n    name: str\n\n    def __hash__(self):\n        return hash((self.api, self.namespace, self.kind, self.name))\n\n    def __repr__(self):\n        message = f\"{self.kind}(api='{self.api}', \"\n        return message + f\"namespace='{self.namespace}', name='{self.name}')\"\n\n\n_label_key_pattern = None\n_label_value_pattern = None\n\n_label_key_regex = None\n_label_value_regex = None\n\n\ndef _get_labels_regex():\n    \"\"\"Build or return the regular expressions that are used to validate the key and\n    value of the labels of the Krake resources.\n\n    The first call builds the expressions, while a second returns the already built\n    ones.\n\n    Returns:\n        (re.Pattern, re.Pattern): a tuple that contains the compiled regular,\n            expressions, the first element to validate the key and the second to\n            validate the value.\n\n    \"\"\"\n    global _label_key_pattern, _label_value_pattern\n    global _label_key_regex, _label_value_regex\n\n    if _label_key_regex and _label_value_regex:\n        return _label_key_regex, _label_value_regex\n\n    # Build the patterns only if not already built\n    max_prefix_size = 253\n    max_key_size = 63\n    max_value_size = max_key_size\n\n    # First and last characters must be alphanumeric. The rest of the string must be\n    # alphanumeric, \"-\", \"_\" or \".\"\n    base_alphanumeric_pattern = \"\\\\w|(\\\\w[\\\\w\\\\-_.]{{0,{length}}}\\\\w)\"\n\n    key_pattern = base_alphanumeric_pattern.format(length=max_key_size - 2)\n    value_pattern = base_alphanumeric_pattern.format(length=max_value_size - 2)\n    prefix_pattern = base_alphanumeric_pattern.format(length=max_prefix_size - 2)\n\n    # The key can be a string of length 63 with the specifications described above,\n    # or have a prefix, then one \"/\" character, then the string of length 63 (called\n    # name).\n    # The prefix itself should have a max length of 253, but otherwise follows the\n    # specifications described above.\n    _label_key_pattern = f\"^(({prefix_pattern})\\\\/)?({key_pattern})$\"\n\n    # The value can be a string of length 63 with the specifications described\n    # above.\n    _label_value_pattern = value_pattern\n\n    _label_key_regex = re.compile(_label_key_pattern, re.ASCII)\n    _label_value_regex = re.compile(_label_value_pattern, re.ASCII)\n\n    return _label_key_regex, _label_value_regex\n\n\ndef validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    key_regex, _ = _get_labels_regex()\n    if not key_regex.fullmatch(key):\n        raise ValidationError(\n            f\"Label key {key!r} does not match the regex {_label_key_pattern!r}.\"\n        )\n\n\ndef validate_value(value):\n    \"\"\"Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    _, value_regex = _get_labels_regex()\n    if not value_regex.fullmatch(value):\n        raise ValidationError(\n            f\"Label value {value!r} does not match\"\n            f\" the regex {_label_value_pattern!r}.\"\n        )\n\n\ndef _validate_labels(labels):\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    errors = []\n    for key, value in labels.items():\n        try:\n            validate_key(key)\n        except (ValidationError, TypeError) as err:\n            errors.append({str(key): str(err)})\n\n        try:\n            validate_value(value)\n        except (ValidationError, TypeError) as err:\n            errors.append({str(value): str(err)})\n\n    if errors:\n        raise ValidationError(list(errors))\n\n\n_resource_name_pattern = None\n_resource_name_regex = None\n\n\ndef _get_resource_name_regex():\n    \"\"\"Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    global _resource_name_regex, _resource_name_pattern\n\n    # Build the patterns only if not already built\n    if _resource_name_regex:\n        return _resource_name_regex\n\n    # First and last characters must be alphanumeric. The rest of the string must be\n    # alphanumeric, \"-\", \"_\" or \".\" and without whitespace as well as have a\n    # max length of 255 and a min length of 1\n    max_name_size = 253  # reduced by 2 for the regex\n    min_name_size = 0  # reduced by 1 for the regex\n    base_alphanumeric_pattern = \"\\\\w|(\\\\w[\\\\w\\\\-_.:]{{{min_length},{length}}}\\\\w)\"\n\n    resource_name_pattern = base_alphanumeric_pattern.format(\n        min_length=min_name_size, length=max_name_size\n    )\n\n    _resource_name_pattern = resource_name_pattern\n    _resource_name_regex = re.compile(_resource_name_pattern, re.ASCII)\n    return _resource_name_regex\n\n\ndef _validate_resource_name(name):\n    \"\"\"Each Krake resource name is checked against a specific pattern.\n    Which characters are not allowed is defined in _get_resource_name_regex\n\n    Args:\n        name(str): the different resource names to validate.\n\n    Raises:\n        ValidationError: if any resource name does not match their respective\n            regular expression.\n    \"\"\"\n    resource_name_regex = _get_resource_name_regex()\n    if not resource_name_regex.fullmatch(name):\n        raise ValidationError(\"Invalid character in resource name.\")\n\n\ndef _validate_resource_namespace(namespace):\n    \"\"\"Each Krake resource namespace is checked against a specific pattern.\n    Which characters are not allowed is defined in _get_resource_name_regex\n\n    Args:\n        namespace(str): the different resource namespaces to validate.\n\n    Raises:\n        ValidationError: if any resource namespace does not match their respective\n            regular expression.\n    \"\"\"\n    resource_namespace_regex = _get_resource_name_regex()\n    if not resource_namespace_regex.fullmatch(namespace):\n        raise ValidationError(\"Invalid character in resource namespace.\")\n\n\nclass Metadata(Serializable):\n    name: str = field(metadata={\"immutable\": True, \"validate\": _validate_resource_name})\n    namespace: str = field(\n        default=None,\n        metadata={\"immutable\": True, \"validate\": _validate_resource_namespace},\n    )\n    labels: dict = field(default_factory=dict, metadata={\"validate\": _validate_labels})\n    finalizers: List[str] = field(default_factory=list)\n\n    uid: str = field(metadata={\"readonly\": True})\n    created: datetime = field(metadata={\"readonly\": True})\n    modified: datetime = field(metadata={\"readonly\": True})\n    deleted: datetime = field(default=None, metadata={\"readonly\": True})\n\n    owners: List[ResourceRef] = field(default_factory=list)\n\n\nclass CoreMetadata(Serializable):\n    name: str\n    uid: str\n\n\nclass ListMetadata(Serializable):\n    pass  # TODO\n\n\nclass ReasonCode(IntEnum):\n    INTERNAL_ERROR = 1  # Default error\n\n    INVALID_RESOURCE = 10  # Invalid values in the Manifest\n    # Kubernetes' resource is not supported by the Kubernetes controller\n    UNSUPPORTED_RESOURCE = 11\n    # The custom resource provided does not exist or is invalid\n    INVALID_CUSTOM_RESOURCE = 12\n\n    CLUSTER_NOT_REACHABLE = 20  # Connectivity issue with the Kubernetes deployment\n    NO_SUITABLE_RESOURCE = 50  # Scheduler issue\n\n    KUBERNETES_ERROR = 60\n\n    CREATE_FAILED = 70\n    RECONCILE_FAILED = 71\n    DELETE_FAILED = 72\n\n    OPENSTACK_ERROR = 80\n    INVALID_CLUSTER_TEMPLATE = 81\n\n    # Related to Metrics and Metric Provider\n    INVALID_METRIC = 91\n    UNREACHABLE_METRICS_PROVIDER = 92\n    UNKNOWN_METRIC = 93\n    UNKNOWN_METRICS_PROVIDER = 94\n\n\nclass Reason(Serializable):\n    code: ReasonCode\n    message: str\n\n\nclass WatchEventType(Enum):\n    ADDED = auto()\n    MODIFIED = auto()\n    DELETED = auto()\n\n\nclass Status(Serializable):\n    reason: Reason = None\n\n\nclass WatchEvent(Serializable):\n    type: WatchEventType\n    object: dict\n\n\nclass Verb(Enum):\n    create = auto()\n    list = auto()\n    list_all = auto()\n    get = auto()\n    update = auto()\n    delete = auto()\n\n\nclass RoleRule(Serializable):\n    api: str\n    resources: List[str]\n    namespaces: List[str]\n    verbs: List[Verb]\n\n\n@persistent(\"/core/roles/{name}\")\nclass Role(ApiObject):\n    api: str = \"core\"\n    kind: str = \"Role\"\n    metadata: Metadata\n    rules: List[RoleRule]\n\n\nclass RoleList(ApiObject):\n    api: str = \"core\"\n    kind: str = \"RoleList\"\n    metadata: ListMetadata\n    items: List[Role]\n\n\n@persistent(\"/core/rolebindings/{name}\")\nclass RoleBinding(ApiObject):\n    api: str = \"core\"\n    kind: str = \"RoleBinding\"\n    metadata: Metadata\n    users: List[str]\n    roles: List[str]\n\n\nclass RoleBindingList(ApiObject):\n    api: str = \"core\"\n    kind: str = \"RoleBindingList\"\n    metadata: ListMetadata\n    items: List[RoleBinding]\n\n\nclass Conflict(Serializable):\n    source: ResourceRef\n    conflicting: List[ResourceRef]\n\n\ndef resource_ref(resource):\n    \"\"\"Create a :class:`ResourceRef` from a :class:`ApiObject`\n\n    Args:\n        resource (.serializable.ApiObject): API object that should be\n            referenced\n\n    Returns:\n        ResourceRef: Corresponding reference to the API object\n    \"\"\"\n    return ResourceRef(\n        api=resource.api,\n        kind=resource.kind,\n        namespace=resource.metadata.namespace,\n        name=resource.metadata.name,\n    )\n\n\nclass MetricSpecProvider(Serializable):\n    name: str\n    metric: str\n\n\nclass MetricSpec(Serializable):\n    min: float\n    max: float\n    provider: MetricSpecProvider\n\n\nclass BaseMetric(ApiObject):\n    api: str = \"core\"\n    kind: str = None\n    metadata: Metadata\n    spec: MetricSpec\n\n\n@persistent(\"/core/globalmetrics/{name}\")\nclass GlobalMetric(BaseMetric):\n    api: str = \"core\"\n    kind: str = \"GlobalMetric\"\n    metadata: Metadata\n    spec: MetricSpec\n\n\n@persistent(\"/core/metrics/{namespace}/{name}\")\nclass Metric(BaseMetric):\n    api: str = \"core\"\n    kind: str = \"Metric\"\n    metadata: Metadata\n    spec: MetricSpec\n\n\nclass MetricList(ApiObject):\n    api: str = \"core\"\n    kind: str = \"MetricList\"\n    metadata: ListMetadata\n    items: List[Metric]\n\n\nclass GlobalMetricList(ApiObject):\n    api: str = \"core\"\n    kind: str = \"GlobalMetricList\"\n    metadata: ListMetadata\n    items: List[GlobalMetric]\n\n\nclass MetricsProviderSpec(PolymorphicContainer):\n    type: str\n\n\n@MetricsProviderSpec.register(\"prometheus\")\nclass PrometheusSpec(Serializable):\n    url: str\n\n\n@MetricsProviderSpec.register(\"kafka\")\nclass KafkaSpec(Serializable):\n    \"\"\"Specifications to connect to a KSQL database, and retrieve a specific row from a\n    specific table.\n\n    Attributes:\n        comparison_column (str): name of the column where the value will be compared to\n            the metric name, to select the right metric.\n        value_column (str): name of the column where the value of a metric is stored.\n        table (str): the name of the KSQL table where the metric is defined.\n        url (str): endpoint of the KSQL database.\n\n    \"\"\"\n\n    comparison_column: str\n    value_column: str\n    table: str\n    url: str\n\n\n@MetricsProviderSpec.register(\"static\")\nclass StaticSpec(Serializable):\n    metrics: Dict[str, float]\n\n\nclass BaseMetricsProvider(ApiObject):\n    api: str = \"core\"\n    kind: str = None\n    metadata: Metadata\n    spec: MetricsProviderSpec\n\n\n@persistent(\"/core/globalmetricsproviders/{name}\")\nclass GlobalMetricsProvider(BaseMetricsProvider):\n    api: str = \"core\"\n    kind: str = \"GlobalMetricsProvider\"\n    metadata: Metadata\n    spec: MetricsProviderSpec\n\n\n@persistent(\"/core/metricsproviders/{namespace}/{name}\")\nclass MetricsProvider(BaseMetricsProvider):\n    api: str = \"core\"\n    kind: str = \"MetricsProvider\"\n    metadata: Metadata\n    spec: MetricsProviderSpec\n\n\nclass MetricsProviderList(ApiObject):\n    api: str = \"core\"\n    kind: str = \"MetricsProviderList\"\n    metadata: ListMetadata\n    items: List[MetricsProvider]\n\n\nclass GlobalMetricsProviderList(ApiObject):\n    api: str = \"core\"\n    kind: str = \"GlobalMetricsProviderList\"\n    metadata: ListMetadata\n    items: List[GlobalMetricsProvider]\n\n\nclass MetricRef(Serializable):\n    name: str\n    weight: float\n    namespaced: bool = False\n",
            "file_path": "krake/krake/data/core.py",
            "human_label": "Check that keys and values in the given labels by validate_key() and validate_value().",
            "level": "file_runnable",
            "lineno": "116",
            "name": "_validate_labels",
            "oracle_context": "{ \"apis\" : \"['items', 'append', 'validate_value', 'validate_key', 'list', 'str']\", \"classes\" : \"['ValidationError']\", \"vars\" : \"[]\" }",
            "package": "core",
            "project": "rak-n-rok/Krake",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b86a4fb4d922cb0e688cf8",
            "all_context": "{ \"import\" : \"dataclasses enum typing re datetime  marshmallow dataclasses enum typing serializable datetime \", \"file\" : \"_label_key_pattern ; _label_value_pattern ; _label_key_regex ; _label_value_regex ; _get_labels_regex() ; validate_key(key) ; validate_value(value) ; _validate_labels(labels) ; _resource_name_pattern ; _resource_name_regex ; _get_resource_name_regex() ; _validate_resource_name(name) ; _validate_resource_namespace(namespace) ; resource_ref(resource) ; \", \"class\" : \"\" }",
            "code": "def validate_value(value):\n    \"\"\"Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    _, value_regex = _get_labels_regex()\n    if not value_regex.fullmatch(value):\n        raise ValidationError(\n            f\"Label value {value!r} does not match\"\n            f\" the regex {_label_value_pattern!r}.\"\n        )\n",
            "dependency": "{ \"builtin\" : false, \"standard_lib\" : false, \"public_lib\" : true, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Validate the given value against the corresponding regular expression.\n\nArgs:\n    value: the string to validate\n\nRaises:\n    ValidationError: if the given value is not conform to the regular expression.",
            "end_lineno": "113",
            "file_content": "import re\nfrom enum import Enum, IntEnum, auto\nfrom datetime import datetime\nfrom dataclasses import field\nfrom typing import List, Dict\n\nfrom marshmallow import ValidationError\n\nfrom . import persistent\nfrom .serializable import Serializable, ApiObject, PolymorphicContainer\n\n\nclass ResourceRef(Serializable):\n    api: str\n    namespace: str = field(default=None)\n    kind: str\n    name: str\n\n    def __hash__(self):\n        return hash((self.api, self.namespace, self.kind, self.name))\n\n    def __repr__(self):\n        message = f\"{self.kind}(api='{self.api}', \"\n        return message + f\"namespace='{self.namespace}', name='{self.name}')\"\n\n\n_label_key_pattern = None\n_label_value_pattern = None\n\n_label_key_regex = None\n_label_value_regex = None\n\n\ndef _get_labels_regex():\n    \"\"\"Build or return the regular expressions that are used to validate the key and\n    value of the labels of the Krake resources.\n\n    The first call builds the expressions, while a second returns the already built\n    ones.\n\n    Returns:\n        (re.Pattern, re.Pattern): a tuple that contains the compiled regular,\n            expressions, the first element to validate the key and the second to\n            validate the value.\n\n    \"\"\"\n    global _label_key_pattern, _label_value_pattern\n    global _label_key_regex, _label_value_regex\n\n    if _label_key_regex and _label_value_regex:\n        return _label_key_regex, _label_value_regex\n\n    # Build the patterns only if not already built\n    max_prefix_size = 253\n    max_key_size = 63\n    max_value_size = max_key_size\n\n    # First and last characters must be alphanumeric. The rest of the string must be\n    # alphanumeric, \"-\", \"_\" or \".\"\n    base_alphanumeric_pattern = \"\\\\w|(\\\\w[\\\\w\\\\-_.]{{0,{length}}}\\\\w)\"\n\n    key_pattern = base_alphanumeric_pattern.format(length=max_key_size - 2)\n    value_pattern = base_alphanumeric_pattern.format(length=max_value_size - 2)\n    prefix_pattern = base_alphanumeric_pattern.format(length=max_prefix_size - 2)\n\n    # The key can be a string of length 63 with the specifications described above,\n    # or have a prefix, then one \"/\" character, then the string of length 63 (called\n    # name).\n    # The prefix itself should have a max length of 253, but otherwise follows the\n    # specifications described above.\n    _label_key_pattern = f\"^(({prefix_pattern})\\\\/)?({key_pattern})$\"\n\n    # The value can be a string of length 63 with the specifications described\n    # above.\n    _label_value_pattern = value_pattern\n\n    _label_key_regex = re.compile(_label_key_pattern, re.ASCII)\n    _label_value_regex = re.compile(_label_value_pattern, re.ASCII)\n\n    return _label_key_regex, _label_value_regex\n\n\ndef validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    key_regex, _ = _get_labels_regex()\n    if not key_regex.fullmatch(key):\n        raise ValidationError(\n            f\"Label key {key!r} does not match the regex {_label_key_pattern!r}.\"\n        )\n\n\ndef validate_value(value):\n    \"\"\"Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    _, value_regex = _get_labels_regex()\n    if not value_regex.fullmatch(value):\n        raise ValidationError(\n            f\"Label value {value!r} does not match\"\n            f\" the regex {_label_value_pattern!r}.\"\n        )\n\n\ndef _validate_labels(labels):\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    errors = []\n    for key, value in labels.items():\n        try:\n            validate_key(key)\n        except (ValidationError, TypeError) as err:\n            errors.append({str(key): str(err)})\n\n        try:\n            validate_value(value)\n        except (ValidationError, TypeError) as err:\n            errors.append({str(value): str(err)})\n\n    if errors:\n        raise ValidationError(list(errors))\n\n\n_resource_name_pattern = None\n_resource_name_regex = None\n\n\ndef _get_resource_name_regex():\n    \"\"\"Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    global _resource_name_regex, _resource_name_pattern\n\n    # Build the patterns only if not already built\n    if _resource_name_regex:\n        return _resource_name_regex\n\n    # First and last characters must be alphanumeric. The rest of the string must be\n    # alphanumeric, \"-\", \"_\" or \".\" and without whitespace as well as have a\n    # max length of 255 and a min length of 1\n    max_name_size = 253  # reduced by 2 for the regex\n    min_name_size = 0  # reduced by 1 for the regex\n    base_alphanumeric_pattern = \"\\\\w|(\\\\w[\\\\w\\\\-_.:]{{{min_length},{length}}}\\\\w)\"\n\n    resource_name_pattern = base_alphanumeric_pattern.format(\n        min_length=min_name_size, length=max_name_size\n    )\n\n    _resource_name_pattern = resource_name_pattern\n    _resource_name_regex = re.compile(_resource_name_pattern, re.ASCII)\n    return _resource_name_regex\n\n\ndef _validate_resource_name(name):\n    \"\"\"Each Krake resource name is checked against a specific pattern.\n    Which characters are not allowed is defined in _get_resource_name_regex\n\n    Args:\n        name(str): the different resource names to validate.\n\n    Raises:\n        ValidationError: if any resource name does not match their respective\n            regular expression.\n    \"\"\"\n    resource_name_regex = _get_resource_name_regex()\n    if not resource_name_regex.fullmatch(name):\n        raise ValidationError(\"Invalid character in resource name.\")\n\n\ndef _validate_resource_namespace(namespace):\n    \"\"\"Each Krake resource namespace is checked against a specific pattern.\n    Which characters are not allowed is defined in _get_resource_name_regex\n\n    Args:\n        namespace(str): the different resource namespaces to validate.\n\n    Raises:\n        ValidationError: if any resource namespace does not match their respective\n            regular expression.\n    \"\"\"\n    resource_namespace_regex = _get_resource_name_regex()\n    if not resource_namespace_regex.fullmatch(namespace):\n        raise ValidationError(\"Invalid character in resource namespace.\")\n\n\nclass Metadata(Serializable):\n    name: str = field(metadata={\"immutable\": True, \"validate\": _validate_resource_name})\n    namespace: str = field(\n        default=None,\n        metadata={\"immutable\": True, \"validate\": _validate_resource_namespace},\n    )\n    labels: dict = field(default_factory=dict, metadata={\"validate\": _validate_labels})\n    finalizers: List[str] = field(default_factory=list)\n\n    uid: str = field(metadata={\"readonly\": True})\n    created: datetime = field(metadata={\"readonly\": True})\n    modified: datetime = field(metadata={\"readonly\": True})\n    deleted: datetime = field(default=None, metadata={\"readonly\": True})\n\n    owners: List[ResourceRef] = field(default_factory=list)\n\n\nclass CoreMetadata(Serializable):\n    name: str\n    uid: str\n\n\nclass ListMetadata(Serializable):\n    pass  # TODO\n\n\nclass ReasonCode(IntEnum):\n    INTERNAL_ERROR = 1  # Default error\n\n    INVALID_RESOURCE = 10  # Invalid values in the Manifest\n    # Kubernetes' resource is not supported by the Kubernetes controller\n    UNSUPPORTED_RESOURCE = 11\n    # The custom resource provided does not exist or is invalid\n    INVALID_CUSTOM_RESOURCE = 12\n\n    CLUSTER_NOT_REACHABLE = 20  # Connectivity issue with the Kubernetes deployment\n    NO_SUITABLE_RESOURCE = 50  # Scheduler issue\n\n    KUBERNETES_ERROR = 60\n\n    CREATE_FAILED = 70\n    RECONCILE_FAILED = 71\n    DELETE_FAILED = 72\n\n    OPENSTACK_ERROR = 80\n    INVALID_CLUSTER_TEMPLATE = 81\n\n    # Related to Metrics and Metric Provider\n    INVALID_METRIC = 91\n    UNREACHABLE_METRICS_PROVIDER = 92\n    UNKNOWN_METRIC = 93\n    UNKNOWN_METRICS_PROVIDER = 94\n\n\nclass Reason(Serializable):\n    code: ReasonCode\n    message: str\n\n\nclass WatchEventType(Enum):\n    ADDED = auto()\n    MODIFIED = auto()\n    DELETED = auto()\n\n\nclass Status(Serializable):\n    reason: Reason = None\n\n\nclass WatchEvent(Serializable):\n    type: WatchEventType\n    object: dict\n\n\nclass Verb(Enum):\n    create = auto()\n    list = auto()\n    list_all = auto()\n    get = auto()\n    update = auto()\n    delete = auto()\n\n\nclass RoleRule(Serializable):\n    api: str\n    resources: List[str]\n    namespaces: List[str]\n    verbs: List[Verb]\n\n\n@persistent(\"/core/roles/{name}\")\nclass Role(ApiObject):\n    api: str = \"core\"\n    kind: str = \"Role\"\n    metadata: Metadata\n    rules: List[RoleRule]\n\n\nclass RoleList(ApiObject):\n    api: str = \"core\"\n    kind: str = \"RoleList\"\n    metadata: ListMetadata\n    items: List[Role]\n\n\n@persistent(\"/core/rolebindings/{name}\")\nclass RoleBinding(ApiObject):\n    api: str = \"core\"\n    kind: str = \"RoleBinding\"\n    metadata: Metadata\n    users: List[str]\n    roles: List[str]\n\n\nclass RoleBindingList(ApiObject):\n    api: str = \"core\"\n    kind: str = \"RoleBindingList\"\n    metadata: ListMetadata\n    items: List[RoleBinding]\n\n\nclass Conflict(Serializable):\n    source: ResourceRef\n    conflicting: List[ResourceRef]\n\n\ndef resource_ref(resource):\n    \"\"\"Create a :class:`ResourceRef` from a :class:`ApiObject`\n\n    Args:\n        resource (.serializable.ApiObject): API object that should be\n            referenced\n\n    Returns:\n        ResourceRef: Corresponding reference to the API object\n    \"\"\"\n    return ResourceRef(\n        api=resource.api,\n        kind=resource.kind,\n        namespace=resource.metadata.namespace,\n        name=resource.metadata.name,\n    )\n\n\nclass MetricSpecProvider(Serializable):\n    name: str\n    metric: str\n\n\nclass MetricSpec(Serializable):\n    min: float\n    max: float\n    provider: MetricSpecProvider\n\n\nclass BaseMetric(ApiObject):\n    api: str = \"core\"\n    kind: str = None\n    metadata: Metadata\n    spec: MetricSpec\n\n\n@persistent(\"/core/globalmetrics/{name}\")\nclass GlobalMetric(BaseMetric):\n    api: str = \"core\"\n    kind: str = \"GlobalMetric\"\n    metadata: Metadata\n    spec: MetricSpec\n\n\n@persistent(\"/core/metrics/{namespace}/{name}\")\nclass Metric(BaseMetric):\n    api: str = \"core\"\n    kind: str = \"Metric\"\n    metadata: Metadata\n    spec: MetricSpec\n\n\nclass MetricList(ApiObject):\n    api: str = \"core\"\n    kind: str = \"MetricList\"\n    metadata: ListMetadata\n    items: List[Metric]\n\n\nclass GlobalMetricList(ApiObject):\n    api: str = \"core\"\n    kind: str = \"GlobalMetricList\"\n    metadata: ListMetadata\n    items: List[GlobalMetric]\n\n\nclass MetricsProviderSpec(PolymorphicContainer):\n    type: str\n\n\n@MetricsProviderSpec.register(\"prometheus\")\nclass PrometheusSpec(Serializable):\n    url: str\n\n\n@MetricsProviderSpec.register(\"kafka\")\nclass KafkaSpec(Serializable):\n    \"\"\"Specifications to connect to a KSQL database, and retrieve a specific row from a\n    specific table.\n\n    Attributes:\n        comparison_column (str): name of the column where the value will be compared to\n            the metric name, to select the right metric.\n        value_column (str): name of the column where the value of a metric is stored.\n        table (str): the name of the KSQL table where the metric is defined.\n        url (str): endpoint of the KSQL database.\n\n    \"\"\"\n\n    comparison_column: str\n    value_column: str\n    table: str\n    url: str\n\n\n@MetricsProviderSpec.register(\"static\")\nclass StaticSpec(Serializable):\n    metrics: Dict[str, float]\n\n\nclass BaseMetricsProvider(ApiObject):\n    api: str = \"core\"\n    kind: str = None\n    metadata: Metadata\n    spec: MetricsProviderSpec\n\n\n@persistent(\"/core/globalmetricsproviders/{name}\")\nclass GlobalMetricsProvider(BaseMetricsProvider):\n    api: str = \"core\"\n    kind: str = \"GlobalMetricsProvider\"\n    metadata: Metadata\n    spec: MetricsProviderSpec\n\n\n@persistent(\"/core/metricsproviders/{namespace}/{name}\")\nclass MetricsProvider(BaseMetricsProvider):\n    api: str = \"core\"\n    kind: str = \"MetricsProvider\"\n    metadata: Metadata\n    spec: MetricsProviderSpec\n\n\nclass MetricsProviderList(ApiObject):\n    api: str = \"core\"\n    kind: str = \"MetricsProviderList\"\n    metadata: ListMetadata\n    items: List[MetricsProvider]\n\n\nclass GlobalMetricsProviderList(ApiObject):\n    api: str = \"core\"\n    kind: str = \"GlobalMetricsProviderList\"\n    metadata: ListMetadata\n    items: List[GlobalMetricsProvider]\n\n\nclass MetricRef(Serializable):\n    name: str\n    weight: float\n    namespaced: bool = False\n",
            "file_path": "krake/krake/data/core.py",
            "human_label": "Validate the given value against the corresponding regular expression.",
            "level": "file_runnable",
            "lineno": "99",
            "name": "validate_value",
            "oracle_context": "{ \"apis\" : \"['_get_labels_regex', 'fullmatch']\", \"classes\" : \"['ValidationError']\", \"vars\" : \"['_label_value_pattern']\" }",
            "package": "core",
            "project": "rak-n-rok/Krake",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b86a4fb4d922cb0e688cf7",
            "all_context": "{ \"import\" : \"dataclasses enum typing re datetime  marshmallow dataclasses enum typing serializable datetime \", \"file\" : \"_label_key_pattern ; _label_value_pattern ; _label_key_regex ; _label_value_regex ; _get_labels_regex() ; validate_key(key) ; validate_value(value) ; _validate_labels(labels) ; _resource_name_pattern ; _resource_name_regex ; _get_resource_name_regex() ; _validate_resource_name(name) ; _validate_resource_namespace(namespace) ; resource_ref(resource) ; \", \"class\" : \"\" }",
            "code": "def validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    key_regex, _ = _get_labels_regex()\n    if not key_regex.fullmatch(key):\n        raise ValidationError(\n            f\"Label key {key!r} does not match the regex {_label_key_pattern!r}.\"\n        )\n",
            "dependency": "{ \"builtin\" : false, \"standard_lib\" : false, \"public_lib\" : true, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Validate the given key against the corresponding regular expression.\n\nArgs:\n    key: the string to validate\n\nRaises:\n    ValidationError: if the given key is not conform to the regular expression.",
            "end_lineno": "96",
            "file_content": "import re\nfrom enum import Enum, IntEnum, auto\nfrom datetime import datetime\nfrom dataclasses import field\nfrom typing import List, Dict\n\nfrom marshmallow import ValidationError\n\nfrom . import persistent\nfrom .serializable import Serializable, ApiObject, PolymorphicContainer\n\n\nclass ResourceRef(Serializable):\n    api: str\n    namespace: str = field(default=None)\n    kind: str\n    name: str\n\n    def __hash__(self):\n        return hash((self.api, self.namespace, self.kind, self.name))\n\n    def __repr__(self):\n        message = f\"{self.kind}(api='{self.api}', \"\n        return message + f\"namespace='{self.namespace}', name='{self.name}')\"\n\n\n_label_key_pattern = None\n_label_value_pattern = None\n\n_label_key_regex = None\n_label_value_regex = None\n\n\ndef _get_labels_regex():\n    \"\"\"Build or return the regular expressions that are used to validate the key and\n    value of the labels of the Krake resources.\n\n    The first call builds the expressions, while a second returns the already built\n    ones.\n\n    Returns:\n        (re.Pattern, re.Pattern): a tuple that contains the compiled regular,\n            expressions, the first element to validate the key and the second to\n            validate the value.\n\n    \"\"\"\n    global _label_key_pattern, _label_value_pattern\n    global _label_key_regex, _label_value_regex\n\n    if _label_key_regex and _label_value_regex:\n        return _label_key_regex, _label_value_regex\n\n    # Build the patterns only if not already built\n    max_prefix_size = 253\n    max_key_size = 63\n    max_value_size = max_key_size\n\n    # First and last characters must be alphanumeric. The rest of the string must be\n    # alphanumeric, \"-\", \"_\" or \".\"\n    base_alphanumeric_pattern = \"\\\\w|(\\\\w[\\\\w\\\\-_.]{{0,{length}}}\\\\w)\"\n\n    key_pattern = base_alphanumeric_pattern.format(length=max_key_size - 2)\n    value_pattern = base_alphanumeric_pattern.format(length=max_value_size - 2)\n    prefix_pattern = base_alphanumeric_pattern.format(length=max_prefix_size - 2)\n\n    # The key can be a string of length 63 with the specifications described above,\n    # or have a prefix, then one \"/\" character, then the string of length 63 (called\n    # name).\n    # The prefix itself should have a max length of 253, but otherwise follows the\n    # specifications described above.\n    _label_key_pattern = f\"^(({prefix_pattern})\\\\/)?({key_pattern})$\"\n\n    # The value can be a string of length 63 with the specifications described\n    # above.\n    _label_value_pattern = value_pattern\n\n    _label_key_regex = re.compile(_label_key_pattern, re.ASCII)\n    _label_value_regex = re.compile(_label_value_pattern, re.ASCII)\n\n    return _label_key_regex, _label_value_regex\n\n\ndef validate_key(key):\n    \"\"\"Validate the given key against the corresponding regular expression.\n\n    Args:\n        key: the string to validate\n\n    Raises:\n        ValidationError: if the given key is not conform to the regular expression.\n    \"\"\"\n    key_regex, _ = _get_labels_regex()\n    if not key_regex.fullmatch(key):\n        raise ValidationError(\n            f\"Label key {key!r} does not match the regex {_label_key_pattern!r}.\"\n        )\n\n\ndef validate_value(value):\n    \"\"\"Validate the given value against the corresponding regular expression.\n\n    Args:\n        value: the string to validate\n\n    Raises:\n        ValidationError: if the given value is not conform to the regular expression.\n    \"\"\"\n    _, value_regex = _get_labels_regex()\n    if not value_regex.fullmatch(value):\n        raise ValidationError(\n            f\"Label value {value!r} does not match\"\n            f\" the regex {_label_value_pattern!r}.\"\n        )\n\n\ndef _validate_labels(labels):\n    \"\"\"Check that keys and values in the given labels match against their corresponding\n    regular expressions.\n\n    Args:\n        labels (dict): the different labels to validate.\n\n    Raises:\n        ValidationError: if any of the keys and labels does not match their respective\n            regular expression. The error contains as message the list of all errors\n            which occurred in the labels. Each element of the list is a dictionary with\n            one key-value pair:\n            - key: the label key or label value for which an error occurred as string.\n            - value: the error message.\n\n            .. code:: python\n\n                # Example:\n                labels = {\n                    \"key1\": \"valid\",\n                    \"key2\": [\"invalid\"],\n                    \"$$\": \"invalid\",\n                    True: True,\n                }\n                try:\n                    _validate_labels(labels)\n                except ValidationError as err:\n                    assert err.messages == [\n                        {\"['invalid']\": 'expected string or bytes-like object'},\n                        {'$$': \"Label key '$$' does not match the regex [...]\"},\n                        {'True': 'expected string or bytes-like object'},\n                        {'True': 'expected string or bytes-like object'},\n                    ]\n    \"\"\"\n    errors = []\n    for key, value in labels.items():\n        try:\n            validate_key(key)\n        except (ValidationError, TypeError) as err:\n            errors.append({str(key): str(err)})\n\n        try:\n            validate_value(value)\n        except (ValidationError, TypeError) as err:\n            errors.append({str(value): str(err)})\n\n    if errors:\n        raise ValidationError(list(errors))\n\n\n_resource_name_pattern = None\n_resource_name_regex = None\n\n\ndef _get_resource_name_regex():\n    \"\"\"Build or return the regular expressions that are used to validate\n    the name of the Krake resources.\n\n    Returns:\n        (re.Pattern): the compiled regular expressions, to validate\n        the resource name.\n    \"\"\"\n    global _resource_name_regex, _resource_name_pattern\n\n    # Build the patterns only if not already built\n    if _resource_name_regex:\n        return _resource_name_regex\n\n    # First and last characters must be alphanumeric. The rest of the string must be\n    # alphanumeric, \"-\", \"_\" or \".\" and without whitespace as well as have a\n    # max length of 255 and a min length of 1\n    max_name_size = 253  # reduced by 2 for the regex\n    min_name_size = 0  # reduced by 1 for the regex\n    base_alphanumeric_pattern = \"\\\\w|(\\\\w[\\\\w\\\\-_.:]{{{min_length},{length}}}\\\\w)\"\n\n    resource_name_pattern = base_alphanumeric_pattern.format(\n        min_length=min_name_size, length=max_name_size\n    )\n\n    _resource_name_pattern = resource_name_pattern\n    _resource_name_regex = re.compile(_resource_name_pattern, re.ASCII)\n    return _resource_name_regex\n\n\ndef _validate_resource_name(name):\n    \"\"\"Each Krake resource name is checked against a specific pattern.\n    Which characters are not allowed is defined in _get_resource_name_regex\n\n    Args:\n        name(str): the different resource names to validate.\n\n    Raises:\n        ValidationError: if any resource name does not match their respective\n            regular expression.\n    \"\"\"\n    resource_name_regex = _get_resource_name_regex()\n    if not resource_name_regex.fullmatch(name):\n        raise ValidationError(\"Invalid character in resource name.\")\n\n\ndef _validate_resource_namespace(namespace):\n    \"\"\"Each Krake resource namespace is checked against a specific pattern.\n    Which characters are not allowed is defined in _get_resource_name_regex\n\n    Args:\n        namespace(str): the different resource namespaces to validate.\n\n    Raises:\n        ValidationError: if any resource namespace does not match their respective\n            regular expression.\n    \"\"\"\n    resource_namespace_regex = _get_resource_name_regex()\n    if not resource_namespace_regex.fullmatch(namespace):\n        raise ValidationError(\"Invalid character in resource namespace.\")\n\n\nclass Metadata(Serializable):\n    name: str = field(metadata={\"immutable\": True, \"validate\": _validate_resource_name})\n    namespace: str = field(\n        default=None,\n        metadata={\"immutable\": True, \"validate\": _validate_resource_namespace},\n    )\n    labels: dict = field(default_factory=dict, metadata={\"validate\": _validate_labels})\n    finalizers: List[str] = field(default_factory=list)\n\n    uid: str = field(metadata={\"readonly\": True})\n    created: datetime = field(metadata={\"readonly\": True})\n    modified: datetime = field(metadata={\"readonly\": True})\n    deleted: datetime = field(default=None, metadata={\"readonly\": True})\n\n    owners: List[ResourceRef] = field(default_factory=list)\n\n\nclass CoreMetadata(Serializable):\n    name: str\n    uid: str\n\n\nclass ListMetadata(Serializable):\n    pass  # TODO\n\n\nclass ReasonCode(IntEnum):\n    INTERNAL_ERROR = 1  # Default error\n\n    INVALID_RESOURCE = 10  # Invalid values in the Manifest\n    # Kubernetes' resource is not supported by the Kubernetes controller\n    UNSUPPORTED_RESOURCE = 11\n    # The custom resource provided does not exist or is invalid\n    INVALID_CUSTOM_RESOURCE = 12\n\n    CLUSTER_NOT_REACHABLE = 20  # Connectivity issue with the Kubernetes deployment\n    NO_SUITABLE_RESOURCE = 50  # Scheduler issue\n\n    KUBERNETES_ERROR = 60\n\n    CREATE_FAILED = 70\n    RECONCILE_FAILED = 71\n    DELETE_FAILED = 72\n\n    OPENSTACK_ERROR = 80\n    INVALID_CLUSTER_TEMPLATE = 81\n\n    # Related to Metrics and Metric Provider\n    INVALID_METRIC = 91\n    UNREACHABLE_METRICS_PROVIDER = 92\n    UNKNOWN_METRIC = 93\n    UNKNOWN_METRICS_PROVIDER = 94\n\n\nclass Reason(Serializable):\n    code: ReasonCode\n    message: str\n\n\nclass WatchEventType(Enum):\n    ADDED = auto()\n    MODIFIED = auto()\n    DELETED = auto()\n\n\nclass Status(Serializable):\n    reason: Reason = None\n\n\nclass WatchEvent(Serializable):\n    type: WatchEventType\n    object: dict\n\n\nclass Verb(Enum):\n    create = auto()\n    list = auto()\n    list_all = auto()\n    get = auto()\n    update = auto()\n    delete = auto()\n\n\nclass RoleRule(Serializable):\n    api: str\n    resources: List[str]\n    namespaces: List[str]\n    verbs: List[Verb]\n\n\n@persistent(\"/core/roles/{name}\")\nclass Role(ApiObject):\n    api: str = \"core\"\n    kind: str = \"Role\"\n    metadata: Metadata\n    rules: List[RoleRule]\n\n\nclass RoleList(ApiObject):\n    api: str = \"core\"\n    kind: str = \"RoleList\"\n    metadata: ListMetadata\n    items: List[Role]\n\n\n@persistent(\"/core/rolebindings/{name}\")\nclass RoleBinding(ApiObject):\n    api: str = \"core\"\n    kind: str = \"RoleBinding\"\n    metadata: Metadata\n    users: List[str]\n    roles: List[str]\n\n\nclass RoleBindingList(ApiObject):\n    api: str = \"core\"\n    kind: str = \"RoleBindingList\"\n    metadata: ListMetadata\n    items: List[RoleBinding]\n\n\nclass Conflict(Serializable):\n    source: ResourceRef\n    conflicting: List[ResourceRef]\n\n\ndef resource_ref(resource):\n    \"\"\"Create a :class:`ResourceRef` from a :class:`ApiObject`\n\n    Args:\n        resource (.serializable.ApiObject): API object that should be\n            referenced\n\n    Returns:\n        ResourceRef: Corresponding reference to the API object\n    \"\"\"\n    return ResourceRef(\n        api=resource.api,\n        kind=resource.kind,\n        namespace=resource.metadata.namespace,\n        name=resource.metadata.name,\n    )\n\n\nclass MetricSpecProvider(Serializable):\n    name: str\n    metric: str\n\n\nclass MetricSpec(Serializable):\n    min: float\n    max: float\n    provider: MetricSpecProvider\n\n\nclass BaseMetric(ApiObject):\n    api: str = \"core\"\n    kind: str = None\n    metadata: Metadata\n    spec: MetricSpec\n\n\n@persistent(\"/core/globalmetrics/{name}\")\nclass GlobalMetric(BaseMetric):\n    api: str = \"core\"\n    kind: str = \"GlobalMetric\"\n    metadata: Metadata\n    spec: MetricSpec\n\n\n@persistent(\"/core/metrics/{namespace}/{name}\")\nclass Metric(BaseMetric):\n    api: str = \"core\"\n    kind: str = \"Metric\"\n    metadata: Metadata\n    spec: MetricSpec\n\n\nclass MetricList(ApiObject):\n    api: str = \"core\"\n    kind: str = \"MetricList\"\n    metadata: ListMetadata\n    items: List[Metric]\n\n\nclass GlobalMetricList(ApiObject):\n    api: str = \"core\"\n    kind: str = \"GlobalMetricList\"\n    metadata: ListMetadata\n    items: List[GlobalMetric]\n\n\nclass MetricsProviderSpec(PolymorphicContainer):\n    type: str\n\n\n@MetricsProviderSpec.register(\"prometheus\")\nclass PrometheusSpec(Serializable):\n    url: str\n\n\n@MetricsProviderSpec.register(\"kafka\")\nclass KafkaSpec(Serializable):\n    \"\"\"Specifications to connect to a KSQL database, and retrieve a specific row from a\n    specific table.\n\n    Attributes:\n        comparison_column (str): name of the column where the value will be compared to\n            the metric name, to select the right metric.\n        value_column (str): name of the column where the value of a metric is stored.\n        table (str): the name of the KSQL table where the metric is defined.\n        url (str): endpoint of the KSQL database.\n\n    \"\"\"\n\n    comparison_column: str\n    value_column: str\n    table: str\n    url: str\n\n\n@MetricsProviderSpec.register(\"static\")\nclass StaticSpec(Serializable):\n    metrics: Dict[str, float]\n\n\nclass BaseMetricsProvider(ApiObject):\n    api: str = \"core\"\n    kind: str = None\n    metadata: Metadata\n    spec: MetricsProviderSpec\n\n\n@persistent(\"/core/globalmetricsproviders/{name}\")\nclass GlobalMetricsProvider(BaseMetricsProvider):\n    api: str = \"core\"\n    kind: str = \"GlobalMetricsProvider\"\n    metadata: Metadata\n    spec: MetricsProviderSpec\n\n\n@persistent(\"/core/metricsproviders/{namespace}/{name}\")\nclass MetricsProvider(BaseMetricsProvider):\n    api: str = \"core\"\n    kind: str = \"MetricsProvider\"\n    metadata: Metadata\n    spec: MetricsProviderSpec\n\n\nclass MetricsProviderList(ApiObject):\n    api: str = \"core\"\n    kind: str = \"MetricsProviderList\"\n    metadata: ListMetadata\n    items: List[MetricsProvider]\n\n\nclass GlobalMetricsProviderList(ApiObject):\n    api: str = \"core\"\n    kind: str = \"GlobalMetricsProviderList\"\n    metadata: ListMetadata\n    items: List[GlobalMetricsProvider]\n\n\nclass MetricRef(Serializable):\n    name: str\n    weight: float\n    namespaced: bool = False\n",
            "file_path": "krake/krake/data/core.py",
            "human_label": "Validate the given key against the corresponding regular expression.",
            "level": "file_runnable",
            "lineno": "83",
            "name": "validate_key",
            "oracle_context": "{ \"apis\" : \"['_get_labels_regex', 'fullmatch']\", \"classes\" : \"['ValidationError']\", \"vars\" : \"['_label_key_pattern']\" }",
            "package": "core",
            "project": "rak-n-rok/Krake",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b86a01b4d922cb0e688ccc",
            "all_context": "{ \"import\" : \"inspect logging asyncio collections secrets operator base64 contextlib enum functools datetime typing random copy logging aiohttp asyncio secrets enum functools krake datetime typing yarl \", \"file\" : \"logger ; listen ; update_last_applied_manifest_dict_from_resp(last_applied_manifest,observer_schema,response) ; update_last_applied_manifest_list_from_resp(last_applied_manifest,observer_schema,response) ; update_last_applied_manifest_from_resp(app,response) ; update_last_observed_manifest_from_resp(app,response) ; update_last_observed_manifest_dict(observed_resource,response) ; update_last_observed_manifest_list(observed_resource,response) ; update_last_applied_manifest_dict_from_spec(resource_status_new,resource_status_old,resource_observed) ; update_last_applied_manifest_list_from_spec(resource_status_new,resource_status_old,resource_observed) ; update_last_applied_manifest_from_spec(app) ; utc_difference() ; generate_certificate(config) ; generate_default_observer_schema(app) ; generate_default_observer_schema_dict(manifest_dict,first_level) ; generate_default_observer_schema_list(manifest_list) ; \", \"class\" : \"\" }",
            "code": "def generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n\n    \"\"\"\n    observer_schema_dict = {}\n\n    for key, value in manifest_dict.items():\n\n        if isinstance(value, dict):\n            observer_schema_dict[key] = generate_default_observer_schema_dict(value)\n\n        elif isinstance(value, list):\n            observer_schema_dict[key] = generate_default_observer_schema_list(value)\n\n        else:\n            observer_schema_dict[key] = None\n\n    if first_level:\n        observer_schema_dict[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        observer_schema_dict[\"kind\"] = manifest_dict[\"kind\"]\n        observer_schema_dict[\"metadata\"][\"name\"] = manifest_dict[\"metadata\"][\"name\"]\n\n        if (\n            \"spec\" in manifest_dict\n            and \"type\" in manifest_dict[\"spec\"]\n            and manifest_dict[\"spec\"][\"type\"] == \"LoadBalancer\"\n        ):\n            observer_schema_dict[\"status\"] = {\"load_balancer\": {\"ingress\": None}}\n\n    return observer_schema_dict\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Together with :func:``generate_default_observer_schema_list``, this function is\ncalled recursively to generate part of a default ``observer_schema`` from part of a\nKubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\nArgs:\n    manifest_dict (dict): Partial Kubernetes resources\n    first_level (bool, optional): If True, indicates that the dictionary represents\n        the whole observer schema of a Kubernetes resource\n\nReturns:\n    dict: Generated partial observer_schema\n\nThis function creates a new dictionary from ``manifest_dict`` and replaces all\nnon-list and non-dict values by ``None``.\n\nIn case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\nresource), the values of the identifying fields are copied from the manifest file.",
            "end_lineno": "1085",
            "file_content": "\"\"\"This module defines the Hook Dispatcher and listeners for registering and\nexecuting hooks. Hook Dispatcher emits hooks based on :class:`Hook` attributes which\ndefine when the hook will be executed.\n\n\"\"\"\nimport asyncio\nimport logging\nimport random\nfrom base64 import b64encode\nfrom collections import defaultdict\nfrom contextlib import suppress\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom functools import reduce\nfrom operator import getitem\nfrom enum import Enum, auto\nfrom inspect import iscoroutinefunction\nfrom OpenSSL import crypto\nfrom typing import NamedTuple\n\nimport yarl\nfrom aiohttp import ClientConnectorError\n\nfrom krake.controller import Observer\nfrom krake.controller.kubernetes.client import KubernetesClient, InvalidManifestError\nfrom krake.utils import camel_to_snake_case, get_kubernetes_resource_idx\nfrom kubernetes_asyncio.client.rest import ApiException\nfrom kubernetes_asyncio.client.api_client import ApiClient\nfrom kubernetes_asyncio import client\nfrom krake.data.kubernetes import ClusterState, Application, Cluster\nfrom yarl import URL\nfrom secrets import token_urlsafe\n\nfrom kubernetes_asyncio.client import (\n    Configuration,\n    V1Secret,\n    V1EnvVar,\n    V1VolumeMount,\n    V1Volume,\n    V1SecretKeySelector,\n    V1EnvVarSource,\n)\nfrom kubernetes_asyncio.config.kube_config import KubeConfigLoader\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass HookType(Enum):\n    ResourcePreCreate = auto()\n    ResourcePostCreate = auto()\n    ResourcePreUpdate = auto()\n    ResourcePostUpdate = auto()\n    ResourcePreDelete = auto()\n    ResourcePostDelete = auto()\n    ApplicationMangling = auto()\n    ApplicationPreMigrate = auto()\n    ApplicationPostMigrate = auto()\n    ApplicationPreReconcile = auto()\n    ApplicationPostReconcile = auto()\n    ApplicationPreDelete = auto()\n    ApplicationPostDelete = auto()\n    ClusterCreation = auto()\n    ClusterDeletion = auto()\n\n\nclass HookDispatcher(object):\n    \"\"\"Simple wrapper around a registry of handlers associated to :class:`Hook`\n     attributes. Each :class:`Hook` attribute defines when the handler will be\n     executed.\n\n    Listeners for certain hooks can be registered via :meth:`on`. Registered\n    listeners are executed via :meth:`hook`.\n\n    Example:\n        .. code:: python\n\n        listen = HookDispatcher()\n\n        @listen.on(HookType.PreApply)\n        def to_perform_before_app_creation(app, cluster, resource, controller):\n            # Do Stuff\n\n        @listen.on(HookType.PostApply)\n        def another_to_perform_after_app_creation(app, cluster, resource, resp):\n            # Do Stuff\n\n        @listen.on(HookType.PostDelete)\n        def to_perform_after_app_deletion(app, cluster, resource, resp):\n            # Do Stuff\n\n    \"\"\"\n\n    def __init__(self):\n        self.registry = defaultdict(list)\n\n    def on(self, hook):\n        \"\"\"Decorator function to add a new handler to the registry.\n\n        Args:\n            hook (HookType): Hook attribute for which to register the handler.\n\n        Returns:\n            callable: Decorator for registering listeners for the specified\n            hook.\n\n        \"\"\"\n\n        def decorator(handler):\n            self.registry[hook].append(handler)\n\n            return handler\n\n        return decorator\n\n    async def hook(self, hook, **kwargs):\n        \"\"\"Execute the list of handlers associated to the provided :class:`Hook`\n        attribute.\n\n        Args:\n            hook (HookType): The hook attribute for which to execute handlers.\n\n        \"\"\"\n        try:\n            handlers = self.registry[hook]\n        except KeyError:\n            pass\n        else:\n            for handler in handlers:\n                if iscoroutinefunction(handler):\n                    await handler(**kwargs)\n                else:\n                    handler(**kwargs)\n\n\nlisten = HookDispatcher()\n\n\n@listen.on(HookType.ResourcePostCreate)\n@listen.on(HookType.ResourcePostUpdate)\nasync def register_service(app, cluster, resource, response):\n    \"\"\"Register endpoint of Kubernetes Service object on creation and update.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        cluster (krake.data.kubernetes.Cluster): The cluster on which the\n            application is running\n        resource (dict): Kubernetes object description as specified in the\n            specification of the application.\n        response (kubernetes_asyncio.client.V1Service): Response of the\n            Kubernetes API\n\n    \"\"\"\n    if resource[\"kind\"] != \"Service\":\n        return\n\n    service_name = resource[\"metadata\"][\"name\"]\n\n    if response.spec and response.spec.type == \"LoadBalancer\":\n        # For a \"LoadBalancer\" type of Service, an external IP is given in the cluster\n        # by a load balancer controller to the service. In this case, the \"port\"\n        # specified in the spec is reachable from the outside.\n        if (\n            not response.status.load_balancer\n            or not response.status.load_balancer.ingress\n        ):\n            # When a \"LoadBalancer\" type of service is created, the IP is given by an\n            # additional controller (e.g. a controller that requests a floating IP to an\n            # OpenStack infrastructure). This process can take some time, but the\n            # Service itself already exist before the IP is assigned. In the case of an\n            # error with the controller, the IP is also not given. This \"<pending>\" IP\n            # just expresses that the Service exists, but the IP is not ready yet.\n            external_ip = \"<pending>\"\n        else:\n            external_ip = response.status.load_balancer.ingress[0].ip\n\n        if not response.spec.ports:\n            external_port = \"<pending>\"\n        else:\n            external_port = response.spec.ports[0].port\n        app.status.services[service_name] = f\"{external_ip}:{external_port}\"\n        return\n\n    node_port = None\n    # Ensure that ports are specified\n    if response.spec and response.spec.ports:\n        node_port = response.spec.ports[0].node_port\n\n    # If the service does not have a node port, remove a potential reference\n    # and return.\n    if node_port is None:\n        try:\n            del app.status.services[service_name]\n        except KeyError:\n            pass\n        return\n\n    # Determine URL of Kubernetes cluster API\n    loader = KubeConfigLoader(cluster.spec.kubeconfig)\n    config = Configuration()\n    await loader.load_and_set(config)\n    cluster_url = yarl.URL(config.host)\n\n    app.status.services[service_name] = f\"{cluster_url.host}:{node_port}\"\n\n\n@listen.on(HookType.ResourcePostDelete)\nasync def unregister_service(app, resource, **kwargs):\n    \"\"\"Unregister endpoint of Kubernetes Service object on deletion.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        resource (dict): Kubernetes object description as specified in the\n            specification of the application.\n\n    \"\"\"\n    if resource[\"kind\"] != \"Service\":\n        return\n\n    service_name = resource[\"metadata\"][\"name\"]\n    try:\n        del app.status.services[service_name]\n    except KeyError:\n        pass\n\n\n@listen.on(HookType.ResourcePostDelete)\nasync def remove_resource_from_last_observed_manifest(app, resource, **kwargs):\n    \"\"\"Remove a given resource from the last_observed_manifest after its deletion\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        resource (dict): Kubernetes object description as specified in the\n            specification of the application.\n\n    \"\"\"\n    try:\n        idx = get_kubernetes_resource_idx(app.status.last_observed_manifest, resource)\n    except IndexError:\n        return\n\n    app.status.last_observed_manifest.pop(idx)\n\n\ndef update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n\n    \"\"\"\n    for key, value in observer_schema.items():\n\n        # Keys in the response are in camelCase\n        camel_key = camel_to_snake_case(key)\n\n        if camel_key not in response:\n            # An observed key should always be present in the k8s response\n            raise KeyError(\n                f\"Observed key {camel_key} is not present in response {response}\"\n            )\n\n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                # The dictionary is observed, but not present in\n                # last_applied_manifest\n                last_applied_manifest[key] = {}\n\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], observer_schema[key], response[camel_key]\n            )\n\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                # The list is observed, but not present in last_applied_manifest\n                last_applied_manifest[key] = []\n\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], observer_schema[key], response[camel_key]\n            )\n\n        elif key not in last_applied_manifest:\n            # If key not present in last_applied_manifest, and value is neither a\n            # dict nor a list, simply add it.\n            last_applied_manifest[key] = response[camel_key]\n\n\ndef update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n\n    \"\"\"\n    # Looping over the observed resource, except the last element which is the\n    # special control dictionary\n    for idx, val in enumerate(observer_schema[:-1]):\n\n        if idx >= len(response):\n            # Element is observed but not present in k8s response, so following\n            # elements will also not exist.\n            #\n            # This doesn't raise an Exception as observing the element of a list\n            # doesn't ensure its presence. The list length is controlled by the\n            # special control dictionary\n            return\n\n        if isinstance(val, dict):\n            if idx >= len(last_applied_manifest):\n                # The dict is observed, but not present in last_applied_manifest\n                last_applied_manifest.append({})\n\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n\n        elif isinstance(response[idx], list):\n            if idx >= len(last_applied_manifest):\n                # The list is observed, but not present in last_applied_manifest\n                last_applied_manifest.append([])\n\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n\n        elif idx >= len(last_applied_manifest):\n            # Element is not yet present in last_applied_manifest. Adding it.\n            last_applied_manifest.append(response[idx])\n\n\n@listen.on(HookType.ResourcePostCreate)\n@listen.on(HookType.ResourcePostUpdate)\ndef update_last_applied_manifest_from_resp(app, response, **kwargs):\n    \"\"\"Hook run after the creation or update of an application in order to update the\n    `status.last_applied_manifest` using the k8s response.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        response (kubernetes_asyncio.client.V1Status): Response of the Kubernetes API\n\n    After a Kubernetes resource has been created/updated, the\n    `status.last_applied_manifest` has to be updated. All fields already initialized\n    (either from the mangling of `spec.manifest`, or by a previous call to this\n    function) should be left untouched. Only observed fields which are not present in\n    `status.last_applied_manifest` should be initialized.\n\n    \"\"\"\n\n    if isinstance(response, dict):\n        # The Kubernetes API couldn't deserialize the k8s response into an object\n        resp = response\n    else:\n        # The Kubernetes API deserialized the k8s response into an object\n        resp = response.to_dict()\n\n    idx_applied = get_kubernetes_resource_idx(app.status.last_applied_manifest, resp)\n\n    idx_observed = get_kubernetes_resource_idx(app.status.mangled_observer_schema, resp)\n\n    update_last_applied_manifest_dict_from_resp(\n        app.status.last_applied_manifest[idx_applied],\n        app.status.mangled_observer_schema[idx_observed],\n        resp,\n    )\n\n\n@listen.on(HookType.ResourcePostCreate)\n@listen.on(HookType.ResourcePostUpdate)\ndef update_last_observed_manifest_from_resp(app, response, **kwargs):\n    \"\"\"Handler to run after the creation or update of a Kubernetes resource to update\n    the last_observed_manifest from the response of the Kubernetes API.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        response (kubernetes_asyncio.client.V1Service): Response of the\n            Kubernetes API\n\n    The target last_observed_manifest holds the value of all observed fields plus the\n    special control dictionaries for the list length\n\n    \"\"\"\n    if isinstance(response, dict):\n        # The Kubernetes API couldn't deserialize the k8s response into an object\n        resp = response\n    else:\n        # The Kubernetes API deserialized the k8s response into an object\n        resp = response.to_dict()\n\n    try:\n        idx_observed = get_kubernetes_resource_idx(\n            app.status.mangled_observer_schema,\n            resp,\n        )\n    except IndexError:\n        # All created resources should be observed\n        raise\n\n    try:\n        idx_last_observed = get_kubernetes_resource_idx(\n            app.status.last_observed_manifest,\n            resp,\n        )\n    except IndexError:\n        # If the resource is not yes present in last_observed_manifest, append it.\n        idx_last_observed = len(app.status.last_observed_manifest)\n        app.status.last_observed_manifest.append({})\n\n    # Overwrite the last_observed_manifest for this resource\n    app.status.last_observed_manifest[\n        idx_last_observed\n    ] = update_last_observed_manifest_dict(\n        app.status.mangled_observer_schema[idx_observed], resp\n    )\n\n\ndef update_last_observed_manifest_dict(observed_resource, response):\n    \"\"\"Together with :func:``update_last_observed_manifest_list``, recursively\n    crafts the ``last_observed_manifest`` from the Kubernetes :attr:``response``.\n\n    Args:\n        observed_resource (dict): The schema to observe for the partial given resource\n        response (dict): The partial Kubernetes response for this resource.\n\n    Raises:\n        KeyError: If an observed key is not present in the Kubernetes response\n\n    Returns:\n        dict: The dictionary of observed keys and their value\n\n    Get the value of all observed fields from the Kubernetes response\n    \"\"\"\n    res = {}\n    for key, value in observed_resource.items():\n\n        camel_key = camel_to_snake_case(key)\n        if camel_key not in response:\n            raise KeyError(\n                f\"Observed key {camel_key} is not present in response {response}\"\n            )\n\n        if isinstance(value, dict):\n            res[key] = update_last_observed_manifest_dict(value, response[camel_key])\n\n        elif isinstance(value, list):\n            res[key] = update_last_observed_manifest_list(value, response[camel_key])\n\n        else:\n            res[key] = response[camel_key]\n\n    return res\n\n\ndef update_last_observed_manifest_list(observed_resource, response):\n    \"\"\"Together with :func:``update_last_observed_manifest_dict``, recursively\n    crafts the ``last_observed_manifest`` from the Kubernetes :attr:``response``.\n\n    Args:\n        observed_resource (list): the schema to observe for the partial given resource\n        response (list): the partial Kubernetes response for this resource.\n\n    Returns:\n        list: The list of observed elements, plus the special list length control\n            dictionary\n\n    Get the value of all observed elements from the Kubernetes response\n    \"\"\"\n\n    if not response:\n        return [{\"observer_schema_list_current_length\": 0}]\n\n    res = []\n    # Looping over the observed resource, except the last element which is the special\n    # control dictionary\n    for idx, val in enumerate(observed_resource[:-1]):\n\n        if idx >= len(response):\n            # Element is not present in the Kubernetes response, nothing more to do\n            break\n\n        if type(response[idx]) == dict:\n            res.append(update_last_observed_manifest_dict(val, response[idx]))\n\n        elif type(response[idx]) == list:\n            res.append(update_last_observed_manifest_list(val, response[idx]))\n\n        else:\n            res.append(response[idx])\n\n    # Append the special control dictionary to the list\n    res.append({\"observer_schema_list_current_length\": len(response)})\n\n    return res\n\n\ndef update_last_applied_manifest_dict_from_spec(\n    resource_status_new, resource_status_old, resource_observed\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_list_from_spec``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n\n    Args:\n        resource_status_new (dict): partial ``last_applied_manifest`` being updated\n        resource_status_old (dict): partial of the current ``last_applied_manifest``\n        resource_observed (dict): partial observer_schema for the manifest file\n            being updated\n\n    \"\"\"\n    for key, value in resource_observed.items():\n\n        if key not in resource_status_old:\n            continue\n\n        if key in resource_status_new:\n\n            if isinstance(value, dict):\n                update_last_applied_manifest_dict_from_spec(\n                    resource_status_new[key],\n                    resource_status_old[key],\n                    resource_observed[key],\n                )\n\n            elif isinstance(value, list):\n                update_last_applied_manifest_list_from_spec(\n                    resource_status_new[key],\n                    resource_status_old[key],\n                    resource_observed[key],\n                )\n\n        else:\n            # If the key is not present the spec.manifest, we first need to\n            # initialize it\n\n            if isinstance(value, dict):\n                resource_status_new[key] = {}\n                update_last_applied_manifest_dict_from_spec(\n                    resource_status_new[key],\n                    resource_status_old[key],\n                    resource_observed[key],\n                )\n\n            elif isinstance(value, list):\n                resource_status_new[key] = []\n                update_last_applied_manifest_list_from_spec(\n                    resource_status_new[key],\n                    resource_status_old[key],\n                    resource_observed[key],\n                )\n\n            else:\n                resource_status_new[key] = resource_status_old[key]\n\n\ndef update_last_applied_manifest_list_from_spec(\n    resource_status_new, resource_status_old, resource_observed\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_dict_from_spec``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n\n    Args:\n        resource_status_new (list): partial ``last_applied_manifest`` being updated\n        resource_status_old (list): partial of the current ``last_applied_manifest``\n        resource_observed (list): partial observer_schema for the manifest file\n            being updated\n\n    \"\"\"\n\n    # Looping over the observed resource, except the last element which is the\n    # special control dictionary\n    for idx, val in enumerate(resource_observed[:-1]):\n\n        if idx >= len(resource_status_old):\n            # The element in not in the current last_applied_manifest, and neither\n            # is the rest of the list\n            break\n\n        if idx < len(resource_status_new):\n            # The element is present in spec.manifest and in the current\n            # last_applied_manifest. Updating observed fields\n\n            if isinstance(val, dict):\n                update_last_applied_manifest_dict_from_spec(\n                    resource_status_new[idx],\n                    resource_status_old[idx],\n                    resource_observed[idx],\n                )\n\n            elif isinstance(val, list):\n                update_last_applied_manifest_list_from_spec(\n                    resource_status_new[idx],\n                    resource_status_old[idx],\n                    resource_observed[idx],\n                )\n\n        else:\n            # If the element is not present in the spec.manifest, we first have to\n            # initialize it.\n\n            if isinstance(val, dict):\n                resource_status_new.append({})\n                update_last_applied_manifest_dict_from_spec(\n                    resource_status_new[idx],\n                    resource_status_old[idx],\n                    resource_observed[idx],\n                )\n\n            elif isinstance(val, list):\n                resource_status_new.append([])\n                update_last_applied_manifest_list_from_spec(\n                    resource_status_new[idx],\n                    resource_status_old[idx],\n                    resource_observed[idx],\n                )\n\n            else:\n                resource_status_new.append(resource_status_old[idx])\n\n\ndef update_last_applied_manifest_from_spec(app):\n    \"\"\"Update the status.last_applied_manifest of an application from spec.manifests\n\n    Args:\n        app (krake.data.kubernetes.Application): Application to update\n\n    This function is called on application creation and updates. The\n    last_applied_manifest of an application is initialized as a copy of spec.manifest,\n    and is augmented by all known observed fields not yet initialized (i.e. all observed\n    fields or resources which are present in the current last_applied_manifest but not\n    in the spec.manifest)\n\n    \"\"\"\n\n    # The new last_applied_manifest is initialized as a copy of the spec.manifest, and\n    # augmented by all observed fields which are present in the current\n    # last_applied_manifest but not in the original spec.manifest\n    new_last_applied_manifest = deepcopy(app.spec.manifest)\n\n    # Loop over observed resources and observed fields, and check if they should be\n    # added to the new last_applied_manifest (i.e. present in the current\n    # last_applied_manifest but not in spec.manifest)\n    for resource_observed in app.status.mangled_observer_schema:\n\n        # If the resource is not present in the current last_applied_manifest, there is\n        # nothing to do. Whether the resource was initialized by spec.manifest doesn't\n        # matter.\n        try:\n            idx_status_old = get_kubernetes_resource_idx(\n                app.status.last_applied_manifest, resource_observed\n            )\n        except IndexError:\n            continue\n\n        # As the resource is present in the current last_applied_manifest, we need to go\n        # through it to check if observed fields should be set to their current value\n        # (i.e. fields are present in the current last_applied_manifest, but not in\n        # spec.manifest)\n        try:\n            # Check if the observed resource is present in spec.manifest\n            idx_status_new = get_kubernetes_resource_idx(\n                new_last_applied_manifest, resource_observed\n            )\n        except IndexError:\n            # The resource is observed but is not present in the spec.manifest.\n            # Create an empty resource, which will be augmented in\n            # update_last_applied_manifest_dict_from_spec with the observed and known\n            # fields.\n            new_last_applied_manifest.append({})\n            idx_status_new = len(new_last_applied_manifest) - 1\n\n        update_last_applied_manifest_dict_from_spec(\n            new_last_applied_manifest[idx_status_new],\n            app.status.last_applied_manifest[idx_status_old],\n            resource_observed,\n        )\n\n    app.status.last_applied_manifest = new_last_applied_manifest\n\n\nclass KubernetesApplicationObserver(Observer):\n    \"\"\"Observer specific for Kubernetes Applications. One observer is created for each\n    Application managed by the Controller, but not one per Kubernetes resource\n    (Deployment, Service...). If several resources are defined by an Application, they\n    are all monitored by the same observer.\n\n    The observer gets the actual status of the resources on the cluster using the\n    Kubernetes API, and compare it to the status stored in the API.\n\n    The observer is:\n     * started at initial Krake resource creation;\n\n     * deleted when a resource needs to be updated, then started again when it is done;\n\n     * simply deleted on resource deletion.\n\n    Args:\n        cluster (krake.data.kubernetes.Cluster): the cluster on which the observed\n            Application is created.\n        resource (krake.data.kubernetes.Application): the application that will be\n            observed.\n        on_res_update (coroutine): a coroutine called when a resource's actual status\n            differs from the status sent by the database. Its signature is:\n            ``(resource) -> updated_resource``. ``updated_resource`` is the instance of\n            the resource that is up-to-date with the API. The Observer internal instance\n            of the resource to observe will be updated. If the API cannot be contacted,\n            ``None`` can be returned. In this case the internal instance of the Observer\n            will not be updated.\n        time_step (int, optional): how frequently the Observer should watch the actual\n            status of the resources.\n\n    \"\"\"\n\n    def __init__(self, cluster, resource, on_res_update, time_step=2):\n        super().__init__(resource, on_res_update, time_step)\n        self.cluster = cluster\n\n    async def poll_resource(self):\n        \"\"\"Fetch the current status of the Application monitored by the Observer.\n\n        Returns:\n            krake.data.core.Status: the status object created using information from the\n                real world Applications resource.\n\n        \"\"\"\n        app = self.resource\n\n        status = deepcopy(app.status)\n        status.last_observed_manifest = []\n        # For each observed kubernetes resource of the Application,\n        # get its current status on the cluster.\n        for desired_resource in app.status.last_applied_manifest:\n            kube = KubernetesClient(self.cluster.spec.kubeconfig)\n            idx_observed = get_kubernetes_resource_idx(\n                app.status.mangled_observer_schema, desired_resource\n            )\n            observed_resource = app.status.mangled_observer_schema[idx_observed]\n            async with kube:\n                try:\n                    group, version, kind, name, namespace = kube.get_immutables(\n                        desired_resource\n                    )\n                    resource_api = await kube.get_resource_api(group, version, kind)\n                    resp = await resource_api.read(kind, name, namespace)\n                except ApiException as err:\n                    if err.status == 404:\n                        # Resource does not exist\n                        continue\n                    # Otherwise, log the unexpected errors\n                    logger.error(err)\n\n            observed_manifest = update_last_observed_manifest_dict(\n                observed_resource, resp.to_dict()\n            )\n            status.last_observed_manifest.append(observed_manifest)\n\n        return status\n\n\nclass KubernetesClusterObserver(Observer):\n    \"\"\"Observer specific for Kubernetes Clusters. One observer is created for each\n    Cluster managed by the Controller.\n\n    The observer gets the actual status of the cluster using the\n    Kubernetes API, and compare it to the status stored in the API.\n\n    The observer is:\n     * started at initial Krake resource creation;\n\n     * deleted when a resource needs to be updated, then started again when it is done;\n\n     * simply deleted on resource deletion.\n\n    Args:\n        cluster (krake.data.kubernetes.Cluster): the cluster which will be observed.\n        on_res_update (coroutine): a coroutine called when a resource's actual status\n            differs from the status sent by the database. Its signature is:\n            ``(resource) -> updated_resource``. ``updated_resource`` is the instance of\n            the resource that is up-to-date with the API. The Observer internal instance\n            of the resource to observe will be updated. If the API cannot be contacted,\n            ``None`` can be returned. In this case the internal instance of the Observer\n            will not be updated.\n        time_step (int, optional): how frequently the Observer should watch the actual\n            status of the resources.\n\n    \"\"\"\n\n    def __init__(self, cluster, on_res_update, time_step=2):\n        super().__init__(cluster, on_res_update, time_step)\n        self.cluster = cluster\n\n    async def poll_resource(self):\n        \"\"\"Fetch the current status of the Cluster monitored by the Observer.\n\n        Returns:\n            krake.data.core.Status: the status object created using information from the\n                real world Cluster.\n\n        \"\"\"\n        status = deepcopy(self.cluster.status)\n        # For each observed kubernetes cluster registered in Krake,\n        # get its current node status.\n        loader = KubeConfigLoader(self.cluster.spec.kubeconfig)\n        config = Configuration()\n        await loader.load_and_set(config)\n        kube = ApiClient(config)\n\n        async with kube as api:\n            v1 = client.CoreV1Api(api)\n            try:\n                response = await v1.list_node()\n\n            except ClientConnectorError as err:\n                status.state = ClusterState.OFFLINE\n                self.cluster.status.state = ClusterState.OFFLINE\n                # Log the error\n                logger.debug(err)\n                return status\n\n            condition_dict = {\n                \"MemoryPressure\": [],\n                \"DiskPressure\": [],\n                \"PIDPressure\": [],\n                \"Ready\": [],\n            }\n\n            for item in response.items:\n                for condition in item.status.conditions:\n                    condition_dict[condition.type].append(condition.status)\n                if (\n                    condition_dict[\"MemoryPressure\"] == [\"True\"]\n                    or condition_dict[\"DiskPressure\"] == [\"True\"]\n                    or condition_dict[\"PIDPressure\"] == [\"True\"]\n                ):\n                    status.state = ClusterState.UNHEALTHY\n                    self.cluster.status.state = ClusterState.UNHEALTHY\n                    return status\n                elif (\n                    condition_dict[\"Ready\"] == [\"True\"]\n                    and status.state is ClusterState.OFFLINE\n                ):\n                    status.state = ClusterState.CONNECTING\n                    self.cluster.status.state = ClusterState.CONNECTING\n                    return status\n                elif condition_dict[\"Ready\"] == [\"True\"]:\n                    status.state = ClusterState.ONLINE\n                    self.cluster.status.state = ClusterState.ONLINE\n                    return status\n                else:\n                    status.state = ClusterState.NOTREADY\n                    self.cluster.status.state = ClusterState.NOTREADY\n                    return status\n\n\n@listen.on(HookType.ApplicationPostReconcile)\n@listen.on(HookType.ApplicationPostMigrate)\n@listen.on(HookType.ClusterCreation)\nasync def register_observer(controller, resource, start=True, **kwargs):\n    \"\"\"Create an observer for the given Application or Cluster, and start it as a\n    background task if wanted.\n\n    If an observer already existed for this Application or Cluster, it is stopped\n    and deleted.\n\n    Args:\n        controller (KubernetesController): the controller for which the observer will be\n            added in the list of working observers.\n        resource (krake.data.kubernetes.Application): the Application to observe or\n        resource (krake.data.kubernetes.Cluster): the Cluster to observe.\n        start (bool, optional): if False, does not start the observer as background\n            task.\n\n    \"\"\"\n    if resource.kind == Application.kind:\n        cluster = await controller.kubernetes_api.read_cluster(\n            namespace=resource.status.running_on.namespace,\n            name=resource.status.running_on.name,\n        )\n\n        observer = KubernetesApplicationObserver(\n            cluster,\n            resource,\n            controller.on_status_update,\n            time_step=controller.observer_time_step,\n        )\n\n    elif resource.kind == Cluster.kind:\n        observer = KubernetesClusterObserver(\n            resource,\n            controller.on_status_update,\n            time_step=controller.observer_time_step,\n        )\n    else:\n        logger.debug(\"Unknown resource kind. No observer was registered.\", resource)\n        return\n\n    logger.debug(f\"Start observer for {resource.kind} %r\", resource.metadata.name)\n    task = None\n    if start:\n        task = controller.loop.create_task(observer.run())\n\n    controller.observers[resource.metadata.uid] = (observer, task)\n\n\n@listen.on(HookType.ApplicationPreReconcile)\n@listen.on(HookType.ApplicationPreMigrate)\n@listen.on(HookType.ApplicationPreDelete)\n@listen.on(HookType.ClusterDeletion)\nasync def unregister_observer(controller, resource, **kwargs):\n    \"\"\"Stop and delete the observer for the given Application or Cluster. If no observer\n    is started, do nothing.\n\n    Args:\n        controller (KubernetesController): the controller for which the observer will be\n            removed from the list of working observers.\n        resource (krake.data.kubernetes.Application): the Application whose observer\n        will be stopped or\n        resource (krake.data.kubernetes.Cluster): the Cluster whose observer will be\n        stopped.\n\n    \"\"\"\n    if resource.metadata.uid not in controller.observers:\n        return\n\n    logger.debug(f\"Stop observer for {resource.kind} %r\", resource.metadata.name)\n    _, task = controller.observers.pop(resource.metadata.uid)\n    task.cancel()\n\n    with suppress(asyncio.CancelledError):\n        await task\n\n\ndef utc_difference():\n    \"\"\"Get the difference in seconds between the current time and the current UTC time.\n\n    Returns:\n        int: the time difference in seconds.\n\n    \"\"\"\n    delta = datetime.now() - datetime.utcnow()\n    return delta.seconds\n\n\ndef generate_certificate(config):\n    \"\"\"Create and sign a new certificate using the one defined in the complete hook\n    configuration as intermediate certificate.\n\n    Args:\n        config (krake.data.config.CompleteHookConfiguration): the configuration of the\n            complete hook.\n\n    Returns:\n        CertificatePair: the content of the certificate created and its corresponding\n            key.\n\n    \"\"\"\n    with open(config.intermediate_src, \"rb\") as f:\n        intermediate_src = crypto.load_certificate(crypto.FILETYPE_PEM, f.read())\n    with open(config.intermediate_key_src, \"rb\") as f:\n        intermediate_key_src = crypto.load_privatekey(crypto.FILETYPE_PEM, f.read())\n\n    client_cert = crypto.X509()\n\n    # Set general information\n    client_cert.set_version(3)\n    client_cert.set_serial_number(random.randint(50000000000000, 100000000000000))\n    # If not set before, TLS will not accept to use this certificate in UTC cases, as\n    # the server time may be earlier.\n    time_offset = utc_difference() * -1\n    client_cert.gmtime_adj_notBefore(time_offset)\n    client_cert.gmtime_adj_notAfter(1 * 365 * 24 * 60 * 60)\n\n    # Set issuer and subject\n    intermediate_subject = intermediate_src.get_subject()\n    client_cert.set_issuer(intermediate_subject)\n    client_subj = crypto.X509Name(intermediate_subject)\n    client_subj.CN = config.hook_user\n    client_cert.set_subject(client_subj)\n\n    # Create and set the private key\n    client_key = crypto.PKey()\n    client_key.generate_key(crypto.TYPE_RSA, 2048)\n    client_cert.set_pubkey(client_key)\n\n    client_cert.sign(intermediate_key_src, \"sha256\")\n\n    cert_dump = crypto.dump_certificate(crypto.FILETYPE_PEM, client_cert).decode()\n    key_dump = crypto.dump_privatekey(crypto.FILETYPE_PEM, client_key).decode()\n    return CertificatePair(cert=cert_dump, key=key_dump)\n\n\ndef generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n\n    app.status.mangled_observer_schema = deepcopy(app.spec.observer_schema)\n\n    for resource_manifest in app.spec.manifest:\n        try:\n            get_kubernetes_resource_idx(\n                app.status.mangled_observer_schema, resource_manifest\n            )\n\n        except IndexError:\n            # Only create a default observer schema, if a custom observer schema hasn't\n            # been set by the user.\n            app.status.mangled_observer_schema.append(\n                generate_default_observer_schema_dict(\n                    resource_manifest,\n                    first_level=True,\n                )\n            )\n\n\ndef generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n\n    \"\"\"\n    observer_schema_dict = {}\n\n    for key, value in manifest_dict.items():\n\n        if isinstance(value, dict):\n            observer_schema_dict[key] = generate_default_observer_schema_dict(value)\n\n        elif isinstance(value, list):\n            observer_schema_dict[key] = generate_default_observer_schema_list(value)\n\n        else:\n            observer_schema_dict[key] = None\n\n    if first_level:\n        observer_schema_dict[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        observer_schema_dict[\"kind\"] = manifest_dict[\"kind\"]\n        observer_schema_dict[\"metadata\"][\"name\"] = manifest_dict[\"metadata\"][\"name\"]\n\n        if (\n            \"spec\" in manifest_dict\n            and \"type\" in manifest_dict[\"spec\"]\n            and manifest_dict[\"spec\"][\"type\"] == \"LoadBalancer\"\n        ):\n            observer_schema_dict[\"status\"] = {\"load_balancer\": {\"ingress\": None}}\n\n    return observer_schema_dict\n\n\ndef generate_default_observer_schema_list(manifest_list):\n    \"\"\"Together with :func:``generate_default_observer_schema_dict``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_list`` or ``manifest_dict``.\n\n    Args:\n        manifest_list (list): Partial Kubernetes resources\n\n    Returns:\n        list: Generated partial observer_schema\n\n    This function creates a new list from ``manifest_list`` and replaces all non-list\n    and non-dict elements by ``None``.\n\n    Additionally, it generates the default list control dictionary, using the current\n    length of the list as default minimum and maximum values.\n\n    \"\"\"\n    observer_schema_list = []\n\n    for value in manifest_list:\n\n        if isinstance(value, dict):\n            observer_schema_list.append(generate_default_observer_schema_dict(value))\n\n        elif isinstance(value, list):\n            observer_schema_list.append(generate_default_observer_schema_list(value))\n\n        else:\n            observer_schema_list.append(None)\n\n    observer_schema_list.append(\n        {\n            \"observer_schema_list_min_length\": len(manifest_list),\n            \"observer_schema_list_max_length\": len(manifest_list),\n        }\n    )\n\n    return observer_schema_list\n\n\n@listen.on(HookType.ApplicationMangling)\nasync def complete(app, api_endpoint, ssl_context, config):\n    \"\"\"Execute application complete hook defined by :class:`Complete`.\n    Hook mangles given application and injects complete hooks variables.\n\n    Application complete hook is disabled by default.\n    User enables this hook by the --hook-complete argument in rok cli.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application object processed\n            when the hook is called\n        api_endpoint (str): the given API endpoint\n        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint\n        config (krake.data.config.HooksConfiguration): Complete hook\n            configuration.\n\n    \"\"\"\n    if \"complete\" not in app.spec.hooks:\n        return\n\n    # Use the endpoint of the API only if the external endpoint has not been set.\n    if config.complete.external_endpoint:\n        api_endpoint = config.complete.external_endpoint\n\n    app.status.complete_token = \\\n        app.status.complete_token if app.status.complete_token else token_urlsafe()\n\n    # Generate only once the certificate and key for a specific Application\n    generated_cert = CertificatePair(\n        cert=app.status.complete_cert, key=app.status.complete_key\n    )\n    if ssl_context and generated_cert == (None, None):\n        generated_cert = generate_certificate(config.complete)\n        app.status.complete_cert = generated_cert.cert\n        app.status.complete_key = generated_cert.key\n\n    hook = Complete(\n        api_endpoint,\n        ssl_context,\n        hook_user=config.complete.hook_user,\n        cert_dest=config.complete.cert_dest,\n        env_token=config.complete.env_token,\n        env_url=config.complete.env_url,\n    )\n    hook.mangle_app(\n        app.metadata.name,\n        app.metadata.namespace,\n        app.status.complete_token,\n        app.status.last_applied_manifest,\n        config.complete.intermediate_src,\n        generated_cert,\n        app.status.mangled_observer_schema,\n        \"complete\"\n    )\n\n\n@listen.on(HookType.ApplicationMangling)\nasync def shutdown(app, api_endpoint, ssl_context, config):\n    \"\"\"Executes an application shutdown hook defined by :class:`Shutdown`.\n    The hook mangles the given application and injects shutdown hooks variables.\n\n    Application shutdown hook is disabled by default.\n    User enables this hook by the --hook-shutdown argument in rok cli.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application object processed\n            when the hook is called\n        api_endpoint (str): the given API endpoint\n        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint\n        config (krake.data.config.HooksConfiguration): Shutdown hook\n            configuration.\n\n    \"\"\"\n    if \"shutdown\" not in app.spec.hooks:\n        return\n\n    # Use the endpoint of the API only if the external endpoint has not been set.\n    if config.shutdown.external_endpoint:\n        api_endpoint = config.shutdown.external_endpoint\n\n    app.status.shutdown_token = \\\n        app.status.shutdown_token if app.status.shutdown_token else token_urlsafe()\n\n    # Generate only once the certificate and key for a specific Application\n    generated_cert = CertificatePair(\n        cert=app.status.shutdown_cert, key=app.status.shutdown_key\n    )\n    if ssl_context and generated_cert == (None, None):\n        generated_cert = generate_certificate(config.shutdown)\n        app.status.shutdown_cert = generated_cert.cert\n        app.status.shutdown_key = generated_cert.key\n\n    hook = Shutdown(\n        api_endpoint,\n        ssl_context,\n        hook_user=config.shutdown.hook_user,\n        cert_dest=config.shutdown.cert_dest,\n        env_token=config.shutdown.env_token,\n        env_url=config.shutdown.env_url,\n    )\n    hook.mangle_app(\n        app.metadata.name,\n        app.metadata.namespace,\n        app.status.shutdown_token,\n        app.status.last_applied_manifest,\n        config.shutdown.intermediate_src,\n        generated_cert,\n        app.status.mangled_observer_schema,\n        \"shutdown\"\n    )\n\n\n@listen.on(HookType.ResourcePreDelete)\nasync def pre_shutdown(controller, app, **kwargs):\n    \"\"\"\n\n    Args:\n        app (krake.data.kubernetes.Application): Application object processed\n            when the hook is called\n    \"\"\"\n    if \"shutdown\" not in app.spec.hooks:\n        return\n\n    return\n\n\nclass SubResource(NamedTuple):\n    group: str\n    name: str\n    body: dict\n    path: tuple\n\n\nclass CertificatePair(NamedTuple):\n    \"\"\"Tuple which contains a certificate and its corresponding key.\n\n    Attributes:\n        cert (str): content of a certificate.\n        key (str): content of the key that corresponds to the certificate.\n\n    \"\"\"\n\n    cert: str\n    key: str\n\n\nclass Hook(object):\n\n    hook_resources = ()\n\n    ca_name = \"ca-bundle.pem\"\n    cert_name = \"cert.pem\"\n    key_name = \"key.pem\"\n\n    def __init__(\n        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n    ):\n        self.api_endpoint = api_endpoint\n        self.ssl_context = ssl_context\n        self.hook_user = hook_user\n        self.cert_dest = cert_dest\n        self.env_token = env_token\n        self.env_url = env_url\n\n    def mangle_app(\n        self,\n        name,\n        namespace,\n        token,\n        last_applied_manifest,\n        intermediate_src,\n        generated_cert,\n        mangled_observer_schema,\n        hook_type=\"\",\n    ):\n        \"\"\"Mangle a given application and inject complete hook resources and\n        sub-resources into the :attr:`last_applied_manifest` object by :meth:`mangle`.\n        Also mangle the observer_schema as new resources and sub-resources should\n        be observed.\n\n        :attr:`last_applied_manifest` is created as a deep copy of the desired\n        application resources, as defined by user. It can be updated by custom hook\n        resources or modified by custom hook sub-resources. It is used as a desired\n        state for the Krake deployment process.\n\n        Args:\n            name (str): Application name\n            namespace (str): Application namespace\n            token (str): Complete hook authentication token\n            last_applied_manifest (list): Application resources\n            intermediate_src (str): content of the certificate that is used to sign new\n                certificates for the complete hook.\n            generated_cert (CertificatePair): tuple that contains the content of the\n                new signed certificate for the Application, and the content of its\n                corresponding key.\n            mangled_observer_schema (list): Observed fields\n            hook_type (str, optional): Name of the hook the app should be mangled for\n\n        \"\"\"\n\n        secret_certs_name = \"-\".join([name, \"krake\", hook_type, \"secret\", \"certs\"])\n        secret_token_name = \"-\".join([name, \"krake\", hook_type, \"secret\", \"token\"])\n        volume_name = \"-\".join([name, \"krake\", hook_type, \"volume\"])\n        ca_certs = (\n            self.ssl_context.get_ca_certs(binary_form=True)\n            if self.ssl_context\n            else None\n        )\n\n        # Extract all different namespaces\n        # FIXME: too many assumptions here: do we create one ConfigMap for each\n        #  namespace?\n        resource_namespaces = {\n            resource[\"metadata\"].get(\"namespace\", \"default\")\n            for resource in last_applied_manifest\n        }\n\n        hook_resources = []\n        hook_sub_resources = []\n        if ca_certs:\n            hook_resources.extend(\n                [\n                    self.secret_certs(\n                        secret_certs_name,\n                        resource_namespace,\n                        intermediate_src=intermediate_src,\n                        generated_cert=generated_cert,\n                        ca_certs=ca_certs,\n                    )\n                    for resource_namespace in resource_namespaces\n                ]\n            )\n            hook_sub_resources.extend(\n                [*self.volumes(secret_certs_name, volume_name, self.cert_dest)]\n            )\n\n        hook_resources.extend(\n            [\n                self.secret_token(\n                    secret_token_name,\n                    name,\n                    namespace,\n                    resource_namespace,\n                    self.api_endpoint,\n                    token,\n                )\n                for resource_namespace in resource_namespaces\n            ]\n        )\n        hook_sub_resources.extend(\n            [\n                *self.env_vars(secret_token_name),\n            ]\n        )\n\n        self.mangle(\n            hook_resources,\n            last_applied_manifest,\n            mangled_observer_schema,\n        )\n        self.mangle(\n            hook_sub_resources,\n            last_applied_manifest,\n            mangled_observer_schema,\n            is_sub_resource=True,\n        )\n\n    def mangle(\n        self,\n        items,\n        last_applied_manifest,\n        mangled_observer_schema,\n        is_sub_resource=False,\n    ):\n        \"\"\"Mangle applications desired state with custom hook resources or\n        sub-resources.\n\n        Example:\n            .. code:: python\n\n            last_applied_manifest = [\n                {\n                    'apiVersion': 'v1',\n                    'kind': 'Pod',\n                    'metadata': {'name': 'test', 'namespace': 'default'},\n                    'spec': {'containers': [{'name': 'test'}]}\n                }\n            ]\n            mangled_observer_schema = [\n                {\n                    'apiVersion': 'v1',\n                    'kind': 'Pod',\n                    'metadata': {'name': 'test', 'namespace': 'default'},\n                    'spec': {\n                        'containers': [\n                            {'name': None},\n                            {\n                                'observer_schema_list_max_length': 1,\n                                'observer_schema_list_min_length': 1,\n                            },\n                        ]\n                    },\n                }\n            ]\n            hook_resources = [\n                {\n                    'apiVersion': 'v1',\n                    'kind': 'Secret',\n                    'metadata': {'name': 'sct', 'namespace': 'default'}\n                }\n            ]\n            hook_sub_resources = [\n                SubResource(\n                    group='env', name='env', body={'name': 'test', 'value': 'test'},\n                    path=(('spec', 'containers'),)\n                )\n            ]\n\n            mangle(\n                hook_resources,\n                last_applied_manifest,\n                mangled_observer_schema,\n            )\n            mangle(\n                hook_sub_resources,\n                last_applied_manifest,\n                mangled_observer_schema,\n                is_sub_resource=True\n            )\n\n            assert last_applied_manifest == [\n                {\n                    \"apiVersion\": \"v1\",\n                    \"kind\": \"Pod\",\n                    \"metadata\": {\"name\": \"test\", 'namespace': 'default'},\n                    \"spec\": {\n                        \"containers\": [\n                            {\n                                \"name\": \"test\",\n                                \"env\": [{\"name\": \"test\", \"value\": \"test\"}]\n                            }\n                        ]\n                    },\n                },\n                {\"apiVersion\": \"v1\", \"kind\": \"Secret\", \"metadata\": {\"name\": \"sct\"}},\n            ]\n\n            assert mangled_observer_schema == [\n                {\n                    \"apiVersion\": \"v1\",\n                    \"kind\": \"Pod\",\n                    \"metadata\": {\"name\": \"test\", \"namespace\": None},\n                    \"spec\": {\n                        \"containers\": [\n                            {\n                                \"name\": None,\n                                \"env\": [\n                                    {\"name\": None, \"value\": None},\n                                    {\n                                        \"observer_schema_list_max_length\": 1,\n                                        \"observer_schema_list_min_length\": 1,\n                                    },\n                                ],\n                            },\n                            {\n                                \"observer_schema_list_max_length\": 1,\n                                \"observer_schema_list_min_length\": 1,\n                            },\n                        ]\n                    },\n                },\n                {\n                    \"apiVersion\": \"v1\",\n                    \"kind\": \"Secret\",\n                    \"metadata\": {\"name\": \"sct\", \"namespace\": None},\n                },\n            ]\n\n        Args:\n            items (list[SubResource]): Custom hook resources or sub-resources\n            last_applied_manifest (list): Application resources\n            mangled_observer_schema (list): Observed resources\n            is_sub_resource (bool, optional): if False, the function only extend the\n                list of Kubernetes resources defined in :attr:`last_applied_manifest`\n                with new hook resources. Otherwise, the function injects each new hook\n                sub-resource into the :attr:`last_applied_manifest` object\n                sub-resources. Defaults to False.\n\n        \"\"\"\n\n        if not items:\n            return\n\n        if not is_sub_resource:\n            last_applied_manifest.extend(items)\n            for sub_resource in items:\n                # Generate the default observer schema for each resource\n                mangled_observer_schema.append(\n                    generate_default_observer_schema_dict(\n                        sub_resource,\n                        first_level=True,\n                    )\n                )\n            return\n\n        def inject(sub_resource, sub_resource_to_mangle, observed_resource_to_mangle):\n            \"\"\"Inject a hooks defined sub-resource into a Kubernetes sub-resource.\n\n            Args:\n                sub_resource (SubResource): Hook sub-resource that needs to be injected\n                    into :attr:`last_applied_manifest`\n                sub_resource_to_mangle (object): Kubernetes sub-resources from\n                    :attr:`last_applied_manifest` which need to be processed\n                observed_resource_to_mangle (dict): partial mangled_observer_schema\n                    corresponding to the Kubernetes sub-resource.\n\n            Raises:\n                InvalidManifestError: if the sub-resource which will be mangled is not a\n                    list or a dict.\n\n            \"\"\"\n\n            # Create sub-resource group if not present in the Kubernetes sub-resource\n            if sub_resource.group not in sub_resource_to_mangle:\n                # FIXME: This assumes the subresource group contains a list\n                sub_resource_to_mangle.update({sub_resource.group: []})\n\n            # Create sub-resource group if not present in the observed fields\n            if sub_resource.group not in observed_resource_to_mangle:\n                observed_resource_to_mangle.update(\n                    {\n                        sub_resource.group: [\n                            {\n                                \"observer_schema_list_min_length\": 0,\n                                \"observer_schema_list_max_length\": 0,\n                            }\n                        ]\n                    }\n                )\n\n            # Inject sub-resource\n            # If sub-resource name is already there update it, if not, append it\n            if sub_resource.name in [\n                g[\"name\"] for g in sub_resource_to_mangle[sub_resource.group]\n            ]:\n                # FIXME: Assuming we are dealing with a list\n                for idx, item in enumerate(sub_resource_to_mangle[sub_resource.group]):\n                    if item[\"name\"]:\n                        if hasattr(item, \"body\"):\n                            sub_resource_to_mangle[item.group][idx] = item[\"body\"]\n            else:\n                sub_resource_to_mangle[sub_resource.group].append(sub_resource.body)\n\n            # Make sure the value is observed\n            if sub_resource.name not in [\n                g[\"name\"] for g in observed_resource_to_mangle[sub_resource.group][:-1]\n            ]:\n                observed_resource_to_mangle[sub_resource.group].insert(\n                    -1, generate_default_observer_schema_dict(sub_resource.body)\n                )\n                observed_resource_to_mangle[sub_resource.group][-1][\n                    \"observer_schema_list_min_length\"\n                ] += 1\n                observed_resource_to_mangle[sub_resource.group][-1][\n                    \"observer_schema_list_max_length\"\n                ] += 1\n\n        for resource in last_applied_manifest:\n            # Complete hook is applied only on defined Kubernetes resources\n            if resource[\"kind\"] not in self.hook_resources:\n                continue\n\n            for sub_resource in items:\n                sub_resources_to_mangle = None\n                idx_observed = get_kubernetes_resource_idx(\n                    mangled_observer_schema, resource\n                )\n                for keys in sub_resource.path:\n                    try:\n                        sub_resources_to_mangle = reduce(getitem, keys, resource)\n                    except KeyError:\n                        continue\n\n                    break\n\n                # Create the path to the observed sub-resource, if it doesn't yet exist\n                try:\n                    observed_sub_resources = reduce(\n                        getitem, keys, mangled_observer_schema[idx_observed]\n                    )\n                except KeyError:\n                    Complete.create_path(\n                        mangled_observer_schema[idx_observed], list(keys)\n                    )\n                    observed_sub_resources = reduce(\n                        getitem, keys, mangled_observer_schema[idx_observed]\n                    )\n\n                if isinstance(sub_resources_to_mangle, list):\n                    for idx, sub_resource_to_mangle in enumerate(\n                        sub_resources_to_mangle\n                    ):\n\n                        # Ensure that each element of the list is observed.\n                        idx_observed = idx\n                        if idx >= len(observed_sub_resources[:-1]):\n                            idx_observed = len(observed_sub_resources[:-1])\n                            # FIXME: Assuming each element of the list contains a\n                            # dictionary, therefore initializing new elements with an\n                            # empty dict\n                            observed_sub_resources.insert(-1, {})\n                        observed_sub_resource = observed_sub_resources[idx_observed]\n\n                        # FIXME: This is assuming a list always contains dict\n                        inject(\n                            sub_resource, sub_resource_to_mangle, observed_sub_resource\n                        )\n\n                elif isinstance(sub_resources_to_mangle, dict):\n                    inject(\n                        sub_resource, sub_resources_to_mangle, observed_sub_resources\n                    )\n\n                else:\n                    message = (\n                        f\"The sub-resource to mangle {sub_resources_to_mangle!r} has an\"\n                        \"invalid type, should be in '[dict, list]'\"\n                    )\n                    raise InvalidManifestError(message)\n\n    @staticmethod\n    def attribute_map(obj):\n        \"\"\"Convert a Kubernetes object to dict based on its attribute mapping\n\n        Example:\n            .. code:: python\n\n            from kubernetes_asyncio.client import V1VolumeMount\n\n            d = attribute_map(\n                    V1VolumeMount(name=\"name\", mount_path=\"path\")\n            )\n            assert d == {'mountPath': 'path', 'name': 'name'}\n\n        Args:\n            obj (object): Kubernetes object\n\n        Returns:\n            dict: Converted Kubernetes object\n\n        \"\"\"\n        return {\n            obj.attribute_map[attr]: getattr(obj, attr)\n            for attr, _ in obj.to_dict().items()\n            if getattr(obj, attr) is not None\n        }\n\n    @staticmethod\n    def create_path(mangled_observer_schema, keys):\n        \"\"\"Create the path to the observed field in the observer schema.\n\n        When a sub-resource is mangled, it should be observed. This function creates\n        the path to the subresource to observe.\n\n        Args:\n            mangled_observer_schema (dict): Partial observer schema of a resource\n            keys (list): list of keys forming the path to the sub-resource to\n                observe\n\n        FIXME: This assumes we are only adding keys to dict. We don't consider lists\n\n        \"\"\"\n\n        # Unpack the first key first, as it contains the base directory\n        key = keys.pop(0)\n\n        # If the key is the last of the list, we reached the end of the path.\n        if len(keys) == 0:\n            mangled_observer_schema[key] = None\n            return\n\n        if key not in mangled_observer_schema:\n            mangled_observer_schema[key] = {}\n        Hook.create_path(mangled_observer_schema[key], keys)\n\n    def secret_certs(\n        self,\n        secret_name,\n        namespace,\n        ca_certs=None,\n        intermediate_src=None,\n        generated_cert=None,\n    ):\n        \"\"\"Create a complete hooks secret resource.\n\n        Complete hook secret stores Krake CAs and client certificates to communicate\n        with the Krake API.\n\n        Args:\n            secret_name (str): Secret name\n            namespace (str): Kubernetes namespace where the Secret will be created.\n            ca_certs (list): Krake CA list\n            intermediate_src (str): content of the certificate that is used to sign new\n                certificates for the complete hook.\n            generated_cert (CertificatePair): tuple that contains the content of the\n                new signed certificate for the Application, and the content of its\n                corresponding key.\n\n        Returns:\n            dict: complete hook secret resource\n\n        \"\"\"\n        ca_certs_pem = \"\"\n        for ca_cert in ca_certs:\n            x509 = crypto.load_certificate(crypto.FILETYPE_ASN1, ca_cert)\n            ca_certs_pem += crypto.dump_certificate(crypto.FILETYPE_PEM, x509).decode()\n\n        # Add the intermediate certificate into the chain\n        with open(intermediate_src, \"r\") as f:\n            intermediate_src_content = f.read()\n        ca_certs_pem += intermediate_src_content\n\n        data = {\n            self.ca_name: self._encode_to_64(ca_certs_pem),\n            self.cert_name: self._encode_to_64(generated_cert.cert),\n            self.key_name: self._encode_to_64(generated_cert.key),\n        }\n        return self.secret(secret_name, data, namespace)\n\n    def secret_token(\n        self, secret_name, name, namespace, resource_namespace, api_endpoint, token\n    ):\n        \"\"\"Create a hooks secret resource.\n\n        The hook secret stores Krake authentication token\n        and hook URL for given application.\n\n        Args:\n            secret_name (str): Secret name\n            name (str): Application name\n            namespace (str): Application namespace\n            resource_namespace (str): Kubernetes namespace where the\n                Secret will be created.\n            api_endpoint (str): Krake API endpoint\n            token (str): Complete hook authentication token\n\n        Returns:\n            dict: complete hook secret resource\n\n        \"\"\"\n        pass\n\n    def volumes(self, secret_name, volume_name, mount_path):\n        \"\"\"Create complete hooks volume and volume mount sub-resources\n\n        Complete hook volume gives access to hook's secret, which stores\n        Krake CAs and client certificates to communicate with the Krake API.\n        Complete hook volume mount puts the volume into the application\n\n        Args:\n            secret_name (str): Secret name\n            volume_name (str): Volume name\n            mount_path (list): Volume mount path\n\n        Returns:\n            list: List of complete hook volume and volume mount sub-resources\n\n        \"\"\"\n        volume = V1Volume(name=volume_name, secret={\"secretName\": secret_name})\n        volume_mount = V1VolumeMount(name=volume_name, mount_path=mount_path)\n        return [\n            SubResource(\n                group=\"volumes\",\n                name=volume.name,\n                body=self.attribute_map(volume),\n                path=((\"spec\", \"template\", \"spec\"), (\"spec\",)),\n            ),\n            SubResource(\n                group=\"volumeMounts\",\n                name=volume_mount.name,\n                body=self.attribute_map(volume_mount),\n                path=(\n                    (\"spec\", \"template\", \"spec\", \"containers\"),\n                    (\"spec\", \"containers\"),  # kind: Pod\n                ),\n            ),\n        ]\n\n    @staticmethod\n    def _encode_to_64(string):\n        \"\"\"Compute the base 64 encoding of a string.\n\n        Args:\n            string (str): the string to encode.\n\n        Returns:\n            str: the result of the encoding.\n\n        \"\"\"\n        return b64encode(string.encode()).decode()\n\n    def secret(self, secret_name, secret_data, namespace, _type=\"Opaque\"):\n        \"\"\"Create a secret resource.\n\n        Args:\n            secret_name (str): Secret name\n            secret_data (dict): Secret data\n            namespace (str): Kubernetes namespace where the Secret will be created.\n            _type (str, optional): Secret type. Defaults to Opaque.\n\n        Returns:\n            dict: secret resource\n\n        \"\"\"\n        return self.attribute_map(\n            V1Secret(\n                api_version=\"v1\",\n                kind=\"Secret\",\n                data=secret_data,\n                metadata={\"name\": secret_name, \"namespace\": namespace},\n                type=_type,\n            )\n        )\n\n    @staticmethod\n    def create_hook_url(name, namespace, api_endpoint):\n        \"\"\"Create an applications' hook URL.\n        Function needs to be specified for each hook.\n\n        Args:\n            name (str): Application name\n            namespace (str): Application namespace\n            api_endpoint (str): Krake API endpoint\n\n        Returns:\n            str: Application shutdown url\n\n        \"\"\"\n        pass\n\n    def env_vars(self, secret_name):\n        \"\"\"Create the hooks' environment variables sub-resources.\n        Function needs to be specified for each hook.\n\n        Creates hook environment variables to store Krake authentication token\n        and a hook URL for the given applications.\n\n        Args:\n            secret_name (str): Secret name\n\n        Returns:\n            list: List of shutdown hook environment variables sub-resources\n\n        \"\"\"\n        pass\n\n\nclass Complete(Hook):\n    \"\"\"Mangle given application and inject complete hooks variables into it.\n\n    Hook injects a Kubernetes secret, which stores Krake authentication token\n    and the Krake complete hook URL for the given application. The variables\n    from Kubernetes secret are imported as environment variables\n    into the application resource definition. Only resources defined in\n    :args:`hook_resources` can be modified.\n\n    Names of environment variables are defined in the application controller\n    configuration file.\n\n    If TLS is enabled on the Krake API, the complete hook injects a Kubernetes secret,\n    and it's corresponding volume and volume mount definitions for the Krake CA,\n    the client certificate with the right CN, and its key. The directory where the\n    secret is mounted is defined in the configuration.\n\n    Args:\n        api_endpoint (str): the given API endpoint\n        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint\n        cert_dest (str, optional): Path of the directory where the CA, client\n            certificate and key to the Krake API will be stored.\n        env_token (str, optional): Name of the environment variable, which stores Krake\n            authentication token.\n        env_url (str, optional): Name of the environment variable,\n            which stores Krake complete hook URL.\n\n    \"\"\"\n\n    hook_resources = (\"Pod\", \"Deployment\", \"ReplicationController\")\n\n    def __init__(\n        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n    ):\n        super().__init__(\n            api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n        )\n        self.env_url = env_url\n\n    def secret_token(\n        self, secret_name, name, namespace, resource_namespace, api_endpoint, token\n    ):\n        \"\"\"Create complete hooks secret resource.\n\n        Complete hook secret stores Krake authentication token\n        and complete hook URL for given application.\n\n        Args:\n            secret_name (str): Secret name\n            name (str): Application name\n            namespace (str): Application namespace\n            resource_namespace (str): Kubernetes namespace where the\n                Secret will be created.\n            api_endpoint (str): Krake API endpoint\n            token (str): Complete hook authentication token\n\n        Returns:\n            dict: complete hook secret resource\n\n        \"\"\"\n        complete_url = self.create_hook_url(name, namespace, api_endpoint)\n        data = {\n            self.env_token.lower(): self._encode_to_64(token),\n            self.env_url.lower(): self._encode_to_64(complete_url),\n        }\n        return self.secret(secret_name, data, resource_namespace)\n\n    @staticmethod\n    def create_hook_url(name, namespace, api_endpoint):\n        \"\"\"Create an applications' complete URL.\n\n        Args:\n            name (str): Application name\n            namespace (str): Application namespace\n            api_endpoint (str): Krake API endpoint\n\n        Returns:\n            str: Application complete url\n\n        \"\"\"\n        api_url = URL(api_endpoint)\n        return str(\n            api_url.with_path(\n                f\"/kubernetes/namespaces/{namespace}/applications/{name}/complete\"\n            )\n        )\n\n    def env_vars(self, secret_name):\n        \"\"\"Create complete hooks environment variables sub-resources\n\n        Create complete hook environment variables store Krake authentication token\n        and complete hook URL for given application.\n\n        Args:\n            secret_name (str): Secret name\n\n        Returns:\n            list: List of complete hook environment variables sub-resources\n\n        \"\"\"\n        sub_resources = []\n\n        env_token = V1EnvVar(\n            name=self.env_token,\n            value_from=self.attribute_map(\n                V1EnvVarSource(\n                    secret_key_ref=self.attribute_map(\n                        V1SecretKeySelector(\n                            name=secret_name, key=self.env_token.lower()\n                        )\n                    )\n                )\n            ),\n        )\n        env_url = V1EnvVar(\n            name=self.env_url,\n            value_from=self.attribute_map(\n                V1EnvVarSource(\n                    secret_key_ref=self.attribute_map(\n                        V1SecretKeySelector(name=secret_name, key=self.env_url.lower())\n                    )\n                )\n            ),\n        )\n\n        for env in (env_token, env_url):\n            sub_resources.append(\n                SubResource(\n                    group=\"env\",\n                    name=env.name,\n                    body=self.attribute_map(env),\n                    path=(\n                        (\"spec\", \"template\", \"spec\", \"containers\"),\n                        (\"spec\", \"containers\"),  # kind: Pod\n                    ),\n                )\n            )\n        return sub_resources\n\n\nclass Shutdown(Hook):\n    \"\"\"Mangle given application and inject shutdown hooks variables into it.\n\n    Hook injects a Kubernetes secret, which stores Krake authentication token\n    and the Krake complete hook URL for the given application. The variables\n    from the Kubernetes secret are imported as environment variables\n    into the application resource definition. Only resources defined in\n    :args:`hook_resources` can be modified.\n\n    Names of environment variables are defined in the application controller\n    configuration file.\n\n    If TLS is enabled on the Krake API, the shutdown hook injects a Kubernetes secret,\n    and it's corresponding volume and volume mount definitions for the Krake CA,\n    the client certificate with the right CN, and its key. The directory where the\n    secret is mounted is defined in the configuration.\n\n    Args:\n        api_endpoint (str): the given API endpoint\n        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint\n        cert_dest (str, optional): Path of the directory where the CA, client\n            certificate and key to the Krake API will be stored.\n        env_token (str, optional): Name of the environment variable, which stores Krake\n            authentication token.\n        env_url (str, optional): Name of the environment variable,\n            which stores Krake complete hook URL.\n\n    \"\"\"\n\n    hook_resources = (\"Pod\", \"Deployment\", \"ReplicationController\")\n\n    def __init__(\n        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n    ):\n        super().__init__(\n            api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n        )\n        self.env_url = env_url\n\n    def secret_token(\n        self, secret_name, name, namespace, resource_namespace, api_endpoint, token\n    ):\n        \"\"\"Create shutdown hooks secret resource.\n\n        Shutdown hook secret stores Krake authentication token\n        and shutdown hook URL for given application.\n\n        Args:\n            secret_name (str): Secret name\n            name (str): Application name\n            namespace (str): Application namespace\n            resource_namespace (str): Kubernetes namespace where the\n                Secret will be created.\n            api_endpoint (str): Krake API endpoint\n            token (str): Shutdown hook authentication token\n\n        Returns:\n            dict: shutdown hook secret resource\n\n        \"\"\"\n        shutdown_url = self.create_hook_url(name, namespace, api_endpoint)\n        data = {\n            self.env_token.lower(): self._encode_to_64(token),\n            self.env_url.lower(): self._encode_to_64(shutdown_url),\n        }\n        return self.secret(secret_name, data, resource_namespace)\n\n    @staticmethod\n    def create_hook_url(name, namespace, api_endpoint):\n        \"\"\"Create an applications' shutdown URL.\n\n        Args:\n            name (str): Application name\n            namespace (str): Application namespace\n            api_endpoint (str): Krake API endpoint\n\n        Returns:\n            str: Application shutdown url\n\n        \"\"\"\n        api_url = URL(api_endpoint)\n        return str(\n            api_url.with_path(\n                f\"/kubernetes/namespaces/{namespace}/applications/{name}/shutdown\"\n            )\n        )\n\n    def env_vars(self, secret_name):\n        \"\"\"Create shutdown hooks environment variables sub-resources.\n\n        Creates shutdown hook environment variables to store Krake authentication token\n        and a shutdown hook URL for given applications.\n\n        Args:\n            secret_name (str): Secret name\n\n        Returns:\n            list: List of shutdown hook environment variables sub-resources\n\n        \"\"\"\n        sub_resources = []\n\n        env_resources = []\n\n        env_token = V1EnvVar(\n            name=self.env_token,\n            value_from=self.attribute_map(\n                V1EnvVarSource(\n                    secret_key_ref=self.attribute_map(\n                        V1SecretKeySelector(\n                            name=secret_name,\n                            key=self.env_token.lower()\n                        )\n                    )\n                )\n            )\n        )\n        env_resources.append(env_token)\n\n        env_url = V1EnvVar(\n            name=self.env_url,\n            value_from=self.attribute_map(\n                V1EnvVarSource(\n                    secret_key_ref=self.attribute_map(\n                        V1SecretKeySelector(name=secret_name, key=self.env_url.lower())\n                    )\n                )\n            ),\n        )\n        env_resources.append(env_url)\n\n        for env in env_resources:\n            sub_resources.append(\n                SubResource(\n                    group=\"env\",\n                    name=env.name,\n                    body=self.attribute_map(env),\n                    path=(\n                        (\"spec\", \"template\", \"spec\", \"containers\"),\n                        (\"spec\", \"containers\"),  # kind: Pod\n                    ),\n                )\n            )\n        return sub_resources\n",
            "file_path": "krake/krake/controller/kubernetes/hooks.py",
            "human_label": "The values corresponding to different keys in the new dict are generated based on the value type (such as dict and list) in the manifest_dict file. Then new dictionary is returned.",
            "level": "file_runnable",
            "lineno": "1040",
            "name": "generate_default_observer_schema_dict",
            "oracle_context": "{ \"apis\" : \"['isinstance', 'items', 'generate_default_observer_schema_list']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }",
            "package": "hooks",
            "project": "rak-n-rok/Krake",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b869ebb4d922cb0e688cc6",
            "all_context": "{ \"import\" : \"inspect logging asyncio collections secrets operator base64 contextlib enum functools datetime typing random copy logging aiohttp asyncio secrets enum functools krake datetime typing yarl \", \"file\" : \"logger ; listen ; update_last_applied_manifest_dict_from_resp(last_applied_manifest,observer_schema,response) ; update_last_applied_manifest_list_from_resp(last_applied_manifest,observer_schema,response) ; update_last_applied_manifest_from_resp(app,response) ; update_last_observed_manifest_from_resp(app,response) ; update_last_observed_manifest_dict(observed_resource,response) ; update_last_observed_manifest_list(observed_resource,response) ; update_last_applied_manifest_dict_from_spec(resource_status_new,resource_status_old,resource_observed) ; update_last_applied_manifest_list_from_spec(resource_status_new,resource_status_old,resource_observed) ; update_last_applied_manifest_from_spec(app) ; utc_difference() ; generate_certificate(config) ; generate_default_observer_schema(app) ; generate_default_observer_schema_dict(manifest_dict,first_level) ; generate_default_observer_schema_list(manifest_list) ; \", \"class\" : \"\" }",
            "code": "def update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n\n    \"\"\"\n    # Looping over the observed resource, except the last element which is the\n    # special control dictionary\n    for idx, val in enumerate(observer_schema[:-1]):\n\n        if idx >= len(response):\n            # Element is observed but not present in k8s response, so following\n            # elements will also not exist.\n            #\n            # This doesn't raise an Exception as observing the element of a list\n            # doesn't ensure its presence. The list length is controlled by the\n            # special control dictionary\n            return\n\n        if isinstance(val, dict):\n            if idx >= len(last_applied_manifest):\n                # The dict is observed, but not present in last_applied_manifest\n                last_applied_manifest.append({})\n\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n\n        elif isinstance(response[idx], list):\n            if idx >= len(last_applied_manifest):\n                # The list is observed, but not present in last_applied_manifest\n                last_applied_manifest.append([])\n\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n\n        elif idx >= len(last_applied_manifest):\n            # Element is not yet present in last_applied_manifest. Adding it.\n            last_applied_manifest.append(response[idx])\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Together with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present",
            "end_lineno": "351",
            "file_content": "\"\"\"This module defines the Hook Dispatcher and listeners for registering and\nexecuting hooks. Hook Dispatcher emits hooks based on :class:`Hook` attributes which\ndefine when the hook will be executed.\n\n\"\"\"\nimport asyncio\nimport logging\nimport random\nfrom base64 import b64encode\nfrom collections import defaultdict\nfrom contextlib import suppress\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom functools import reduce\nfrom operator import getitem\nfrom enum import Enum, auto\nfrom inspect import iscoroutinefunction\nfrom OpenSSL import crypto\nfrom typing import NamedTuple\n\nimport yarl\nfrom aiohttp import ClientConnectorError\n\nfrom krake.controller import Observer\nfrom krake.controller.kubernetes.client import KubernetesClient, InvalidManifestError\nfrom krake.utils import camel_to_snake_case, get_kubernetes_resource_idx\nfrom kubernetes_asyncio.client.rest import ApiException\nfrom kubernetes_asyncio.client.api_client import ApiClient\nfrom kubernetes_asyncio import client\nfrom krake.data.kubernetes import ClusterState, Application, Cluster\nfrom yarl import URL\nfrom secrets import token_urlsafe\n\nfrom kubernetes_asyncio.client import (\n    Configuration,\n    V1Secret,\n    V1EnvVar,\n    V1VolumeMount,\n    V1Volume,\n    V1SecretKeySelector,\n    V1EnvVarSource,\n)\nfrom kubernetes_asyncio.config.kube_config import KubeConfigLoader\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass HookType(Enum):\n    ResourcePreCreate = auto()\n    ResourcePostCreate = auto()\n    ResourcePreUpdate = auto()\n    ResourcePostUpdate = auto()\n    ResourcePreDelete = auto()\n    ResourcePostDelete = auto()\n    ApplicationMangling = auto()\n    ApplicationPreMigrate = auto()\n    ApplicationPostMigrate = auto()\n    ApplicationPreReconcile = auto()\n    ApplicationPostReconcile = auto()\n    ApplicationPreDelete = auto()\n    ApplicationPostDelete = auto()\n    ClusterCreation = auto()\n    ClusterDeletion = auto()\n\n\nclass HookDispatcher(object):\n    \"\"\"Simple wrapper around a registry of handlers associated to :class:`Hook`\n     attributes. Each :class:`Hook` attribute defines when the handler will be\n     executed.\n\n    Listeners for certain hooks can be registered via :meth:`on`. Registered\n    listeners are executed via :meth:`hook`.\n\n    Example:\n        .. code:: python\n\n        listen = HookDispatcher()\n\n        @listen.on(HookType.PreApply)\n        def to_perform_before_app_creation(app, cluster, resource, controller):\n            # Do Stuff\n\n        @listen.on(HookType.PostApply)\n        def another_to_perform_after_app_creation(app, cluster, resource, resp):\n            # Do Stuff\n\n        @listen.on(HookType.PostDelete)\n        def to_perform_after_app_deletion(app, cluster, resource, resp):\n            # Do Stuff\n\n    \"\"\"\n\n    def __init__(self):\n        self.registry = defaultdict(list)\n\n    def on(self, hook):\n        \"\"\"Decorator function to add a new handler to the registry.\n\n        Args:\n            hook (HookType): Hook attribute for which to register the handler.\n\n        Returns:\n            callable: Decorator for registering listeners for the specified\n            hook.\n\n        \"\"\"\n\n        def decorator(handler):\n            self.registry[hook].append(handler)\n\n            return handler\n\n        return decorator\n\n    async def hook(self, hook, **kwargs):\n        \"\"\"Execute the list of handlers associated to the provided :class:`Hook`\n        attribute.\n\n        Args:\n            hook (HookType): The hook attribute for which to execute handlers.\n\n        \"\"\"\n        try:\n            handlers = self.registry[hook]\n        except KeyError:\n            pass\n        else:\n            for handler in handlers:\n                if iscoroutinefunction(handler):\n                    await handler(**kwargs)\n                else:\n                    handler(**kwargs)\n\n\nlisten = HookDispatcher()\n\n\n@listen.on(HookType.ResourcePostCreate)\n@listen.on(HookType.ResourcePostUpdate)\nasync def register_service(app, cluster, resource, response):\n    \"\"\"Register endpoint of Kubernetes Service object on creation and update.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        cluster (krake.data.kubernetes.Cluster): The cluster on which the\n            application is running\n        resource (dict): Kubernetes object description as specified in the\n            specification of the application.\n        response (kubernetes_asyncio.client.V1Service): Response of the\n            Kubernetes API\n\n    \"\"\"\n    if resource[\"kind\"] != \"Service\":\n        return\n\n    service_name = resource[\"metadata\"][\"name\"]\n\n    if response.spec and response.spec.type == \"LoadBalancer\":\n        # For a \"LoadBalancer\" type of Service, an external IP is given in the cluster\n        # by a load balancer controller to the service. In this case, the \"port\"\n        # specified in the spec is reachable from the outside.\n        if (\n            not response.status.load_balancer\n            or not response.status.load_balancer.ingress\n        ):\n            # When a \"LoadBalancer\" type of service is created, the IP is given by an\n            # additional controller (e.g. a controller that requests a floating IP to an\n            # OpenStack infrastructure). This process can take some time, but the\n            # Service itself already exist before the IP is assigned. In the case of an\n            # error with the controller, the IP is also not given. This \"<pending>\" IP\n            # just expresses that the Service exists, but the IP is not ready yet.\n            external_ip = \"<pending>\"\n        else:\n            external_ip = response.status.load_balancer.ingress[0].ip\n\n        if not response.spec.ports:\n            external_port = \"<pending>\"\n        else:\n            external_port = response.spec.ports[0].port\n        app.status.services[service_name] = f\"{external_ip}:{external_port}\"\n        return\n\n    node_port = None\n    # Ensure that ports are specified\n    if response.spec and response.spec.ports:\n        node_port = response.spec.ports[0].node_port\n\n    # If the service does not have a node port, remove a potential reference\n    # and return.\n    if node_port is None:\n        try:\n            del app.status.services[service_name]\n        except KeyError:\n            pass\n        return\n\n    # Determine URL of Kubernetes cluster API\n    loader = KubeConfigLoader(cluster.spec.kubeconfig)\n    config = Configuration()\n    await loader.load_and_set(config)\n    cluster_url = yarl.URL(config.host)\n\n    app.status.services[service_name] = f\"{cluster_url.host}:{node_port}\"\n\n\n@listen.on(HookType.ResourcePostDelete)\nasync def unregister_service(app, resource, **kwargs):\n    \"\"\"Unregister endpoint of Kubernetes Service object on deletion.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        resource (dict): Kubernetes object description as specified in the\n            specification of the application.\n\n    \"\"\"\n    if resource[\"kind\"] != \"Service\":\n        return\n\n    service_name = resource[\"metadata\"][\"name\"]\n    try:\n        del app.status.services[service_name]\n    except KeyError:\n        pass\n\n\n@listen.on(HookType.ResourcePostDelete)\nasync def remove_resource_from_last_observed_manifest(app, resource, **kwargs):\n    \"\"\"Remove a given resource from the last_observed_manifest after its deletion\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        resource (dict): Kubernetes object description as specified in the\n            specification of the application.\n\n    \"\"\"\n    try:\n        idx = get_kubernetes_resource_idx(app.status.last_observed_manifest, resource)\n    except IndexError:\n        return\n\n    app.status.last_observed_manifest.pop(idx)\n\n\ndef update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n\n    \"\"\"\n    for key, value in observer_schema.items():\n\n        # Keys in the response are in camelCase\n        camel_key = camel_to_snake_case(key)\n\n        if camel_key not in response:\n            # An observed key should always be present in the k8s response\n            raise KeyError(\n                f\"Observed key {camel_key} is not present in response {response}\"\n            )\n\n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                # The dictionary is observed, but not present in\n                # last_applied_manifest\n                last_applied_manifest[key] = {}\n\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], observer_schema[key], response[camel_key]\n            )\n\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                # The list is observed, but not present in last_applied_manifest\n                last_applied_manifest[key] = []\n\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], observer_schema[key], response[camel_key]\n            )\n\n        elif key not in last_applied_manifest:\n            # If key not present in last_applied_manifest, and value is neither a\n            # dict nor a list, simply add it.\n            last_applied_manifest[key] = response[camel_key]\n\n\ndef update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n\n    \"\"\"\n    # Looping over the observed resource, except the last element which is the\n    # special control dictionary\n    for idx, val in enumerate(observer_schema[:-1]):\n\n        if idx >= len(response):\n            # Element is observed but not present in k8s response, so following\n            # elements will also not exist.\n            #\n            # This doesn't raise an Exception as observing the element of a list\n            # doesn't ensure its presence. The list length is controlled by the\n            # special control dictionary\n            return\n\n        if isinstance(val, dict):\n            if idx >= len(last_applied_manifest):\n                # The dict is observed, but not present in last_applied_manifest\n                last_applied_manifest.append({})\n\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n\n        elif isinstance(response[idx], list):\n            if idx >= len(last_applied_manifest):\n                # The list is observed, but not present in last_applied_manifest\n                last_applied_manifest.append([])\n\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n\n        elif idx >= len(last_applied_manifest):\n            # Element is not yet present in last_applied_manifest. Adding it.\n            last_applied_manifest.append(response[idx])\n\n\n@listen.on(HookType.ResourcePostCreate)\n@listen.on(HookType.ResourcePostUpdate)\ndef update_last_applied_manifest_from_resp(app, response, **kwargs):\n    \"\"\"Hook run after the creation or update of an application in order to update the\n    `status.last_applied_manifest` using the k8s response.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        response (kubernetes_asyncio.client.V1Status): Response of the Kubernetes API\n\n    After a Kubernetes resource has been created/updated, the\n    `status.last_applied_manifest` has to be updated. All fields already initialized\n    (either from the mangling of `spec.manifest`, or by a previous call to this\n    function) should be left untouched. Only observed fields which are not present in\n    `status.last_applied_manifest` should be initialized.\n\n    \"\"\"\n\n    if isinstance(response, dict):\n        # The Kubernetes API couldn't deserialize the k8s response into an object\n        resp = response\n    else:\n        # The Kubernetes API deserialized the k8s response into an object\n        resp = response.to_dict()\n\n    idx_applied = get_kubernetes_resource_idx(app.status.last_applied_manifest, resp)\n\n    idx_observed = get_kubernetes_resource_idx(app.status.mangled_observer_schema, resp)\n\n    update_last_applied_manifest_dict_from_resp(\n        app.status.last_applied_manifest[idx_applied],\n        app.status.mangled_observer_schema[idx_observed],\n        resp,\n    )\n\n\n@listen.on(HookType.ResourcePostCreate)\n@listen.on(HookType.ResourcePostUpdate)\ndef update_last_observed_manifest_from_resp(app, response, **kwargs):\n    \"\"\"Handler to run after the creation or update of a Kubernetes resource to update\n    the last_observed_manifest from the response of the Kubernetes API.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        response (kubernetes_asyncio.client.V1Service): Response of the\n            Kubernetes API\n\n    The target last_observed_manifest holds the value of all observed fields plus the\n    special control dictionaries for the list length\n\n    \"\"\"\n    if isinstance(response, dict):\n        # The Kubernetes API couldn't deserialize the k8s response into an object\n        resp = response\n    else:\n        # The Kubernetes API deserialized the k8s response into an object\n        resp = response.to_dict()\n\n    try:\n        idx_observed = get_kubernetes_resource_idx(\n            app.status.mangled_observer_schema,\n            resp,\n        )\n    except IndexError:\n        # All created resources should be observed\n        raise\n\n    try:\n        idx_last_observed = get_kubernetes_resource_idx(\n            app.status.last_observed_manifest,\n            resp,\n        )\n    except IndexError:\n        # If the resource is not yes present in last_observed_manifest, append it.\n        idx_last_observed = len(app.status.last_observed_manifest)\n        app.status.last_observed_manifest.append({})\n\n    # Overwrite the last_observed_manifest for this resource\n    app.status.last_observed_manifest[\n        idx_last_observed\n    ] = update_last_observed_manifest_dict(\n        app.status.mangled_observer_schema[idx_observed], resp\n    )\n\n\ndef update_last_observed_manifest_dict(observed_resource, response):\n    \"\"\"Together with :func:``update_last_observed_manifest_list``, recursively\n    crafts the ``last_observed_manifest`` from the Kubernetes :attr:``response``.\n\n    Args:\n        observed_resource (dict): The schema to observe for the partial given resource\n        response (dict): The partial Kubernetes response for this resource.\n\n    Raises:\n        KeyError: If an observed key is not present in the Kubernetes response\n\n    Returns:\n        dict: The dictionary of observed keys and their value\n\n    Get the value of all observed fields from the Kubernetes response\n    \"\"\"\n    res = {}\n    for key, value in observed_resource.items():\n\n        camel_key = camel_to_snake_case(key)\n        if camel_key not in response:\n            raise KeyError(\n                f\"Observed key {camel_key} is not present in response {response}\"\n            )\n\n        if isinstance(value, dict):\n            res[key] = update_last_observed_manifest_dict(value, response[camel_key])\n\n        elif isinstance(value, list):\n            res[key] = update_last_observed_manifest_list(value, response[camel_key])\n\n        else:\n            res[key] = response[camel_key]\n\n    return res\n\n\ndef update_last_observed_manifest_list(observed_resource, response):\n    \"\"\"Together with :func:``update_last_observed_manifest_dict``, recursively\n    crafts the ``last_observed_manifest`` from the Kubernetes :attr:``response``.\n\n    Args:\n        observed_resource (list): the schema to observe for the partial given resource\n        response (list): the partial Kubernetes response for this resource.\n\n    Returns:\n        list: The list of observed elements, plus the special list length control\n            dictionary\n\n    Get the value of all observed elements from the Kubernetes response\n    \"\"\"\n\n    if not response:\n        return [{\"observer_schema_list_current_length\": 0}]\n\n    res = []\n    # Looping over the observed resource, except the last element which is the special\n    # control dictionary\n    for idx, val in enumerate(observed_resource[:-1]):\n\n        if idx >= len(response):\n            # Element is not present in the Kubernetes response, nothing more to do\n            break\n\n        if type(response[idx]) == dict:\n            res.append(update_last_observed_manifest_dict(val, response[idx]))\n\n        elif type(response[idx]) == list:\n            res.append(update_last_observed_manifest_list(val, response[idx]))\n\n        else:\n            res.append(response[idx])\n\n    # Append the special control dictionary to the list\n    res.append({\"observer_schema_list_current_length\": len(response)})\n\n    return res\n\n\ndef update_last_applied_manifest_dict_from_spec(\n    resource_status_new, resource_status_old, resource_observed\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_list_from_spec``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n\n    Args:\n        resource_status_new (dict): partial ``last_applied_manifest`` being updated\n        resource_status_old (dict): partial of the current ``last_applied_manifest``\n        resource_observed (dict): partial observer_schema for the manifest file\n            being updated\n\n    \"\"\"\n    for key, value in resource_observed.items():\n\n        if key not in resource_status_old:\n            continue\n\n        if key in resource_status_new:\n\n            if isinstance(value, dict):\n                update_last_applied_manifest_dict_from_spec(\n                    resource_status_new[key],\n                    resource_status_old[key],\n                    resource_observed[key],\n                )\n\n            elif isinstance(value, list):\n                update_last_applied_manifest_list_from_spec(\n                    resource_status_new[key],\n                    resource_status_old[key],\n                    resource_observed[key],\n                )\n\n        else:\n            # If the key is not present the spec.manifest, we first need to\n            # initialize it\n\n            if isinstance(value, dict):\n                resource_status_new[key] = {}\n                update_last_applied_manifest_dict_from_spec(\n                    resource_status_new[key],\n                    resource_status_old[key],\n                    resource_observed[key],\n                )\n\n            elif isinstance(value, list):\n                resource_status_new[key] = []\n                update_last_applied_manifest_list_from_spec(\n                    resource_status_new[key],\n                    resource_status_old[key],\n                    resource_observed[key],\n                )\n\n            else:\n                resource_status_new[key] = resource_status_old[key]\n\n\ndef update_last_applied_manifest_list_from_spec(\n    resource_status_new, resource_status_old, resource_observed\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_dict_from_spec``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n\n    Args:\n        resource_status_new (list): partial ``last_applied_manifest`` being updated\n        resource_status_old (list): partial of the current ``last_applied_manifest``\n        resource_observed (list): partial observer_schema for the manifest file\n            being updated\n\n    \"\"\"\n\n    # Looping over the observed resource, except the last element which is the\n    # special control dictionary\n    for idx, val in enumerate(resource_observed[:-1]):\n\n        if idx >= len(resource_status_old):\n            # The element in not in the current last_applied_manifest, and neither\n            # is the rest of the list\n            break\n\n        if idx < len(resource_status_new):\n            # The element is present in spec.manifest and in the current\n            # last_applied_manifest. Updating observed fields\n\n            if isinstance(val, dict):\n                update_last_applied_manifest_dict_from_spec(\n                    resource_status_new[idx],\n                    resource_status_old[idx],\n                    resource_observed[idx],\n                )\n\n            elif isinstance(val, list):\n                update_last_applied_manifest_list_from_spec(\n                    resource_status_new[idx],\n                    resource_status_old[idx],\n                    resource_observed[idx],\n                )\n\n        else:\n            # If the element is not present in the spec.manifest, we first have to\n            # initialize it.\n\n            if isinstance(val, dict):\n                resource_status_new.append({})\n                update_last_applied_manifest_dict_from_spec(\n                    resource_status_new[idx],\n                    resource_status_old[idx],\n                    resource_observed[idx],\n                )\n\n            elif isinstance(val, list):\n                resource_status_new.append([])\n                update_last_applied_manifest_list_from_spec(\n                    resource_status_new[idx],\n                    resource_status_old[idx],\n                    resource_observed[idx],\n                )\n\n            else:\n                resource_status_new.append(resource_status_old[idx])\n\n\ndef update_last_applied_manifest_from_spec(app):\n    \"\"\"Update the status.last_applied_manifest of an application from spec.manifests\n\n    Args:\n        app (krake.data.kubernetes.Application): Application to update\n\n    This function is called on application creation and updates. The\n    last_applied_manifest of an application is initialized as a copy of spec.manifest,\n    and is augmented by all known observed fields not yet initialized (i.e. all observed\n    fields or resources which are present in the current last_applied_manifest but not\n    in the spec.manifest)\n\n    \"\"\"\n\n    # The new last_applied_manifest is initialized as a copy of the spec.manifest, and\n    # augmented by all observed fields which are present in the current\n    # last_applied_manifest but not in the original spec.manifest\n    new_last_applied_manifest = deepcopy(app.spec.manifest)\n\n    # Loop over observed resources and observed fields, and check if they should be\n    # added to the new last_applied_manifest (i.e. present in the current\n    # last_applied_manifest but not in spec.manifest)\n    for resource_observed in app.status.mangled_observer_schema:\n\n        # If the resource is not present in the current last_applied_manifest, there is\n        # nothing to do. Whether the resource was initialized by spec.manifest doesn't\n        # matter.\n        try:\n            idx_status_old = get_kubernetes_resource_idx(\n                app.status.last_applied_manifest, resource_observed\n            )\n        except IndexError:\n            continue\n\n        # As the resource is present in the current last_applied_manifest, we need to go\n        # through it to check if observed fields should be set to their current value\n        # (i.e. fields are present in the current last_applied_manifest, but not in\n        # spec.manifest)\n        try:\n            # Check if the observed resource is present in spec.manifest\n            idx_status_new = get_kubernetes_resource_idx(\n                new_last_applied_manifest, resource_observed\n            )\n        except IndexError:\n            # The resource is observed but is not present in the spec.manifest.\n            # Create an empty resource, which will be augmented in\n            # update_last_applied_manifest_dict_from_spec with the observed and known\n            # fields.\n            new_last_applied_manifest.append({})\n            idx_status_new = len(new_last_applied_manifest) - 1\n\n        update_last_applied_manifest_dict_from_spec(\n            new_last_applied_manifest[idx_status_new],\n            app.status.last_applied_manifest[idx_status_old],\n            resource_observed,\n        )\n\n    app.status.last_applied_manifest = new_last_applied_manifest\n\n\nclass KubernetesApplicationObserver(Observer):\n    \"\"\"Observer specific for Kubernetes Applications. One observer is created for each\n    Application managed by the Controller, but not one per Kubernetes resource\n    (Deployment, Service...). If several resources are defined by an Application, they\n    are all monitored by the same observer.\n\n    The observer gets the actual status of the resources on the cluster using the\n    Kubernetes API, and compare it to the status stored in the API.\n\n    The observer is:\n     * started at initial Krake resource creation;\n\n     * deleted when a resource needs to be updated, then started again when it is done;\n\n     * simply deleted on resource deletion.\n\n    Args:\n        cluster (krake.data.kubernetes.Cluster): the cluster on which the observed\n            Application is created.\n        resource (krake.data.kubernetes.Application): the application that will be\n            observed.\n        on_res_update (coroutine): a coroutine called when a resource's actual status\n            differs from the status sent by the database. Its signature is:\n            ``(resource) -> updated_resource``. ``updated_resource`` is the instance of\n            the resource that is up-to-date with the API. The Observer internal instance\n            of the resource to observe will be updated. If the API cannot be contacted,\n            ``None`` can be returned. In this case the internal instance of the Observer\n            will not be updated.\n        time_step (int, optional): how frequently the Observer should watch the actual\n            status of the resources.\n\n    \"\"\"\n\n    def __init__(self, cluster, resource, on_res_update, time_step=2):\n        super().__init__(resource, on_res_update, time_step)\n        self.cluster = cluster\n\n    async def poll_resource(self):\n        \"\"\"Fetch the current status of the Application monitored by the Observer.\n\n        Returns:\n            krake.data.core.Status: the status object created using information from the\n                real world Applications resource.\n\n        \"\"\"\n        app = self.resource\n\n        status = deepcopy(app.status)\n        status.last_observed_manifest = []\n        # For each observed kubernetes resource of the Application,\n        # get its current status on the cluster.\n        for desired_resource in app.status.last_applied_manifest:\n            kube = KubernetesClient(self.cluster.spec.kubeconfig)\n            idx_observed = get_kubernetes_resource_idx(\n                app.status.mangled_observer_schema, desired_resource\n            )\n            observed_resource = app.status.mangled_observer_schema[idx_observed]\n            async with kube:\n                try:\n                    group, version, kind, name, namespace = kube.get_immutables(\n                        desired_resource\n                    )\n                    resource_api = await kube.get_resource_api(group, version, kind)\n                    resp = await resource_api.read(kind, name, namespace)\n                except ApiException as err:\n                    if err.status == 404:\n                        # Resource does not exist\n                        continue\n                    # Otherwise, log the unexpected errors\n                    logger.error(err)\n\n            observed_manifest = update_last_observed_manifest_dict(\n                observed_resource, resp.to_dict()\n            )\n            status.last_observed_manifest.append(observed_manifest)\n\n        return status\n\n\nclass KubernetesClusterObserver(Observer):\n    \"\"\"Observer specific for Kubernetes Clusters. One observer is created for each\n    Cluster managed by the Controller.\n\n    The observer gets the actual status of the cluster using the\n    Kubernetes API, and compare it to the status stored in the API.\n\n    The observer is:\n     * started at initial Krake resource creation;\n\n     * deleted when a resource needs to be updated, then started again when it is done;\n\n     * simply deleted on resource deletion.\n\n    Args:\n        cluster (krake.data.kubernetes.Cluster): the cluster which will be observed.\n        on_res_update (coroutine): a coroutine called when a resource's actual status\n            differs from the status sent by the database. Its signature is:\n            ``(resource) -> updated_resource``. ``updated_resource`` is the instance of\n            the resource that is up-to-date with the API. The Observer internal instance\n            of the resource to observe will be updated. If the API cannot be contacted,\n            ``None`` can be returned. In this case the internal instance of the Observer\n            will not be updated.\n        time_step (int, optional): how frequently the Observer should watch the actual\n            status of the resources.\n\n    \"\"\"\n\n    def __init__(self, cluster, on_res_update, time_step=2):\n        super().__init__(cluster, on_res_update, time_step)\n        self.cluster = cluster\n\n    async def poll_resource(self):\n        \"\"\"Fetch the current status of the Cluster monitored by the Observer.\n\n        Returns:\n            krake.data.core.Status: the status object created using information from the\n                real world Cluster.\n\n        \"\"\"\n        status = deepcopy(self.cluster.status)\n        # For each observed kubernetes cluster registered in Krake,\n        # get its current node status.\n        loader = KubeConfigLoader(self.cluster.spec.kubeconfig)\n        config = Configuration()\n        await loader.load_and_set(config)\n        kube = ApiClient(config)\n\n        async with kube as api:\n            v1 = client.CoreV1Api(api)\n            try:\n                response = await v1.list_node()\n\n            except ClientConnectorError as err:\n                status.state = ClusterState.OFFLINE\n                self.cluster.status.state = ClusterState.OFFLINE\n                # Log the error\n                logger.debug(err)\n                return status\n\n            condition_dict = {\n                \"MemoryPressure\": [],\n                \"DiskPressure\": [],\n                \"PIDPressure\": [],\n                \"Ready\": [],\n            }\n\n            for item in response.items:\n                for condition in item.status.conditions:\n                    condition_dict[condition.type].append(condition.status)\n                if (\n                    condition_dict[\"MemoryPressure\"] == [\"True\"]\n                    or condition_dict[\"DiskPressure\"] == [\"True\"]\n                    or condition_dict[\"PIDPressure\"] == [\"True\"]\n                ):\n                    status.state = ClusterState.UNHEALTHY\n                    self.cluster.status.state = ClusterState.UNHEALTHY\n                    return status\n                elif (\n                    condition_dict[\"Ready\"] == [\"True\"]\n                    and status.state is ClusterState.OFFLINE\n                ):\n                    status.state = ClusterState.CONNECTING\n                    self.cluster.status.state = ClusterState.CONNECTING\n                    return status\n                elif condition_dict[\"Ready\"] == [\"True\"]:\n                    status.state = ClusterState.ONLINE\n                    self.cluster.status.state = ClusterState.ONLINE\n                    return status\n                else:\n                    status.state = ClusterState.NOTREADY\n                    self.cluster.status.state = ClusterState.NOTREADY\n                    return status\n\n\n@listen.on(HookType.ApplicationPostReconcile)\n@listen.on(HookType.ApplicationPostMigrate)\n@listen.on(HookType.ClusterCreation)\nasync def register_observer(controller, resource, start=True, **kwargs):\n    \"\"\"Create an observer for the given Application or Cluster, and start it as a\n    background task if wanted.\n\n    If an observer already existed for this Application or Cluster, it is stopped\n    and deleted.\n\n    Args:\n        controller (KubernetesController): the controller for which the observer will be\n            added in the list of working observers.\n        resource (krake.data.kubernetes.Application): the Application to observe or\n        resource (krake.data.kubernetes.Cluster): the Cluster to observe.\n        start (bool, optional): if False, does not start the observer as background\n            task.\n\n    \"\"\"\n    if resource.kind == Application.kind:\n        cluster = await controller.kubernetes_api.read_cluster(\n            namespace=resource.status.running_on.namespace,\n            name=resource.status.running_on.name,\n        )\n\n        observer = KubernetesApplicationObserver(\n            cluster,\n            resource,\n            controller.on_status_update,\n            time_step=controller.observer_time_step,\n        )\n\n    elif resource.kind == Cluster.kind:\n        observer = KubernetesClusterObserver(\n            resource,\n            controller.on_status_update,\n            time_step=controller.observer_time_step,\n        )\n    else:\n        logger.debug(\"Unknown resource kind. No observer was registered.\", resource)\n        return\n\n    logger.debug(f\"Start observer for {resource.kind} %r\", resource.metadata.name)\n    task = None\n    if start:\n        task = controller.loop.create_task(observer.run())\n\n    controller.observers[resource.metadata.uid] = (observer, task)\n\n\n@listen.on(HookType.ApplicationPreReconcile)\n@listen.on(HookType.ApplicationPreMigrate)\n@listen.on(HookType.ApplicationPreDelete)\n@listen.on(HookType.ClusterDeletion)\nasync def unregister_observer(controller, resource, **kwargs):\n    \"\"\"Stop and delete the observer for the given Application or Cluster. If no observer\n    is started, do nothing.\n\n    Args:\n        controller (KubernetesController): the controller for which the observer will be\n            removed from the list of working observers.\n        resource (krake.data.kubernetes.Application): the Application whose observer\n        will be stopped or\n        resource (krake.data.kubernetes.Cluster): the Cluster whose observer will be\n        stopped.\n\n    \"\"\"\n    if resource.metadata.uid not in controller.observers:\n        return\n\n    logger.debug(f\"Stop observer for {resource.kind} %r\", resource.metadata.name)\n    _, task = controller.observers.pop(resource.metadata.uid)\n    task.cancel()\n\n    with suppress(asyncio.CancelledError):\n        await task\n\n\ndef utc_difference():\n    \"\"\"Get the difference in seconds between the current time and the current UTC time.\n\n    Returns:\n        int: the time difference in seconds.\n\n    \"\"\"\n    delta = datetime.now() - datetime.utcnow()\n    return delta.seconds\n\n\ndef generate_certificate(config):\n    \"\"\"Create and sign a new certificate using the one defined in the complete hook\n    configuration as intermediate certificate.\n\n    Args:\n        config (krake.data.config.CompleteHookConfiguration): the configuration of the\n            complete hook.\n\n    Returns:\n        CertificatePair: the content of the certificate created and its corresponding\n            key.\n\n    \"\"\"\n    with open(config.intermediate_src, \"rb\") as f:\n        intermediate_src = crypto.load_certificate(crypto.FILETYPE_PEM, f.read())\n    with open(config.intermediate_key_src, \"rb\") as f:\n        intermediate_key_src = crypto.load_privatekey(crypto.FILETYPE_PEM, f.read())\n\n    client_cert = crypto.X509()\n\n    # Set general information\n    client_cert.set_version(3)\n    client_cert.set_serial_number(random.randint(50000000000000, 100000000000000))\n    # If not set before, TLS will not accept to use this certificate in UTC cases, as\n    # the server time may be earlier.\n    time_offset = utc_difference() * -1\n    client_cert.gmtime_adj_notBefore(time_offset)\n    client_cert.gmtime_adj_notAfter(1 * 365 * 24 * 60 * 60)\n\n    # Set issuer and subject\n    intermediate_subject = intermediate_src.get_subject()\n    client_cert.set_issuer(intermediate_subject)\n    client_subj = crypto.X509Name(intermediate_subject)\n    client_subj.CN = config.hook_user\n    client_cert.set_subject(client_subj)\n\n    # Create and set the private key\n    client_key = crypto.PKey()\n    client_key.generate_key(crypto.TYPE_RSA, 2048)\n    client_cert.set_pubkey(client_key)\n\n    client_cert.sign(intermediate_key_src, \"sha256\")\n\n    cert_dump = crypto.dump_certificate(crypto.FILETYPE_PEM, client_cert).decode()\n    key_dump = crypto.dump_privatekey(crypto.FILETYPE_PEM, client_key).decode()\n    return CertificatePair(cert=cert_dump, key=key_dump)\n\n\ndef generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n\n    app.status.mangled_observer_schema = deepcopy(app.spec.observer_schema)\n\n    for resource_manifest in app.spec.manifest:\n        try:\n            get_kubernetes_resource_idx(\n                app.status.mangled_observer_schema, resource_manifest\n            )\n\n        except IndexError:\n            # Only create a default observer schema, if a custom observer schema hasn't\n            # been set by the user.\n            app.status.mangled_observer_schema.append(\n                generate_default_observer_schema_dict(\n                    resource_manifest,\n                    first_level=True,\n                )\n            )\n\n\ndef generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n\n    \"\"\"\n    observer_schema_dict = {}\n\n    for key, value in manifest_dict.items():\n\n        if isinstance(value, dict):\n            observer_schema_dict[key] = generate_default_observer_schema_dict(value)\n\n        elif isinstance(value, list):\n            observer_schema_dict[key] = generate_default_observer_schema_list(value)\n\n        else:\n            observer_schema_dict[key] = None\n\n    if first_level:\n        observer_schema_dict[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        observer_schema_dict[\"kind\"] = manifest_dict[\"kind\"]\n        observer_schema_dict[\"metadata\"][\"name\"] = manifest_dict[\"metadata\"][\"name\"]\n\n        if (\n            \"spec\" in manifest_dict\n            and \"type\" in manifest_dict[\"spec\"]\n            and manifest_dict[\"spec\"][\"type\"] == \"LoadBalancer\"\n        ):\n            observer_schema_dict[\"status\"] = {\"load_balancer\": {\"ingress\": None}}\n\n    return observer_schema_dict\n\n\ndef generate_default_observer_schema_list(manifest_list):\n    \"\"\"Together with :func:``generate_default_observer_schema_dict``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_list`` or ``manifest_dict``.\n\n    Args:\n        manifest_list (list): Partial Kubernetes resources\n\n    Returns:\n        list: Generated partial observer_schema\n\n    This function creates a new list from ``manifest_list`` and replaces all non-list\n    and non-dict elements by ``None``.\n\n    Additionally, it generates the default list control dictionary, using the current\n    length of the list as default minimum and maximum values.\n\n    \"\"\"\n    observer_schema_list = []\n\n    for value in manifest_list:\n\n        if isinstance(value, dict):\n            observer_schema_list.append(generate_default_observer_schema_dict(value))\n\n        elif isinstance(value, list):\n            observer_schema_list.append(generate_default_observer_schema_list(value))\n\n        else:\n            observer_schema_list.append(None)\n\n    observer_schema_list.append(\n        {\n            \"observer_schema_list_min_length\": len(manifest_list),\n            \"observer_schema_list_max_length\": len(manifest_list),\n        }\n    )\n\n    return observer_schema_list\n\n\n@listen.on(HookType.ApplicationMangling)\nasync def complete(app, api_endpoint, ssl_context, config):\n    \"\"\"Execute application complete hook defined by :class:`Complete`.\n    Hook mangles given application and injects complete hooks variables.\n\n    Application complete hook is disabled by default.\n    User enables this hook by the --hook-complete argument in rok cli.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application object processed\n            when the hook is called\n        api_endpoint (str): the given API endpoint\n        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint\n        config (krake.data.config.HooksConfiguration): Complete hook\n            configuration.\n\n    \"\"\"\n    if \"complete\" not in app.spec.hooks:\n        return\n\n    # Use the endpoint of the API only if the external endpoint has not been set.\n    if config.complete.external_endpoint:\n        api_endpoint = config.complete.external_endpoint\n\n    app.status.complete_token = \\\n        app.status.complete_token if app.status.complete_token else token_urlsafe()\n\n    # Generate only once the certificate and key for a specific Application\n    generated_cert = CertificatePair(\n        cert=app.status.complete_cert, key=app.status.complete_key\n    )\n    if ssl_context and generated_cert == (None, None):\n        generated_cert = generate_certificate(config.complete)\n        app.status.complete_cert = generated_cert.cert\n        app.status.complete_key = generated_cert.key\n\n    hook = Complete(\n        api_endpoint,\n        ssl_context,\n        hook_user=config.complete.hook_user,\n        cert_dest=config.complete.cert_dest,\n        env_token=config.complete.env_token,\n        env_url=config.complete.env_url,\n    )\n    hook.mangle_app(\n        app.metadata.name,\n        app.metadata.namespace,\n        app.status.complete_token,\n        app.status.last_applied_manifest,\n        config.complete.intermediate_src,\n        generated_cert,\n        app.status.mangled_observer_schema,\n        \"complete\"\n    )\n\n\n@listen.on(HookType.ApplicationMangling)\nasync def shutdown(app, api_endpoint, ssl_context, config):\n    \"\"\"Executes an application shutdown hook defined by :class:`Shutdown`.\n    The hook mangles the given application and injects shutdown hooks variables.\n\n    Application shutdown hook is disabled by default.\n    User enables this hook by the --hook-shutdown argument in rok cli.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application object processed\n            when the hook is called\n        api_endpoint (str): the given API endpoint\n        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint\n        config (krake.data.config.HooksConfiguration): Shutdown hook\n            configuration.\n\n    \"\"\"\n    if \"shutdown\" not in app.spec.hooks:\n        return\n\n    # Use the endpoint of the API only if the external endpoint has not been set.\n    if config.shutdown.external_endpoint:\n        api_endpoint = config.shutdown.external_endpoint\n\n    app.status.shutdown_token = \\\n        app.status.shutdown_token if app.status.shutdown_token else token_urlsafe()\n\n    # Generate only once the certificate and key for a specific Application\n    generated_cert = CertificatePair(\n        cert=app.status.shutdown_cert, key=app.status.shutdown_key\n    )\n    if ssl_context and generated_cert == (None, None):\n        generated_cert = generate_certificate(config.shutdown)\n        app.status.shutdown_cert = generated_cert.cert\n        app.status.shutdown_key = generated_cert.key\n\n    hook = Shutdown(\n        api_endpoint,\n        ssl_context,\n        hook_user=config.shutdown.hook_user,\n        cert_dest=config.shutdown.cert_dest,\n        env_token=config.shutdown.env_token,\n        env_url=config.shutdown.env_url,\n    )\n    hook.mangle_app(\n        app.metadata.name,\n        app.metadata.namespace,\n        app.status.shutdown_token,\n        app.status.last_applied_manifest,\n        config.shutdown.intermediate_src,\n        generated_cert,\n        app.status.mangled_observer_schema,\n        \"shutdown\"\n    )\n\n\n@listen.on(HookType.ResourcePreDelete)\nasync def pre_shutdown(controller, app, **kwargs):\n    \"\"\"\n\n    Args:\n        app (krake.data.kubernetes.Application): Application object processed\n            when the hook is called\n    \"\"\"\n    if \"shutdown\" not in app.spec.hooks:\n        return\n\n    return\n\n\nclass SubResource(NamedTuple):\n    group: str\n    name: str\n    body: dict\n    path: tuple\n\n\nclass CertificatePair(NamedTuple):\n    \"\"\"Tuple which contains a certificate and its corresponding key.\n\n    Attributes:\n        cert (str): content of a certificate.\n        key (str): content of the key that corresponds to the certificate.\n\n    \"\"\"\n\n    cert: str\n    key: str\n\n\nclass Hook(object):\n\n    hook_resources = ()\n\n    ca_name = \"ca-bundle.pem\"\n    cert_name = \"cert.pem\"\n    key_name = \"key.pem\"\n\n    def __init__(\n        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n    ):\n        self.api_endpoint = api_endpoint\n        self.ssl_context = ssl_context\n        self.hook_user = hook_user\n        self.cert_dest = cert_dest\n        self.env_token = env_token\n        self.env_url = env_url\n\n    def mangle_app(\n        self,\n        name,\n        namespace,\n        token,\n        last_applied_manifest,\n        intermediate_src,\n        generated_cert,\n        mangled_observer_schema,\n        hook_type=\"\",\n    ):\n        \"\"\"Mangle a given application and inject complete hook resources and\n        sub-resources into the :attr:`last_applied_manifest` object by :meth:`mangle`.\n        Also mangle the observer_schema as new resources and sub-resources should\n        be observed.\n\n        :attr:`last_applied_manifest` is created as a deep copy of the desired\n        application resources, as defined by user. It can be updated by custom hook\n        resources or modified by custom hook sub-resources. It is used as a desired\n        state for the Krake deployment process.\n\n        Args:\n            name (str): Application name\n            namespace (str): Application namespace\n            token (str): Complete hook authentication token\n            last_applied_manifest (list): Application resources\n            intermediate_src (str): content of the certificate that is used to sign new\n                certificates for the complete hook.\n            generated_cert (CertificatePair): tuple that contains the content of the\n                new signed certificate for the Application, and the content of its\n                corresponding key.\n            mangled_observer_schema (list): Observed fields\n            hook_type (str, optional): Name of the hook the app should be mangled for\n\n        \"\"\"\n\n        secret_certs_name = \"-\".join([name, \"krake\", hook_type, \"secret\", \"certs\"])\n        secret_token_name = \"-\".join([name, \"krake\", hook_type, \"secret\", \"token\"])\n        volume_name = \"-\".join([name, \"krake\", hook_type, \"volume\"])\n        ca_certs = (\n            self.ssl_context.get_ca_certs(binary_form=True)\n            if self.ssl_context\n            else None\n        )\n\n        # Extract all different namespaces\n        # FIXME: too many assumptions here: do we create one ConfigMap for each\n        #  namespace?\n        resource_namespaces = {\n            resource[\"metadata\"].get(\"namespace\", \"default\")\n            for resource in last_applied_manifest\n        }\n\n        hook_resources = []\n        hook_sub_resources = []\n        if ca_certs:\n            hook_resources.extend(\n                [\n                    self.secret_certs(\n                        secret_certs_name,\n                        resource_namespace,\n                        intermediate_src=intermediate_src,\n                        generated_cert=generated_cert,\n                        ca_certs=ca_certs,\n                    )\n                    for resource_namespace in resource_namespaces\n                ]\n            )\n            hook_sub_resources.extend(\n                [*self.volumes(secret_certs_name, volume_name, self.cert_dest)]\n            )\n\n        hook_resources.extend(\n            [\n                self.secret_token(\n                    secret_token_name,\n                    name,\n                    namespace,\n                    resource_namespace,\n                    self.api_endpoint,\n                    token,\n                )\n                for resource_namespace in resource_namespaces\n            ]\n        )\n        hook_sub_resources.extend(\n            [\n                *self.env_vars(secret_token_name),\n            ]\n        )\n\n        self.mangle(\n            hook_resources,\n            last_applied_manifest,\n            mangled_observer_schema,\n        )\n        self.mangle(\n            hook_sub_resources,\n            last_applied_manifest,\n            mangled_observer_schema,\n            is_sub_resource=True,\n        )\n\n    def mangle(\n        self,\n        items,\n        last_applied_manifest,\n        mangled_observer_schema,\n        is_sub_resource=False,\n    ):\n        \"\"\"Mangle applications desired state with custom hook resources or\n        sub-resources.\n\n        Example:\n            .. code:: python\n\n            last_applied_manifest = [\n                {\n                    'apiVersion': 'v1',\n                    'kind': 'Pod',\n                    'metadata': {'name': 'test', 'namespace': 'default'},\n                    'spec': {'containers': [{'name': 'test'}]}\n                }\n            ]\n            mangled_observer_schema = [\n                {\n                    'apiVersion': 'v1',\n                    'kind': 'Pod',\n                    'metadata': {'name': 'test', 'namespace': 'default'},\n                    'spec': {\n                        'containers': [\n                            {'name': None},\n                            {\n                                'observer_schema_list_max_length': 1,\n                                'observer_schema_list_min_length': 1,\n                            },\n                        ]\n                    },\n                }\n            ]\n            hook_resources = [\n                {\n                    'apiVersion': 'v1',\n                    'kind': 'Secret',\n                    'metadata': {'name': 'sct', 'namespace': 'default'}\n                }\n            ]\n            hook_sub_resources = [\n                SubResource(\n                    group='env', name='env', body={'name': 'test', 'value': 'test'},\n                    path=(('spec', 'containers'),)\n                )\n            ]\n\n            mangle(\n                hook_resources,\n                last_applied_manifest,\n                mangled_observer_schema,\n            )\n            mangle(\n                hook_sub_resources,\n                last_applied_manifest,\n                mangled_observer_schema,\n                is_sub_resource=True\n            )\n\n            assert last_applied_manifest == [\n                {\n                    \"apiVersion\": \"v1\",\n                    \"kind\": \"Pod\",\n                    \"metadata\": {\"name\": \"test\", 'namespace': 'default'},\n                    \"spec\": {\n                        \"containers\": [\n                            {\n                                \"name\": \"test\",\n                                \"env\": [{\"name\": \"test\", \"value\": \"test\"}]\n                            }\n                        ]\n                    },\n                },\n                {\"apiVersion\": \"v1\", \"kind\": \"Secret\", \"metadata\": {\"name\": \"sct\"}},\n            ]\n\n            assert mangled_observer_schema == [\n                {\n                    \"apiVersion\": \"v1\",\n                    \"kind\": \"Pod\",\n                    \"metadata\": {\"name\": \"test\", \"namespace\": None},\n                    \"spec\": {\n                        \"containers\": [\n                            {\n                                \"name\": None,\n                                \"env\": [\n                                    {\"name\": None, \"value\": None},\n                                    {\n                                        \"observer_schema_list_max_length\": 1,\n                                        \"observer_schema_list_min_length\": 1,\n                                    },\n                                ],\n                            },\n                            {\n                                \"observer_schema_list_max_length\": 1,\n                                \"observer_schema_list_min_length\": 1,\n                            },\n                        ]\n                    },\n                },\n                {\n                    \"apiVersion\": \"v1\",\n                    \"kind\": \"Secret\",\n                    \"metadata\": {\"name\": \"sct\", \"namespace\": None},\n                },\n            ]\n\n        Args:\n            items (list[SubResource]): Custom hook resources or sub-resources\n            last_applied_manifest (list): Application resources\n            mangled_observer_schema (list): Observed resources\n            is_sub_resource (bool, optional): if False, the function only extend the\n                list of Kubernetes resources defined in :attr:`last_applied_manifest`\n                with new hook resources. Otherwise, the function injects each new hook\n                sub-resource into the :attr:`last_applied_manifest` object\n                sub-resources. Defaults to False.\n\n        \"\"\"\n\n        if not items:\n            return\n\n        if not is_sub_resource:\n            last_applied_manifest.extend(items)\n            for sub_resource in items:\n                # Generate the default observer schema for each resource\n                mangled_observer_schema.append(\n                    generate_default_observer_schema_dict(\n                        sub_resource,\n                        first_level=True,\n                    )\n                )\n            return\n\n        def inject(sub_resource, sub_resource_to_mangle, observed_resource_to_mangle):\n            \"\"\"Inject a hooks defined sub-resource into a Kubernetes sub-resource.\n\n            Args:\n                sub_resource (SubResource): Hook sub-resource that needs to be injected\n                    into :attr:`last_applied_manifest`\n                sub_resource_to_mangle (object): Kubernetes sub-resources from\n                    :attr:`last_applied_manifest` which need to be processed\n                observed_resource_to_mangle (dict): partial mangled_observer_schema\n                    corresponding to the Kubernetes sub-resource.\n\n            Raises:\n                InvalidManifestError: if the sub-resource which will be mangled is not a\n                    list or a dict.\n\n            \"\"\"\n\n            # Create sub-resource group if not present in the Kubernetes sub-resource\n            if sub_resource.group not in sub_resource_to_mangle:\n                # FIXME: This assumes the subresource group contains a list\n                sub_resource_to_mangle.update({sub_resource.group: []})\n\n            # Create sub-resource group if not present in the observed fields\n            if sub_resource.group not in observed_resource_to_mangle:\n                observed_resource_to_mangle.update(\n                    {\n                        sub_resource.group: [\n                            {\n                                \"observer_schema_list_min_length\": 0,\n                                \"observer_schema_list_max_length\": 0,\n                            }\n                        ]\n                    }\n                )\n\n            # Inject sub-resource\n            # If sub-resource name is already there update it, if not, append it\n            if sub_resource.name in [\n                g[\"name\"] for g in sub_resource_to_mangle[sub_resource.group]\n            ]:\n                # FIXME: Assuming we are dealing with a list\n                for idx, item in enumerate(sub_resource_to_mangle[sub_resource.group]):\n                    if item[\"name\"]:\n                        if hasattr(item, \"body\"):\n                            sub_resource_to_mangle[item.group][idx] = item[\"body\"]\n            else:\n                sub_resource_to_mangle[sub_resource.group].append(sub_resource.body)\n\n            # Make sure the value is observed\n            if sub_resource.name not in [\n                g[\"name\"] for g in observed_resource_to_mangle[sub_resource.group][:-1]\n            ]:\n                observed_resource_to_mangle[sub_resource.group].insert(\n                    -1, generate_default_observer_schema_dict(sub_resource.body)\n                )\n                observed_resource_to_mangle[sub_resource.group][-1][\n                    \"observer_schema_list_min_length\"\n                ] += 1\n                observed_resource_to_mangle[sub_resource.group][-1][\n                    \"observer_schema_list_max_length\"\n                ] += 1\n\n        for resource in last_applied_manifest:\n            # Complete hook is applied only on defined Kubernetes resources\n            if resource[\"kind\"] not in self.hook_resources:\n                continue\n\n            for sub_resource in items:\n                sub_resources_to_mangle = None\n                idx_observed = get_kubernetes_resource_idx(\n                    mangled_observer_schema, resource\n                )\n                for keys in sub_resource.path:\n                    try:\n                        sub_resources_to_mangle = reduce(getitem, keys, resource)\n                    except KeyError:\n                        continue\n\n                    break\n\n                # Create the path to the observed sub-resource, if it doesn't yet exist\n                try:\n                    observed_sub_resources = reduce(\n                        getitem, keys, mangled_observer_schema[idx_observed]\n                    )\n                except KeyError:\n                    Complete.create_path(\n                        mangled_observer_schema[idx_observed], list(keys)\n                    )\n                    observed_sub_resources = reduce(\n                        getitem, keys, mangled_observer_schema[idx_observed]\n                    )\n\n                if isinstance(sub_resources_to_mangle, list):\n                    for idx, sub_resource_to_mangle in enumerate(\n                        sub_resources_to_mangle\n                    ):\n\n                        # Ensure that each element of the list is observed.\n                        idx_observed = idx\n                        if idx >= len(observed_sub_resources[:-1]):\n                            idx_observed = len(observed_sub_resources[:-1])\n                            # FIXME: Assuming each element of the list contains a\n                            # dictionary, therefore initializing new elements with an\n                            # empty dict\n                            observed_sub_resources.insert(-1, {})\n                        observed_sub_resource = observed_sub_resources[idx_observed]\n\n                        # FIXME: This is assuming a list always contains dict\n                        inject(\n                            sub_resource, sub_resource_to_mangle, observed_sub_resource\n                        )\n\n                elif isinstance(sub_resources_to_mangle, dict):\n                    inject(\n                        sub_resource, sub_resources_to_mangle, observed_sub_resources\n                    )\n\n                else:\n                    message = (\n                        f\"The sub-resource to mangle {sub_resources_to_mangle!r} has an\"\n                        \"invalid type, should be in '[dict, list]'\"\n                    )\n                    raise InvalidManifestError(message)\n\n    @staticmethod\n    def attribute_map(obj):\n        \"\"\"Convert a Kubernetes object to dict based on its attribute mapping\n\n        Example:\n            .. code:: python\n\n            from kubernetes_asyncio.client import V1VolumeMount\n\n            d = attribute_map(\n                    V1VolumeMount(name=\"name\", mount_path=\"path\")\n            )\n            assert d == {'mountPath': 'path', 'name': 'name'}\n\n        Args:\n            obj (object): Kubernetes object\n\n        Returns:\n            dict: Converted Kubernetes object\n\n        \"\"\"\n        return {\n            obj.attribute_map[attr]: getattr(obj, attr)\n            for attr, _ in obj.to_dict().items()\n            if getattr(obj, attr) is not None\n        }\n\n    @staticmethod\n    def create_path(mangled_observer_schema, keys):\n        \"\"\"Create the path to the observed field in the observer schema.\n\n        When a sub-resource is mangled, it should be observed. This function creates\n        the path to the subresource to observe.\n\n        Args:\n            mangled_observer_schema (dict): Partial observer schema of a resource\n            keys (list): list of keys forming the path to the sub-resource to\n                observe\n\n        FIXME: This assumes we are only adding keys to dict. We don't consider lists\n\n        \"\"\"\n\n        # Unpack the first key first, as it contains the base directory\n        key = keys.pop(0)\n\n        # If the key is the last of the list, we reached the end of the path.\n        if len(keys) == 0:\n            mangled_observer_schema[key] = None\n            return\n\n        if key not in mangled_observer_schema:\n            mangled_observer_schema[key] = {}\n        Hook.create_path(mangled_observer_schema[key], keys)\n\n    def secret_certs(\n        self,\n        secret_name,\n        namespace,\n        ca_certs=None,\n        intermediate_src=None,\n        generated_cert=None,\n    ):\n        \"\"\"Create a complete hooks secret resource.\n\n        Complete hook secret stores Krake CAs and client certificates to communicate\n        with the Krake API.\n\n        Args:\n            secret_name (str): Secret name\n            namespace (str): Kubernetes namespace where the Secret will be created.\n            ca_certs (list): Krake CA list\n            intermediate_src (str): content of the certificate that is used to sign new\n                certificates for the complete hook.\n            generated_cert (CertificatePair): tuple that contains the content of the\n                new signed certificate for the Application, and the content of its\n                corresponding key.\n\n        Returns:\n            dict: complete hook secret resource\n\n        \"\"\"\n        ca_certs_pem = \"\"\n        for ca_cert in ca_certs:\n            x509 = crypto.load_certificate(crypto.FILETYPE_ASN1, ca_cert)\n            ca_certs_pem += crypto.dump_certificate(crypto.FILETYPE_PEM, x509).decode()\n\n        # Add the intermediate certificate into the chain\n        with open(intermediate_src, \"r\") as f:\n            intermediate_src_content = f.read()\n        ca_certs_pem += intermediate_src_content\n\n        data = {\n            self.ca_name: self._encode_to_64(ca_certs_pem),\n            self.cert_name: self._encode_to_64(generated_cert.cert),\n            self.key_name: self._encode_to_64(generated_cert.key),\n        }\n        return self.secret(secret_name, data, namespace)\n\n    def secret_token(\n        self, secret_name, name, namespace, resource_namespace, api_endpoint, token\n    ):\n        \"\"\"Create a hooks secret resource.\n\n        The hook secret stores Krake authentication token\n        and hook URL for given application.\n\n        Args:\n            secret_name (str): Secret name\n            name (str): Application name\n            namespace (str): Application namespace\n            resource_namespace (str): Kubernetes namespace where the\n                Secret will be created.\n            api_endpoint (str): Krake API endpoint\n            token (str): Complete hook authentication token\n\n        Returns:\n            dict: complete hook secret resource\n\n        \"\"\"\n        pass\n\n    def volumes(self, secret_name, volume_name, mount_path):\n        \"\"\"Create complete hooks volume and volume mount sub-resources\n\n        Complete hook volume gives access to hook's secret, which stores\n        Krake CAs and client certificates to communicate with the Krake API.\n        Complete hook volume mount puts the volume into the application\n\n        Args:\n            secret_name (str): Secret name\n            volume_name (str): Volume name\n            mount_path (list): Volume mount path\n\n        Returns:\n            list: List of complete hook volume and volume mount sub-resources\n\n        \"\"\"\n        volume = V1Volume(name=volume_name, secret={\"secretName\": secret_name})\n        volume_mount = V1VolumeMount(name=volume_name, mount_path=mount_path)\n        return [\n            SubResource(\n                group=\"volumes\",\n                name=volume.name,\n                body=self.attribute_map(volume),\n                path=((\"spec\", \"template\", \"spec\"), (\"spec\",)),\n            ),\n            SubResource(\n                group=\"volumeMounts\",\n                name=volume_mount.name,\n                body=self.attribute_map(volume_mount),\n                path=(\n                    (\"spec\", \"template\", \"spec\", \"containers\"),\n                    (\"spec\", \"containers\"),  # kind: Pod\n                ),\n            ),\n        ]\n\n    @staticmethod\n    def _encode_to_64(string):\n        \"\"\"Compute the base 64 encoding of a string.\n\n        Args:\n            string (str): the string to encode.\n\n        Returns:\n            str: the result of the encoding.\n\n        \"\"\"\n        return b64encode(string.encode()).decode()\n\n    def secret(self, secret_name, secret_data, namespace, _type=\"Opaque\"):\n        \"\"\"Create a secret resource.\n\n        Args:\n            secret_name (str): Secret name\n            secret_data (dict): Secret data\n            namespace (str): Kubernetes namespace where the Secret will be created.\n            _type (str, optional): Secret type. Defaults to Opaque.\n\n        Returns:\n            dict: secret resource\n\n        \"\"\"\n        return self.attribute_map(\n            V1Secret(\n                api_version=\"v1\",\n                kind=\"Secret\",\n                data=secret_data,\n                metadata={\"name\": secret_name, \"namespace\": namespace},\n                type=_type,\n            )\n        )\n\n    @staticmethod\n    def create_hook_url(name, namespace, api_endpoint):\n        \"\"\"Create an applications' hook URL.\n        Function needs to be specified for each hook.\n\n        Args:\n            name (str): Application name\n            namespace (str): Application namespace\n            api_endpoint (str): Krake API endpoint\n\n        Returns:\n            str: Application shutdown url\n\n        \"\"\"\n        pass\n\n    def env_vars(self, secret_name):\n        \"\"\"Create the hooks' environment variables sub-resources.\n        Function needs to be specified for each hook.\n\n        Creates hook environment variables to store Krake authentication token\n        and a hook URL for the given applications.\n\n        Args:\n            secret_name (str): Secret name\n\n        Returns:\n            list: List of shutdown hook environment variables sub-resources\n\n        \"\"\"\n        pass\n\n\nclass Complete(Hook):\n    \"\"\"Mangle given application and inject complete hooks variables into it.\n\n    Hook injects a Kubernetes secret, which stores Krake authentication token\n    and the Krake complete hook URL for the given application. The variables\n    from Kubernetes secret are imported as environment variables\n    into the application resource definition. Only resources defined in\n    :args:`hook_resources` can be modified.\n\n    Names of environment variables are defined in the application controller\n    configuration file.\n\n    If TLS is enabled on the Krake API, the complete hook injects a Kubernetes secret,\n    and it's corresponding volume and volume mount definitions for the Krake CA,\n    the client certificate with the right CN, and its key. The directory where the\n    secret is mounted is defined in the configuration.\n\n    Args:\n        api_endpoint (str): the given API endpoint\n        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint\n        cert_dest (str, optional): Path of the directory where the CA, client\n            certificate and key to the Krake API will be stored.\n        env_token (str, optional): Name of the environment variable, which stores Krake\n            authentication token.\n        env_url (str, optional): Name of the environment variable,\n            which stores Krake complete hook URL.\n\n    \"\"\"\n\n    hook_resources = (\"Pod\", \"Deployment\", \"ReplicationController\")\n\n    def __init__(\n        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n    ):\n        super().__init__(\n            api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n        )\n        self.env_url = env_url\n\n    def secret_token(\n        self, secret_name, name, namespace, resource_namespace, api_endpoint, token\n    ):\n        \"\"\"Create complete hooks secret resource.\n\n        Complete hook secret stores Krake authentication token\n        and complete hook URL for given application.\n\n        Args:\n            secret_name (str): Secret name\n            name (str): Application name\n            namespace (str): Application namespace\n            resource_namespace (str): Kubernetes namespace where the\n                Secret will be created.\n            api_endpoint (str): Krake API endpoint\n            token (str): Complete hook authentication token\n\n        Returns:\n            dict: complete hook secret resource\n\n        \"\"\"\n        complete_url = self.create_hook_url(name, namespace, api_endpoint)\n        data = {\n            self.env_token.lower(): self._encode_to_64(token),\n            self.env_url.lower(): self._encode_to_64(complete_url),\n        }\n        return self.secret(secret_name, data, resource_namespace)\n\n    @staticmethod\n    def create_hook_url(name, namespace, api_endpoint):\n        \"\"\"Create an applications' complete URL.\n\n        Args:\n            name (str): Application name\n            namespace (str): Application namespace\n            api_endpoint (str): Krake API endpoint\n\n        Returns:\n            str: Application complete url\n\n        \"\"\"\n        api_url = URL(api_endpoint)\n        return str(\n            api_url.with_path(\n                f\"/kubernetes/namespaces/{namespace}/applications/{name}/complete\"\n            )\n        )\n\n    def env_vars(self, secret_name):\n        \"\"\"Create complete hooks environment variables sub-resources\n\n        Create complete hook environment variables store Krake authentication token\n        and complete hook URL for given application.\n\n        Args:\n            secret_name (str): Secret name\n\n        Returns:\n            list: List of complete hook environment variables sub-resources\n\n        \"\"\"\n        sub_resources = []\n\n        env_token = V1EnvVar(\n            name=self.env_token,\n            value_from=self.attribute_map(\n                V1EnvVarSource(\n                    secret_key_ref=self.attribute_map(\n                        V1SecretKeySelector(\n                            name=secret_name, key=self.env_token.lower()\n                        )\n                    )\n                )\n            ),\n        )\n        env_url = V1EnvVar(\n            name=self.env_url,\n            value_from=self.attribute_map(\n                V1EnvVarSource(\n                    secret_key_ref=self.attribute_map(\n                        V1SecretKeySelector(name=secret_name, key=self.env_url.lower())\n                    )\n                )\n            ),\n        )\n\n        for env in (env_token, env_url):\n            sub_resources.append(\n                SubResource(\n                    group=\"env\",\n                    name=env.name,\n                    body=self.attribute_map(env),\n                    path=(\n                        (\"spec\", \"template\", \"spec\", \"containers\"),\n                        (\"spec\", \"containers\"),  # kind: Pod\n                    ),\n                )\n            )\n        return sub_resources\n\n\nclass Shutdown(Hook):\n    \"\"\"Mangle given application and inject shutdown hooks variables into it.\n\n    Hook injects a Kubernetes secret, which stores Krake authentication token\n    and the Krake complete hook URL for the given application. The variables\n    from the Kubernetes secret are imported as environment variables\n    into the application resource definition. Only resources defined in\n    :args:`hook_resources` can be modified.\n\n    Names of environment variables are defined in the application controller\n    configuration file.\n\n    If TLS is enabled on the Krake API, the shutdown hook injects a Kubernetes secret,\n    and it's corresponding volume and volume mount definitions for the Krake CA,\n    the client certificate with the right CN, and its key. The directory where the\n    secret is mounted is defined in the configuration.\n\n    Args:\n        api_endpoint (str): the given API endpoint\n        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint\n        cert_dest (str, optional): Path of the directory where the CA, client\n            certificate and key to the Krake API will be stored.\n        env_token (str, optional): Name of the environment variable, which stores Krake\n            authentication token.\n        env_url (str, optional): Name of the environment variable,\n            which stores Krake complete hook URL.\n\n    \"\"\"\n\n    hook_resources = (\"Pod\", \"Deployment\", \"ReplicationController\")\n\n    def __init__(\n        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n    ):\n        super().__init__(\n            api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n        )\n        self.env_url = env_url\n\n    def secret_token(\n        self, secret_name, name, namespace, resource_namespace, api_endpoint, token\n    ):\n        \"\"\"Create shutdown hooks secret resource.\n\n        Shutdown hook secret stores Krake authentication token\n        and shutdown hook URL for given application.\n\n        Args:\n            secret_name (str): Secret name\n            name (str): Application name\n            namespace (str): Application namespace\n            resource_namespace (str): Kubernetes namespace where the\n                Secret will be created.\n            api_endpoint (str): Krake API endpoint\n            token (str): Shutdown hook authentication token\n\n        Returns:\n            dict: shutdown hook secret resource\n\n        \"\"\"\n        shutdown_url = self.create_hook_url(name, namespace, api_endpoint)\n        data = {\n            self.env_token.lower(): self._encode_to_64(token),\n            self.env_url.lower(): self._encode_to_64(shutdown_url),\n        }\n        return self.secret(secret_name, data, resource_namespace)\n\n    @staticmethod\n    def create_hook_url(name, namespace, api_endpoint):\n        \"\"\"Create an applications' shutdown URL.\n\n        Args:\n            name (str): Application name\n            namespace (str): Application namespace\n            api_endpoint (str): Krake API endpoint\n\n        Returns:\n            str: Application shutdown url\n\n        \"\"\"\n        api_url = URL(api_endpoint)\n        return str(\n            api_url.with_path(\n                f\"/kubernetes/namespaces/{namespace}/applications/{name}/shutdown\"\n            )\n        )\n\n    def env_vars(self, secret_name):\n        \"\"\"Create shutdown hooks environment variables sub-resources.\n\n        Creates shutdown hook environment variables to store Krake authentication token\n        and a shutdown hook URL for given applications.\n\n        Args:\n            secret_name (str): Secret name\n\n        Returns:\n            list: List of shutdown hook environment variables sub-resources\n\n        \"\"\"\n        sub_resources = []\n\n        env_resources = []\n\n        env_token = V1EnvVar(\n            name=self.env_token,\n            value_from=self.attribute_map(\n                V1EnvVarSource(\n                    secret_key_ref=self.attribute_map(\n                        V1SecretKeySelector(\n                            name=secret_name,\n                            key=self.env_token.lower()\n                        )\n                    )\n                )\n            )\n        )\n        env_resources.append(env_token)\n\n        env_url = V1EnvVar(\n            name=self.env_url,\n            value_from=self.attribute_map(\n                V1EnvVarSource(\n                    secret_key_ref=self.attribute_map(\n                        V1SecretKeySelector(name=secret_name, key=self.env_url.lower())\n                    )\n                )\n            ),\n        )\n        env_resources.append(env_url)\n\n        for env in env_resources:\n            sub_resources.append(\n                SubResource(\n                    group=\"env\",\n                    name=env.name,\n                    body=self.attribute_map(env),\n                    path=(\n                        (\"spec\", \"template\", \"spec\", \"containers\"),\n                        (\"spec\", \"containers\"),  # kind: Pod\n                    ),\n                )\n            )\n        return sub_resources\n",
            "file_path": "krake/krake/controller/kubernetes/hooks.py",
            "human_label": "Together with :func:``update_last_applied_manifest_dict_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (list): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (list): partial ``observer_schema``\n    response (list): partial response from the Kubernetes API.\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present",
            "level": "file_runnable",
            "lineno": "301",
            "name": "update_last_applied_manifest_list_from_resp",
            "oracle_context": "{ \"apis\" : \"['isinstance', 'append', 'update_last_applied_manifest_dict_from_resp', 'len', 'enumerate']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }",
            "package": "hooks",
            "project": "rak-n-rok/Krake",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b869eab4d922cb0e688cc5",
            "all_context": "{ \"import\" : \"inspect logging asyncio collections secrets operator base64 contextlib enum functools datetime typing random copy logging aiohttp asyncio secrets enum functools krake datetime typing yarl \", \"file\" : \"logger ; listen ; update_last_applied_manifest_dict_from_resp(last_applied_manifest,observer_schema,response) ; update_last_applied_manifest_list_from_resp(last_applied_manifest,observer_schema,response) ; update_last_applied_manifest_from_resp(app,response) ; update_last_observed_manifest_from_resp(app,response) ; update_last_observed_manifest_dict(observed_resource,response) ; update_last_observed_manifest_list(observed_resource,response) ; update_last_applied_manifest_dict_from_spec(resource_status_new,resource_status_old,resource_observed) ; update_last_applied_manifest_list_from_spec(resource_status_new,resource_status_old,resource_observed) ; update_last_applied_manifest_from_spec(app) ; utc_difference() ; generate_certificate(config) ; generate_default_observer_schema(app) ; generate_default_observer_schema_dict(manifest_dict,first_level) ; generate_default_observer_schema_list(manifest_list) ; \", \"class\" : \"\" }",
            "code": "def update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n\n    \"\"\"\n    for key, value in observer_schema.items():\n\n        # Keys in the response are in camelCase\n        camel_key = camel_to_snake_case(key)\n\n        if camel_key not in response:\n            # An observed key should always be present in the k8s response\n            raise KeyError(\n                f\"Observed key {camel_key} is not present in response {response}\"\n            )\n\n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                # The dictionary is observed, but not present in\n                # last_applied_manifest\n                last_applied_manifest[key] = {}\n\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], observer_schema[key], response[camel_key]\n            )\n\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                # The list is observed, but not present in last_applied_manifest\n                last_applied_manifest[key] = []\n\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], observer_schema[key], response[camel_key]\n            )\n\n        elif key not in last_applied_manifest:\n            # If key not present in last_applied_manifest, and value is neither a\n            # dict nor a list, simply add it.\n            last_applied_manifest[key] = response[camel_key]\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : false, \"public_lib\" : true, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Together with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present",
            "end_lineno": "298",
            "file_content": "\"\"\"This module defines the Hook Dispatcher and listeners for registering and\nexecuting hooks. Hook Dispatcher emits hooks based on :class:`Hook` attributes which\ndefine when the hook will be executed.\n\n\"\"\"\nimport asyncio\nimport logging\nimport random\nfrom base64 import b64encode\nfrom collections import defaultdict\nfrom contextlib import suppress\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom functools import reduce\nfrom operator import getitem\nfrom enum import Enum, auto\nfrom inspect import iscoroutinefunction\nfrom OpenSSL import crypto\nfrom typing import NamedTuple\n\nimport yarl\nfrom aiohttp import ClientConnectorError\n\nfrom krake.controller import Observer\nfrom krake.controller.kubernetes.client import KubernetesClient, InvalidManifestError\nfrom krake.utils import camel_to_snake_case, get_kubernetes_resource_idx\nfrom kubernetes_asyncio.client.rest import ApiException\nfrom kubernetes_asyncio.client.api_client import ApiClient\nfrom kubernetes_asyncio import client\nfrom krake.data.kubernetes import ClusterState, Application, Cluster\nfrom yarl import URL\nfrom secrets import token_urlsafe\n\nfrom kubernetes_asyncio.client import (\n    Configuration,\n    V1Secret,\n    V1EnvVar,\n    V1VolumeMount,\n    V1Volume,\n    V1SecretKeySelector,\n    V1EnvVarSource,\n)\nfrom kubernetes_asyncio.config.kube_config import KubeConfigLoader\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass HookType(Enum):\n    ResourcePreCreate = auto()\n    ResourcePostCreate = auto()\n    ResourcePreUpdate = auto()\n    ResourcePostUpdate = auto()\n    ResourcePreDelete = auto()\n    ResourcePostDelete = auto()\n    ApplicationMangling = auto()\n    ApplicationPreMigrate = auto()\n    ApplicationPostMigrate = auto()\n    ApplicationPreReconcile = auto()\n    ApplicationPostReconcile = auto()\n    ApplicationPreDelete = auto()\n    ApplicationPostDelete = auto()\n    ClusterCreation = auto()\n    ClusterDeletion = auto()\n\n\nclass HookDispatcher(object):\n    \"\"\"Simple wrapper around a registry of handlers associated to :class:`Hook`\n     attributes. Each :class:`Hook` attribute defines when the handler will be\n     executed.\n\n    Listeners for certain hooks can be registered via :meth:`on`. Registered\n    listeners are executed via :meth:`hook`.\n\n    Example:\n        .. code:: python\n\n        listen = HookDispatcher()\n\n        @listen.on(HookType.PreApply)\n        def to_perform_before_app_creation(app, cluster, resource, controller):\n            # Do Stuff\n\n        @listen.on(HookType.PostApply)\n        def another_to_perform_after_app_creation(app, cluster, resource, resp):\n            # Do Stuff\n\n        @listen.on(HookType.PostDelete)\n        def to_perform_after_app_deletion(app, cluster, resource, resp):\n            # Do Stuff\n\n    \"\"\"\n\n    def __init__(self):\n        self.registry = defaultdict(list)\n\n    def on(self, hook):\n        \"\"\"Decorator function to add a new handler to the registry.\n\n        Args:\n            hook (HookType): Hook attribute for which to register the handler.\n\n        Returns:\n            callable: Decorator for registering listeners for the specified\n            hook.\n\n        \"\"\"\n\n        def decorator(handler):\n            self.registry[hook].append(handler)\n\n            return handler\n\n        return decorator\n\n    async def hook(self, hook, **kwargs):\n        \"\"\"Execute the list of handlers associated to the provided :class:`Hook`\n        attribute.\n\n        Args:\n            hook (HookType): The hook attribute for which to execute handlers.\n\n        \"\"\"\n        try:\n            handlers = self.registry[hook]\n        except KeyError:\n            pass\n        else:\n            for handler in handlers:\n                if iscoroutinefunction(handler):\n                    await handler(**kwargs)\n                else:\n                    handler(**kwargs)\n\n\nlisten = HookDispatcher()\n\n\n@listen.on(HookType.ResourcePostCreate)\n@listen.on(HookType.ResourcePostUpdate)\nasync def register_service(app, cluster, resource, response):\n    \"\"\"Register endpoint of Kubernetes Service object on creation and update.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        cluster (krake.data.kubernetes.Cluster): The cluster on which the\n            application is running\n        resource (dict): Kubernetes object description as specified in the\n            specification of the application.\n        response (kubernetes_asyncio.client.V1Service): Response of the\n            Kubernetes API\n\n    \"\"\"\n    if resource[\"kind\"] != \"Service\":\n        return\n\n    service_name = resource[\"metadata\"][\"name\"]\n\n    if response.spec and response.spec.type == \"LoadBalancer\":\n        # For a \"LoadBalancer\" type of Service, an external IP is given in the cluster\n        # by a load balancer controller to the service. In this case, the \"port\"\n        # specified in the spec is reachable from the outside.\n        if (\n            not response.status.load_balancer\n            or not response.status.load_balancer.ingress\n        ):\n            # When a \"LoadBalancer\" type of service is created, the IP is given by an\n            # additional controller (e.g. a controller that requests a floating IP to an\n            # OpenStack infrastructure). This process can take some time, but the\n            # Service itself already exist before the IP is assigned. In the case of an\n            # error with the controller, the IP is also not given. This \"<pending>\" IP\n            # just expresses that the Service exists, but the IP is not ready yet.\n            external_ip = \"<pending>\"\n        else:\n            external_ip = response.status.load_balancer.ingress[0].ip\n\n        if not response.spec.ports:\n            external_port = \"<pending>\"\n        else:\n            external_port = response.spec.ports[0].port\n        app.status.services[service_name] = f\"{external_ip}:{external_port}\"\n        return\n\n    node_port = None\n    # Ensure that ports are specified\n    if response.spec and response.spec.ports:\n        node_port = response.spec.ports[0].node_port\n\n    # If the service does not have a node port, remove a potential reference\n    # and return.\n    if node_port is None:\n        try:\n            del app.status.services[service_name]\n        except KeyError:\n            pass\n        return\n\n    # Determine URL of Kubernetes cluster API\n    loader = KubeConfigLoader(cluster.spec.kubeconfig)\n    config = Configuration()\n    await loader.load_and_set(config)\n    cluster_url = yarl.URL(config.host)\n\n    app.status.services[service_name] = f\"{cluster_url.host}:{node_port}\"\n\n\n@listen.on(HookType.ResourcePostDelete)\nasync def unregister_service(app, resource, **kwargs):\n    \"\"\"Unregister endpoint of Kubernetes Service object on deletion.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        resource (dict): Kubernetes object description as specified in the\n            specification of the application.\n\n    \"\"\"\n    if resource[\"kind\"] != \"Service\":\n        return\n\n    service_name = resource[\"metadata\"][\"name\"]\n    try:\n        del app.status.services[service_name]\n    except KeyError:\n        pass\n\n\n@listen.on(HookType.ResourcePostDelete)\nasync def remove_resource_from_last_observed_manifest(app, resource, **kwargs):\n    \"\"\"Remove a given resource from the last_observed_manifest after its deletion\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        resource (dict): Kubernetes object description as specified in the\n            specification of the application.\n\n    \"\"\"\n    try:\n        idx = get_kubernetes_resource_idx(app.status.last_observed_manifest, resource)\n    except IndexError:\n        return\n\n    app.status.last_observed_manifest.pop(idx)\n\n\ndef update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n\n    \"\"\"\n    for key, value in observer_schema.items():\n\n        # Keys in the response are in camelCase\n        camel_key = camel_to_snake_case(key)\n\n        if camel_key not in response:\n            # An observed key should always be present in the k8s response\n            raise KeyError(\n                f\"Observed key {camel_key} is not present in response {response}\"\n            )\n\n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                # The dictionary is observed, but not present in\n                # last_applied_manifest\n                last_applied_manifest[key] = {}\n\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], observer_schema[key], response[camel_key]\n            )\n\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                # The list is observed, but not present in last_applied_manifest\n                last_applied_manifest[key] = []\n\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], observer_schema[key], response[camel_key]\n            )\n\n        elif key not in last_applied_manifest:\n            # If key not present in last_applied_manifest, and value is neither a\n            # dict nor a list, simply add it.\n            last_applied_manifest[key] = response[camel_key]\n\n\ndef update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n\n    \"\"\"\n    # Looping over the observed resource, except the last element which is the\n    # special control dictionary\n    for idx, val in enumerate(observer_schema[:-1]):\n\n        if idx >= len(response):\n            # Element is observed but not present in k8s response, so following\n            # elements will also not exist.\n            #\n            # This doesn't raise an Exception as observing the element of a list\n            # doesn't ensure its presence. The list length is controlled by the\n            # special control dictionary\n            return\n\n        if isinstance(val, dict):\n            if idx >= len(last_applied_manifest):\n                # The dict is observed, but not present in last_applied_manifest\n                last_applied_manifest.append({})\n\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n\n        elif isinstance(response[idx], list):\n            if idx >= len(last_applied_manifest):\n                # The list is observed, but not present in last_applied_manifest\n                last_applied_manifest.append([])\n\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n\n        elif idx >= len(last_applied_manifest):\n            # Element is not yet present in last_applied_manifest. Adding it.\n            last_applied_manifest.append(response[idx])\n\n\n@listen.on(HookType.ResourcePostCreate)\n@listen.on(HookType.ResourcePostUpdate)\ndef update_last_applied_manifest_from_resp(app, response, **kwargs):\n    \"\"\"Hook run after the creation or update of an application in order to update the\n    `status.last_applied_manifest` using the k8s response.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        response (kubernetes_asyncio.client.V1Status): Response of the Kubernetes API\n\n    After a Kubernetes resource has been created/updated, the\n    `status.last_applied_manifest` has to be updated. All fields already initialized\n    (either from the mangling of `spec.manifest`, or by a previous call to this\n    function) should be left untouched. Only observed fields which are not present in\n    `status.last_applied_manifest` should be initialized.\n\n    \"\"\"\n\n    if isinstance(response, dict):\n        # The Kubernetes API couldn't deserialize the k8s response into an object\n        resp = response\n    else:\n        # The Kubernetes API deserialized the k8s response into an object\n        resp = response.to_dict()\n\n    idx_applied = get_kubernetes_resource_idx(app.status.last_applied_manifest, resp)\n\n    idx_observed = get_kubernetes_resource_idx(app.status.mangled_observer_schema, resp)\n\n    update_last_applied_manifest_dict_from_resp(\n        app.status.last_applied_manifest[idx_applied],\n        app.status.mangled_observer_schema[idx_observed],\n        resp,\n    )\n\n\n@listen.on(HookType.ResourcePostCreate)\n@listen.on(HookType.ResourcePostUpdate)\ndef update_last_observed_manifest_from_resp(app, response, **kwargs):\n    \"\"\"Handler to run after the creation or update of a Kubernetes resource to update\n    the last_observed_manifest from the response of the Kubernetes API.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        response (kubernetes_asyncio.client.V1Service): Response of the\n            Kubernetes API\n\n    The target last_observed_manifest holds the value of all observed fields plus the\n    special control dictionaries for the list length\n\n    \"\"\"\n    if isinstance(response, dict):\n        # The Kubernetes API couldn't deserialize the k8s response into an object\n        resp = response\n    else:\n        # The Kubernetes API deserialized the k8s response into an object\n        resp = response.to_dict()\n\n    try:\n        idx_observed = get_kubernetes_resource_idx(\n            app.status.mangled_observer_schema,\n            resp,\n        )\n    except IndexError:\n        # All created resources should be observed\n        raise\n\n    try:\n        idx_last_observed = get_kubernetes_resource_idx(\n            app.status.last_observed_manifest,\n            resp,\n        )\n    except IndexError:\n        # If the resource is not yes present in last_observed_manifest, append it.\n        idx_last_observed = len(app.status.last_observed_manifest)\n        app.status.last_observed_manifest.append({})\n\n    # Overwrite the last_observed_manifest for this resource\n    app.status.last_observed_manifest[\n        idx_last_observed\n    ] = update_last_observed_manifest_dict(\n        app.status.mangled_observer_schema[idx_observed], resp\n    )\n\n\ndef update_last_observed_manifest_dict(observed_resource, response):\n    \"\"\"Together with :func:``update_last_observed_manifest_list``, recursively\n    crafts the ``last_observed_manifest`` from the Kubernetes :attr:``response``.\n\n    Args:\n        observed_resource (dict): The schema to observe for the partial given resource\n        response (dict): The partial Kubernetes response for this resource.\n\n    Raises:\n        KeyError: If an observed key is not present in the Kubernetes response\n\n    Returns:\n        dict: The dictionary of observed keys and their value\n\n    Get the value of all observed fields from the Kubernetes response\n    \"\"\"\n    res = {}\n    for key, value in observed_resource.items():\n\n        camel_key = camel_to_snake_case(key)\n        if camel_key not in response:\n            raise KeyError(\n                f\"Observed key {camel_key} is not present in response {response}\"\n            )\n\n        if isinstance(value, dict):\n            res[key] = update_last_observed_manifest_dict(value, response[camel_key])\n\n        elif isinstance(value, list):\n            res[key] = update_last_observed_manifest_list(value, response[camel_key])\n\n        else:\n            res[key] = response[camel_key]\n\n    return res\n\n\ndef update_last_observed_manifest_list(observed_resource, response):\n    \"\"\"Together with :func:``update_last_observed_manifest_dict``, recursively\n    crafts the ``last_observed_manifest`` from the Kubernetes :attr:``response``.\n\n    Args:\n        observed_resource (list): the schema to observe for the partial given resource\n        response (list): the partial Kubernetes response for this resource.\n\n    Returns:\n        list: The list of observed elements, plus the special list length control\n            dictionary\n\n    Get the value of all observed elements from the Kubernetes response\n    \"\"\"\n\n    if not response:\n        return [{\"observer_schema_list_current_length\": 0}]\n\n    res = []\n    # Looping over the observed resource, except the last element which is the special\n    # control dictionary\n    for idx, val in enumerate(observed_resource[:-1]):\n\n        if idx >= len(response):\n            # Element is not present in the Kubernetes response, nothing more to do\n            break\n\n        if type(response[idx]) == dict:\n            res.append(update_last_observed_manifest_dict(val, response[idx]))\n\n        elif type(response[idx]) == list:\n            res.append(update_last_observed_manifest_list(val, response[idx]))\n\n        else:\n            res.append(response[idx])\n\n    # Append the special control dictionary to the list\n    res.append({\"observer_schema_list_current_length\": len(response)})\n\n    return res\n\n\ndef update_last_applied_manifest_dict_from_spec(\n    resource_status_new, resource_status_old, resource_observed\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_list_from_spec``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n\n    Args:\n        resource_status_new (dict): partial ``last_applied_manifest`` being updated\n        resource_status_old (dict): partial of the current ``last_applied_manifest``\n        resource_observed (dict): partial observer_schema for the manifest file\n            being updated\n\n    \"\"\"\n    for key, value in resource_observed.items():\n\n        if key not in resource_status_old:\n            continue\n\n        if key in resource_status_new:\n\n            if isinstance(value, dict):\n                update_last_applied_manifest_dict_from_spec(\n                    resource_status_new[key],\n                    resource_status_old[key],\n                    resource_observed[key],\n                )\n\n            elif isinstance(value, list):\n                update_last_applied_manifest_list_from_spec(\n                    resource_status_new[key],\n                    resource_status_old[key],\n                    resource_observed[key],\n                )\n\n        else:\n            # If the key is not present the spec.manifest, we first need to\n            # initialize it\n\n            if isinstance(value, dict):\n                resource_status_new[key] = {}\n                update_last_applied_manifest_dict_from_spec(\n                    resource_status_new[key],\n                    resource_status_old[key],\n                    resource_observed[key],\n                )\n\n            elif isinstance(value, list):\n                resource_status_new[key] = []\n                update_last_applied_manifest_list_from_spec(\n                    resource_status_new[key],\n                    resource_status_old[key],\n                    resource_observed[key],\n                )\n\n            else:\n                resource_status_new[key] = resource_status_old[key]\n\n\ndef update_last_applied_manifest_list_from_spec(\n    resource_status_new, resource_status_old, resource_observed\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_dict_from_spec``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n\n    Args:\n        resource_status_new (list): partial ``last_applied_manifest`` being updated\n        resource_status_old (list): partial of the current ``last_applied_manifest``\n        resource_observed (list): partial observer_schema for the manifest file\n            being updated\n\n    \"\"\"\n\n    # Looping over the observed resource, except the last element which is the\n    # special control dictionary\n    for idx, val in enumerate(resource_observed[:-1]):\n\n        if idx >= len(resource_status_old):\n            # The element in not in the current last_applied_manifest, and neither\n            # is the rest of the list\n            break\n\n        if idx < len(resource_status_new):\n            # The element is present in spec.manifest and in the current\n            # last_applied_manifest. Updating observed fields\n\n            if isinstance(val, dict):\n                update_last_applied_manifest_dict_from_spec(\n                    resource_status_new[idx],\n                    resource_status_old[idx],\n                    resource_observed[idx],\n                )\n\n            elif isinstance(val, list):\n                update_last_applied_manifest_list_from_spec(\n                    resource_status_new[idx],\n                    resource_status_old[idx],\n                    resource_observed[idx],\n                )\n\n        else:\n            # If the element is not present in the spec.manifest, we first have to\n            # initialize it.\n\n            if isinstance(val, dict):\n                resource_status_new.append({})\n                update_last_applied_manifest_dict_from_spec(\n                    resource_status_new[idx],\n                    resource_status_old[idx],\n                    resource_observed[idx],\n                )\n\n            elif isinstance(val, list):\n                resource_status_new.append([])\n                update_last_applied_manifest_list_from_spec(\n                    resource_status_new[idx],\n                    resource_status_old[idx],\n                    resource_observed[idx],\n                )\n\n            else:\n                resource_status_new.append(resource_status_old[idx])\n\n\ndef update_last_applied_manifest_from_spec(app):\n    \"\"\"Update the status.last_applied_manifest of an application from spec.manifests\n\n    Args:\n        app (krake.data.kubernetes.Application): Application to update\n\n    This function is called on application creation and updates. The\n    last_applied_manifest of an application is initialized as a copy of spec.manifest,\n    and is augmented by all known observed fields not yet initialized (i.e. all observed\n    fields or resources which are present in the current last_applied_manifest but not\n    in the spec.manifest)\n\n    \"\"\"\n\n    # The new last_applied_manifest is initialized as a copy of the spec.manifest, and\n    # augmented by all observed fields which are present in the current\n    # last_applied_manifest but not in the original spec.manifest\n    new_last_applied_manifest = deepcopy(app.spec.manifest)\n\n    # Loop over observed resources and observed fields, and check if they should be\n    # added to the new last_applied_manifest (i.e. present in the current\n    # last_applied_manifest but not in spec.manifest)\n    for resource_observed in app.status.mangled_observer_schema:\n\n        # If the resource is not present in the current last_applied_manifest, there is\n        # nothing to do. Whether the resource was initialized by spec.manifest doesn't\n        # matter.\n        try:\n            idx_status_old = get_kubernetes_resource_idx(\n                app.status.last_applied_manifest, resource_observed\n            )\n        except IndexError:\n            continue\n\n        # As the resource is present in the current last_applied_manifest, we need to go\n        # through it to check if observed fields should be set to their current value\n        # (i.e. fields are present in the current last_applied_manifest, but not in\n        # spec.manifest)\n        try:\n            # Check if the observed resource is present in spec.manifest\n            idx_status_new = get_kubernetes_resource_idx(\n                new_last_applied_manifest, resource_observed\n            )\n        except IndexError:\n            # The resource is observed but is not present in the spec.manifest.\n            # Create an empty resource, which will be augmented in\n            # update_last_applied_manifest_dict_from_spec with the observed and known\n            # fields.\n            new_last_applied_manifest.append({})\n            idx_status_new = len(new_last_applied_manifest) - 1\n\n        update_last_applied_manifest_dict_from_spec(\n            new_last_applied_manifest[idx_status_new],\n            app.status.last_applied_manifest[idx_status_old],\n            resource_observed,\n        )\n\n    app.status.last_applied_manifest = new_last_applied_manifest\n\n\nclass KubernetesApplicationObserver(Observer):\n    \"\"\"Observer specific for Kubernetes Applications. One observer is created for each\n    Application managed by the Controller, but not one per Kubernetes resource\n    (Deployment, Service...). If several resources are defined by an Application, they\n    are all monitored by the same observer.\n\n    The observer gets the actual status of the resources on the cluster using the\n    Kubernetes API, and compare it to the status stored in the API.\n\n    The observer is:\n     * started at initial Krake resource creation;\n\n     * deleted when a resource needs to be updated, then started again when it is done;\n\n     * simply deleted on resource deletion.\n\n    Args:\n        cluster (krake.data.kubernetes.Cluster): the cluster on which the observed\n            Application is created.\n        resource (krake.data.kubernetes.Application): the application that will be\n            observed.\n        on_res_update (coroutine): a coroutine called when a resource's actual status\n            differs from the status sent by the database. Its signature is:\n            ``(resource) -> updated_resource``. ``updated_resource`` is the instance of\n            the resource that is up-to-date with the API. The Observer internal instance\n            of the resource to observe will be updated. If the API cannot be contacted,\n            ``None`` can be returned. In this case the internal instance of the Observer\n            will not be updated.\n        time_step (int, optional): how frequently the Observer should watch the actual\n            status of the resources.\n\n    \"\"\"\n\n    def __init__(self, cluster, resource, on_res_update, time_step=2):\n        super().__init__(resource, on_res_update, time_step)\n        self.cluster = cluster\n\n    async def poll_resource(self):\n        \"\"\"Fetch the current status of the Application monitored by the Observer.\n\n        Returns:\n            krake.data.core.Status: the status object created using information from the\n                real world Applications resource.\n\n        \"\"\"\n        app = self.resource\n\n        status = deepcopy(app.status)\n        status.last_observed_manifest = []\n        # For each observed kubernetes resource of the Application,\n        # get its current status on the cluster.\n        for desired_resource in app.status.last_applied_manifest:\n            kube = KubernetesClient(self.cluster.spec.kubeconfig)\n            idx_observed = get_kubernetes_resource_idx(\n                app.status.mangled_observer_schema, desired_resource\n            )\n            observed_resource = app.status.mangled_observer_schema[idx_observed]\n            async with kube:\n                try:\n                    group, version, kind, name, namespace = kube.get_immutables(\n                        desired_resource\n                    )\n                    resource_api = await kube.get_resource_api(group, version, kind)\n                    resp = await resource_api.read(kind, name, namespace)\n                except ApiException as err:\n                    if err.status == 404:\n                        # Resource does not exist\n                        continue\n                    # Otherwise, log the unexpected errors\n                    logger.error(err)\n\n            observed_manifest = update_last_observed_manifest_dict(\n                observed_resource, resp.to_dict()\n            )\n            status.last_observed_manifest.append(observed_manifest)\n\n        return status\n\n\nclass KubernetesClusterObserver(Observer):\n    \"\"\"Observer specific for Kubernetes Clusters. One observer is created for each\n    Cluster managed by the Controller.\n\n    The observer gets the actual status of the cluster using the\n    Kubernetes API, and compare it to the status stored in the API.\n\n    The observer is:\n     * started at initial Krake resource creation;\n\n     * deleted when a resource needs to be updated, then started again when it is done;\n\n     * simply deleted on resource deletion.\n\n    Args:\n        cluster (krake.data.kubernetes.Cluster): the cluster which will be observed.\n        on_res_update (coroutine): a coroutine called when a resource's actual status\n            differs from the status sent by the database. Its signature is:\n            ``(resource) -> updated_resource``. ``updated_resource`` is the instance of\n            the resource that is up-to-date with the API. The Observer internal instance\n            of the resource to observe will be updated. If the API cannot be contacted,\n            ``None`` can be returned. In this case the internal instance of the Observer\n            will not be updated.\n        time_step (int, optional): how frequently the Observer should watch the actual\n            status of the resources.\n\n    \"\"\"\n\n    def __init__(self, cluster, on_res_update, time_step=2):\n        super().__init__(cluster, on_res_update, time_step)\n        self.cluster = cluster\n\n    async def poll_resource(self):\n        \"\"\"Fetch the current status of the Cluster monitored by the Observer.\n\n        Returns:\n            krake.data.core.Status: the status object created using information from the\n                real world Cluster.\n\n        \"\"\"\n        status = deepcopy(self.cluster.status)\n        # For each observed kubernetes cluster registered in Krake,\n        # get its current node status.\n        loader = KubeConfigLoader(self.cluster.spec.kubeconfig)\n        config = Configuration()\n        await loader.load_and_set(config)\n        kube = ApiClient(config)\n\n        async with kube as api:\n            v1 = client.CoreV1Api(api)\n            try:\n                response = await v1.list_node()\n\n            except ClientConnectorError as err:\n                status.state = ClusterState.OFFLINE\n                self.cluster.status.state = ClusterState.OFFLINE\n                # Log the error\n                logger.debug(err)\n                return status\n\n            condition_dict = {\n                \"MemoryPressure\": [],\n                \"DiskPressure\": [],\n                \"PIDPressure\": [],\n                \"Ready\": [],\n            }\n\n            for item in response.items:\n                for condition in item.status.conditions:\n                    condition_dict[condition.type].append(condition.status)\n                if (\n                    condition_dict[\"MemoryPressure\"] == [\"True\"]\n                    or condition_dict[\"DiskPressure\"] == [\"True\"]\n                    or condition_dict[\"PIDPressure\"] == [\"True\"]\n                ):\n                    status.state = ClusterState.UNHEALTHY\n                    self.cluster.status.state = ClusterState.UNHEALTHY\n                    return status\n                elif (\n                    condition_dict[\"Ready\"] == [\"True\"]\n                    and status.state is ClusterState.OFFLINE\n                ):\n                    status.state = ClusterState.CONNECTING\n                    self.cluster.status.state = ClusterState.CONNECTING\n                    return status\n                elif condition_dict[\"Ready\"] == [\"True\"]:\n                    status.state = ClusterState.ONLINE\n                    self.cluster.status.state = ClusterState.ONLINE\n                    return status\n                else:\n                    status.state = ClusterState.NOTREADY\n                    self.cluster.status.state = ClusterState.NOTREADY\n                    return status\n\n\n@listen.on(HookType.ApplicationPostReconcile)\n@listen.on(HookType.ApplicationPostMigrate)\n@listen.on(HookType.ClusterCreation)\nasync def register_observer(controller, resource, start=True, **kwargs):\n    \"\"\"Create an observer for the given Application or Cluster, and start it as a\n    background task if wanted.\n\n    If an observer already existed for this Application or Cluster, it is stopped\n    and deleted.\n\n    Args:\n        controller (KubernetesController): the controller for which the observer will be\n            added in the list of working observers.\n        resource (krake.data.kubernetes.Application): the Application to observe or\n        resource (krake.data.kubernetes.Cluster): the Cluster to observe.\n        start (bool, optional): if False, does not start the observer as background\n            task.\n\n    \"\"\"\n    if resource.kind == Application.kind:\n        cluster = await controller.kubernetes_api.read_cluster(\n            namespace=resource.status.running_on.namespace,\n            name=resource.status.running_on.name,\n        )\n\n        observer = KubernetesApplicationObserver(\n            cluster,\n            resource,\n            controller.on_status_update,\n            time_step=controller.observer_time_step,\n        )\n\n    elif resource.kind == Cluster.kind:\n        observer = KubernetesClusterObserver(\n            resource,\n            controller.on_status_update,\n            time_step=controller.observer_time_step,\n        )\n    else:\n        logger.debug(\"Unknown resource kind. No observer was registered.\", resource)\n        return\n\n    logger.debug(f\"Start observer for {resource.kind} %r\", resource.metadata.name)\n    task = None\n    if start:\n        task = controller.loop.create_task(observer.run())\n\n    controller.observers[resource.metadata.uid] = (observer, task)\n\n\n@listen.on(HookType.ApplicationPreReconcile)\n@listen.on(HookType.ApplicationPreMigrate)\n@listen.on(HookType.ApplicationPreDelete)\n@listen.on(HookType.ClusterDeletion)\nasync def unregister_observer(controller, resource, **kwargs):\n    \"\"\"Stop and delete the observer for the given Application or Cluster. If no observer\n    is started, do nothing.\n\n    Args:\n        controller (KubernetesController): the controller for which the observer will be\n            removed from the list of working observers.\n        resource (krake.data.kubernetes.Application): the Application whose observer\n        will be stopped or\n        resource (krake.data.kubernetes.Cluster): the Cluster whose observer will be\n        stopped.\n\n    \"\"\"\n    if resource.metadata.uid not in controller.observers:\n        return\n\n    logger.debug(f\"Stop observer for {resource.kind} %r\", resource.metadata.name)\n    _, task = controller.observers.pop(resource.metadata.uid)\n    task.cancel()\n\n    with suppress(asyncio.CancelledError):\n        await task\n\n\ndef utc_difference():\n    \"\"\"Get the difference in seconds between the current time and the current UTC time.\n\n    Returns:\n        int: the time difference in seconds.\n\n    \"\"\"\n    delta = datetime.now() - datetime.utcnow()\n    return delta.seconds\n\n\ndef generate_certificate(config):\n    \"\"\"Create and sign a new certificate using the one defined in the complete hook\n    configuration as intermediate certificate.\n\n    Args:\n        config (krake.data.config.CompleteHookConfiguration): the configuration of the\n            complete hook.\n\n    Returns:\n        CertificatePair: the content of the certificate created and its corresponding\n            key.\n\n    \"\"\"\n    with open(config.intermediate_src, \"rb\") as f:\n        intermediate_src = crypto.load_certificate(crypto.FILETYPE_PEM, f.read())\n    with open(config.intermediate_key_src, \"rb\") as f:\n        intermediate_key_src = crypto.load_privatekey(crypto.FILETYPE_PEM, f.read())\n\n    client_cert = crypto.X509()\n\n    # Set general information\n    client_cert.set_version(3)\n    client_cert.set_serial_number(random.randint(50000000000000, 100000000000000))\n    # If not set before, TLS will not accept to use this certificate in UTC cases, as\n    # the server time may be earlier.\n    time_offset = utc_difference() * -1\n    client_cert.gmtime_adj_notBefore(time_offset)\n    client_cert.gmtime_adj_notAfter(1 * 365 * 24 * 60 * 60)\n\n    # Set issuer and subject\n    intermediate_subject = intermediate_src.get_subject()\n    client_cert.set_issuer(intermediate_subject)\n    client_subj = crypto.X509Name(intermediate_subject)\n    client_subj.CN = config.hook_user\n    client_cert.set_subject(client_subj)\n\n    # Create and set the private key\n    client_key = crypto.PKey()\n    client_key.generate_key(crypto.TYPE_RSA, 2048)\n    client_cert.set_pubkey(client_key)\n\n    client_cert.sign(intermediate_key_src, \"sha256\")\n\n    cert_dump = crypto.dump_certificate(crypto.FILETYPE_PEM, client_cert).decode()\n    key_dump = crypto.dump_privatekey(crypto.FILETYPE_PEM, client_key).decode()\n    return CertificatePair(cert=cert_dump, key=key_dump)\n\n\ndef generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n\n    app.status.mangled_observer_schema = deepcopy(app.spec.observer_schema)\n\n    for resource_manifest in app.spec.manifest:\n        try:\n            get_kubernetes_resource_idx(\n                app.status.mangled_observer_schema, resource_manifest\n            )\n\n        except IndexError:\n            # Only create a default observer schema, if a custom observer schema hasn't\n            # been set by the user.\n            app.status.mangled_observer_schema.append(\n                generate_default_observer_schema_dict(\n                    resource_manifest,\n                    first_level=True,\n                )\n            )\n\n\ndef generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n\n    \"\"\"\n    observer_schema_dict = {}\n\n    for key, value in manifest_dict.items():\n\n        if isinstance(value, dict):\n            observer_schema_dict[key] = generate_default_observer_schema_dict(value)\n\n        elif isinstance(value, list):\n            observer_schema_dict[key] = generate_default_observer_schema_list(value)\n\n        else:\n            observer_schema_dict[key] = None\n\n    if first_level:\n        observer_schema_dict[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        observer_schema_dict[\"kind\"] = manifest_dict[\"kind\"]\n        observer_schema_dict[\"metadata\"][\"name\"] = manifest_dict[\"metadata\"][\"name\"]\n\n        if (\n            \"spec\" in manifest_dict\n            and \"type\" in manifest_dict[\"spec\"]\n            and manifest_dict[\"spec\"][\"type\"] == \"LoadBalancer\"\n        ):\n            observer_schema_dict[\"status\"] = {\"load_balancer\": {\"ingress\": None}}\n\n    return observer_schema_dict\n\n\ndef generate_default_observer_schema_list(manifest_list):\n    \"\"\"Together with :func:``generate_default_observer_schema_dict``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_list`` or ``manifest_dict``.\n\n    Args:\n        manifest_list (list): Partial Kubernetes resources\n\n    Returns:\n        list: Generated partial observer_schema\n\n    This function creates a new list from ``manifest_list`` and replaces all non-list\n    and non-dict elements by ``None``.\n\n    Additionally, it generates the default list control dictionary, using the current\n    length of the list as default minimum and maximum values.\n\n    \"\"\"\n    observer_schema_list = []\n\n    for value in manifest_list:\n\n        if isinstance(value, dict):\n            observer_schema_list.append(generate_default_observer_schema_dict(value))\n\n        elif isinstance(value, list):\n            observer_schema_list.append(generate_default_observer_schema_list(value))\n\n        else:\n            observer_schema_list.append(None)\n\n    observer_schema_list.append(\n        {\n            \"observer_schema_list_min_length\": len(manifest_list),\n            \"observer_schema_list_max_length\": len(manifest_list),\n        }\n    )\n\n    return observer_schema_list\n\n\n@listen.on(HookType.ApplicationMangling)\nasync def complete(app, api_endpoint, ssl_context, config):\n    \"\"\"Execute application complete hook defined by :class:`Complete`.\n    Hook mangles given application and injects complete hooks variables.\n\n    Application complete hook is disabled by default.\n    User enables this hook by the --hook-complete argument in rok cli.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application object processed\n            when the hook is called\n        api_endpoint (str): the given API endpoint\n        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint\n        config (krake.data.config.HooksConfiguration): Complete hook\n            configuration.\n\n    \"\"\"\n    if \"complete\" not in app.spec.hooks:\n        return\n\n    # Use the endpoint of the API only if the external endpoint has not been set.\n    if config.complete.external_endpoint:\n        api_endpoint = config.complete.external_endpoint\n\n    app.status.complete_token = \\\n        app.status.complete_token if app.status.complete_token else token_urlsafe()\n\n    # Generate only once the certificate and key for a specific Application\n    generated_cert = CertificatePair(\n        cert=app.status.complete_cert, key=app.status.complete_key\n    )\n    if ssl_context and generated_cert == (None, None):\n        generated_cert = generate_certificate(config.complete)\n        app.status.complete_cert = generated_cert.cert\n        app.status.complete_key = generated_cert.key\n\n    hook = Complete(\n        api_endpoint,\n        ssl_context,\n        hook_user=config.complete.hook_user,\n        cert_dest=config.complete.cert_dest,\n        env_token=config.complete.env_token,\n        env_url=config.complete.env_url,\n    )\n    hook.mangle_app(\n        app.metadata.name,\n        app.metadata.namespace,\n        app.status.complete_token,\n        app.status.last_applied_manifest,\n        config.complete.intermediate_src,\n        generated_cert,\n        app.status.mangled_observer_schema,\n        \"complete\"\n    )\n\n\n@listen.on(HookType.ApplicationMangling)\nasync def shutdown(app, api_endpoint, ssl_context, config):\n    \"\"\"Executes an application shutdown hook defined by :class:`Shutdown`.\n    The hook mangles the given application and injects shutdown hooks variables.\n\n    Application shutdown hook is disabled by default.\n    User enables this hook by the --hook-shutdown argument in rok cli.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application object processed\n            when the hook is called\n        api_endpoint (str): the given API endpoint\n        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint\n        config (krake.data.config.HooksConfiguration): Shutdown hook\n            configuration.\n\n    \"\"\"\n    if \"shutdown\" not in app.spec.hooks:\n        return\n\n    # Use the endpoint of the API only if the external endpoint has not been set.\n    if config.shutdown.external_endpoint:\n        api_endpoint = config.shutdown.external_endpoint\n\n    app.status.shutdown_token = \\\n        app.status.shutdown_token if app.status.shutdown_token else token_urlsafe()\n\n    # Generate only once the certificate and key for a specific Application\n    generated_cert = CertificatePair(\n        cert=app.status.shutdown_cert, key=app.status.shutdown_key\n    )\n    if ssl_context and generated_cert == (None, None):\n        generated_cert = generate_certificate(config.shutdown)\n        app.status.shutdown_cert = generated_cert.cert\n        app.status.shutdown_key = generated_cert.key\n\n    hook = Shutdown(\n        api_endpoint,\n        ssl_context,\n        hook_user=config.shutdown.hook_user,\n        cert_dest=config.shutdown.cert_dest,\n        env_token=config.shutdown.env_token,\n        env_url=config.shutdown.env_url,\n    )\n    hook.mangle_app(\n        app.metadata.name,\n        app.metadata.namespace,\n        app.status.shutdown_token,\n        app.status.last_applied_manifest,\n        config.shutdown.intermediate_src,\n        generated_cert,\n        app.status.mangled_observer_schema,\n        \"shutdown\"\n    )\n\n\n@listen.on(HookType.ResourcePreDelete)\nasync def pre_shutdown(controller, app, **kwargs):\n    \"\"\"\n\n    Args:\n        app (krake.data.kubernetes.Application): Application object processed\n            when the hook is called\n    \"\"\"\n    if \"shutdown\" not in app.spec.hooks:\n        return\n\n    return\n\n\nclass SubResource(NamedTuple):\n    group: str\n    name: str\n    body: dict\n    path: tuple\n\n\nclass CertificatePair(NamedTuple):\n    \"\"\"Tuple which contains a certificate and its corresponding key.\n\n    Attributes:\n        cert (str): content of a certificate.\n        key (str): content of the key that corresponds to the certificate.\n\n    \"\"\"\n\n    cert: str\n    key: str\n\n\nclass Hook(object):\n\n    hook_resources = ()\n\n    ca_name = \"ca-bundle.pem\"\n    cert_name = \"cert.pem\"\n    key_name = \"key.pem\"\n\n    def __init__(\n        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n    ):\n        self.api_endpoint = api_endpoint\n        self.ssl_context = ssl_context\n        self.hook_user = hook_user\n        self.cert_dest = cert_dest\n        self.env_token = env_token\n        self.env_url = env_url\n\n    def mangle_app(\n        self,\n        name,\n        namespace,\n        token,\n        last_applied_manifest,\n        intermediate_src,\n        generated_cert,\n        mangled_observer_schema,\n        hook_type=\"\",\n    ):\n        \"\"\"Mangle a given application and inject complete hook resources and\n        sub-resources into the :attr:`last_applied_manifest` object by :meth:`mangle`.\n        Also mangle the observer_schema as new resources and sub-resources should\n        be observed.\n\n        :attr:`last_applied_manifest` is created as a deep copy of the desired\n        application resources, as defined by user. It can be updated by custom hook\n        resources or modified by custom hook sub-resources. It is used as a desired\n        state for the Krake deployment process.\n\n        Args:\n            name (str): Application name\n            namespace (str): Application namespace\n            token (str): Complete hook authentication token\n            last_applied_manifest (list): Application resources\n            intermediate_src (str): content of the certificate that is used to sign new\n                certificates for the complete hook.\n            generated_cert (CertificatePair): tuple that contains the content of the\n                new signed certificate for the Application, and the content of its\n                corresponding key.\n            mangled_observer_schema (list): Observed fields\n            hook_type (str, optional): Name of the hook the app should be mangled for\n\n        \"\"\"\n\n        secret_certs_name = \"-\".join([name, \"krake\", hook_type, \"secret\", \"certs\"])\n        secret_token_name = \"-\".join([name, \"krake\", hook_type, \"secret\", \"token\"])\n        volume_name = \"-\".join([name, \"krake\", hook_type, \"volume\"])\n        ca_certs = (\n            self.ssl_context.get_ca_certs(binary_form=True)\n            if self.ssl_context\n            else None\n        )\n\n        # Extract all different namespaces\n        # FIXME: too many assumptions here: do we create one ConfigMap for each\n        #  namespace?\n        resource_namespaces = {\n            resource[\"metadata\"].get(\"namespace\", \"default\")\n            for resource in last_applied_manifest\n        }\n\n        hook_resources = []\n        hook_sub_resources = []\n        if ca_certs:\n            hook_resources.extend(\n                [\n                    self.secret_certs(\n                        secret_certs_name,\n                        resource_namespace,\n                        intermediate_src=intermediate_src,\n                        generated_cert=generated_cert,\n                        ca_certs=ca_certs,\n                    )\n                    for resource_namespace in resource_namespaces\n                ]\n            )\n            hook_sub_resources.extend(\n                [*self.volumes(secret_certs_name, volume_name, self.cert_dest)]\n            )\n\n        hook_resources.extend(\n            [\n                self.secret_token(\n                    secret_token_name,\n                    name,\n                    namespace,\n                    resource_namespace,\n                    self.api_endpoint,\n                    token,\n                )\n                for resource_namespace in resource_namespaces\n            ]\n        )\n        hook_sub_resources.extend(\n            [\n                *self.env_vars(secret_token_name),\n            ]\n        )\n\n        self.mangle(\n            hook_resources,\n            last_applied_manifest,\n            mangled_observer_schema,\n        )\n        self.mangle(\n            hook_sub_resources,\n            last_applied_manifest,\n            mangled_observer_schema,\n            is_sub_resource=True,\n        )\n\n    def mangle(\n        self,\n        items,\n        last_applied_manifest,\n        mangled_observer_schema,\n        is_sub_resource=False,\n    ):\n        \"\"\"Mangle applications desired state with custom hook resources or\n        sub-resources.\n\n        Example:\n            .. code:: python\n\n            last_applied_manifest = [\n                {\n                    'apiVersion': 'v1',\n                    'kind': 'Pod',\n                    'metadata': {'name': 'test', 'namespace': 'default'},\n                    'spec': {'containers': [{'name': 'test'}]}\n                }\n            ]\n            mangled_observer_schema = [\n                {\n                    'apiVersion': 'v1',\n                    'kind': 'Pod',\n                    'metadata': {'name': 'test', 'namespace': 'default'},\n                    'spec': {\n                        'containers': [\n                            {'name': None},\n                            {\n                                'observer_schema_list_max_length': 1,\n                                'observer_schema_list_min_length': 1,\n                            },\n                        ]\n                    },\n                }\n            ]\n            hook_resources = [\n                {\n                    'apiVersion': 'v1',\n                    'kind': 'Secret',\n                    'metadata': {'name': 'sct', 'namespace': 'default'}\n                }\n            ]\n            hook_sub_resources = [\n                SubResource(\n                    group='env', name='env', body={'name': 'test', 'value': 'test'},\n                    path=(('spec', 'containers'),)\n                )\n            ]\n\n            mangle(\n                hook_resources,\n                last_applied_manifest,\n                mangled_observer_schema,\n            )\n            mangle(\n                hook_sub_resources,\n                last_applied_manifest,\n                mangled_observer_schema,\n                is_sub_resource=True\n            )\n\n            assert last_applied_manifest == [\n                {\n                    \"apiVersion\": \"v1\",\n                    \"kind\": \"Pod\",\n                    \"metadata\": {\"name\": \"test\", 'namespace': 'default'},\n                    \"spec\": {\n                        \"containers\": [\n                            {\n                                \"name\": \"test\",\n                                \"env\": [{\"name\": \"test\", \"value\": \"test\"}]\n                            }\n                        ]\n                    },\n                },\n                {\"apiVersion\": \"v1\", \"kind\": \"Secret\", \"metadata\": {\"name\": \"sct\"}},\n            ]\n\n            assert mangled_observer_schema == [\n                {\n                    \"apiVersion\": \"v1\",\n                    \"kind\": \"Pod\",\n                    \"metadata\": {\"name\": \"test\", \"namespace\": None},\n                    \"spec\": {\n                        \"containers\": [\n                            {\n                                \"name\": None,\n                                \"env\": [\n                                    {\"name\": None, \"value\": None},\n                                    {\n                                        \"observer_schema_list_max_length\": 1,\n                                        \"observer_schema_list_min_length\": 1,\n                                    },\n                                ],\n                            },\n                            {\n                                \"observer_schema_list_max_length\": 1,\n                                \"observer_schema_list_min_length\": 1,\n                            },\n                        ]\n                    },\n                },\n                {\n                    \"apiVersion\": \"v1\",\n                    \"kind\": \"Secret\",\n                    \"metadata\": {\"name\": \"sct\", \"namespace\": None},\n                },\n            ]\n\n        Args:\n            items (list[SubResource]): Custom hook resources or sub-resources\n            last_applied_manifest (list): Application resources\n            mangled_observer_schema (list): Observed resources\n            is_sub_resource (bool, optional): if False, the function only extend the\n                list of Kubernetes resources defined in :attr:`last_applied_manifest`\n                with new hook resources. Otherwise, the function injects each new hook\n                sub-resource into the :attr:`last_applied_manifest` object\n                sub-resources. Defaults to False.\n\n        \"\"\"\n\n        if not items:\n            return\n\n        if not is_sub_resource:\n            last_applied_manifest.extend(items)\n            for sub_resource in items:\n                # Generate the default observer schema for each resource\n                mangled_observer_schema.append(\n                    generate_default_observer_schema_dict(\n                        sub_resource,\n                        first_level=True,\n                    )\n                )\n            return\n\n        def inject(sub_resource, sub_resource_to_mangle, observed_resource_to_mangle):\n            \"\"\"Inject a hooks defined sub-resource into a Kubernetes sub-resource.\n\n            Args:\n                sub_resource (SubResource): Hook sub-resource that needs to be injected\n                    into :attr:`last_applied_manifest`\n                sub_resource_to_mangle (object): Kubernetes sub-resources from\n                    :attr:`last_applied_manifest` which need to be processed\n                observed_resource_to_mangle (dict): partial mangled_observer_schema\n                    corresponding to the Kubernetes sub-resource.\n\n            Raises:\n                InvalidManifestError: if the sub-resource which will be mangled is not a\n                    list or a dict.\n\n            \"\"\"\n\n            # Create sub-resource group if not present in the Kubernetes sub-resource\n            if sub_resource.group not in sub_resource_to_mangle:\n                # FIXME: This assumes the subresource group contains a list\n                sub_resource_to_mangle.update({sub_resource.group: []})\n\n            # Create sub-resource group if not present in the observed fields\n            if sub_resource.group not in observed_resource_to_mangle:\n                observed_resource_to_mangle.update(\n                    {\n                        sub_resource.group: [\n                            {\n                                \"observer_schema_list_min_length\": 0,\n                                \"observer_schema_list_max_length\": 0,\n                            }\n                        ]\n                    }\n                )\n\n            # Inject sub-resource\n            # If sub-resource name is already there update it, if not, append it\n            if sub_resource.name in [\n                g[\"name\"] for g in sub_resource_to_mangle[sub_resource.group]\n            ]:\n                # FIXME: Assuming we are dealing with a list\n                for idx, item in enumerate(sub_resource_to_mangle[sub_resource.group]):\n                    if item[\"name\"]:\n                        if hasattr(item, \"body\"):\n                            sub_resource_to_mangle[item.group][idx] = item[\"body\"]\n            else:\n                sub_resource_to_mangle[sub_resource.group].append(sub_resource.body)\n\n            # Make sure the value is observed\n            if sub_resource.name not in [\n                g[\"name\"] for g in observed_resource_to_mangle[sub_resource.group][:-1]\n            ]:\n                observed_resource_to_mangle[sub_resource.group].insert(\n                    -1, generate_default_observer_schema_dict(sub_resource.body)\n                )\n                observed_resource_to_mangle[sub_resource.group][-1][\n                    \"observer_schema_list_min_length\"\n                ] += 1\n                observed_resource_to_mangle[sub_resource.group][-1][\n                    \"observer_schema_list_max_length\"\n                ] += 1\n\n        for resource in last_applied_manifest:\n            # Complete hook is applied only on defined Kubernetes resources\n            if resource[\"kind\"] not in self.hook_resources:\n                continue\n\n            for sub_resource in items:\n                sub_resources_to_mangle = None\n                idx_observed = get_kubernetes_resource_idx(\n                    mangled_observer_schema, resource\n                )\n                for keys in sub_resource.path:\n                    try:\n                        sub_resources_to_mangle = reduce(getitem, keys, resource)\n                    except KeyError:\n                        continue\n\n                    break\n\n                # Create the path to the observed sub-resource, if it doesn't yet exist\n                try:\n                    observed_sub_resources = reduce(\n                        getitem, keys, mangled_observer_schema[idx_observed]\n                    )\n                except KeyError:\n                    Complete.create_path(\n                        mangled_observer_schema[idx_observed], list(keys)\n                    )\n                    observed_sub_resources = reduce(\n                        getitem, keys, mangled_observer_schema[idx_observed]\n                    )\n\n                if isinstance(sub_resources_to_mangle, list):\n                    for idx, sub_resource_to_mangle in enumerate(\n                        sub_resources_to_mangle\n                    ):\n\n                        # Ensure that each element of the list is observed.\n                        idx_observed = idx\n                        if idx >= len(observed_sub_resources[:-1]):\n                            idx_observed = len(observed_sub_resources[:-1])\n                            # FIXME: Assuming each element of the list contains a\n                            # dictionary, therefore initializing new elements with an\n                            # empty dict\n                            observed_sub_resources.insert(-1, {})\n                        observed_sub_resource = observed_sub_resources[idx_observed]\n\n                        # FIXME: This is assuming a list always contains dict\n                        inject(\n                            sub_resource, sub_resource_to_mangle, observed_sub_resource\n                        )\n\n                elif isinstance(sub_resources_to_mangle, dict):\n                    inject(\n                        sub_resource, sub_resources_to_mangle, observed_sub_resources\n                    )\n\n                else:\n                    message = (\n                        f\"The sub-resource to mangle {sub_resources_to_mangle!r} has an\"\n                        \"invalid type, should be in '[dict, list]'\"\n                    )\n                    raise InvalidManifestError(message)\n\n    @staticmethod\n    def attribute_map(obj):\n        \"\"\"Convert a Kubernetes object to dict based on its attribute mapping\n\n        Example:\n            .. code:: python\n\n            from kubernetes_asyncio.client import V1VolumeMount\n\n            d = attribute_map(\n                    V1VolumeMount(name=\"name\", mount_path=\"path\")\n            )\n            assert d == {'mountPath': 'path', 'name': 'name'}\n\n        Args:\n            obj (object): Kubernetes object\n\n        Returns:\n            dict: Converted Kubernetes object\n\n        \"\"\"\n        return {\n            obj.attribute_map[attr]: getattr(obj, attr)\n            for attr, _ in obj.to_dict().items()\n            if getattr(obj, attr) is not None\n        }\n\n    @staticmethod\n    def create_path(mangled_observer_schema, keys):\n        \"\"\"Create the path to the observed field in the observer schema.\n\n        When a sub-resource is mangled, it should be observed. This function creates\n        the path to the subresource to observe.\n\n        Args:\n            mangled_observer_schema (dict): Partial observer schema of a resource\n            keys (list): list of keys forming the path to the sub-resource to\n                observe\n\n        FIXME: This assumes we are only adding keys to dict. We don't consider lists\n\n        \"\"\"\n\n        # Unpack the first key first, as it contains the base directory\n        key = keys.pop(0)\n\n        # If the key is the last of the list, we reached the end of the path.\n        if len(keys) == 0:\n            mangled_observer_schema[key] = None\n            return\n\n        if key not in mangled_observer_schema:\n            mangled_observer_schema[key] = {}\n        Hook.create_path(mangled_observer_schema[key], keys)\n\n    def secret_certs(\n        self,\n        secret_name,\n        namespace,\n        ca_certs=None,\n        intermediate_src=None,\n        generated_cert=None,\n    ):\n        \"\"\"Create a complete hooks secret resource.\n\n        Complete hook secret stores Krake CAs and client certificates to communicate\n        with the Krake API.\n\n        Args:\n            secret_name (str): Secret name\n            namespace (str): Kubernetes namespace where the Secret will be created.\n            ca_certs (list): Krake CA list\n            intermediate_src (str): content of the certificate that is used to sign new\n                certificates for the complete hook.\n            generated_cert (CertificatePair): tuple that contains the content of the\n                new signed certificate for the Application, and the content of its\n                corresponding key.\n\n        Returns:\n            dict: complete hook secret resource\n\n        \"\"\"\n        ca_certs_pem = \"\"\n        for ca_cert in ca_certs:\n            x509 = crypto.load_certificate(crypto.FILETYPE_ASN1, ca_cert)\n            ca_certs_pem += crypto.dump_certificate(crypto.FILETYPE_PEM, x509).decode()\n\n        # Add the intermediate certificate into the chain\n        with open(intermediate_src, \"r\") as f:\n            intermediate_src_content = f.read()\n        ca_certs_pem += intermediate_src_content\n\n        data = {\n            self.ca_name: self._encode_to_64(ca_certs_pem),\n            self.cert_name: self._encode_to_64(generated_cert.cert),\n            self.key_name: self._encode_to_64(generated_cert.key),\n        }\n        return self.secret(secret_name, data, namespace)\n\n    def secret_token(\n        self, secret_name, name, namespace, resource_namespace, api_endpoint, token\n    ):\n        \"\"\"Create a hooks secret resource.\n\n        The hook secret stores Krake authentication token\n        and hook URL for given application.\n\n        Args:\n            secret_name (str): Secret name\n            name (str): Application name\n            namespace (str): Application namespace\n            resource_namespace (str): Kubernetes namespace where the\n                Secret will be created.\n            api_endpoint (str): Krake API endpoint\n            token (str): Complete hook authentication token\n\n        Returns:\n            dict: complete hook secret resource\n\n        \"\"\"\n        pass\n\n    def volumes(self, secret_name, volume_name, mount_path):\n        \"\"\"Create complete hooks volume and volume mount sub-resources\n\n        Complete hook volume gives access to hook's secret, which stores\n        Krake CAs and client certificates to communicate with the Krake API.\n        Complete hook volume mount puts the volume into the application\n\n        Args:\n            secret_name (str): Secret name\n            volume_name (str): Volume name\n            mount_path (list): Volume mount path\n\n        Returns:\n            list: List of complete hook volume and volume mount sub-resources\n\n        \"\"\"\n        volume = V1Volume(name=volume_name, secret={\"secretName\": secret_name})\n        volume_mount = V1VolumeMount(name=volume_name, mount_path=mount_path)\n        return [\n            SubResource(\n                group=\"volumes\",\n                name=volume.name,\n                body=self.attribute_map(volume),\n                path=((\"spec\", \"template\", \"spec\"), (\"spec\",)),\n            ),\n            SubResource(\n                group=\"volumeMounts\",\n                name=volume_mount.name,\n                body=self.attribute_map(volume_mount),\n                path=(\n                    (\"spec\", \"template\", \"spec\", \"containers\"),\n                    (\"spec\", \"containers\"),  # kind: Pod\n                ),\n            ),\n        ]\n\n    @staticmethod\n    def _encode_to_64(string):\n        \"\"\"Compute the base 64 encoding of a string.\n\n        Args:\n            string (str): the string to encode.\n\n        Returns:\n            str: the result of the encoding.\n\n        \"\"\"\n        return b64encode(string.encode()).decode()\n\n    def secret(self, secret_name, secret_data, namespace, _type=\"Opaque\"):\n        \"\"\"Create a secret resource.\n\n        Args:\n            secret_name (str): Secret name\n            secret_data (dict): Secret data\n            namespace (str): Kubernetes namespace where the Secret will be created.\n            _type (str, optional): Secret type. Defaults to Opaque.\n\n        Returns:\n            dict: secret resource\n\n        \"\"\"\n        return self.attribute_map(\n            V1Secret(\n                api_version=\"v1\",\n                kind=\"Secret\",\n                data=secret_data,\n                metadata={\"name\": secret_name, \"namespace\": namespace},\n                type=_type,\n            )\n        )\n\n    @staticmethod\n    def create_hook_url(name, namespace, api_endpoint):\n        \"\"\"Create an applications' hook URL.\n        Function needs to be specified for each hook.\n\n        Args:\n            name (str): Application name\n            namespace (str): Application namespace\n            api_endpoint (str): Krake API endpoint\n\n        Returns:\n            str: Application shutdown url\n\n        \"\"\"\n        pass\n\n    def env_vars(self, secret_name):\n        \"\"\"Create the hooks' environment variables sub-resources.\n        Function needs to be specified for each hook.\n\n        Creates hook environment variables to store Krake authentication token\n        and a hook URL for the given applications.\n\n        Args:\n            secret_name (str): Secret name\n\n        Returns:\n            list: List of shutdown hook environment variables sub-resources\n\n        \"\"\"\n        pass\n\n\nclass Complete(Hook):\n    \"\"\"Mangle given application and inject complete hooks variables into it.\n\n    Hook injects a Kubernetes secret, which stores Krake authentication token\n    and the Krake complete hook URL for the given application. The variables\n    from Kubernetes secret are imported as environment variables\n    into the application resource definition. Only resources defined in\n    :args:`hook_resources` can be modified.\n\n    Names of environment variables are defined in the application controller\n    configuration file.\n\n    If TLS is enabled on the Krake API, the complete hook injects a Kubernetes secret,\n    and it's corresponding volume and volume mount definitions for the Krake CA,\n    the client certificate with the right CN, and its key. The directory where the\n    secret is mounted is defined in the configuration.\n\n    Args:\n        api_endpoint (str): the given API endpoint\n        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint\n        cert_dest (str, optional): Path of the directory where the CA, client\n            certificate and key to the Krake API will be stored.\n        env_token (str, optional): Name of the environment variable, which stores Krake\n            authentication token.\n        env_url (str, optional): Name of the environment variable,\n            which stores Krake complete hook URL.\n\n    \"\"\"\n\n    hook_resources = (\"Pod\", \"Deployment\", \"ReplicationController\")\n\n    def __init__(\n        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n    ):\n        super().__init__(\n            api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n        )\n        self.env_url = env_url\n\n    def secret_token(\n        self, secret_name, name, namespace, resource_namespace, api_endpoint, token\n    ):\n        \"\"\"Create complete hooks secret resource.\n\n        Complete hook secret stores Krake authentication token\n        and complete hook URL for given application.\n\n        Args:\n            secret_name (str): Secret name\n            name (str): Application name\n            namespace (str): Application namespace\n            resource_namespace (str): Kubernetes namespace where the\n                Secret will be created.\n            api_endpoint (str): Krake API endpoint\n            token (str): Complete hook authentication token\n\n        Returns:\n            dict: complete hook secret resource\n\n        \"\"\"\n        complete_url = self.create_hook_url(name, namespace, api_endpoint)\n        data = {\n            self.env_token.lower(): self._encode_to_64(token),\n            self.env_url.lower(): self._encode_to_64(complete_url),\n        }\n        return self.secret(secret_name, data, resource_namespace)\n\n    @staticmethod\n    def create_hook_url(name, namespace, api_endpoint):\n        \"\"\"Create an applications' complete URL.\n\n        Args:\n            name (str): Application name\n            namespace (str): Application namespace\n            api_endpoint (str): Krake API endpoint\n\n        Returns:\n            str: Application complete url\n\n        \"\"\"\n        api_url = URL(api_endpoint)\n        return str(\n            api_url.with_path(\n                f\"/kubernetes/namespaces/{namespace}/applications/{name}/complete\"\n            )\n        )\n\n    def env_vars(self, secret_name):\n        \"\"\"Create complete hooks environment variables sub-resources\n\n        Create complete hook environment variables store Krake authentication token\n        and complete hook URL for given application.\n\n        Args:\n            secret_name (str): Secret name\n\n        Returns:\n            list: List of complete hook environment variables sub-resources\n\n        \"\"\"\n        sub_resources = []\n\n        env_token = V1EnvVar(\n            name=self.env_token,\n            value_from=self.attribute_map(\n                V1EnvVarSource(\n                    secret_key_ref=self.attribute_map(\n                        V1SecretKeySelector(\n                            name=secret_name, key=self.env_token.lower()\n                        )\n                    )\n                )\n            ),\n        )\n        env_url = V1EnvVar(\n            name=self.env_url,\n            value_from=self.attribute_map(\n                V1EnvVarSource(\n                    secret_key_ref=self.attribute_map(\n                        V1SecretKeySelector(name=secret_name, key=self.env_url.lower())\n                    )\n                )\n            ),\n        )\n\n        for env in (env_token, env_url):\n            sub_resources.append(\n                SubResource(\n                    group=\"env\",\n                    name=env.name,\n                    body=self.attribute_map(env),\n                    path=(\n                        (\"spec\", \"template\", \"spec\", \"containers\"),\n                        (\"spec\", \"containers\"),  # kind: Pod\n                    ),\n                )\n            )\n        return sub_resources\n\n\nclass Shutdown(Hook):\n    \"\"\"Mangle given application and inject shutdown hooks variables into it.\n\n    Hook injects a Kubernetes secret, which stores Krake authentication token\n    and the Krake complete hook URL for the given application. The variables\n    from the Kubernetes secret are imported as environment variables\n    into the application resource definition. Only resources defined in\n    :args:`hook_resources` can be modified.\n\n    Names of environment variables are defined in the application controller\n    configuration file.\n\n    If TLS is enabled on the Krake API, the shutdown hook injects a Kubernetes secret,\n    and it's corresponding volume and volume mount definitions for the Krake CA,\n    the client certificate with the right CN, and its key. The directory where the\n    secret is mounted is defined in the configuration.\n\n    Args:\n        api_endpoint (str): the given API endpoint\n        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint\n        cert_dest (str, optional): Path of the directory where the CA, client\n            certificate and key to the Krake API will be stored.\n        env_token (str, optional): Name of the environment variable, which stores Krake\n            authentication token.\n        env_url (str, optional): Name of the environment variable,\n            which stores Krake complete hook URL.\n\n    \"\"\"\n\n    hook_resources = (\"Pod\", \"Deployment\", \"ReplicationController\")\n\n    def __init__(\n        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n    ):\n        super().__init__(\n            api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n        )\n        self.env_url = env_url\n\n    def secret_token(\n        self, secret_name, name, namespace, resource_namespace, api_endpoint, token\n    ):\n        \"\"\"Create shutdown hooks secret resource.\n\n        Shutdown hook secret stores Krake authentication token\n        and shutdown hook URL for given application.\n\n        Args:\n            secret_name (str): Secret name\n            name (str): Application name\n            namespace (str): Application namespace\n            resource_namespace (str): Kubernetes namespace where the\n                Secret will be created.\n            api_endpoint (str): Krake API endpoint\n            token (str): Shutdown hook authentication token\n\n        Returns:\n            dict: shutdown hook secret resource\n\n        \"\"\"\n        shutdown_url = self.create_hook_url(name, namespace, api_endpoint)\n        data = {\n            self.env_token.lower(): self._encode_to_64(token),\n            self.env_url.lower(): self._encode_to_64(shutdown_url),\n        }\n        return self.secret(secret_name, data, resource_namespace)\n\n    @staticmethod\n    def create_hook_url(name, namespace, api_endpoint):\n        \"\"\"Create an applications' shutdown URL.\n\n        Args:\n            name (str): Application name\n            namespace (str): Application namespace\n            api_endpoint (str): Krake API endpoint\n\n        Returns:\n            str: Application shutdown url\n\n        \"\"\"\n        api_url = URL(api_endpoint)\n        return str(\n            api_url.with_path(\n                f\"/kubernetes/namespaces/{namespace}/applications/{name}/shutdown\"\n            )\n        )\n\n    def env_vars(self, secret_name):\n        \"\"\"Create shutdown hooks environment variables sub-resources.\n\n        Creates shutdown hook environment variables to store Krake authentication token\n        and a shutdown hook URL for given applications.\n\n        Args:\n            secret_name (str): Secret name\n\n        Returns:\n            list: List of shutdown hook environment variables sub-resources\n\n        \"\"\"\n        sub_resources = []\n\n        env_resources = []\n\n        env_token = V1EnvVar(\n            name=self.env_token,\n            value_from=self.attribute_map(\n                V1EnvVarSource(\n                    secret_key_ref=self.attribute_map(\n                        V1SecretKeySelector(\n                            name=secret_name,\n                            key=self.env_token.lower()\n                        )\n                    )\n                )\n            )\n        )\n        env_resources.append(env_token)\n\n        env_url = V1EnvVar(\n            name=self.env_url,\n            value_from=self.attribute_map(\n                V1EnvVarSource(\n                    secret_key_ref=self.attribute_map(\n                        V1SecretKeySelector(name=secret_name, key=self.env_url.lower())\n                    )\n                )\n            ),\n        )\n        env_resources.append(env_url)\n\n        for env in env_resources:\n            sub_resources.append(\n                SubResource(\n                    group=\"env\",\n                    name=env.name,\n                    body=self.attribute_map(env),\n                    path=(\n                        (\"spec\", \"template\", \"spec\", \"containers\"),\n                        (\"spec\", \"containers\"),  # kind: Pod\n                    ),\n                )\n            )\n        return sub_resources\n",
            "file_path": "krake/krake/controller/kubernetes/hooks.py",
            "human_label": "Together with :func:``update_last_applied_manifest_list_from_resp``, this\nfunction is called recursively to update a partial ``last_applied_manifest``\nfrom a partial Kubernetes response\n\nArgs:\n    last_applied_manifest (dict): partial ``last_applied_manifest`` being\n        updated\n    observer_schema (dict): partial ``observer_schema``\n    response (dict): partial response from the Kubernetes API.\n\nRaises:\n    KeyError: If the observed field is not present in the Kubernetes response\n\nThis function go through all observed fields, and initialized their value in\nlast_applied_manifest if they are not yet present",
            "level": "file_runnable",
            "lineno": "245",
            "name": "update_last_applied_manifest_dict_from_resp",
            "oracle_context": "{ \"apis\" : \"['camel_to_snake_case', 'items', 'update_last_applied_manifest_list_from_resp', 'isinstance']\", \"classes\" : \"['camel_to_snake_case', 'KeyError']\", \"vars\" : \"[]\" }",
            "package": "hooks",
            "project": "rak-n-rok/Krake",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b869eab4d922cb0e688cbf",
            "all_context": "{ \"import\" : \"inspect logging asyncio collections secrets operator base64 contextlib enum functools datetime typing random copy logging aiohttp asyncio secrets enum functools krake datetime typing yarl \", \"file\" : \"logger ; listen ; update_last_applied_manifest_dict_from_resp(last_applied_manifest,observer_schema,response) ; update_last_applied_manifest_list_from_resp(last_applied_manifest,observer_schema,response) ; update_last_applied_manifest_from_resp(app,response) ; update_last_observed_manifest_from_resp(app,response) ; update_last_observed_manifest_dict(observed_resource,response) ; update_last_observed_manifest_list(observed_resource,response) ; update_last_applied_manifest_dict_from_spec(resource_status_new,resource_status_old,resource_observed) ; update_last_applied_manifest_list_from_spec(resource_status_new,resource_status_old,resource_observed) ; update_last_applied_manifest_from_spec(app) ; utc_difference() ; generate_certificate(config) ; generate_default_observer_schema(app) ; generate_default_observer_schema_dict(manifest_dict,first_level) ; generate_default_observer_schema_list(manifest_list) ; \", \"class\" : \"\" }",
            "code": "def generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n\n    app.status.mangled_observer_schema = deepcopy(app.spec.observer_schema)\n\n    for resource_manifest in app.spec.manifest:\n        try:\n            get_kubernetes_resource_idx(\n                app.status.mangled_observer_schema, resource_manifest\n            )\n\n        except IndexError:\n            # Only create a default observer schema, if a custom observer schema hasn't\n            # been set by the user.\n            app.status.mangled_observer_schema.append(\n                generate_default_observer_schema_dict(\n                    resource_manifest,\n                    first_level=True,\n                )\n            )\n",
            "dependency": "{ \"builtin\" : false, \"standard_lib\" : true, \"public_lib\" : true, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Generate the default observer schema for each Kubernetes resource present in\n``spec.manifest`` for which a custom observer schema hasn't been specified.\n\nArgs:\n    app (krake.data.kubernetes.Application): The application for which to generate a\n        default observer schema",
            "end_lineno": "1037",
            "file_content": "\"\"\"This module defines the Hook Dispatcher and listeners for registering and\nexecuting hooks. Hook Dispatcher emits hooks based on :class:`Hook` attributes which\ndefine when the hook will be executed.\n\n\"\"\"\nimport asyncio\nimport logging\nimport random\nfrom base64 import b64encode\nfrom collections import defaultdict\nfrom contextlib import suppress\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom functools import reduce\nfrom operator import getitem\nfrom enum import Enum, auto\nfrom inspect import iscoroutinefunction\nfrom OpenSSL import crypto\nfrom typing import NamedTuple\n\nimport yarl\nfrom aiohttp import ClientConnectorError\n\nfrom krake.controller import Observer\nfrom krake.controller.kubernetes.client import KubernetesClient, InvalidManifestError\nfrom krake.utils import camel_to_snake_case, get_kubernetes_resource_idx\nfrom kubernetes_asyncio.client.rest import ApiException\nfrom kubernetes_asyncio.client.api_client import ApiClient\nfrom kubernetes_asyncio import client\nfrom krake.data.kubernetes import ClusterState, Application, Cluster\nfrom yarl import URL\nfrom secrets import token_urlsafe\n\nfrom kubernetes_asyncio.client import (\n    Configuration,\n    V1Secret,\n    V1EnvVar,\n    V1VolumeMount,\n    V1Volume,\n    V1SecretKeySelector,\n    V1EnvVarSource,\n)\nfrom kubernetes_asyncio.config.kube_config import KubeConfigLoader\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass HookType(Enum):\n    ResourcePreCreate = auto()\n    ResourcePostCreate = auto()\n    ResourcePreUpdate = auto()\n    ResourcePostUpdate = auto()\n    ResourcePreDelete = auto()\n    ResourcePostDelete = auto()\n    ApplicationMangling = auto()\n    ApplicationPreMigrate = auto()\n    ApplicationPostMigrate = auto()\n    ApplicationPreReconcile = auto()\n    ApplicationPostReconcile = auto()\n    ApplicationPreDelete = auto()\n    ApplicationPostDelete = auto()\n    ClusterCreation = auto()\n    ClusterDeletion = auto()\n\n\nclass HookDispatcher(object):\n    \"\"\"Simple wrapper around a registry of handlers associated to :class:`Hook`\n     attributes. Each :class:`Hook` attribute defines when the handler will be\n     executed.\n\n    Listeners for certain hooks can be registered via :meth:`on`. Registered\n    listeners are executed via :meth:`hook`.\n\n    Example:\n        .. code:: python\n\n        listen = HookDispatcher()\n\n        @listen.on(HookType.PreApply)\n        def to_perform_before_app_creation(app, cluster, resource, controller):\n            # Do Stuff\n\n        @listen.on(HookType.PostApply)\n        def another_to_perform_after_app_creation(app, cluster, resource, resp):\n            # Do Stuff\n\n        @listen.on(HookType.PostDelete)\n        def to_perform_after_app_deletion(app, cluster, resource, resp):\n            # Do Stuff\n\n    \"\"\"\n\n    def __init__(self):\n        self.registry = defaultdict(list)\n\n    def on(self, hook):\n        \"\"\"Decorator function to add a new handler to the registry.\n\n        Args:\n            hook (HookType): Hook attribute for which to register the handler.\n\n        Returns:\n            callable: Decorator for registering listeners for the specified\n            hook.\n\n        \"\"\"\n\n        def decorator(handler):\n            self.registry[hook].append(handler)\n\n            return handler\n\n        return decorator\n\n    async def hook(self, hook, **kwargs):\n        \"\"\"Execute the list of handlers associated to the provided :class:`Hook`\n        attribute.\n\n        Args:\n            hook (HookType): The hook attribute for which to execute handlers.\n\n        \"\"\"\n        try:\n            handlers = self.registry[hook]\n        except KeyError:\n            pass\n        else:\n            for handler in handlers:\n                if iscoroutinefunction(handler):\n                    await handler(**kwargs)\n                else:\n                    handler(**kwargs)\n\n\nlisten = HookDispatcher()\n\n\n@listen.on(HookType.ResourcePostCreate)\n@listen.on(HookType.ResourcePostUpdate)\nasync def register_service(app, cluster, resource, response):\n    \"\"\"Register endpoint of Kubernetes Service object on creation and update.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        cluster (krake.data.kubernetes.Cluster): The cluster on which the\n            application is running\n        resource (dict): Kubernetes object description as specified in the\n            specification of the application.\n        response (kubernetes_asyncio.client.V1Service): Response of the\n            Kubernetes API\n\n    \"\"\"\n    if resource[\"kind\"] != \"Service\":\n        return\n\n    service_name = resource[\"metadata\"][\"name\"]\n\n    if response.spec and response.spec.type == \"LoadBalancer\":\n        # For a \"LoadBalancer\" type of Service, an external IP is given in the cluster\n        # by a load balancer controller to the service. In this case, the \"port\"\n        # specified in the spec is reachable from the outside.\n        if (\n            not response.status.load_balancer\n            or not response.status.load_balancer.ingress\n        ):\n            # When a \"LoadBalancer\" type of service is created, the IP is given by an\n            # additional controller (e.g. a controller that requests a floating IP to an\n            # OpenStack infrastructure). This process can take some time, but the\n            # Service itself already exist before the IP is assigned. In the case of an\n            # error with the controller, the IP is also not given. This \"<pending>\" IP\n            # just expresses that the Service exists, but the IP is not ready yet.\n            external_ip = \"<pending>\"\n        else:\n            external_ip = response.status.load_balancer.ingress[0].ip\n\n        if not response.spec.ports:\n            external_port = \"<pending>\"\n        else:\n            external_port = response.spec.ports[0].port\n        app.status.services[service_name] = f\"{external_ip}:{external_port}\"\n        return\n\n    node_port = None\n    # Ensure that ports are specified\n    if response.spec and response.spec.ports:\n        node_port = response.spec.ports[0].node_port\n\n    # If the service does not have a node port, remove a potential reference\n    # and return.\n    if node_port is None:\n        try:\n            del app.status.services[service_name]\n        except KeyError:\n            pass\n        return\n\n    # Determine URL of Kubernetes cluster API\n    loader = KubeConfigLoader(cluster.spec.kubeconfig)\n    config = Configuration()\n    await loader.load_and_set(config)\n    cluster_url = yarl.URL(config.host)\n\n    app.status.services[service_name] = f\"{cluster_url.host}:{node_port}\"\n\n\n@listen.on(HookType.ResourcePostDelete)\nasync def unregister_service(app, resource, **kwargs):\n    \"\"\"Unregister endpoint of Kubernetes Service object on deletion.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        resource (dict): Kubernetes object description as specified in the\n            specification of the application.\n\n    \"\"\"\n    if resource[\"kind\"] != \"Service\":\n        return\n\n    service_name = resource[\"metadata\"][\"name\"]\n    try:\n        del app.status.services[service_name]\n    except KeyError:\n        pass\n\n\n@listen.on(HookType.ResourcePostDelete)\nasync def remove_resource_from_last_observed_manifest(app, resource, **kwargs):\n    \"\"\"Remove a given resource from the last_observed_manifest after its deletion\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        resource (dict): Kubernetes object description as specified in the\n            specification of the application.\n\n    \"\"\"\n    try:\n        idx = get_kubernetes_resource_idx(app.status.last_observed_manifest, resource)\n    except IndexError:\n        return\n\n    app.status.last_observed_manifest.pop(idx)\n\n\ndef update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n\n    \"\"\"\n    for key, value in observer_schema.items():\n\n        # Keys in the response are in camelCase\n        camel_key = camel_to_snake_case(key)\n\n        if camel_key not in response:\n            # An observed key should always be present in the k8s response\n            raise KeyError(\n                f\"Observed key {camel_key} is not present in response {response}\"\n            )\n\n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                # The dictionary is observed, but not present in\n                # last_applied_manifest\n                last_applied_manifest[key] = {}\n\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], observer_schema[key], response[camel_key]\n            )\n\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                # The list is observed, but not present in last_applied_manifest\n                last_applied_manifest[key] = []\n\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], observer_schema[key], response[camel_key]\n            )\n\n        elif key not in last_applied_manifest:\n            # If key not present in last_applied_manifest, and value is neither a\n            # dict nor a list, simply add it.\n            last_applied_manifest[key] = response[camel_key]\n\n\ndef update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n\n    \"\"\"\n    # Looping over the observed resource, except the last element which is the\n    # special control dictionary\n    for idx, val in enumerate(observer_schema[:-1]):\n\n        if idx >= len(response):\n            # Element is observed but not present in k8s response, so following\n            # elements will also not exist.\n            #\n            # This doesn't raise an Exception as observing the element of a list\n            # doesn't ensure its presence. The list length is controlled by the\n            # special control dictionary\n            return\n\n        if isinstance(val, dict):\n            if idx >= len(last_applied_manifest):\n                # The dict is observed, but not present in last_applied_manifest\n                last_applied_manifest.append({})\n\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n\n        elif isinstance(response[idx], list):\n            if idx >= len(last_applied_manifest):\n                # The list is observed, but not present in last_applied_manifest\n                last_applied_manifest.append([])\n\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n\n        elif idx >= len(last_applied_manifest):\n            # Element is not yet present in last_applied_manifest. Adding it.\n            last_applied_manifest.append(response[idx])\n\n\n@listen.on(HookType.ResourcePostCreate)\n@listen.on(HookType.ResourcePostUpdate)\ndef update_last_applied_manifest_from_resp(app, response, **kwargs):\n    \"\"\"Hook run after the creation or update of an application in order to update the\n    `status.last_applied_manifest` using the k8s response.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        response (kubernetes_asyncio.client.V1Status): Response of the Kubernetes API\n\n    After a Kubernetes resource has been created/updated, the\n    `status.last_applied_manifest` has to be updated. All fields already initialized\n    (either from the mangling of `spec.manifest`, or by a previous call to this\n    function) should be left untouched. Only observed fields which are not present in\n    `status.last_applied_manifest` should be initialized.\n\n    \"\"\"\n\n    if isinstance(response, dict):\n        # The Kubernetes API couldn't deserialize the k8s response into an object\n        resp = response\n    else:\n        # The Kubernetes API deserialized the k8s response into an object\n        resp = response.to_dict()\n\n    idx_applied = get_kubernetes_resource_idx(app.status.last_applied_manifest, resp)\n\n    idx_observed = get_kubernetes_resource_idx(app.status.mangled_observer_schema, resp)\n\n    update_last_applied_manifest_dict_from_resp(\n        app.status.last_applied_manifest[idx_applied],\n        app.status.mangled_observer_schema[idx_observed],\n        resp,\n    )\n\n\n@listen.on(HookType.ResourcePostCreate)\n@listen.on(HookType.ResourcePostUpdate)\ndef update_last_observed_manifest_from_resp(app, response, **kwargs):\n    \"\"\"Handler to run after the creation or update of a Kubernetes resource to update\n    the last_observed_manifest from the response of the Kubernetes API.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        response (kubernetes_asyncio.client.V1Service): Response of the\n            Kubernetes API\n\n    The target last_observed_manifest holds the value of all observed fields plus the\n    special control dictionaries for the list length\n\n    \"\"\"\n    if isinstance(response, dict):\n        # The Kubernetes API couldn't deserialize the k8s response into an object\n        resp = response\n    else:\n        # The Kubernetes API deserialized the k8s response into an object\n        resp = response.to_dict()\n\n    try:\n        idx_observed = get_kubernetes_resource_idx(\n            app.status.mangled_observer_schema,\n            resp,\n        )\n    except IndexError:\n        # All created resources should be observed\n        raise\n\n    try:\n        idx_last_observed = get_kubernetes_resource_idx(\n            app.status.last_observed_manifest,\n            resp,\n        )\n    except IndexError:\n        # If the resource is not yes present in last_observed_manifest, append it.\n        idx_last_observed = len(app.status.last_observed_manifest)\n        app.status.last_observed_manifest.append({})\n\n    # Overwrite the last_observed_manifest for this resource\n    app.status.last_observed_manifest[\n        idx_last_observed\n    ] = update_last_observed_manifest_dict(\n        app.status.mangled_observer_schema[idx_observed], resp\n    )\n\n\ndef update_last_observed_manifest_dict(observed_resource, response):\n    \"\"\"Together with :func:``update_last_observed_manifest_list``, recursively\n    crafts the ``last_observed_manifest`` from the Kubernetes :attr:``response``.\n\n    Args:\n        observed_resource (dict): The schema to observe for the partial given resource\n        response (dict): The partial Kubernetes response for this resource.\n\n    Raises:\n        KeyError: If an observed key is not present in the Kubernetes response\n\n    Returns:\n        dict: The dictionary of observed keys and their value\n\n    Get the value of all observed fields from the Kubernetes response\n    \"\"\"\n    res = {}\n    for key, value in observed_resource.items():\n\n        camel_key = camel_to_snake_case(key)\n        if camel_key not in response:\n            raise KeyError(\n                f\"Observed key {camel_key} is not present in response {response}\"\n            )\n\n        if isinstance(value, dict):\n            res[key] = update_last_observed_manifest_dict(value, response[camel_key])\n\n        elif isinstance(value, list):\n            res[key] = update_last_observed_manifest_list(value, response[camel_key])\n\n        else:\n            res[key] = response[camel_key]\n\n    return res\n\n\ndef update_last_observed_manifest_list(observed_resource, response):\n    \"\"\"Together with :func:``update_last_observed_manifest_dict``, recursively\n    crafts the ``last_observed_manifest`` from the Kubernetes :attr:``response``.\n\n    Args:\n        observed_resource (list): the schema to observe for the partial given resource\n        response (list): the partial Kubernetes response for this resource.\n\n    Returns:\n        list: The list of observed elements, plus the special list length control\n            dictionary\n\n    Get the value of all observed elements from the Kubernetes response\n    \"\"\"\n\n    if not response:\n        return [{\"observer_schema_list_current_length\": 0}]\n\n    res = []\n    # Looping over the observed resource, except the last element which is the special\n    # control dictionary\n    for idx, val in enumerate(observed_resource[:-1]):\n\n        if idx >= len(response):\n            # Element is not present in the Kubernetes response, nothing more to do\n            break\n\n        if type(response[idx]) == dict:\n            res.append(update_last_observed_manifest_dict(val, response[idx]))\n\n        elif type(response[idx]) == list:\n            res.append(update_last_observed_manifest_list(val, response[idx]))\n\n        else:\n            res.append(response[idx])\n\n    # Append the special control dictionary to the list\n    res.append({\"observer_schema_list_current_length\": len(response)})\n\n    return res\n\n\ndef update_last_applied_manifest_dict_from_spec(\n    resource_status_new, resource_status_old, resource_observed\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_list_from_spec``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n\n    Args:\n        resource_status_new (dict): partial ``last_applied_manifest`` being updated\n        resource_status_old (dict): partial of the current ``last_applied_manifest``\n        resource_observed (dict): partial observer_schema for the manifest file\n            being updated\n\n    \"\"\"\n    for key, value in resource_observed.items():\n\n        if key not in resource_status_old:\n            continue\n\n        if key in resource_status_new:\n\n            if isinstance(value, dict):\n                update_last_applied_manifest_dict_from_spec(\n                    resource_status_new[key],\n                    resource_status_old[key],\n                    resource_observed[key],\n                )\n\n            elif isinstance(value, list):\n                update_last_applied_manifest_list_from_spec(\n                    resource_status_new[key],\n                    resource_status_old[key],\n                    resource_observed[key],\n                )\n\n        else:\n            # If the key is not present the spec.manifest, we first need to\n            # initialize it\n\n            if isinstance(value, dict):\n                resource_status_new[key] = {}\n                update_last_applied_manifest_dict_from_spec(\n                    resource_status_new[key],\n                    resource_status_old[key],\n                    resource_observed[key],\n                )\n\n            elif isinstance(value, list):\n                resource_status_new[key] = []\n                update_last_applied_manifest_list_from_spec(\n                    resource_status_new[key],\n                    resource_status_old[key],\n                    resource_observed[key],\n                )\n\n            else:\n                resource_status_new[key] = resource_status_old[key]\n\n\ndef update_last_applied_manifest_list_from_spec(\n    resource_status_new, resource_status_old, resource_observed\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_dict_from_spec``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n\n    Args:\n        resource_status_new (list): partial ``last_applied_manifest`` being updated\n        resource_status_old (list): partial of the current ``last_applied_manifest``\n        resource_observed (list): partial observer_schema for the manifest file\n            being updated\n\n    \"\"\"\n\n    # Looping over the observed resource, except the last element which is the\n    # special control dictionary\n    for idx, val in enumerate(resource_observed[:-1]):\n\n        if idx >= len(resource_status_old):\n            # The element in not in the current last_applied_manifest, and neither\n            # is the rest of the list\n            break\n\n        if idx < len(resource_status_new):\n            # The element is present in spec.manifest and in the current\n            # last_applied_manifest. Updating observed fields\n\n            if isinstance(val, dict):\n                update_last_applied_manifest_dict_from_spec(\n                    resource_status_new[idx],\n                    resource_status_old[idx],\n                    resource_observed[idx],\n                )\n\n            elif isinstance(val, list):\n                update_last_applied_manifest_list_from_spec(\n                    resource_status_new[idx],\n                    resource_status_old[idx],\n                    resource_observed[idx],\n                )\n\n        else:\n            # If the element is not present in the spec.manifest, we first have to\n            # initialize it.\n\n            if isinstance(val, dict):\n                resource_status_new.append({})\n                update_last_applied_manifest_dict_from_spec(\n                    resource_status_new[idx],\n                    resource_status_old[idx],\n                    resource_observed[idx],\n                )\n\n            elif isinstance(val, list):\n                resource_status_new.append([])\n                update_last_applied_manifest_list_from_spec(\n                    resource_status_new[idx],\n                    resource_status_old[idx],\n                    resource_observed[idx],\n                )\n\n            else:\n                resource_status_new.append(resource_status_old[idx])\n\n\ndef update_last_applied_manifest_from_spec(app):\n    \"\"\"Update the status.last_applied_manifest of an application from spec.manifests\n\n    Args:\n        app (krake.data.kubernetes.Application): Application to update\n\n    This function is called on application creation and updates. The\n    last_applied_manifest of an application is initialized as a copy of spec.manifest,\n    and is augmented by all known observed fields not yet initialized (i.e. all observed\n    fields or resources which are present in the current last_applied_manifest but not\n    in the spec.manifest)\n\n    \"\"\"\n\n    # The new last_applied_manifest is initialized as a copy of the spec.manifest, and\n    # augmented by all observed fields which are present in the current\n    # last_applied_manifest but not in the original spec.manifest\n    new_last_applied_manifest = deepcopy(app.spec.manifest)\n\n    # Loop over observed resources and observed fields, and check if they should be\n    # added to the new last_applied_manifest (i.e. present in the current\n    # last_applied_manifest but not in spec.manifest)\n    for resource_observed in app.status.mangled_observer_schema:\n\n        # If the resource is not present in the current last_applied_manifest, there is\n        # nothing to do. Whether the resource was initialized by spec.manifest doesn't\n        # matter.\n        try:\n            idx_status_old = get_kubernetes_resource_idx(\n                app.status.last_applied_manifest, resource_observed\n            )\n        except IndexError:\n            continue\n\n        # As the resource is present in the current last_applied_manifest, we need to go\n        # through it to check if observed fields should be set to their current value\n        # (i.e. fields are present in the current last_applied_manifest, but not in\n        # spec.manifest)\n        try:\n            # Check if the observed resource is present in spec.manifest\n            idx_status_new = get_kubernetes_resource_idx(\n                new_last_applied_manifest, resource_observed\n            )\n        except IndexError:\n            # The resource is observed but is not present in the spec.manifest.\n            # Create an empty resource, which will be augmented in\n            # update_last_applied_manifest_dict_from_spec with the observed and known\n            # fields.\n            new_last_applied_manifest.append({})\n            idx_status_new = len(new_last_applied_manifest) - 1\n\n        update_last_applied_manifest_dict_from_spec(\n            new_last_applied_manifest[idx_status_new],\n            app.status.last_applied_manifest[idx_status_old],\n            resource_observed,\n        )\n\n    app.status.last_applied_manifest = new_last_applied_manifest\n\n\nclass KubernetesApplicationObserver(Observer):\n    \"\"\"Observer specific for Kubernetes Applications. One observer is created for each\n    Application managed by the Controller, but not one per Kubernetes resource\n    (Deployment, Service...). If several resources are defined by an Application, they\n    are all monitored by the same observer.\n\n    The observer gets the actual status of the resources on the cluster using the\n    Kubernetes API, and compare it to the status stored in the API.\n\n    The observer is:\n     * started at initial Krake resource creation;\n\n     * deleted when a resource needs to be updated, then started again when it is done;\n\n     * simply deleted on resource deletion.\n\n    Args:\n        cluster (krake.data.kubernetes.Cluster): the cluster on which the observed\n            Application is created.\n        resource (krake.data.kubernetes.Application): the application that will be\n            observed.\n        on_res_update (coroutine): a coroutine called when a resource's actual status\n            differs from the status sent by the database. Its signature is:\n            ``(resource) -> updated_resource``. ``updated_resource`` is the instance of\n            the resource that is up-to-date with the API. The Observer internal instance\n            of the resource to observe will be updated. If the API cannot be contacted,\n            ``None`` can be returned. In this case the internal instance of the Observer\n            will not be updated.\n        time_step (int, optional): how frequently the Observer should watch the actual\n            status of the resources.\n\n    \"\"\"\n\n    def __init__(self, cluster, resource, on_res_update, time_step=2):\n        super().__init__(resource, on_res_update, time_step)\n        self.cluster = cluster\n\n    async def poll_resource(self):\n        \"\"\"Fetch the current status of the Application monitored by the Observer.\n\n        Returns:\n            krake.data.core.Status: the status object created using information from the\n                real world Applications resource.\n\n        \"\"\"\n        app = self.resource\n\n        status = deepcopy(app.status)\n        status.last_observed_manifest = []\n        # For each observed kubernetes resource of the Application,\n        # get its current status on the cluster.\n        for desired_resource in app.status.last_applied_manifest:\n            kube = KubernetesClient(self.cluster.spec.kubeconfig)\n            idx_observed = get_kubernetes_resource_idx(\n                app.status.mangled_observer_schema, desired_resource\n            )\n            observed_resource = app.status.mangled_observer_schema[idx_observed]\n            async with kube:\n                try:\n                    group, version, kind, name, namespace = kube.get_immutables(\n                        desired_resource\n                    )\n                    resource_api = await kube.get_resource_api(group, version, kind)\n                    resp = await resource_api.read(kind, name, namespace)\n                except ApiException as err:\n                    if err.status == 404:\n                        # Resource does not exist\n                        continue\n                    # Otherwise, log the unexpected errors\n                    logger.error(err)\n\n            observed_manifest = update_last_observed_manifest_dict(\n                observed_resource, resp.to_dict()\n            )\n            status.last_observed_manifest.append(observed_manifest)\n\n        return status\n\n\nclass KubernetesClusterObserver(Observer):\n    \"\"\"Observer specific for Kubernetes Clusters. One observer is created for each\n    Cluster managed by the Controller.\n\n    The observer gets the actual status of the cluster using the\n    Kubernetes API, and compare it to the status stored in the API.\n\n    The observer is:\n     * started at initial Krake resource creation;\n\n     * deleted when a resource needs to be updated, then started again when it is done;\n\n     * simply deleted on resource deletion.\n\n    Args:\n        cluster (krake.data.kubernetes.Cluster): the cluster which will be observed.\n        on_res_update (coroutine): a coroutine called when a resource's actual status\n            differs from the status sent by the database. Its signature is:\n            ``(resource) -> updated_resource``. ``updated_resource`` is the instance of\n            the resource that is up-to-date with the API. The Observer internal instance\n            of the resource to observe will be updated. If the API cannot be contacted,\n            ``None`` can be returned. In this case the internal instance of the Observer\n            will not be updated.\n        time_step (int, optional): how frequently the Observer should watch the actual\n            status of the resources.\n\n    \"\"\"\n\n    def __init__(self, cluster, on_res_update, time_step=2):\n        super().__init__(cluster, on_res_update, time_step)\n        self.cluster = cluster\n\n    async def poll_resource(self):\n        \"\"\"Fetch the current status of the Cluster monitored by the Observer.\n\n        Returns:\n            krake.data.core.Status: the status object created using information from the\n                real world Cluster.\n\n        \"\"\"\n        status = deepcopy(self.cluster.status)\n        # For each observed kubernetes cluster registered in Krake,\n        # get its current node status.\n        loader = KubeConfigLoader(self.cluster.spec.kubeconfig)\n        config = Configuration()\n        await loader.load_and_set(config)\n        kube = ApiClient(config)\n\n        async with kube as api:\n            v1 = client.CoreV1Api(api)\n            try:\n                response = await v1.list_node()\n\n            except ClientConnectorError as err:\n                status.state = ClusterState.OFFLINE\n                self.cluster.status.state = ClusterState.OFFLINE\n                # Log the error\n                logger.debug(err)\n                return status\n\n            condition_dict = {\n                \"MemoryPressure\": [],\n                \"DiskPressure\": [],\n                \"PIDPressure\": [],\n                \"Ready\": [],\n            }\n\n            for item in response.items:\n                for condition in item.status.conditions:\n                    condition_dict[condition.type].append(condition.status)\n                if (\n                    condition_dict[\"MemoryPressure\"] == [\"True\"]\n                    or condition_dict[\"DiskPressure\"] == [\"True\"]\n                    or condition_dict[\"PIDPressure\"] == [\"True\"]\n                ):\n                    status.state = ClusterState.UNHEALTHY\n                    self.cluster.status.state = ClusterState.UNHEALTHY\n                    return status\n                elif (\n                    condition_dict[\"Ready\"] == [\"True\"]\n                    and status.state is ClusterState.OFFLINE\n                ):\n                    status.state = ClusterState.CONNECTING\n                    self.cluster.status.state = ClusterState.CONNECTING\n                    return status\n                elif condition_dict[\"Ready\"] == [\"True\"]:\n                    status.state = ClusterState.ONLINE\n                    self.cluster.status.state = ClusterState.ONLINE\n                    return status\n                else:\n                    status.state = ClusterState.NOTREADY\n                    self.cluster.status.state = ClusterState.NOTREADY\n                    return status\n\n\n@listen.on(HookType.ApplicationPostReconcile)\n@listen.on(HookType.ApplicationPostMigrate)\n@listen.on(HookType.ClusterCreation)\nasync def register_observer(controller, resource, start=True, **kwargs):\n    \"\"\"Create an observer for the given Application or Cluster, and start it as a\n    background task if wanted.\n\n    If an observer already existed for this Application or Cluster, it is stopped\n    and deleted.\n\n    Args:\n        controller (KubernetesController): the controller for which the observer will be\n            added in the list of working observers.\n        resource (krake.data.kubernetes.Application): the Application to observe or\n        resource (krake.data.kubernetes.Cluster): the Cluster to observe.\n        start (bool, optional): if False, does not start the observer as background\n            task.\n\n    \"\"\"\n    if resource.kind == Application.kind:\n        cluster = await controller.kubernetes_api.read_cluster(\n            namespace=resource.status.running_on.namespace,\n            name=resource.status.running_on.name,\n        )\n\n        observer = KubernetesApplicationObserver(\n            cluster,\n            resource,\n            controller.on_status_update,\n            time_step=controller.observer_time_step,\n        )\n\n    elif resource.kind == Cluster.kind:\n        observer = KubernetesClusterObserver(\n            resource,\n            controller.on_status_update,\n            time_step=controller.observer_time_step,\n        )\n    else:\n        logger.debug(\"Unknown resource kind. No observer was registered.\", resource)\n        return\n\n    logger.debug(f\"Start observer for {resource.kind} %r\", resource.metadata.name)\n    task = None\n    if start:\n        task = controller.loop.create_task(observer.run())\n\n    controller.observers[resource.metadata.uid] = (observer, task)\n\n\n@listen.on(HookType.ApplicationPreReconcile)\n@listen.on(HookType.ApplicationPreMigrate)\n@listen.on(HookType.ApplicationPreDelete)\n@listen.on(HookType.ClusterDeletion)\nasync def unregister_observer(controller, resource, **kwargs):\n    \"\"\"Stop and delete the observer for the given Application or Cluster. If no observer\n    is started, do nothing.\n\n    Args:\n        controller (KubernetesController): the controller for which the observer will be\n            removed from the list of working observers.\n        resource (krake.data.kubernetes.Application): the Application whose observer\n        will be stopped or\n        resource (krake.data.kubernetes.Cluster): the Cluster whose observer will be\n        stopped.\n\n    \"\"\"\n    if resource.metadata.uid not in controller.observers:\n        return\n\n    logger.debug(f\"Stop observer for {resource.kind} %r\", resource.metadata.name)\n    _, task = controller.observers.pop(resource.metadata.uid)\n    task.cancel()\n\n    with suppress(asyncio.CancelledError):\n        await task\n\n\ndef utc_difference():\n    \"\"\"Get the difference in seconds between the current time and the current UTC time.\n\n    Returns:\n        int: the time difference in seconds.\n\n    \"\"\"\n    delta = datetime.now() - datetime.utcnow()\n    return delta.seconds\n\n\ndef generate_certificate(config):\n    \"\"\"Create and sign a new certificate using the one defined in the complete hook\n    configuration as intermediate certificate.\n\n    Args:\n        config (krake.data.config.CompleteHookConfiguration): the configuration of the\n            complete hook.\n\n    Returns:\n        CertificatePair: the content of the certificate created and its corresponding\n            key.\n\n    \"\"\"\n    with open(config.intermediate_src, \"rb\") as f:\n        intermediate_src = crypto.load_certificate(crypto.FILETYPE_PEM, f.read())\n    with open(config.intermediate_key_src, \"rb\") as f:\n        intermediate_key_src = crypto.load_privatekey(crypto.FILETYPE_PEM, f.read())\n\n    client_cert = crypto.X509()\n\n    # Set general information\n    client_cert.set_version(3)\n    client_cert.set_serial_number(random.randint(50000000000000, 100000000000000))\n    # If not set before, TLS will not accept to use this certificate in UTC cases, as\n    # the server time may be earlier.\n    time_offset = utc_difference() * -1\n    client_cert.gmtime_adj_notBefore(time_offset)\n    client_cert.gmtime_adj_notAfter(1 * 365 * 24 * 60 * 60)\n\n    # Set issuer and subject\n    intermediate_subject = intermediate_src.get_subject()\n    client_cert.set_issuer(intermediate_subject)\n    client_subj = crypto.X509Name(intermediate_subject)\n    client_subj.CN = config.hook_user\n    client_cert.set_subject(client_subj)\n\n    # Create and set the private key\n    client_key = crypto.PKey()\n    client_key.generate_key(crypto.TYPE_RSA, 2048)\n    client_cert.set_pubkey(client_key)\n\n    client_cert.sign(intermediate_key_src, \"sha256\")\n\n    cert_dump = crypto.dump_certificate(crypto.FILETYPE_PEM, client_cert).decode()\n    key_dump = crypto.dump_privatekey(crypto.FILETYPE_PEM, client_key).decode()\n    return CertificatePair(cert=cert_dump, key=key_dump)\n\n\ndef generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n\n    app.status.mangled_observer_schema = deepcopy(app.spec.observer_schema)\n\n    for resource_manifest in app.spec.manifest:\n        try:\n            get_kubernetes_resource_idx(\n                app.status.mangled_observer_schema, resource_manifest\n            )\n\n        except IndexError:\n            # Only create a default observer schema, if a custom observer schema hasn't\n            # been set by the user.\n            app.status.mangled_observer_schema.append(\n                generate_default_observer_schema_dict(\n                    resource_manifest,\n                    first_level=True,\n                )\n            )\n\n\ndef generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n\n    \"\"\"\n    observer_schema_dict = {}\n\n    for key, value in manifest_dict.items():\n\n        if isinstance(value, dict):\n            observer_schema_dict[key] = generate_default_observer_schema_dict(value)\n\n        elif isinstance(value, list):\n            observer_schema_dict[key] = generate_default_observer_schema_list(value)\n\n        else:\n            observer_schema_dict[key] = None\n\n    if first_level:\n        observer_schema_dict[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        observer_schema_dict[\"kind\"] = manifest_dict[\"kind\"]\n        observer_schema_dict[\"metadata\"][\"name\"] = manifest_dict[\"metadata\"][\"name\"]\n\n        if (\n            \"spec\" in manifest_dict\n            and \"type\" in manifest_dict[\"spec\"]\n            and manifest_dict[\"spec\"][\"type\"] == \"LoadBalancer\"\n        ):\n            observer_schema_dict[\"status\"] = {\"load_balancer\": {\"ingress\": None}}\n\n    return observer_schema_dict\n\n\ndef generate_default_observer_schema_list(manifest_list):\n    \"\"\"Together with :func:``generate_default_observer_schema_dict``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_list`` or ``manifest_dict``.\n\n    Args:\n        manifest_list (list): Partial Kubernetes resources\n\n    Returns:\n        list: Generated partial observer_schema\n\n    This function creates a new list from ``manifest_list`` and replaces all non-list\n    and non-dict elements by ``None``.\n\n    Additionally, it generates the default list control dictionary, using the current\n    length of the list as default minimum and maximum values.\n\n    \"\"\"\n    observer_schema_list = []\n\n    for value in manifest_list:\n\n        if isinstance(value, dict):\n            observer_schema_list.append(generate_default_observer_schema_dict(value))\n\n        elif isinstance(value, list):\n            observer_schema_list.append(generate_default_observer_schema_list(value))\n\n        else:\n            observer_schema_list.append(None)\n\n    observer_schema_list.append(\n        {\n            \"observer_schema_list_min_length\": len(manifest_list),\n            \"observer_schema_list_max_length\": len(manifest_list),\n        }\n    )\n\n    return observer_schema_list\n\n\n@listen.on(HookType.ApplicationMangling)\nasync def complete(app, api_endpoint, ssl_context, config):\n    \"\"\"Execute application complete hook defined by :class:`Complete`.\n    Hook mangles given application and injects complete hooks variables.\n\n    Application complete hook is disabled by default.\n    User enables this hook by the --hook-complete argument in rok cli.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application object processed\n            when the hook is called\n        api_endpoint (str): the given API endpoint\n        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint\n        config (krake.data.config.HooksConfiguration): Complete hook\n            configuration.\n\n    \"\"\"\n    if \"complete\" not in app.spec.hooks:\n        return\n\n    # Use the endpoint of the API only if the external endpoint has not been set.\n    if config.complete.external_endpoint:\n        api_endpoint = config.complete.external_endpoint\n\n    app.status.complete_token = \\\n        app.status.complete_token if app.status.complete_token else token_urlsafe()\n\n    # Generate only once the certificate and key for a specific Application\n    generated_cert = CertificatePair(\n        cert=app.status.complete_cert, key=app.status.complete_key\n    )\n    if ssl_context and generated_cert == (None, None):\n        generated_cert = generate_certificate(config.complete)\n        app.status.complete_cert = generated_cert.cert\n        app.status.complete_key = generated_cert.key\n\n    hook = Complete(\n        api_endpoint,\n        ssl_context,\n        hook_user=config.complete.hook_user,\n        cert_dest=config.complete.cert_dest,\n        env_token=config.complete.env_token,\n        env_url=config.complete.env_url,\n    )\n    hook.mangle_app(\n        app.metadata.name,\n        app.metadata.namespace,\n        app.status.complete_token,\n        app.status.last_applied_manifest,\n        config.complete.intermediate_src,\n        generated_cert,\n        app.status.mangled_observer_schema,\n        \"complete\"\n    )\n\n\n@listen.on(HookType.ApplicationMangling)\nasync def shutdown(app, api_endpoint, ssl_context, config):\n    \"\"\"Executes an application shutdown hook defined by :class:`Shutdown`.\n    The hook mangles the given application and injects shutdown hooks variables.\n\n    Application shutdown hook is disabled by default.\n    User enables this hook by the --hook-shutdown argument in rok cli.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application object processed\n            when the hook is called\n        api_endpoint (str): the given API endpoint\n        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint\n        config (krake.data.config.HooksConfiguration): Shutdown hook\n            configuration.\n\n    \"\"\"\n    if \"shutdown\" not in app.spec.hooks:\n        return\n\n    # Use the endpoint of the API only if the external endpoint has not been set.\n    if config.shutdown.external_endpoint:\n        api_endpoint = config.shutdown.external_endpoint\n\n    app.status.shutdown_token = \\\n        app.status.shutdown_token if app.status.shutdown_token else token_urlsafe()\n\n    # Generate only once the certificate and key for a specific Application\n    generated_cert = CertificatePair(\n        cert=app.status.shutdown_cert, key=app.status.shutdown_key\n    )\n    if ssl_context and generated_cert == (None, None):\n        generated_cert = generate_certificate(config.shutdown)\n        app.status.shutdown_cert = generated_cert.cert\n        app.status.shutdown_key = generated_cert.key\n\n    hook = Shutdown(\n        api_endpoint,\n        ssl_context,\n        hook_user=config.shutdown.hook_user,\n        cert_dest=config.shutdown.cert_dest,\n        env_token=config.shutdown.env_token,\n        env_url=config.shutdown.env_url,\n    )\n    hook.mangle_app(\n        app.metadata.name,\n        app.metadata.namespace,\n        app.status.shutdown_token,\n        app.status.last_applied_manifest,\n        config.shutdown.intermediate_src,\n        generated_cert,\n        app.status.mangled_observer_schema,\n        \"shutdown\"\n    )\n\n\n@listen.on(HookType.ResourcePreDelete)\nasync def pre_shutdown(controller, app, **kwargs):\n    \"\"\"\n\n    Args:\n        app (krake.data.kubernetes.Application): Application object processed\n            when the hook is called\n    \"\"\"\n    if \"shutdown\" not in app.spec.hooks:\n        return\n\n    return\n\n\nclass SubResource(NamedTuple):\n    group: str\n    name: str\n    body: dict\n    path: tuple\n\n\nclass CertificatePair(NamedTuple):\n    \"\"\"Tuple which contains a certificate and its corresponding key.\n\n    Attributes:\n        cert (str): content of a certificate.\n        key (str): content of the key that corresponds to the certificate.\n\n    \"\"\"\n\n    cert: str\n    key: str\n\n\nclass Hook(object):\n\n    hook_resources = ()\n\n    ca_name = \"ca-bundle.pem\"\n    cert_name = \"cert.pem\"\n    key_name = \"key.pem\"\n\n    def __init__(\n        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n    ):\n        self.api_endpoint = api_endpoint\n        self.ssl_context = ssl_context\n        self.hook_user = hook_user\n        self.cert_dest = cert_dest\n        self.env_token = env_token\n        self.env_url = env_url\n\n    def mangle_app(\n        self,\n        name,\n        namespace,\n        token,\n        last_applied_manifest,\n        intermediate_src,\n        generated_cert,\n        mangled_observer_schema,\n        hook_type=\"\",\n    ):\n        \"\"\"Mangle a given application and inject complete hook resources and\n        sub-resources into the :attr:`last_applied_manifest` object by :meth:`mangle`.\n        Also mangle the observer_schema as new resources and sub-resources should\n        be observed.\n\n        :attr:`last_applied_manifest` is created as a deep copy of the desired\n        application resources, as defined by user. It can be updated by custom hook\n        resources or modified by custom hook sub-resources. It is used as a desired\n        state for the Krake deployment process.\n\n        Args:\n            name (str): Application name\n            namespace (str): Application namespace\n            token (str): Complete hook authentication token\n            last_applied_manifest (list): Application resources\n            intermediate_src (str): content of the certificate that is used to sign new\n                certificates for the complete hook.\n            generated_cert (CertificatePair): tuple that contains the content of the\n                new signed certificate for the Application, and the content of its\n                corresponding key.\n            mangled_observer_schema (list): Observed fields\n            hook_type (str, optional): Name of the hook the app should be mangled for\n\n        \"\"\"\n\n        secret_certs_name = \"-\".join([name, \"krake\", hook_type, \"secret\", \"certs\"])\n        secret_token_name = \"-\".join([name, \"krake\", hook_type, \"secret\", \"token\"])\n        volume_name = \"-\".join([name, \"krake\", hook_type, \"volume\"])\n        ca_certs = (\n            self.ssl_context.get_ca_certs(binary_form=True)\n            if self.ssl_context\n            else None\n        )\n\n        # Extract all different namespaces\n        # FIXME: too many assumptions here: do we create one ConfigMap for each\n        #  namespace?\n        resource_namespaces = {\n            resource[\"metadata\"].get(\"namespace\", \"default\")\n            for resource in last_applied_manifest\n        }\n\n        hook_resources = []\n        hook_sub_resources = []\n        if ca_certs:\n            hook_resources.extend(\n                [\n                    self.secret_certs(\n                        secret_certs_name,\n                        resource_namespace,\n                        intermediate_src=intermediate_src,\n                        generated_cert=generated_cert,\n                        ca_certs=ca_certs,\n                    )\n                    for resource_namespace in resource_namespaces\n                ]\n            )\n            hook_sub_resources.extend(\n                [*self.volumes(secret_certs_name, volume_name, self.cert_dest)]\n            )\n\n        hook_resources.extend(\n            [\n                self.secret_token(\n                    secret_token_name,\n                    name,\n                    namespace,\n                    resource_namespace,\n                    self.api_endpoint,\n                    token,\n                )\n                for resource_namespace in resource_namespaces\n            ]\n        )\n        hook_sub_resources.extend(\n            [\n                *self.env_vars(secret_token_name),\n            ]\n        )\n\n        self.mangle(\n            hook_resources,\n            last_applied_manifest,\n            mangled_observer_schema,\n        )\n        self.mangle(\n            hook_sub_resources,\n            last_applied_manifest,\n            mangled_observer_schema,\n            is_sub_resource=True,\n        )\n\n    def mangle(\n        self,\n        items,\n        last_applied_manifest,\n        mangled_observer_schema,\n        is_sub_resource=False,\n    ):\n        \"\"\"Mangle applications desired state with custom hook resources or\n        sub-resources.\n\n        Example:\n            .. code:: python\n\n            last_applied_manifest = [\n                {\n                    'apiVersion': 'v1',\n                    'kind': 'Pod',\n                    'metadata': {'name': 'test', 'namespace': 'default'},\n                    'spec': {'containers': [{'name': 'test'}]}\n                }\n            ]\n            mangled_observer_schema = [\n                {\n                    'apiVersion': 'v1',\n                    'kind': 'Pod',\n                    'metadata': {'name': 'test', 'namespace': 'default'},\n                    'spec': {\n                        'containers': [\n                            {'name': None},\n                            {\n                                'observer_schema_list_max_length': 1,\n                                'observer_schema_list_min_length': 1,\n                            },\n                        ]\n                    },\n                }\n            ]\n            hook_resources = [\n                {\n                    'apiVersion': 'v1',\n                    'kind': 'Secret',\n                    'metadata': {'name': 'sct', 'namespace': 'default'}\n                }\n            ]\n            hook_sub_resources = [\n                SubResource(\n                    group='env', name='env', body={'name': 'test', 'value': 'test'},\n                    path=(('spec', 'containers'),)\n                )\n            ]\n\n            mangle(\n                hook_resources,\n                last_applied_manifest,\n                mangled_observer_schema,\n            )\n            mangle(\n                hook_sub_resources,\n                last_applied_manifest,\n                mangled_observer_schema,\n                is_sub_resource=True\n            )\n\n            assert last_applied_manifest == [\n                {\n                    \"apiVersion\": \"v1\",\n                    \"kind\": \"Pod\",\n                    \"metadata\": {\"name\": \"test\", 'namespace': 'default'},\n                    \"spec\": {\n                        \"containers\": [\n                            {\n                                \"name\": \"test\",\n                                \"env\": [{\"name\": \"test\", \"value\": \"test\"}]\n                            }\n                        ]\n                    },\n                },\n                {\"apiVersion\": \"v1\", \"kind\": \"Secret\", \"metadata\": {\"name\": \"sct\"}},\n            ]\n\n            assert mangled_observer_schema == [\n                {\n                    \"apiVersion\": \"v1\",\n                    \"kind\": \"Pod\",\n                    \"metadata\": {\"name\": \"test\", \"namespace\": None},\n                    \"spec\": {\n                        \"containers\": [\n                            {\n                                \"name\": None,\n                                \"env\": [\n                                    {\"name\": None, \"value\": None},\n                                    {\n                                        \"observer_schema_list_max_length\": 1,\n                                        \"observer_schema_list_min_length\": 1,\n                                    },\n                                ],\n                            },\n                            {\n                                \"observer_schema_list_max_length\": 1,\n                                \"observer_schema_list_min_length\": 1,\n                            },\n                        ]\n                    },\n                },\n                {\n                    \"apiVersion\": \"v1\",\n                    \"kind\": \"Secret\",\n                    \"metadata\": {\"name\": \"sct\", \"namespace\": None},\n                },\n            ]\n\n        Args:\n            items (list[SubResource]): Custom hook resources or sub-resources\n            last_applied_manifest (list): Application resources\n            mangled_observer_schema (list): Observed resources\n            is_sub_resource (bool, optional): if False, the function only extend the\n                list of Kubernetes resources defined in :attr:`last_applied_manifest`\n                with new hook resources. Otherwise, the function injects each new hook\n                sub-resource into the :attr:`last_applied_manifest` object\n                sub-resources. Defaults to False.\n\n        \"\"\"\n\n        if not items:\n            return\n\n        if not is_sub_resource:\n            last_applied_manifest.extend(items)\n            for sub_resource in items:\n                # Generate the default observer schema for each resource\n                mangled_observer_schema.append(\n                    generate_default_observer_schema_dict(\n                        sub_resource,\n                        first_level=True,\n                    )\n                )\n            return\n\n        def inject(sub_resource, sub_resource_to_mangle, observed_resource_to_mangle):\n            \"\"\"Inject a hooks defined sub-resource into a Kubernetes sub-resource.\n\n            Args:\n                sub_resource (SubResource): Hook sub-resource that needs to be injected\n                    into :attr:`last_applied_manifest`\n                sub_resource_to_mangle (object): Kubernetes sub-resources from\n                    :attr:`last_applied_manifest` which need to be processed\n                observed_resource_to_mangle (dict): partial mangled_observer_schema\n                    corresponding to the Kubernetes sub-resource.\n\n            Raises:\n                InvalidManifestError: if the sub-resource which will be mangled is not a\n                    list or a dict.\n\n            \"\"\"\n\n            # Create sub-resource group if not present in the Kubernetes sub-resource\n            if sub_resource.group not in sub_resource_to_mangle:\n                # FIXME: This assumes the subresource group contains a list\n                sub_resource_to_mangle.update({sub_resource.group: []})\n\n            # Create sub-resource group if not present in the observed fields\n            if sub_resource.group not in observed_resource_to_mangle:\n                observed_resource_to_mangle.update(\n                    {\n                        sub_resource.group: [\n                            {\n                                \"observer_schema_list_min_length\": 0,\n                                \"observer_schema_list_max_length\": 0,\n                            }\n                        ]\n                    }\n                )\n\n            # Inject sub-resource\n            # If sub-resource name is already there update it, if not, append it\n            if sub_resource.name in [\n                g[\"name\"] for g in sub_resource_to_mangle[sub_resource.group]\n            ]:\n                # FIXME: Assuming we are dealing with a list\n                for idx, item in enumerate(sub_resource_to_mangle[sub_resource.group]):\n                    if item[\"name\"]:\n                        if hasattr(item, \"body\"):\n                            sub_resource_to_mangle[item.group][idx] = item[\"body\"]\n            else:\n                sub_resource_to_mangle[sub_resource.group].append(sub_resource.body)\n\n            # Make sure the value is observed\n            if sub_resource.name not in [\n                g[\"name\"] for g in observed_resource_to_mangle[sub_resource.group][:-1]\n            ]:\n                observed_resource_to_mangle[sub_resource.group].insert(\n                    -1, generate_default_observer_schema_dict(sub_resource.body)\n                )\n                observed_resource_to_mangle[sub_resource.group][-1][\n                    \"observer_schema_list_min_length\"\n                ] += 1\n                observed_resource_to_mangle[sub_resource.group][-1][\n                    \"observer_schema_list_max_length\"\n                ] += 1\n\n        for resource in last_applied_manifest:\n            # Complete hook is applied only on defined Kubernetes resources\n            if resource[\"kind\"] not in self.hook_resources:\n                continue\n\n            for sub_resource in items:\n                sub_resources_to_mangle = None\n                idx_observed = get_kubernetes_resource_idx(\n                    mangled_observer_schema, resource\n                )\n                for keys in sub_resource.path:\n                    try:\n                        sub_resources_to_mangle = reduce(getitem, keys, resource)\n                    except KeyError:\n                        continue\n\n                    break\n\n                # Create the path to the observed sub-resource, if it doesn't yet exist\n                try:\n                    observed_sub_resources = reduce(\n                        getitem, keys, mangled_observer_schema[idx_observed]\n                    )\n                except KeyError:\n                    Complete.create_path(\n                        mangled_observer_schema[idx_observed], list(keys)\n                    )\n                    observed_sub_resources = reduce(\n                        getitem, keys, mangled_observer_schema[idx_observed]\n                    )\n\n                if isinstance(sub_resources_to_mangle, list):\n                    for idx, sub_resource_to_mangle in enumerate(\n                        sub_resources_to_mangle\n                    ):\n\n                        # Ensure that each element of the list is observed.\n                        idx_observed = idx\n                        if idx >= len(observed_sub_resources[:-1]):\n                            idx_observed = len(observed_sub_resources[:-1])\n                            # FIXME: Assuming each element of the list contains a\n                            # dictionary, therefore initializing new elements with an\n                            # empty dict\n                            observed_sub_resources.insert(-1, {})\n                        observed_sub_resource = observed_sub_resources[idx_observed]\n\n                        # FIXME: This is assuming a list always contains dict\n                        inject(\n                            sub_resource, sub_resource_to_mangle, observed_sub_resource\n                        )\n\n                elif isinstance(sub_resources_to_mangle, dict):\n                    inject(\n                        sub_resource, sub_resources_to_mangle, observed_sub_resources\n                    )\n\n                else:\n                    message = (\n                        f\"The sub-resource to mangle {sub_resources_to_mangle!r} has an\"\n                        \"invalid type, should be in '[dict, list]'\"\n                    )\n                    raise InvalidManifestError(message)\n\n    @staticmethod\n    def attribute_map(obj):\n        \"\"\"Convert a Kubernetes object to dict based on its attribute mapping\n\n        Example:\n            .. code:: python\n\n            from kubernetes_asyncio.client import V1VolumeMount\n\n            d = attribute_map(\n                    V1VolumeMount(name=\"name\", mount_path=\"path\")\n            )\n            assert d == {'mountPath': 'path', 'name': 'name'}\n\n        Args:\n            obj (object): Kubernetes object\n\n        Returns:\n            dict: Converted Kubernetes object\n\n        \"\"\"\n        return {\n            obj.attribute_map[attr]: getattr(obj, attr)\n            for attr, _ in obj.to_dict().items()\n            if getattr(obj, attr) is not None\n        }\n\n    @staticmethod\n    def create_path(mangled_observer_schema, keys):\n        \"\"\"Create the path to the observed field in the observer schema.\n\n        When a sub-resource is mangled, it should be observed. This function creates\n        the path to the subresource to observe.\n\n        Args:\n            mangled_observer_schema (dict): Partial observer schema of a resource\n            keys (list): list of keys forming the path to the sub-resource to\n                observe\n\n        FIXME: This assumes we are only adding keys to dict. We don't consider lists\n\n        \"\"\"\n\n        # Unpack the first key first, as it contains the base directory\n        key = keys.pop(0)\n\n        # If the key is the last of the list, we reached the end of the path.\n        if len(keys) == 0:\n            mangled_observer_schema[key] = None\n            return\n\n        if key not in mangled_observer_schema:\n            mangled_observer_schema[key] = {}\n        Hook.create_path(mangled_observer_schema[key], keys)\n\n    def secret_certs(\n        self,\n        secret_name,\n        namespace,\n        ca_certs=None,\n        intermediate_src=None,\n        generated_cert=None,\n    ):\n        \"\"\"Create a complete hooks secret resource.\n\n        Complete hook secret stores Krake CAs and client certificates to communicate\n        with the Krake API.\n\n        Args:\n            secret_name (str): Secret name\n            namespace (str): Kubernetes namespace where the Secret will be created.\n            ca_certs (list): Krake CA list\n            intermediate_src (str): content of the certificate that is used to sign new\n                certificates for the complete hook.\n            generated_cert (CertificatePair): tuple that contains the content of the\n                new signed certificate for the Application, and the content of its\n                corresponding key.\n\n        Returns:\n            dict: complete hook secret resource\n\n        \"\"\"\n        ca_certs_pem = \"\"\n        for ca_cert in ca_certs:\n            x509 = crypto.load_certificate(crypto.FILETYPE_ASN1, ca_cert)\n            ca_certs_pem += crypto.dump_certificate(crypto.FILETYPE_PEM, x509).decode()\n\n        # Add the intermediate certificate into the chain\n        with open(intermediate_src, \"r\") as f:\n            intermediate_src_content = f.read()\n        ca_certs_pem += intermediate_src_content\n\n        data = {\n            self.ca_name: self._encode_to_64(ca_certs_pem),\n            self.cert_name: self._encode_to_64(generated_cert.cert),\n            self.key_name: self._encode_to_64(generated_cert.key),\n        }\n        return self.secret(secret_name, data, namespace)\n\n    def secret_token(\n        self, secret_name, name, namespace, resource_namespace, api_endpoint, token\n    ):\n        \"\"\"Create a hooks secret resource.\n\n        The hook secret stores Krake authentication token\n        and hook URL for given application.\n\n        Args:\n            secret_name (str): Secret name\n            name (str): Application name\n            namespace (str): Application namespace\n            resource_namespace (str): Kubernetes namespace where the\n                Secret will be created.\n            api_endpoint (str): Krake API endpoint\n            token (str): Complete hook authentication token\n\n        Returns:\n            dict: complete hook secret resource\n\n        \"\"\"\n        pass\n\n    def volumes(self, secret_name, volume_name, mount_path):\n        \"\"\"Create complete hooks volume and volume mount sub-resources\n\n        Complete hook volume gives access to hook's secret, which stores\n        Krake CAs and client certificates to communicate with the Krake API.\n        Complete hook volume mount puts the volume into the application\n\n        Args:\n            secret_name (str): Secret name\n            volume_name (str): Volume name\n            mount_path (list): Volume mount path\n\n        Returns:\n            list: List of complete hook volume and volume mount sub-resources\n\n        \"\"\"\n        volume = V1Volume(name=volume_name, secret={\"secretName\": secret_name})\n        volume_mount = V1VolumeMount(name=volume_name, mount_path=mount_path)\n        return [\n            SubResource(\n                group=\"volumes\",\n                name=volume.name,\n                body=self.attribute_map(volume),\n                path=((\"spec\", \"template\", \"spec\"), (\"spec\",)),\n            ),\n            SubResource(\n                group=\"volumeMounts\",\n                name=volume_mount.name,\n                body=self.attribute_map(volume_mount),\n                path=(\n                    (\"spec\", \"template\", \"spec\", \"containers\"),\n                    (\"spec\", \"containers\"),  # kind: Pod\n                ),\n            ),\n        ]\n\n    @staticmethod\n    def _encode_to_64(string):\n        \"\"\"Compute the base 64 encoding of a string.\n\n        Args:\n            string (str): the string to encode.\n\n        Returns:\n            str: the result of the encoding.\n\n        \"\"\"\n        return b64encode(string.encode()).decode()\n\n    def secret(self, secret_name, secret_data, namespace, _type=\"Opaque\"):\n        \"\"\"Create a secret resource.\n\n        Args:\n            secret_name (str): Secret name\n            secret_data (dict): Secret data\n            namespace (str): Kubernetes namespace where the Secret will be created.\n            _type (str, optional): Secret type. Defaults to Opaque.\n\n        Returns:\n            dict: secret resource\n\n        \"\"\"\n        return self.attribute_map(\n            V1Secret(\n                api_version=\"v1\",\n                kind=\"Secret\",\n                data=secret_data,\n                metadata={\"name\": secret_name, \"namespace\": namespace},\n                type=_type,\n            )\n        )\n\n    @staticmethod\n    def create_hook_url(name, namespace, api_endpoint):\n        \"\"\"Create an applications' hook URL.\n        Function needs to be specified for each hook.\n\n        Args:\n            name (str): Application name\n            namespace (str): Application namespace\n            api_endpoint (str): Krake API endpoint\n\n        Returns:\n            str: Application shutdown url\n\n        \"\"\"\n        pass\n\n    def env_vars(self, secret_name):\n        \"\"\"Create the hooks' environment variables sub-resources.\n        Function needs to be specified for each hook.\n\n        Creates hook environment variables to store Krake authentication token\n        and a hook URL for the given applications.\n\n        Args:\n            secret_name (str): Secret name\n\n        Returns:\n            list: List of shutdown hook environment variables sub-resources\n\n        \"\"\"\n        pass\n\n\nclass Complete(Hook):\n    \"\"\"Mangle given application and inject complete hooks variables into it.\n\n    Hook injects a Kubernetes secret, which stores Krake authentication token\n    and the Krake complete hook URL for the given application. The variables\n    from Kubernetes secret are imported as environment variables\n    into the application resource definition. Only resources defined in\n    :args:`hook_resources` can be modified.\n\n    Names of environment variables are defined in the application controller\n    configuration file.\n\n    If TLS is enabled on the Krake API, the complete hook injects a Kubernetes secret,\n    and it's corresponding volume and volume mount definitions for the Krake CA,\n    the client certificate with the right CN, and its key. The directory where the\n    secret is mounted is defined in the configuration.\n\n    Args:\n        api_endpoint (str): the given API endpoint\n        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint\n        cert_dest (str, optional): Path of the directory where the CA, client\n            certificate and key to the Krake API will be stored.\n        env_token (str, optional): Name of the environment variable, which stores Krake\n            authentication token.\n        env_url (str, optional): Name of the environment variable,\n            which stores Krake complete hook URL.\n\n    \"\"\"\n\n    hook_resources = (\"Pod\", \"Deployment\", \"ReplicationController\")\n\n    def __init__(\n        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n    ):\n        super().__init__(\n            api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n        )\n        self.env_url = env_url\n\n    def secret_token(\n        self, secret_name, name, namespace, resource_namespace, api_endpoint, token\n    ):\n        \"\"\"Create complete hooks secret resource.\n\n        Complete hook secret stores Krake authentication token\n        and complete hook URL for given application.\n\n        Args:\n            secret_name (str): Secret name\n            name (str): Application name\n            namespace (str): Application namespace\n            resource_namespace (str): Kubernetes namespace where the\n                Secret will be created.\n            api_endpoint (str): Krake API endpoint\n            token (str): Complete hook authentication token\n\n        Returns:\n            dict: complete hook secret resource\n\n        \"\"\"\n        complete_url = self.create_hook_url(name, namespace, api_endpoint)\n        data = {\n            self.env_token.lower(): self._encode_to_64(token),\n            self.env_url.lower(): self._encode_to_64(complete_url),\n        }\n        return self.secret(secret_name, data, resource_namespace)\n\n    @staticmethod\n    def create_hook_url(name, namespace, api_endpoint):\n        \"\"\"Create an applications' complete URL.\n\n        Args:\n            name (str): Application name\n            namespace (str): Application namespace\n            api_endpoint (str): Krake API endpoint\n\n        Returns:\n            str: Application complete url\n\n        \"\"\"\n        api_url = URL(api_endpoint)\n        return str(\n            api_url.with_path(\n                f\"/kubernetes/namespaces/{namespace}/applications/{name}/complete\"\n            )\n        )\n\n    def env_vars(self, secret_name):\n        \"\"\"Create complete hooks environment variables sub-resources\n\n        Create complete hook environment variables store Krake authentication token\n        and complete hook URL for given application.\n\n        Args:\n            secret_name (str): Secret name\n\n        Returns:\n            list: List of complete hook environment variables sub-resources\n\n        \"\"\"\n        sub_resources = []\n\n        env_token = V1EnvVar(\n            name=self.env_token,\n            value_from=self.attribute_map(\n                V1EnvVarSource(\n                    secret_key_ref=self.attribute_map(\n                        V1SecretKeySelector(\n                            name=secret_name, key=self.env_token.lower()\n                        )\n                    )\n                )\n            ),\n        )\n        env_url = V1EnvVar(\n            name=self.env_url,\n            value_from=self.attribute_map(\n                V1EnvVarSource(\n                    secret_key_ref=self.attribute_map(\n                        V1SecretKeySelector(name=secret_name, key=self.env_url.lower())\n                    )\n                )\n            ),\n        )\n\n        for env in (env_token, env_url):\n            sub_resources.append(\n                SubResource(\n                    group=\"env\",\n                    name=env.name,\n                    body=self.attribute_map(env),\n                    path=(\n                        (\"spec\", \"template\", \"spec\", \"containers\"),\n                        (\"spec\", \"containers\"),  # kind: Pod\n                    ),\n                )\n            )\n        return sub_resources\n\n\nclass Shutdown(Hook):\n    \"\"\"Mangle given application and inject shutdown hooks variables into it.\n\n    Hook injects a Kubernetes secret, which stores Krake authentication token\n    and the Krake complete hook URL for the given application. The variables\n    from the Kubernetes secret are imported as environment variables\n    into the application resource definition. Only resources defined in\n    :args:`hook_resources` can be modified.\n\n    Names of environment variables are defined in the application controller\n    configuration file.\n\n    If TLS is enabled on the Krake API, the shutdown hook injects a Kubernetes secret,\n    and it's corresponding volume and volume mount definitions for the Krake CA,\n    the client certificate with the right CN, and its key. The directory where the\n    secret is mounted is defined in the configuration.\n\n    Args:\n        api_endpoint (str): the given API endpoint\n        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint\n        cert_dest (str, optional): Path of the directory where the CA, client\n            certificate and key to the Krake API will be stored.\n        env_token (str, optional): Name of the environment variable, which stores Krake\n            authentication token.\n        env_url (str, optional): Name of the environment variable,\n            which stores Krake complete hook URL.\n\n    \"\"\"\n\n    hook_resources = (\"Pod\", \"Deployment\", \"ReplicationController\")\n\n    def __init__(\n        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n    ):\n        super().__init__(\n            api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n        )\n        self.env_url = env_url\n\n    def secret_token(\n        self, secret_name, name, namespace, resource_namespace, api_endpoint, token\n    ):\n        \"\"\"Create shutdown hooks secret resource.\n\n        Shutdown hook secret stores Krake authentication token\n        and shutdown hook URL for given application.\n\n        Args:\n            secret_name (str): Secret name\n            name (str): Application name\n            namespace (str): Application namespace\n            resource_namespace (str): Kubernetes namespace where the\n                Secret will be created.\n            api_endpoint (str): Krake API endpoint\n            token (str): Shutdown hook authentication token\n\n        Returns:\n            dict: shutdown hook secret resource\n\n        \"\"\"\n        shutdown_url = self.create_hook_url(name, namespace, api_endpoint)\n        data = {\n            self.env_token.lower(): self._encode_to_64(token),\n            self.env_url.lower(): self._encode_to_64(shutdown_url),\n        }\n        return self.secret(secret_name, data, resource_namespace)\n\n    @staticmethod\n    def create_hook_url(name, namespace, api_endpoint):\n        \"\"\"Create an applications' shutdown URL.\n\n        Args:\n            name (str): Application name\n            namespace (str): Application namespace\n            api_endpoint (str): Krake API endpoint\n\n        Returns:\n            str: Application shutdown url\n\n        \"\"\"\n        api_url = URL(api_endpoint)\n        return str(\n            api_url.with_path(\n                f\"/kubernetes/namespaces/{namespace}/applications/{name}/shutdown\"\n            )\n        )\n\n    def env_vars(self, secret_name):\n        \"\"\"Create shutdown hooks environment variables sub-resources.\n\n        Creates shutdown hook environment variables to store Krake authentication token\n        and a shutdown hook URL for given applications.\n\n        Args:\n            secret_name (str): Secret name\n\n        Returns:\n            list: List of shutdown hook environment variables sub-resources\n\n        \"\"\"\n        sub_resources = []\n\n        env_resources = []\n\n        env_token = V1EnvVar(\n            name=self.env_token,\n            value_from=self.attribute_map(\n                V1EnvVarSource(\n                    secret_key_ref=self.attribute_map(\n                        V1SecretKeySelector(\n                            name=secret_name,\n                            key=self.env_token.lower()\n                        )\n                    )\n                )\n            )\n        )\n        env_resources.append(env_token)\n\n        env_url = V1EnvVar(\n            name=self.env_url,\n            value_from=self.attribute_map(\n                V1EnvVarSource(\n                    secret_key_ref=self.attribute_map(\n                        V1SecretKeySelector(name=secret_name, key=self.env_url.lower())\n                    )\n                )\n            ),\n        )\n        env_resources.append(env_url)\n\n        for env in env_resources:\n            sub_resources.append(\n                SubResource(\n                    group=\"env\",\n                    name=env.name,\n                    body=self.attribute_map(env),\n                    path=(\n                        (\"spec\", \"template\", \"spec\", \"containers\"),\n                        (\"spec\", \"containers\"),  # kind: Pod\n                    ),\n                )\n            )\n        return sub_resources\n",
            "file_path": "krake/krake/controller/kubernetes/hooks.py",
            "human_label": "Generate the default observer schema for each Kubernetes resource present in ``spec.manifest`` for which a custom observer schema hasn't been specified.",
            "level": "file_runnable",
            "lineno": "1012",
            "name": "generate_default_observer_schema",
            "oracle_context": "{ \"apis\" : \"['deepcopy', 'get_kubernetes_resource_idx', 'append', 'generate_default_observer_schema_dict']\", \"classes\" : \"['deepcopy', 'get_kubernetes_resource_idx']\", \"vars\" : \"['mangled_observer_schema', 'spec', 'observer_schema', 'status', 'manifest']\" }",
            "package": "hooks",
            "project": "rak-n-rok/Krake",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b43427903eeb48555d3ea5",
            "all_context": "{ \"import\" : \"typing re  typing \", \"file\" : \"_BYTES_ENCODING ; _STYLES ; \", \"class\" : \"self._create_converter(self) ; self.format(self,sql,params) ; self.__class__ ; self.formatmany(self,sql,many_params) ; self._converter ; self._in_regex ; self._create_in_regex(self) ; self.expand_tuples(self) ; self.out_style(self) ; self.__repr__(self) ; self._expand_tuples ; self.__init__(self,in_style,out_style,escape_char,expand_tuples) ; self._in_obj ; self._out_obj ; self._create_in_regex ; self._in_style ; self._escape_char ; self._out_style ; self.escape_char(self) ; self.in_style(self) ; self._create_converter ; \" }",
            "code": "\tdef format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\t\"\"\"\n\t\tConvert the SQL query to use the out-style parameters instead of\n\t\tthe in-style parameters.\n\n\t\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n\t\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t\tcontains the set of in-style parameters. It maps each parameter\n\t\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t\t*params* must be a :class:`~collections.abc.Sequence`.\n\n\t\tReturns a :class:`tuple` containing:\n\n\t\t-\tThe formatted SQL query (:class:`str` or :class:`bytes`).\n\n\t\t-\tThe set of converted out-style parameters (:class:`dict` or\n\t\t\t:class:`list`).\n\t\t\"\"\"\n\t\t# Normalize query encoding to simplify processing.\n\t\tif isinstance(sql, str):\n\t\t\tuse_sql = sql\n\t\t\tstring_type = str\n\t\telif isinstance(sql, bytes):\n\t\t\tuse_sql = sql.decode(_BYTES_ENCODING)\n\t\t\tstring_type = bytes\n\t\telse:\n\t\t\traise TypeError(\"sql:{!r} is not a unicode or byte string.\".format(sql))\n\n\t\t# Replace in-style with out-style parameters.\n\t\tuse_sql, out_params = self._converter.convert(use_sql, params)\n\n\t\t# Make sure the query is returned as the proper string type.\n\t\tif string_type is bytes:\n\t\t\tout_sql = use_sql.encode(_BYTES_ENCODING)\n\t\telse:\n\t\t\tout_sql = use_sql\n\n\t\t# Return converted SQL and out-parameters.\n\t\treturn out_sql, out_params\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : true, \"public_lib\" : true, \"current_class\" : true, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Convert the SQL query to use the out-style parameters instead of\nthe in-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\ncontains the set of in-style parameters. It maps each parameter\n(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\nis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\nIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n*params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       The set of converted out-style parameters (:class:`dict` or\n        :class:`list`).",
            "end_lineno": "373",
            "file_content": "\"\"\"\n:mod:`sqlparams` is a utility package for converting between various SQL\nparameter styles.\n\"\"\"\n\nimport re\nfrom typing import (\n\tAny,\n\tAnyStr,\n\tDict,\n\tIterable,\n\tList,\n\tOptional,\n\tPattern,\n\tSequence,\n\tTuple,\n\tType,\n\tUnion)\n\nfrom . import _converting\nfrom . import _styles\nfrom ._util import _is_iterable\n\nfrom ._meta import (\n\t__author__,\n\t__copyright__,\n\t__credits__,\n\t__license__,\n\t__version__,\n)\n\n_BYTES_ENCODING = 'latin1'\n\"\"\"\nThe encoding to use when parsing a byte query string.\n\"\"\"\n\n_STYLES = {}\n\"\"\"\nMaps parameter style by name.\n\"\"\"\n\n\nclass SQLParams(object):\n\t\"\"\"\n\tThe :class:`.SQLParams` class is used to support named parameters in\n\tSQL queries where they are not otherwise supported (e.g., pyodbc).\n\tThis is done by converting from one parameter style query to another\n\tparameter style query.\n\n\tBy default, when converting to a numeric or ordinal style any\n\t:class:`tuple` parameter will be expanded into \"(?,?,...)\" to support\n\tthe widely used \"IN {tuple}\" SQL expression without leaking any\n\tunescaped values.\n\t\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tin_style: str,\n\t\tout_style: str,\n\t\tescape_char: Union[str, bool, None] = None,\n\t\texpand_tuples: Optional[bool] = None,\n\t) -> None:\n\t\t\"\"\"\n\t\tInstantiates the :class:`.SQLParams` instance.\n\n\t\t*in_style* (:class:`str`) is the parameter style that will be used\n\t\tin an SQL query before being parsed and converted to :attr:`.SQLParams.out_style`.\n\n\t\t*out_style* (:class:`str`) is the parameter style that the SQL query\n\t\twill be converted to.\n\n\t\t*escape_char* (:class:`str`, :class:`bool`, or :data:`None`) is the\n\t\tescape character used to prevent matching a in-style parameter. If\n\t\t:data:`True`, use the default escape character (repeat the initial\n\t\tcharacter to escape it; e.g., \"%%\"). If :data:`False`, do not use an\n\t\tescape character. Default is :data:`None` for :data:`False`.\n\n\t\t*expand_tuples* (:class:`bool` or :data:`None`) is whether to\n\t\texpand tuples into a sequence of parameters. Default is :data:`None`\n\t\tto let it be determined by *out_style* (to maintain backward\n\t\tcompatibility). If *out_style* is a numeric or ordinal style, expand\n\t\ttuples by default (:data:`True`). If *out_style* is a named style,\n\t\tdo not expand tuples by default (:data:`False`).\n\n\t\tThe following parameter styles are supported by both *in_style* and\n\t\t*out_style*:\n\n\t\t-\tFor all named styles the parameter keys must be valid `Python identifiers`_.\n\t\t\tThey cannot start with a digit. This is to help prevent\n\t\t\tincorrectly matching common strings such as datetimes.\n\n\t\t\tNamed styles:\n\n\t\t\t-\t\"named\" indicates parameters will use the named style::\n\n\t\t\t\t\t... WHERE name = :name\n\n\t\t\t-\t\"named_dollar\" indicates parameters will use the named dollar\n\t\t\t\tsign style::\n\n\t\t\t\t\t... WHERE name = $name\n\n\t\t\t\t.. NOTE:: This is not defined by `PEP 249`_.\n\n\t\t\t-\t\"pyformat\" indicates parameters will use the named Python\n\t\t\t\textended format style::\n\n\t\t\t\t\t... WHERE name = %(name)s\n\n\t\t\t\t.. NOTE:: Strictly speaking, `PEP 249`_ only specifies\n\t\t\t\t   \"%(name)s\" for the \"pyformat\" parameter style so only that\n\t\t\t\t   form (without any other conversions or flags) is supported.\n\n\t\t-\tAll numeric styles start at :data:`1`. When using a\n\t\t\t:class:`~collections.abc.Sequence` for the parameters, the 1st\n\t\t\tparameter (e.g., \":1\") will correspond to the 1st element of the\n\t\t\tsequence (i.e., index :data:`0`). When using a :class:`~collections.abc.Mapping`\n\t\t\tfor the parameters, the 1st parameter (e.g., \":1\") will correspond\n\t\t\tto the matching key (i.e., :data:`1` or :data:`\"1\"`).\n\n\t\t\tNumeric styles:\n\n\t\t\t-\t\"numeric\" indicates parameters will use the numeric style::\n\n\t\t\t\t\t... WHERE name = :1\n\n\t\t\t-\t\"numeric_dollar\" indicates parameters will use the numeric\n\t\t\t\tdollar sign style (starts at :data:`1`)::\n\n\t\t\t\t\t... WHERE name = $1\n\n\t\t\t\t.. NOTE:: This is not defined by `PEP 249`_.\n\n\t\t- Ordinal styles:\n\n\t\t\t-\t\"format\" indicates parameters will use the ordinal Python format\n\t\t\t\tstyle::\n\n\t\t\t\t\t... WHERE name = %s\n\n\t\t\t\t.. NOTE:: Strictly speaking, `PEP 249`_ only specifies \"%s\" for\n\t\t\t\t   the \"format\" parameter styles so only that form (without any\n\t\t\t\t   other conversions or flags) is supported.\n\n\t\t\t-\t\"qmark\" indicates parameters will use the ordinal question mark\n\t\t\t\tstyle::\n\n\t\t\t\t\t... WHERE name = ?\n\n\t\t.. _`PEP 249`: http://www.python.org/dev/peps/pep-0249/\n\n\t\t.. _`Python identifiers`: https://docs.python.org/3/reference/lexical_analysis.html#identifiers\n\t\t\"\"\"\n\n\t\tself._converter: _converting._Converter = None\n\t\t\"\"\"\n\t\t*_converter* (:class:`._converting._Converter`) is the parameter\n\t\tconverter to use.\n\t\t\"\"\"\n\n\t\tself._escape_char: Optional[str] = None\n\t\t\"\"\"\n\t\t*_escape_char* (:class:`str` or :data:`None`) is the escape\n\t\tcharacter used to prevent matching a in-style parameter.\n\t\t\"\"\"\n\n\t\tself._expand_tuples: bool = None\n\t\t\"\"\"\n\t\t*_expand_tuples* (:class:`bool`) is whether to convert tuples into a\n\t\tsequence of parameters.\n\t\t\"\"\"\n\n\t\tself._in_obj: _styles._Style = None\n\t\t\"\"\"\n\t\t*_in_obj* (:class:`._styles._Style`) is the in-style parameter object.\n\t\t\"\"\"\n\n\t\tself._in_regex: Pattern = None\n\t\t\"\"\"\n\t\t*_in_regex* (:class:`re.Pattern`) is the regular expression used to\n\t\textract the in-style parameters.\n\t\t\"\"\"\n\n\t\tself._in_style: str = None\n\t\t\"\"\"\n\t\t*_in_style* (:class:`str`) is the parameter style that will be used\n\t\tin an SQL query before being parsed and converted to :attr:`.SQLParams.out_style`.\n\t\t\"\"\"\n\n\t\tself._out_obj: _styles._Style = None\n\t\t\"\"\"\n\t\t*_out_obj* (:class:`._styles._Style`) is the out-style parameter object.\n\t\t\"\"\"\n\n\t\tself._out_style: str = None\n\t\t\"\"\"\n\t\t*_out_style* (:class:`str`) is the parameter style that the SQL query\n\t\twill be converted to.\n\t\t\"\"\"\n\n\t\tif not isinstance(in_style, str):\n\t\t\traise TypeError(\"in_style:{!r} is not a string.\".format(in_style))\n\n\t\tif not isinstance(out_style, str):\n\t\t\traise TypeError(\"out_style:{!r} is not a string.\".format(out_style))\n\n\t\tself._in_style = in_style\n\t\tself._out_style = out_style\n\n\t\tself._in_obj = _styles._STYLES[self._in_style]\n\t\tself._out_obj = _styles._STYLES[self._out_style]\n\n\t\tif escape_char is True:\n\t\t\tuse_char = self._in_obj.escape_char\n\t\telif not escape_char:\n\t\t\tuse_char = None\n\t\telif isinstance(escape_char, str):\n\t\t\tuse_char = escape_char\n\t\telse:\n\t\t\traise TypeError(\"escape_char:{!r} is not a string or bool.\")\n\n\t\tif expand_tuples is None:\n\t\t\texpand_tuples = not isinstance(self._out_obj, _styles._NamedStyle)\n\n\t\tself._escape_char = use_char\n\t\tself._expand_tuples = bool(expand_tuples)\n\n\t\tself._in_regex = self._create_in_regex()\n\t\tself._converter = self._create_converter()\n\n\tdef __repr__(self) -> str:\n\t\t\"\"\"\n\t\tReturns the canonical string representation (:class:`str`) of this\n\t\tinstance.\n\t\t\"\"\"\n\t\treturn \"{}.{}({!r}, {!r})\".format(self.__class__.__module__, self.__class__.__name__, self._in_style, self._out_style)\n\n\tdef _create_converter(self) -> _converting._Converter:\n\t\t\"\"\"\n\t\tCreate the parameter style converter.\n\n\t\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\t\"\"\"\n\t\tassert self._in_regex is not None, self._in_regex\n\t\tassert self._out_obj is not None, self._out_obj\n\n\t\t# Determine converter class.\n\t\tconverter_class: Type[_converting._Converter]\n\t\tif isinstance(self._in_obj, _styles._NamedStyle):\n\t\t\tif isinstance(self._out_obj, _styles._NamedStyle):\n\t\t\t\tconverter_class = _converting._NamedToNamedConverter\n\t\t\telif isinstance(self._out_obj, _styles._NumericStyle):\n\t\t\t\tconverter_class = _converting._NamedToNumericConverter\n\t\t\telif isinstance(self._out_obj, _styles._OrdinalStyle):\n\t\t\t\tconverter_class = _converting._NamedToOrdinalConverter\n\t\t\telse:\n\t\t\t\traise TypeError(\"out_style:{!r} maps to an unexpected type: {!r}\".format(self._out_style, self._out_obj))\n\n\t\telif isinstance(self._in_obj, _styles._NumericStyle):\n\t\t\tif isinstance(self._out_obj, _styles._NamedStyle):\n\t\t\t\tconverter_class = _converting._NumericToNamedConverter\n\t\t\telif isinstance(self._out_obj, _styles._NumericStyle):\n\t\t\t\tconverter_class = _converting._NumericToNumericConverter\n\t\t\telif isinstance(self._out_obj, _styles._OrdinalStyle):\n\t\t\t\tconverter_class = _converting._NumericToOrdinalConverter\n\t\t\telse:\n\t\t\t\traise TypeError(\"out_style:{!r} maps to an unexpected type: {!r}\".format(self._out_style, self._out_obj))\n\n\t\telif isinstance(self._in_obj, _styles._OrdinalStyle):\n\t\t\tif isinstance(self._out_obj, _styles._NamedStyle):\n\t\t\t\tconverter_class = _converting._OrdinalToNamedConverter\n\t\t\telif isinstance(self._out_obj, _styles._NumericStyle):\n\t\t\t\tconverter_class = _converting._OrdinalToNumericConverter\n\t\t\telif isinstance(self._out_obj, _styles._OrdinalStyle):\n\t\t\t\tconverter_class = _converting._OrdinalToOrdinalConverter\n\t\t\telse:\n\t\t\t\traise TypeError(\"out_style:{!r} maps to an unexpected type: {!r}\".format(self._out_style, self._out_obj))\n\n\t\telse:\n\t\t\traise TypeError(\"in_style:{!r} maps to an unexpected type: {!r}\".format(self._in_style, self._in_obj))\n\n\t\t# Create converter.\n\t\tconverter = converter_class(\n\t\t\tescape_char=self._escape_char,\n\t\t\texpand_tuples=self._expand_tuples,\n\t\t\tin_regex=self._in_regex,\n\t\t\tin_style=self._in_obj,\n\t\t\tout_style=self._out_obj,\n\t\t)\n\t\treturn converter\n\n\tdef _create_in_regex(self) -> Pattern:\n\t\t\"\"\"\n\t\tCreate the in-style parameter regular expression.\n\n\t\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\t\"\"\"\n\t\tregex_parts = []\n\n\t\tif self._in_obj.escape_char != \"%\" and self._out_obj.escape_char == \"%\":\n\t\t\tregex_parts.append(\"(?P<out_percent>%)\")\n\n\t\tif self._escape_char:\n\t\t\t# Escaping is enabled.\n\t\t\tescape = self._in_obj.escape_regex.format(char=re.escape(self._escape_char))\n\t\t\tregex_parts.append(escape)\n\n\t\tregex_parts.append(self._in_obj.param_regex)\n\n\t\treturn re.compile(\"|\".join(regex_parts))\n\n\t@property\n\tdef escape_char(self) -> Optional[str]:\n\t\t\"\"\"\n\t\t*escape_char* (:class:`str` or :data:`None`) is the escape character\n\t\tused to prevent matching a in-style parameter.\n\t\t\"\"\"\n\t\treturn self._escape_char\n\n\t@property\n\tdef expand_tuples(self) -> bool:\n\t\t\"\"\"\n\t\t*expand_tuples* (:class:`bool`) is whether to convert tuples into a\n\t\tsequence of parameters.\n\t\t\"\"\"\n\t\treturn self._expand_tuples\n\n\tdef format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\t\"\"\"\n\t\tConvert the SQL query to use the out-style parameters instead of\n\t\tthe in-style parameters.\n\n\t\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n\t\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t\tcontains the set of in-style parameters. It maps each parameter\n\t\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t\t*params* must be a :class:`~collections.abc.Sequence`.\n\n\t\tReturns a :class:`tuple` containing:\n\n\t\t-\tThe formatted SQL query (:class:`str` or :class:`bytes`).\n\n\t\t-\tThe set of converted out-style parameters (:class:`dict` or\n\t\t\t:class:`list`).\n\t\t\"\"\"\n\t\t# Normalize query encoding to simplify processing.\n\t\tif isinstance(sql, str):\n\t\t\tuse_sql = sql\n\t\t\tstring_type = str\n\t\telif isinstance(sql, bytes):\n\t\t\tuse_sql = sql.decode(_BYTES_ENCODING)\n\t\t\tstring_type = bytes\n\t\telse:\n\t\t\traise TypeError(\"sql:{!r} is not a unicode or byte string.\".format(sql))\n\n\t\t# Replace in-style with out-style parameters.\n\t\tuse_sql, out_params = self._converter.convert(use_sql, params)\n\n\t\t# Make sure the query is returned as the proper string type.\n\t\tif string_type is bytes:\n\t\t\tout_sql = use_sql.encode(_BYTES_ENCODING)\n\t\telse:\n\t\t\tout_sql = use_sql\n\n\t\t# Return converted SQL and out-parameters.\n\t\treturn out_sql, out_params\n\n\tdef formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\t\"\"\"\n\t\tConvert the SQL query to use the out-style parameters instead of the\n\t\tin-style parameters.\n\n\t\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n\t\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\t\tof in-style parameters (*params*).\n\n\t\t-\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t\t\tcontains the set of in-style parameters. It maps each parameter\n\t\t\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t\t\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t\t\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t\t\t*params* must be a :class:`~collections.abc.Sequence`.\n\n\t\tReturns a :class:`tuple` containing:\n\n\t\t-\tThe formatted SQL query (:class:`str` or :class:`bytes`).\n\n\t\t-\tA :class:`list` containing each set of converted out-style\n\t\t\tparameters (:class:`dict` or :class:`list`).\n\t\t\"\"\"\n\t\t# Normalize query encoding to simplify processing.\n\t\tif isinstance(sql, str):\n\t\t\tuse_sql = sql\n\t\t\tstring_type = str\n\t\telif isinstance(sql, bytes):\n\t\t\tuse_sql = sql.decode(_BYTES_ENCODING)\n\t\t\tstring_type = bytes\n\t\telse:\n\t\t\traise TypeError(\"sql:{!r} is not a unicode or byte string.\".format(sql))\n\n\t\tif not _is_iterable(many_params):\n\t\t\traise TypeError(\"many_params:{!r} is not iterable.\".format(many_params))\n\n\t\t# Replace in-style with out-style parameters.\n\t\tuse_sql, many_out_params = self._converter.convert_many(use_sql, many_params)\n\n\t\t# Make sure the query is returned as the proper string type.\n\t\tif string_type is bytes:\n\t\t\tout_sql = use_sql.encode(_BYTES_ENCODING)\n\t\telse:\n\t\t\tout_sql = use_sql\n\n\t\t# Return converted SQL and out-parameters.\n\t\treturn out_sql, many_out_params\n\n\t@property\n\tdef in_style(self) -> str:\n\t\t\"\"\"\n\t\t*in_style* (:class:`str`) is the parameter style to expect in an SQL\n\t\tquery when being parsed.\n\t\t\"\"\"\n\t\treturn self._in_style\n\n\t@property\n\tdef out_style(self) -> str:\n\t\t\"\"\"\n\t\t*out_style* (:class:`str`) is the parameter style that the SQL query\n\t\twill be converted to.\n\t\t\"\"\"\n\t\treturn self._out_style\n",
            "file_path": "sqlparams/__init__.py",
            "human_label": "Convert sql using self._converter.convert",
            "level": "file_runnable",
            "lineno": "328",
            "name": "format",
            "oracle_context": "{ \"apis\" : \"['decode', 'convert', 'isinstance', 'encode']\", \"classes\" : \"['TypeError', 'Sequence', 'Tuple', 'Dict', 'Union', 'AnyStr', 'Any']\", \"vars\" : \"['Str', 'format', '_BYTES_ENCODING', '_converter']\" }",
            "package": "__init__",
            "project": "cpburnz/python-sql-parameters",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b43428903eeb48555d3eaa",
            "all_context": "{ \"import\" : \"typing re  typing \", \"file\" : \"\", \"class\" : \"self._create_converter(self) ; self.format(self,sql,params) ; self.__class__ ; self.formatmany(self,sql,many_params) ; self._converter ; self._in_regex ; self._create_in_regex(self) ; self.expand_tuples(self) ; self.out_style(self) ; self.__repr__(self) ; self._expand_tuples ; self.__init__(self,in_style,out_style,escape_char,expand_tuples) ; self._in_obj ; self._out_obj ; self._create_in_regex ; self._in_style ; self._escape_char ; self._out_style ; self.escape_char(self) ; self.in_style(self) ; self._create_converter ; \" }",
            "code": "\tdef formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\t\"\"\"\n\t\tConvert the SQL query to use the out-style parameters instead of the\n\t\tin-style parameters.\n\n\t\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n\t\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\t\tof in-style parameters (*params*).\n\n\t\t-\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t\t\tcontains the set of in-style parameters. It maps each parameter\n\t\t\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t\t\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t\t\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t\t\t*params* must be a :class:`~collections.abc.Sequence`.\n\n\t\tReturns a :class:`tuple` containing:\n\n\t\t-\tThe formatted SQL query (:class:`str` or :class:`bytes`).\n\n\t\t-\tA :class:`list` containing each set of converted out-style\n\t\t\tparameters (:class:`dict` or :class:`list`).\n\t\t\"\"\"\n\t\t# Normalize query encoding to simplify processing.\n\t\tif isinstance(sql, str):\n\t\t\tuse_sql = sql\n\t\t\tstring_type = str\n\t\telif isinstance(sql, bytes):\n\t\t\tuse_sql = sql.decode(_BYTES_ENCODING)\n\t\t\tstring_type = bytes\n\t\telse:\n\t\t\traise TypeError(\"sql:{!r} is not a unicode or byte string.\".format(sql))\n\n\t\tif not _is_iterable(many_params):\n\t\t\traise TypeError(\"many_params:{!r} is not iterable.\".format(many_params))\n\n\t\t# Replace in-style with out-style parameters.\n\t\tuse_sql, many_out_params = self._converter.convert_many(use_sql, many_params)\n\n\t\t# Make sure the query is returned as the proper string type.\n\t\tif string_type is bytes:\n\t\t\tout_sql = use_sql.encode(_BYTES_ENCODING)\n\t\telse:\n\t\t\tout_sql = use_sql\n\n\t\t# Return converted SQL and out-parameters.\n\t\treturn out_sql, many_out_params\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : true, \"public_lib\" : true, \"current_class\" : true, \"current_file\" : true, \"current_project\" : true, \"external\" : false }",
            "docstring": "Convert the SQL query to use the out-style parameters instead of the\nin-style parameters.\n\n*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n*many_params* (:class:`~collections.abc.Iterable`) contains each set\nof in-style parameters (*params*).\n\n-       *params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n        contains the set of in-style parameters. It maps each parameter\n        (:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n        is a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n        If :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n        *params* must be a :class:`~collections.abc.Sequence`.\n\nReturns a :class:`tuple` containing:\n\n-       The formatted SQL query (:class:`str` or :class:`bytes`).\n\n-       A :class:`list` containing each set of converted out-style\n        parameters (:class:`dict` or :class:`list`).",
            "end_lineno": "426",
            "file_content": "\"\"\"\n:mod:`sqlparams` is a utility package for converting between various SQL\nparameter styles.\n\"\"\"\n\nimport re\nfrom typing import (\n\tAny,\n\tAnyStr,\n\tDict,\n\tIterable,\n\tList,\n\tOptional,\n\tPattern,\n\tSequence,\n\tTuple,\n\tType,\n\tUnion)\n\nfrom . import _converting\nfrom . import _styles\nfrom ._util import _is_iterable\n\nfrom ._meta import (\n\t__author__,\n\t__copyright__,\n\t__credits__,\n\t__license__,\n\t__version__,\n)\n\n_BYTES_ENCODING = 'latin1'\n\"\"\"\nThe encoding to use when parsing a byte query string.\n\"\"\"\n\n_STYLES = {}\n\"\"\"\nMaps parameter style by name.\n\"\"\"\n\n\nclass SQLParams(object):\n\t\"\"\"\n\tThe :class:`.SQLParams` class is used to support named parameters in\n\tSQL queries where they are not otherwise supported (e.g., pyodbc).\n\tThis is done by converting from one parameter style query to another\n\tparameter style query.\n\n\tBy default, when converting to a numeric or ordinal style any\n\t:class:`tuple` parameter will be expanded into \"(?,?,...)\" to support\n\tthe widely used \"IN {tuple}\" SQL expression without leaking any\n\tunescaped values.\n\t\"\"\"\n\n\tdef __init__(\n\t\tself,\n\t\tin_style: str,\n\t\tout_style: str,\n\t\tescape_char: Union[str, bool, None] = None,\n\t\texpand_tuples: Optional[bool] = None,\n\t) -> None:\n\t\t\"\"\"\n\t\tInstantiates the :class:`.SQLParams` instance.\n\n\t\t*in_style* (:class:`str`) is the parameter style that will be used\n\t\tin an SQL query before being parsed and converted to :attr:`.SQLParams.out_style`.\n\n\t\t*out_style* (:class:`str`) is the parameter style that the SQL query\n\t\twill be converted to.\n\n\t\t*escape_char* (:class:`str`, :class:`bool`, or :data:`None`) is the\n\t\tescape character used to prevent matching a in-style parameter. If\n\t\t:data:`True`, use the default escape character (repeat the initial\n\t\tcharacter to escape it; e.g., \"%%\"). If :data:`False`, do not use an\n\t\tescape character. Default is :data:`None` for :data:`False`.\n\n\t\t*expand_tuples* (:class:`bool` or :data:`None`) is whether to\n\t\texpand tuples into a sequence of parameters. Default is :data:`None`\n\t\tto let it be determined by *out_style* (to maintain backward\n\t\tcompatibility). If *out_style* is a numeric or ordinal style, expand\n\t\ttuples by default (:data:`True`). If *out_style* is a named style,\n\t\tdo not expand tuples by default (:data:`False`).\n\n\t\tThe following parameter styles are supported by both *in_style* and\n\t\t*out_style*:\n\n\t\t-\tFor all named styles the parameter keys must be valid `Python identifiers`_.\n\t\t\tThey cannot start with a digit. This is to help prevent\n\t\t\tincorrectly matching common strings such as datetimes.\n\n\t\t\tNamed styles:\n\n\t\t\t-\t\"named\" indicates parameters will use the named style::\n\n\t\t\t\t\t... WHERE name = :name\n\n\t\t\t-\t\"named_dollar\" indicates parameters will use the named dollar\n\t\t\t\tsign style::\n\n\t\t\t\t\t... WHERE name = $name\n\n\t\t\t\t.. NOTE:: This is not defined by `PEP 249`_.\n\n\t\t\t-\t\"pyformat\" indicates parameters will use the named Python\n\t\t\t\textended format style::\n\n\t\t\t\t\t... WHERE name = %(name)s\n\n\t\t\t\t.. NOTE:: Strictly speaking, `PEP 249`_ only specifies\n\t\t\t\t   \"%(name)s\" for the \"pyformat\" parameter style so only that\n\t\t\t\t   form (without any other conversions or flags) is supported.\n\n\t\t-\tAll numeric styles start at :data:`1`. When using a\n\t\t\t:class:`~collections.abc.Sequence` for the parameters, the 1st\n\t\t\tparameter (e.g., \":1\") will correspond to the 1st element of the\n\t\t\tsequence (i.e., index :data:`0`). When using a :class:`~collections.abc.Mapping`\n\t\t\tfor the parameters, the 1st parameter (e.g., \":1\") will correspond\n\t\t\tto the matching key (i.e., :data:`1` or :data:`\"1\"`).\n\n\t\t\tNumeric styles:\n\n\t\t\t-\t\"numeric\" indicates parameters will use the numeric style::\n\n\t\t\t\t\t... WHERE name = :1\n\n\t\t\t-\t\"numeric_dollar\" indicates parameters will use the numeric\n\t\t\t\tdollar sign style (starts at :data:`1`)::\n\n\t\t\t\t\t... WHERE name = $1\n\n\t\t\t\t.. NOTE:: This is not defined by `PEP 249`_.\n\n\t\t- Ordinal styles:\n\n\t\t\t-\t\"format\" indicates parameters will use the ordinal Python format\n\t\t\t\tstyle::\n\n\t\t\t\t\t... WHERE name = %s\n\n\t\t\t\t.. NOTE:: Strictly speaking, `PEP 249`_ only specifies \"%s\" for\n\t\t\t\t   the \"format\" parameter styles so only that form (without any\n\t\t\t\t   other conversions or flags) is supported.\n\n\t\t\t-\t\"qmark\" indicates parameters will use the ordinal question mark\n\t\t\t\tstyle::\n\n\t\t\t\t\t... WHERE name = ?\n\n\t\t.. _`PEP 249`: http://www.python.org/dev/peps/pep-0249/\n\n\t\t.. _`Python identifiers`: https://docs.python.org/3/reference/lexical_analysis.html#identifiers\n\t\t\"\"\"\n\n\t\tself._converter: _converting._Converter = None\n\t\t\"\"\"\n\t\t*_converter* (:class:`._converting._Converter`) is the parameter\n\t\tconverter to use.\n\t\t\"\"\"\n\n\t\tself._escape_char: Optional[str] = None\n\t\t\"\"\"\n\t\t*_escape_char* (:class:`str` or :data:`None`) is the escape\n\t\tcharacter used to prevent matching a in-style parameter.\n\t\t\"\"\"\n\n\t\tself._expand_tuples: bool = None\n\t\t\"\"\"\n\t\t*_expand_tuples* (:class:`bool`) is whether to convert tuples into a\n\t\tsequence of parameters.\n\t\t\"\"\"\n\n\t\tself._in_obj: _styles._Style = None\n\t\t\"\"\"\n\t\t*_in_obj* (:class:`._styles._Style`) is the in-style parameter object.\n\t\t\"\"\"\n\n\t\tself._in_regex: Pattern = None\n\t\t\"\"\"\n\t\t*_in_regex* (:class:`re.Pattern`) is the regular expression used to\n\t\textract the in-style parameters.\n\t\t\"\"\"\n\n\t\tself._in_style: str = None\n\t\t\"\"\"\n\t\t*_in_style* (:class:`str`) is the parameter style that will be used\n\t\tin an SQL query before being parsed and converted to :attr:`.SQLParams.out_style`.\n\t\t\"\"\"\n\n\t\tself._out_obj: _styles._Style = None\n\t\t\"\"\"\n\t\t*_out_obj* (:class:`._styles._Style`) is the out-style parameter object.\n\t\t\"\"\"\n\n\t\tself._out_style: str = None\n\t\t\"\"\"\n\t\t*_out_style* (:class:`str`) is the parameter style that the SQL query\n\t\twill be converted to.\n\t\t\"\"\"\n\n\t\tif not isinstance(in_style, str):\n\t\t\traise TypeError(\"in_style:{!r} is not a string.\".format(in_style))\n\n\t\tif not isinstance(out_style, str):\n\t\t\traise TypeError(\"out_style:{!r} is not a string.\".format(out_style))\n\n\t\tself._in_style = in_style\n\t\tself._out_style = out_style\n\n\t\tself._in_obj = _styles._STYLES[self._in_style]\n\t\tself._out_obj = _styles._STYLES[self._out_style]\n\n\t\tif escape_char is True:\n\t\t\tuse_char = self._in_obj.escape_char\n\t\telif not escape_char:\n\t\t\tuse_char = None\n\t\telif isinstance(escape_char, str):\n\t\t\tuse_char = escape_char\n\t\telse:\n\t\t\traise TypeError(\"escape_char:{!r} is not a string or bool.\")\n\n\t\tif expand_tuples is None:\n\t\t\texpand_tuples = not isinstance(self._out_obj, _styles._NamedStyle)\n\n\t\tself._escape_char = use_char\n\t\tself._expand_tuples = bool(expand_tuples)\n\n\t\tself._in_regex = self._create_in_regex()\n\t\tself._converter = self._create_converter()\n\n\tdef __repr__(self) -> str:\n\t\t\"\"\"\n\t\tReturns the canonical string representation (:class:`str`) of this\n\t\tinstance.\n\t\t\"\"\"\n\t\treturn \"{}.{}({!r}, {!r})\".format(self.__class__.__module__, self.__class__.__name__, self._in_style, self._out_style)\n\n\tdef _create_converter(self) -> _converting._Converter:\n\t\t\"\"\"\n\t\tCreate the parameter style converter.\n\n\t\tReturns the parameter style converter (:class:`._converting._Converter`).\n\t\t\"\"\"\n\t\tassert self._in_regex is not None, self._in_regex\n\t\tassert self._out_obj is not None, self._out_obj\n\n\t\t# Determine converter class.\n\t\tconverter_class: Type[_converting._Converter]\n\t\tif isinstance(self._in_obj, _styles._NamedStyle):\n\t\t\tif isinstance(self._out_obj, _styles._NamedStyle):\n\t\t\t\tconverter_class = _converting._NamedToNamedConverter\n\t\t\telif isinstance(self._out_obj, _styles._NumericStyle):\n\t\t\t\tconverter_class = _converting._NamedToNumericConverter\n\t\t\telif isinstance(self._out_obj, _styles._OrdinalStyle):\n\t\t\t\tconverter_class = _converting._NamedToOrdinalConverter\n\t\t\telse:\n\t\t\t\traise TypeError(\"out_style:{!r} maps to an unexpected type: {!r}\".format(self._out_style, self._out_obj))\n\n\t\telif isinstance(self._in_obj, _styles._NumericStyle):\n\t\t\tif isinstance(self._out_obj, _styles._NamedStyle):\n\t\t\t\tconverter_class = _converting._NumericToNamedConverter\n\t\t\telif isinstance(self._out_obj, _styles._NumericStyle):\n\t\t\t\tconverter_class = _converting._NumericToNumericConverter\n\t\t\telif isinstance(self._out_obj, _styles._OrdinalStyle):\n\t\t\t\tconverter_class = _converting._NumericToOrdinalConverter\n\t\t\telse:\n\t\t\t\traise TypeError(\"out_style:{!r} maps to an unexpected type: {!r}\".format(self._out_style, self._out_obj))\n\n\t\telif isinstance(self._in_obj, _styles._OrdinalStyle):\n\t\t\tif isinstance(self._out_obj, _styles._NamedStyle):\n\t\t\t\tconverter_class = _converting._OrdinalToNamedConverter\n\t\t\telif isinstance(self._out_obj, _styles._NumericStyle):\n\t\t\t\tconverter_class = _converting._OrdinalToNumericConverter\n\t\t\telif isinstance(self._out_obj, _styles._OrdinalStyle):\n\t\t\t\tconverter_class = _converting._OrdinalToOrdinalConverter\n\t\t\telse:\n\t\t\t\traise TypeError(\"out_style:{!r} maps to an unexpected type: {!r}\".format(self._out_style, self._out_obj))\n\n\t\telse:\n\t\t\traise TypeError(\"in_style:{!r} maps to an unexpected type: {!r}\".format(self._in_style, self._in_obj))\n\n\t\t# Create converter.\n\t\tconverter = converter_class(\n\t\t\tescape_char=self._escape_char,\n\t\t\texpand_tuples=self._expand_tuples,\n\t\t\tin_regex=self._in_regex,\n\t\t\tin_style=self._in_obj,\n\t\t\tout_style=self._out_obj,\n\t\t)\n\t\treturn converter\n\n\tdef _create_in_regex(self) -> Pattern:\n\t\t\"\"\"\n\t\tCreate the in-style parameter regular expression.\n\n\t\tReturns the in-style parameter regular expression (:class:`re.Pattern`).\n\t\t\"\"\"\n\t\tregex_parts = []\n\n\t\tif self._in_obj.escape_char != \"%\" and self._out_obj.escape_char == \"%\":\n\t\t\tregex_parts.append(\"(?P<out_percent>%)\")\n\n\t\tif self._escape_char:\n\t\t\t# Escaping is enabled.\n\t\t\tescape = self._in_obj.escape_regex.format(char=re.escape(self._escape_char))\n\t\t\tregex_parts.append(escape)\n\n\t\tregex_parts.append(self._in_obj.param_regex)\n\n\t\treturn re.compile(\"|\".join(regex_parts))\n\n\t@property\n\tdef escape_char(self) -> Optional[str]:\n\t\t\"\"\"\n\t\t*escape_char* (:class:`str` or :data:`None`) is the escape character\n\t\tused to prevent matching a in-style parameter.\n\t\t\"\"\"\n\t\treturn self._escape_char\n\n\t@property\n\tdef expand_tuples(self) -> bool:\n\t\t\"\"\"\n\t\t*expand_tuples* (:class:`bool`) is whether to convert tuples into a\n\t\tsequence of parameters.\n\t\t\"\"\"\n\t\treturn self._expand_tuples\n\n\tdef format(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tparams: Union[Dict[Union[str, int], Any], Sequence[Any]],\n\t) -> Tuple[AnyStr, Union[Dict[Union[str, int], Any], Sequence[Any]]]:\n\t\t\"\"\"\n\t\tConvert the SQL query to use the out-style parameters instead of\n\t\tthe in-style parameters.\n\n\t\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n\t\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t\tcontains the set of in-style parameters. It maps each parameter\n\t\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style, then\n\t\t*params* must be a :class:`~collections.abc.Sequence`.\n\n\t\tReturns a :class:`tuple` containing:\n\n\t\t-\tThe formatted SQL query (:class:`str` or :class:`bytes`).\n\n\t\t-\tThe set of converted out-style parameters (:class:`dict` or\n\t\t\t:class:`list`).\n\t\t\"\"\"\n\t\t# Normalize query encoding to simplify processing.\n\t\tif isinstance(sql, str):\n\t\t\tuse_sql = sql\n\t\t\tstring_type = str\n\t\telif isinstance(sql, bytes):\n\t\t\tuse_sql = sql.decode(_BYTES_ENCODING)\n\t\t\tstring_type = bytes\n\t\telse:\n\t\t\traise TypeError(\"sql:{!r} is not a unicode or byte string.\".format(sql))\n\n\t\t# Replace in-style with out-style parameters.\n\t\tuse_sql, out_params = self._converter.convert(use_sql, params)\n\n\t\t# Make sure the query is returned as the proper string type.\n\t\tif string_type is bytes:\n\t\t\tout_sql = use_sql.encode(_BYTES_ENCODING)\n\t\telse:\n\t\t\tout_sql = use_sql\n\n\t\t# Return converted SQL and out-parameters.\n\t\treturn out_sql, out_params\n\n\tdef formatmany(\n\t\tself,\n\t\tsql: AnyStr,\n\t\tmany_params: Union[Iterable[Dict[Union[str, int], Any]], Iterable[Sequence[Any]]],\n\t) -> Tuple[AnyStr, Union[List[Dict[Union[str, int], Any]], List[Sequence[Any]]]]:\n\t\t\"\"\"\n\t\tConvert the SQL query to use the out-style parameters instead of the\n\t\tin-style parameters.\n\n\t\t*sql* (:class:`str` or :class:`bytes`) is the SQL query.\n\n\t\t*many_params* (:class:`~collections.abc.Iterable`) contains each set\n\t\tof in-style parameters (*params*).\n\n\t\t-\t*params* (:class:`~collections.abc.Mapping` or :class:`~collections.abc.Sequence`)\n\t\t\tcontains the set of in-style parameters. It maps each parameter\n\t\t\t(:class:`str` or :class:`int`) to value. If :attr:`.SQLParams.in_style`\n\t\t\tis a named parameter style. then *params* must be a :class:`~collections.abc.Mapping`.\n\t\t\tIf :attr:`.SQLParams.in_style` is an ordinal parameter style. then\n\t\t\t*params* must be a :class:`~collections.abc.Sequence`.\n\n\t\tReturns a :class:`tuple` containing:\n\n\t\t-\tThe formatted SQL query (:class:`str` or :class:`bytes`).\n\n\t\t-\tA :class:`list` containing each set of converted out-style\n\t\t\tparameters (:class:`dict` or :class:`list`).\n\t\t\"\"\"\n\t\t# Normalize query encoding to simplify processing.\n\t\tif isinstance(sql, str):\n\t\t\tuse_sql = sql\n\t\t\tstring_type = str\n\t\telif isinstance(sql, bytes):\n\t\t\tuse_sql = sql.decode(_BYTES_ENCODING)\n\t\t\tstring_type = bytes\n\t\telse:\n\t\t\traise TypeError(\"sql:{!r} is not a unicode or byte string.\".format(sql))\n\n\t\tif not _is_iterable(many_params):\n\t\t\traise TypeError(\"many_params:{!r} is not iterable.\".format(many_params))\n\n\t\t# Replace in-style with out-style parameters.\n\t\tuse_sql, many_out_params = self._converter.convert_many(use_sql, many_params)\n\n\t\t# Make sure the query is returned as the proper string type.\n\t\tif string_type is bytes:\n\t\t\tout_sql = use_sql.encode(_BYTES_ENCODING)\n\t\telse:\n\t\t\tout_sql = use_sql\n\n\t\t# Return converted SQL and out-parameters.\n\t\treturn out_sql, many_out_params\n\n\t@property\n\tdef in_style(self) -> str:\n\t\t\"\"\"\n\t\t*in_style* (:class:`str`) is the parameter style to expect in an SQL\n\t\tquery when being parsed.\n\t\t\"\"\"\n\t\treturn self._in_style\n\n\t@property\n\tdef out_style(self) -> str:\n\t\t\"\"\"\n\t\t*out_style* (:class:`str`) is the parameter style that the SQL query\n\t\twill be converted to.\n\t\t\"\"\"\n\t\treturn self._out_style\n",
            "file_path": "sqlparams/__init__.py",
            "human_label": "Convert sql using self._converter.convert_many",
            "level": "project_runnable",
            "lineno": "375",
            "name": "formatmany",
            "oracle_context": "{ \"apis\" : \"['decode', 'isinstance', 'format', 'convert_many', '_is_iterable', 'encode']\", \"classes\" : \"['TypeError', 'List', 'Sequence', '_is_iterable', 'Tuple', 'Dict', 'Union', 'AnyStr', 'Iterable', 'Any']\", \"vars\" : \"['Str', '_BYTES_ENCODING', '_converter']\" }",
            "package": "__init__",
            "project": "cpburnz/python-sql-parameters",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b45df05108cfac7f2109ce",
            "all_context": "{ \"import\" : \"json re namaste pyfs fs digest \", \"file\" : \"\", \"class\" : \"self.validate_inventory_digest_match(self,inv_file,inv_digest_file) ; self.status_str ; self.validate_inventory(self,inv_file,where,extract_spec_version) ; self.registered_extensions ; self.read_inventory_digest ; self.obj_fs ; self.root_inv_validator ; self.validate_inventory_digest ; self.initialize(self) ; self.check_additional_digests(self,filepath,known_digests,additional_digests,error_code) ; self.validate_object_root(self,version_dirs,already_checked) ; self.validate_extensions_dir(self) ; self.inventory_digest_files ; self.validate_inventory ; self.check_additional_digests ; self.content_directory ; self.validate_version_inventories ; self.validate_extensions_dir ; self.status_str(self,prefix) ; self.validate_content(self,inventory,version_dirs,prior_manifest_digests,prior_fixity_digests) ; self.id ; self.__str__(self) ; self.lax_digests ; self.read_inventory_digest(self,inv_digest_file) ; self.validate_inventory_digest(self,inv_file,digest_algorithm,where) ; self.log ; self.digest_algorithm ; self.validate_content ; self.validate_inventory_digest_match ; self.spec_version ; self.check_digests ; self.validate_object_root ; self.initialize ; self.validate(self,path) ; self.__init__(self,log,show_warnings,show_errors,check_digests,lax_digests,lang) ; self.validate_version_inventories(self,version_dirs) ; \" }",
            "code": "    def validate(self, path):\n        \"\"\"Validate OCFL object at path or pyfs root.\n\n        Returns True if valid (warnings permitted), False otherwise.\n        \"\"\"\n        self.initialize()\n        try:\n            if isinstance(path, str):\n                self.obj_fs = open_fs(path)\n            else:\n                self.obj_fs = path\n                path = self.obj_fs.desc('')\n        except fs.errors.CreateFailed:\n            self.log.error('E003e', path=path)\n            return False\n        # Object declaration, set spec version number. If there are multiple declarations,\n        # look for the lastest object version then report any others as errors\n        namastes = find_namastes(0, pyfs=self.obj_fs)\n        if len(namastes) == 0:\n            self.log.error('E003a', assumed_version=self.spec_version)\n        else:\n            spec_version = None\n            for namaste in namastes:\n                # Extract and check spec version number\n                this_file_version = None\n                for version in ('1.1', '1.0'):\n                    if namaste.filename == '0=ocfl_object_' + version:\n                        this_file_version = version\n                        break\n                if this_file_version is None:\n                    self.log.error('E006', filename=namaste.filename)\n                elif spec_version is None or this_file_version > spec_version:\n                    spec_version = this_file_version\n                    if not namaste.content_ok(pyfs=self.obj_fs):\n                        self.log.error('E007', filename=namaste.filename)\n            if spec_version is None:\n                self.log.error('E003c', assumed_version=self.spec_version)\n            else:\n                self.spec_version = spec_version\n                if len(namastes) > 1:\n                    self.log.error('E003b', files=len(namastes), using_version=self.spec_version)\n        # Object root inventory file\n        inv_file = 'inventory.json'\n        if not self.obj_fs.exists(inv_file):\n            self.log.error('E063')\n            return False\n        try:\n            inventory, inv_validator = self.validate_inventory(inv_file)\n            inventory_is_valid = self.log.num_errors == 0\n            self.root_inv_validator = inv_validator\n            all_versions = inv_validator.all_versions\n            self.id = inv_validator.id\n            self.content_directory = inv_validator.content_directory\n            self.digest_algorithm = inv_validator.digest_algorithm\n            self.validate_inventory_digest(inv_file, self.digest_algorithm)\n            # Object root\n            self.validate_object_root(all_versions, already_checked=[namaste.filename for namaste in namastes])\n            # Version inventory files\n            (prior_manifest_digests, prior_fixity_digests) = self.validate_version_inventories(all_versions)\n            if inventory_is_valid:\n                # Object content\n                self.validate_content(inventory, all_versions, prior_manifest_digests, prior_fixity_digests)\n        except ValidatorAbortException:\n            pass\n        return self.log.num_errors == 0\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : false, \"public_lib\" : true, \"current_class\" : true, \"current_file\" : false, \"current_project\" : false, \"external\" : false }",
            "docstring": "Validate OCFL object at path or pyfs root.\n\nReturns True if valid (warnings permitted), False otherwise.",
            "end_lineno": "137",
            "file_content": "\"\"\"OCFL Validator.\n\nPhilosophy of this code is to keep it separate from the implementations\nof Store, Object and Version used to build and manipulate OCFL data, but\nto leverage lower level functions such as digest creation etc.. Code style\nis plain/verbose with detailed and specific validation errors that might\nhelp someone debug an implementation.\n\nThis code uses PyFilesystem (import fs) exclusively for access to files. This\nshould enable application beyond the operating system filesystem.\n\"\"\"\nimport json\nimport re\nimport fs\n\nfrom .digest import file_digest, normalized_digest\nfrom .inventory_validator import InventoryValidator\nfrom .namaste import find_namastes\nfrom .pyfs import open_fs, ocfl_walk, ocfl_files_identical\nfrom .validation_logger import ValidationLogger\n\n\nclass ValidatorAbortException(Exception):\n    \"\"\"Exception class to bail out of validation.\"\"\"\n\n\nclass Validator():\n    \"\"\"Class for OCFL Validator.\"\"\"\n\n    def __init__(self, log=None, show_warnings=False, show_errors=True, check_digests=True, lax_digests=False, lang='en'):\n        \"\"\"Initialize OCFL validator.\"\"\"\n        self.log = log\n        self.check_digests = check_digests\n        self.lax_digests = lax_digests\n        if self.log is None:\n            self.log = ValidationLogger(show_warnings=show_warnings, show_errors=show_errors, lang=lang)\n        self.registered_extensions = [\n            '0001-digest-algorithms', '0002-flat-direct-storage-layout',\n            '0003-hash-and-id-n-tuple-storage-layout', '0004-hashed-n-tuple-storage-layout',\n            '0005-mutable-head'\n        ]\n        # The following actually initialized in initialize() method\n        self.id = None\n        self.spec_version = None\n        self.digest_algorithm = None\n        self.content_directory = None\n        self.inventory_digest_files = None\n        self.root_inv_validator = None\n        self.obj_fs = None\n        self.initialize()\n\n    def initialize(self):\n        \"\"\"Initialize object state.\n\n        Must be called between attempts to validate objects.\n        \"\"\"\n        self.id = None\n        self.spec_version = '1.0'  # default to latest published version\n        self.digest_algorithm = 'sha512'\n        self.content_directory = 'content'\n        self.inventory_digest_files = {}  # index by version_dir, algorithms may differ\n        self.root_inv_validator = None\n        self.obj_fs = None\n\n    def status_str(self, prefix=''):\n        \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n        return self.log.status_str(prefix=prefix)\n\n    def __str__(self):\n        \"\"\"Return string representation of validation log.\"\"\"\n        return self.status_str()\n\n    def validate(self, path):\n        \"\"\"Validate OCFL object at path or pyfs root.\n\n        Returns True if valid (warnings permitted), False otherwise.\n        \"\"\"\n        self.initialize()\n        try:\n            if isinstance(path, str):\n                self.obj_fs = open_fs(path)\n            else:\n                self.obj_fs = path\n                path = self.obj_fs.desc('')\n        except fs.errors.CreateFailed:\n            self.log.error('E003e', path=path)\n            return False\n        # Object declaration, set spec version number. If there are multiple declarations,\n        # look for the lastest object version then report any others as errors\n        namastes = find_namastes(0, pyfs=self.obj_fs)\n        if len(namastes) == 0:\n            self.log.error('E003a', assumed_version=self.spec_version)\n        else:\n            spec_version = None\n            for namaste in namastes:\n                # Extract and check spec version number\n                this_file_version = None\n                for version in ('1.1', '1.0'):\n                    if namaste.filename == '0=ocfl_object_' + version:\n                        this_file_version = version\n                        break\n                if this_file_version is None:\n                    self.log.error('E006', filename=namaste.filename)\n                elif spec_version is None or this_file_version > spec_version:\n                    spec_version = this_file_version\n                    if not namaste.content_ok(pyfs=self.obj_fs):\n                        self.log.error('E007', filename=namaste.filename)\n            if spec_version is None:\n                self.log.error('E003c', assumed_version=self.spec_version)\n            else:\n                self.spec_version = spec_version\n                if len(namastes) > 1:\n                    self.log.error('E003b', files=len(namastes), using_version=self.spec_version)\n        # Object root inventory file\n        inv_file = 'inventory.json'\n        if not self.obj_fs.exists(inv_file):\n            self.log.error('E063')\n            return False\n        try:\n            inventory, inv_validator = self.validate_inventory(inv_file)\n            inventory_is_valid = self.log.num_errors == 0\n            self.root_inv_validator = inv_validator\n            all_versions = inv_validator.all_versions\n            self.id = inv_validator.id\n            self.content_directory = inv_validator.content_directory\n            self.digest_algorithm = inv_validator.digest_algorithm\n            self.validate_inventory_digest(inv_file, self.digest_algorithm)\n            # Object root\n            self.validate_object_root(all_versions, already_checked=[namaste.filename for namaste in namastes])\n            # Version inventory files\n            (prior_manifest_digests, prior_fixity_digests) = self.validate_version_inventories(all_versions)\n            if inventory_is_valid:\n                # Object content\n                self.validate_content(inventory, all_versions, prior_manifest_digests, prior_fixity_digests)\n        except ValidatorAbortException:\n            pass\n        return self.log.num_errors == 0\n\n    def validate_inventory(self, inv_file, where='root', extract_spec_version=False):\n        \"\"\"Validate a given inventory file, record errors with self.log.error().\n\n        Returns inventory object for use in later validation\n        of object content. Does not look at anything else in the\n        object itself.\n\n        where - used for reporting messages of where inventory is in object\n\n        extract_spec_version - if set True will attempt to take spec_version from the\n            inventory itself instead of using the spec_version provided\n        \"\"\"\n        try:\n            with self.obj_fs.openbin(inv_file, 'r') as fh:\n                inventory = json.load(fh)\n        except json.decoder.JSONDecodeError as e:\n            self.log.error('E033', where=where, explanation=str(e))\n            raise ValidatorAbortException\n        inv_validator = InventoryValidator(log=self.log, where=where,\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return inventory, inv_validator\n\n    def validate_inventory_digest(self, inv_file, digest_algorithm, where=\"root\"):\n        \"\"\"Validate the appropriate inventory digest file in path.\"\"\"\n        inv_digest_file = inv_file + '.' + digest_algorithm\n        if not self.obj_fs.exists(inv_digest_file):\n            self.log.error('E058a', where=where, path=inv_digest_file)\n        else:\n            self.validate_inventory_digest_match(inv_file, inv_digest_file)\n\n    def validate_inventory_digest_match(self, inv_file, inv_digest_file):\n        \"\"\"Validate a given inventory digest for a given inventory file.\n\n        On error throws exception with debugging string intended to\n        be presented to a user.\n        \"\"\"\n        if not self.check_digests:\n            return\n        m = re.match(r'''.*\\.(\\w+)$''', inv_digest_file)\n        if m:\n            digest_algorithm = m.group(1)\n            try:\n                digest_recorded = self.read_inventory_digest(inv_digest_file)\n                digest_actual = file_digest(inv_file, digest_algorithm, pyfs=self.obj_fs)\n                if digest_actual != digest_recorded:\n                    self.log.error(\"E060\", inv_file=inv_file, actual=digest_actual, recorded=digest_recorded, inv_digest_file=inv_digest_file)\n            except Exception as e:  # pylint: disable=broad-except\n                self.log.error(\"E061\", description=str(e))\n        else:\n            self.log.error(\"E058b\", inv_digest_file=inv_digest_file)\n\n    def validate_object_root(self, version_dirs, already_checked):\n        \"\"\"Validate object root.\n\n        All expected_files must be present and no other files.\n        All expected_dirs must be present and no other dirs.\n        \"\"\"\n        expected_files = ['0=ocfl_object_' + self.spec_version, 'inventory.json',\n                          'inventory.json.' + self.digest_algorithm]\n        for entry in self.obj_fs.scandir(''):\n            if entry.is_file:\n                if entry.name not in expected_files and entry.name not in already_checked:\n                    self.log.error('E001a', file=entry.name)\n            elif entry.is_dir:\n                if entry.name in version_dirs:\n                    pass\n                elif entry.name == 'extensions':\n                    self.validate_extensions_dir()\n                elif re.match(r'''v\\d+$''', entry.name):\n                    # Looks like a version directory so give more specific error\n                    self.log.error('E046b', dir=entry.name)\n                else:\n                    # Simply an unexpected directory\n                    self.log.error('E001b', dir=entry.name)\n            else:\n                self.log.error('E001c', entry=entry.name)\n\n    def validate_extensions_dir(self):\n        \"\"\"Validate content of extensions directory inside object root.\n\n        Validate the extensions directory by checking that there aren't any\n        entries in the extensions directory that aren't directories themselves.\n        Where there are extension directories they SHOULD be registered and\n        this code relies up the registered_extensions property to list known\n        extensions.\n        \"\"\"\n        for entry in self.obj_fs.scandir('extensions'):\n            if entry.is_dir:\n                if entry.name not in self.registered_extensions:\n                    self.log.warning('W013', entry=entry.name)\n            else:\n                self.log.error('E067', entry=entry.name)\n\n    def validate_version_inventories(self, version_dirs):\n        \"\"\"Each version SHOULD have an inventory up to that point.\n\n        Also keep a record of any content digests different from those in the root inventory\n        so that we can also check them when validating the content.\n\n        version_dirs is an array of version directory names and is assumed to be in\n        version sequence (1, 2, 3...).\n        \"\"\"\n        prior_manifest_digests = {}  # file -> algorithm -> digest -> [versions]\n        prior_fixity_digests = {}  # file -> algorithm -> digest -> [versions]\n        if len(version_dirs) == 0:\n            return prior_manifest_digests, prior_fixity_digests\n        last_version = version_dirs[-1]\n        prev_version_dir = \"NONE\"  # will be set for first directory with inventory\n        prev_spec_version = '1.0'  # lowest version\n        for version_dir in version_dirs:\n            inv_file = fs.path.join(version_dir, 'inventory.json')\n            if not self.obj_fs.exists(inv_file):\n                self.log.warning('W010', where=version_dir)\n                continue\n            # There is an inventory file for this version directory, check it\n            if version_dir == last_version:\n                # Don't validate in this case. Per the spec the inventory in the last version\n                # MUST be identical to the copy in the object root, just check that\n                root_inv_file = 'inventory.json'\n                if not ocfl_files_identical(self.obj_fs, inv_file, root_inv_file):\n                    self.log.error('E064', root_inv_file=root_inv_file, inv_file=inv_file)\n                else:\n                    # We could also just compare digest files but this gives a more helpful error for\n                    # which file has the incorrect digest if they don't match\n                    self.validate_inventory_digest(inv_file, self.digest_algorithm, where=version_dir)\n                self.inventory_digest_files[version_dir] = 'inventory.json.' + self.digest_algorithm\n                this_spec_version = self.spec_version\n            else:\n                # Note that inventories in prior versions may use different digest algorithms\n                # from the current invenotory. Also,\n                # an may accord with the same or earlier versions of the specification\n                version_inventory, inv_validator = self.validate_inventory(inv_file, where=version_dir, extract_spec_version=True)\n                this_spec_version = inv_validator.spec_version\n                digest_algorithm = inv_validator.digest_algorithm\n                self.validate_inventory_digest(inv_file, digest_algorithm, where=version_dir)\n                self.inventory_digest_files[version_dir] = 'inventory.json.' + digest_algorithm\n                if self.id and 'id' in version_inventory:\n                    if version_inventory['id'] != self.id:\n                        self.log.error('E037b', where=version_dir, root_id=self.id, version_id=version_inventory['id'])\n                if 'manifest' in version_inventory:\n                    # Check that all files listed in prior inventories are in manifest\n                    not_seen = set(prior_manifest_digests.keys())\n                    for digest in version_inventory['manifest']:\n                        for filepath in version_inventory['manifest'][digest]:\n                            # We rely on the validation to check that anything present is OK\n                            if filepath in not_seen:\n                                not_seen.remove(filepath)\n                    if len(not_seen) > 0:\n                        self.log.error('E023b', where=version_dir, missing_filepaths=', '.join(sorted(not_seen)))\n                    # Record all prior digests\n                    for unnormalized_digest in version_inventory['manifest']:\n                        digest = normalized_digest(unnormalized_digest, digest_type=digest_algorithm)\n                        for filepath in version_inventory['manifest'][unnormalized_digest]:\n                            if filepath not in prior_manifest_digests:\n                                prior_manifest_digests[filepath] = {}\n                            if digest_algorithm not in prior_manifest_digests[filepath]:\n                                prior_manifest_digests[filepath][digest_algorithm] = {}\n                            if digest not in prior_manifest_digests[filepath][digest_algorithm]:\n                                prior_manifest_digests[filepath][digest_algorithm][digest] = []\n                            prior_manifest_digests[filepath][digest_algorithm][digest].append(version_dir)\n                # Is this inventory an appropriate prior version of the object root inventory?\n                if self.root_inv_validator is not None:\n                    self.root_inv_validator.validate_as_prior_version(inv_validator)\n                # Fixity blocks are independent in each version. Record all values and the versions\n                # they occur in for later checks against content\n                if 'fixity' in version_inventory:\n                    for digest_algorithm in version_inventory['fixity']:\n                        for unnormalized_digest in version_inventory['fixity'][digest_algorithm]:\n                            digest = normalized_digest(unnormalized_digest, digest_type=digest_algorithm)\n                            for filepath in version_inventory['fixity'][digest_algorithm][unnormalized_digest]:\n                                if filepath not in prior_fixity_digests:\n                                    prior_fixity_digests[filepath] = {}\n                                if digest_algorithm not in prior_fixity_digests[filepath]:\n                                    prior_fixity_digests[filepath][digest_algorithm] = {}\n                                if digest not in prior_fixity_digests[filepath][digest_algorithm]:\n                                    prior_fixity_digests[filepath][digest_algorithm][digest] = []\n                                prior_fixity_digests[filepath][digest_algorithm][digest].append(version_dir)\n            # We are validating the inventories in sequence and each new version must\n            # follow the same or later spec version to previous inventories\n            if prev_spec_version > this_spec_version:\n                self.log.error('E103', where=version_dir, this_spec_version=this_spec_version,\n                               prev_version_dir=prev_version_dir, prev_spec_version=prev_spec_version)\n            prev_version_dir = version_dir\n            prev_spec_version = this_spec_version\n        return prior_manifest_digests, prior_fixity_digests\n\n    def validate_content(self, inventory, version_dirs, prior_manifest_digests, prior_fixity_digests):\n        \"\"\"Validate file presence and content against inventory.\n\n        The root inventory in `inventory` is assumed to be valid and safe to use\n        for construction of file paths etc..\n        \"\"\"\n        files_seen = set()\n        # Check files in each version directory\n        for version_dir in version_dirs:\n            try:\n                # Check contents of version directory except content_directory\n                for entry in self.obj_fs.listdir(version_dir):\n                    if ((entry == 'inventory.json')\n                            or (version_dir in self.inventory_digest_files and entry == self.inventory_digest_files[version_dir])):\n                        pass\n                    elif entry == self.content_directory:\n                        # Check content_directory\n                        content_path = fs.path.join(version_dir, self.content_directory)\n                        num_content_files_in_version = 0\n                        for dirpath, dirs, files in ocfl_walk(self.obj_fs, content_path):\n                            if dirpath != '/' + content_path and (len(dirs) + len(files)) == 0:\n                                self.log.error(\"E024\", where=version_dir, path=dirpath)\n                            for file in files:\n                                files_seen.add(fs.path.join(dirpath, file).lstrip('/'))\n                                num_content_files_in_version += 1\n                        if num_content_files_in_version == 0:\n                            self.log.warning(\"W003\", where=version_dir)\n                    elif self.obj_fs.isdir(fs.path.join(version_dir, entry)):\n                        self.log.warning(\"W002\", where=version_dir, entry=entry)\n                    else:\n                        self.log.error(\"E015\", where=version_dir, entry=entry)\n            except (fs.errors.ResourceNotFound, fs.errors.DirectoryExpected):\n                self.log.error('E046a', version_dir=version_dir)\n        # Extract any digests in fixity and organize by filepath\n        fixity_digests = {}\n        if 'fixity' in inventory:\n            for digest_algorithm in inventory['fixity']:\n                for digest in inventory['fixity'][digest_algorithm]:\n                    for filepath in inventory['fixity'][digest_algorithm][digest]:\n                        if filepath in files_seen:\n                            if filepath not in fixity_digests:\n                                fixity_digests[filepath] = {}\n                            if digest_algorithm not in fixity_digests[filepath]:\n                                fixity_digests[filepath][digest_algorithm] = {}\n                            if digest not in fixity_digests[filepath][digest_algorithm]:\n                                fixity_digests[filepath][digest_algorithm][digest] = ['root']\n                        else:\n                            self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, digest=digest, content_path=filepath)\n        # Check all files in root manifest\n        if 'manifest' in inventory:\n            for digest in inventory['manifest']:\n                for filepath in inventory['manifest'][digest]:\n                    if filepath not in files_seen:\n                        self.log.error('E092b', where='root', content_path=filepath)\n                    else:\n                        if self.check_digests:\n                            content_digest = file_digest(filepath, digest_type=self.digest_algorithm, pyfs=self.obj_fs)\n                            if content_digest != normalized_digest(digest, digest_type=self.digest_algorithm):\n                                self.log.error('E092a', where='root', digest_algorithm=self.digest_algorithm, digest=digest, content_path=filepath, content_digest=content_digest)\n                            known_digests = {self.digest_algorithm: content_digest}\n                            # Are there digest values in the fixity block?\n                            self.check_additional_digests(filepath, known_digests, fixity_digests, 'E093a')\n                            # Are there other digests for this same file from other inventories?\n                            self.check_additional_digests(filepath, known_digests, prior_manifest_digests, 'E092a')\n                            self.check_additional_digests(filepath, known_digests, prior_fixity_digests, 'E093a')\n                        files_seen.discard(filepath)\n        # Anything left in files_seen is not mentioned in the inventory\n        if len(files_seen) > 0:\n            self.log.error('E023a', where='root', extra_files=', '.join(sorted(files_seen)))\n\n    def check_additional_digests(self, filepath, known_digests, additional_digests, error_code):\n        \"\"\"Check all the additional digests for filepath.\n\n        This method is intended to be used both for manifest digests in prior versions and\n        for fixity digests. The digests_seen dict is used to store any values calculated\n        so that we don't recalculate digests that might appear multiple times. It is added to\n        with any additional values calculated.\n\n        Parameters:\n            filepath - path of file in object (`v1/content/something` etc.)\n            known_digests - dict of algorithm->digest that we have calculated\n            additional_digests - dict: filepath -> algorithm -> digest -> [versions appears in]\n            error_code - error code to log on mismatch (E092a for manifest, E093a for fixity)\n        \"\"\"\n        if filepath in additional_digests:\n            for digest_algorithm in additional_digests[filepath]:\n                if digest_algorithm in known_digests:\n                    # Don't recompute anything, just use it if we've seen it before\n                    content_digest = known_digests[digest_algorithm]\n                else:\n                    content_digest = file_digest(filepath, digest_type=digest_algorithm, pyfs=self.obj_fs)\n                    known_digests[digest_algorithm] = content_digest\n                for digest in additional_digests[filepath][digest_algorithm]:\n                    if content_digest != normalized_digest(digest, digest_type=digest_algorithm):\n                        where = ','.join(additional_digests[filepath][digest_algorithm][digest])\n                        self.log.error(error_code, where=where, digest_algorithm=digest_algorithm, digest=digest, content_path=filepath, content_digest=content_digest)\n\n    def read_inventory_digest(self, inv_digest_file):\n        \"\"\"Read inventory digest from sidecar file.\n\n        Raise exception if there is an error, else return digest.\n        \"\"\"\n        with self.obj_fs.open(inv_digest_file, 'r') as fh:\n            line = fh.readline()\n            # we ignore any following lines, could raise exception\n        m = re.match(r'''(\\w+)\\s+(\\S+)\\s*$''', line)\n        if not m:\n            raise Exception(\"Bad inventory digest file %s, wrong format\" % (inv_digest_file))\n        if m.group(2) != 'inventory.json':\n            raise Exception(\"Bad inventory name in inventory digest file %s\" % (inv_digest_file))\n        return m.group(1)\n",
            "file_path": "ocfl/validator.py",
            "human_label": "Returns True if OCFL object at path or pyfs root, False otherwise.",
            "level": "class_runnable",
            "lineno": "73",
            "name": "validate",
            "oracle_context": "{ \"apis\" : \"['open_fs', 'desc', 'isinstance', 'exists', 'validate_content', 'error', 'validate_inventory_digest', 'validate_version_inventories', 'content_ok', 'len', 'validate_inventory', 'find_namastes', 'validate_object_root', 'initialize']\", \"classes\" : \"['find_namastes', 'ValidatorAbortException', 'fs', 'open_fs']\", \"vars\" : \"['all_versions', 'root_inv_validator', 'id', 'filename', 'CreateFailed', 'digest_algorithm', 'num_errors', 'obj_fs', 'log', 'spec_version', 'errors', 'content_directory']\" }",
            "package": "validator",
            "project": "zimeon/ocfl-py",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b45df15108cfac7f2109dd",
            "all_context": "{ \"import\" : \"json re namaste pyfs fs digest \", \"file\" : \"\", \"class\" : \"self.validate_inventory_digest_match(self,inv_file,inv_digest_file) ; self.status_str ; self.validate_inventory(self,inv_file,where,extract_spec_version) ; self.registered_extensions ; self.read_inventory_digest ; self.obj_fs ; self.root_inv_validator ; self.validate_inventory_digest ; self.initialize(self) ; self.check_additional_digests(self,filepath,known_digests,additional_digests,error_code) ; self.validate_object_root(self,version_dirs,already_checked) ; self.validate_extensions_dir(self) ; self.inventory_digest_files ; self.validate_inventory ; self.check_additional_digests ; self.content_directory ; self.validate_version_inventories ; self.validate_extensions_dir ; self.status_str(self,prefix) ; self.validate_content(self,inventory,version_dirs,prior_manifest_digests,prior_fixity_digests) ; self.id ; self.__str__(self) ; self.lax_digests ; self.read_inventory_digest(self,inv_digest_file) ; self.validate_inventory_digest(self,inv_file,digest_algorithm,where) ; self.log ; self.digest_algorithm ; self.validate_content ; self.validate_inventory_digest_match ; self.spec_version ; self.check_digests ; self.validate_object_root ; self.initialize ; self.validate(self,path) ; self.__init__(self,log,show_warnings,show_errors,check_digests,lax_digests,lang) ; self.validate_version_inventories(self,version_dirs) ; \" }",
            "code": "    def status_str(self, prefix=''):\n        \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n        return self.log.status_str(prefix=prefix)\n",
            "dependency": "{ \"builtin\" : false, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : true, \"current_file\" : false, \"current_project\" : false, \"external\" : false }",
            "docstring": "Return string representation of validation log, with optional prefix.",
            "end_lineno": "67",
            "file_content": "\"\"\"OCFL Validator.\n\nPhilosophy of this code is to keep it separate from the implementations\nof Store, Object and Version used to build and manipulate OCFL data, but\nto leverage lower level functions such as digest creation etc.. Code style\nis plain/verbose with detailed and specific validation errors that might\nhelp someone debug an implementation.\n\nThis code uses PyFilesystem (import fs) exclusively for access to files. This\nshould enable application beyond the operating system filesystem.\n\"\"\"\nimport json\nimport re\nimport fs\n\nfrom .digest import file_digest, normalized_digest\nfrom .inventory_validator import InventoryValidator\nfrom .namaste import find_namastes\nfrom .pyfs import open_fs, ocfl_walk, ocfl_files_identical\nfrom .validation_logger import ValidationLogger\n\n\nclass ValidatorAbortException(Exception):\n    \"\"\"Exception class to bail out of validation.\"\"\"\n\n\nclass Validator():\n    \"\"\"Class for OCFL Validator.\"\"\"\n\n    def __init__(self, log=None, show_warnings=False, show_errors=True, check_digests=True, lax_digests=False, lang='en'):\n        \"\"\"Initialize OCFL validator.\"\"\"\n        self.log = log\n        self.check_digests = check_digests\n        self.lax_digests = lax_digests\n        if self.log is None:\n            self.log = ValidationLogger(show_warnings=show_warnings, show_errors=show_errors, lang=lang)\n        self.registered_extensions = [\n            '0001-digest-algorithms', '0002-flat-direct-storage-layout',\n            '0003-hash-and-id-n-tuple-storage-layout', '0004-hashed-n-tuple-storage-layout',\n            '0005-mutable-head'\n        ]\n        # The following actually initialized in initialize() method\n        self.id = None\n        self.spec_version = None\n        self.digest_algorithm = None\n        self.content_directory = None\n        self.inventory_digest_files = None\n        self.root_inv_validator = None\n        self.obj_fs = None\n        self.initialize()\n\n    def initialize(self):\n        \"\"\"Initialize object state.\n\n        Must be called between attempts to validate objects.\n        \"\"\"\n        self.id = None\n        self.spec_version = '1.0'  # default to latest published version\n        self.digest_algorithm = 'sha512'\n        self.content_directory = 'content'\n        self.inventory_digest_files = {}  # index by version_dir, algorithms may differ\n        self.root_inv_validator = None\n        self.obj_fs = None\n\n    def status_str(self, prefix=''):\n        \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n        return self.log.status_str(prefix=prefix)\n\n    def __str__(self):\n        \"\"\"Return string representation of validation log.\"\"\"\n        return self.status_str()\n\n    def validate(self, path):\n        \"\"\"Validate OCFL object at path or pyfs root.\n\n        Returns True if valid (warnings permitted), False otherwise.\n        \"\"\"\n        self.initialize()\n        try:\n            if isinstance(path, str):\n                self.obj_fs = open_fs(path)\n            else:\n                self.obj_fs = path\n                path = self.obj_fs.desc('')\n        except fs.errors.CreateFailed:\n            self.log.error('E003e', path=path)\n            return False\n        # Object declaration, set spec version number. If there are multiple declarations,\n        # look for the lastest object version then report any others as errors\n        namastes = find_namastes(0, pyfs=self.obj_fs)\n        if len(namastes) == 0:\n            self.log.error('E003a', assumed_version=self.spec_version)\n        else:\n            spec_version = None\n            for namaste in namastes:\n                # Extract and check spec version number\n                this_file_version = None\n                for version in ('1.1', '1.0'):\n                    if namaste.filename == '0=ocfl_object_' + version:\n                        this_file_version = version\n                        break\n                if this_file_version is None:\n                    self.log.error('E006', filename=namaste.filename)\n                elif spec_version is None or this_file_version > spec_version:\n                    spec_version = this_file_version\n                    if not namaste.content_ok(pyfs=self.obj_fs):\n                        self.log.error('E007', filename=namaste.filename)\n            if spec_version is None:\n                self.log.error('E003c', assumed_version=self.spec_version)\n            else:\n                self.spec_version = spec_version\n                if len(namastes) > 1:\n                    self.log.error('E003b', files=len(namastes), using_version=self.spec_version)\n        # Object root inventory file\n        inv_file = 'inventory.json'\n        if not self.obj_fs.exists(inv_file):\n            self.log.error('E063')\n            return False\n        try:\n            inventory, inv_validator = self.validate_inventory(inv_file)\n            inventory_is_valid = self.log.num_errors == 0\n            self.root_inv_validator = inv_validator\n            all_versions = inv_validator.all_versions\n            self.id = inv_validator.id\n            self.content_directory = inv_validator.content_directory\n            self.digest_algorithm = inv_validator.digest_algorithm\n            self.validate_inventory_digest(inv_file, self.digest_algorithm)\n            # Object root\n            self.validate_object_root(all_versions, already_checked=[namaste.filename for namaste in namastes])\n            # Version inventory files\n            (prior_manifest_digests, prior_fixity_digests) = self.validate_version_inventories(all_versions)\n            if inventory_is_valid:\n                # Object content\n                self.validate_content(inventory, all_versions, prior_manifest_digests, prior_fixity_digests)\n        except ValidatorAbortException:\n            pass\n        return self.log.num_errors == 0\n\n    def validate_inventory(self, inv_file, where='root', extract_spec_version=False):\n        \"\"\"Validate a given inventory file, record errors with self.log.error().\n\n        Returns inventory object for use in later validation\n        of object content. Does not look at anything else in the\n        object itself.\n\n        where - used for reporting messages of where inventory is in object\n\n        extract_spec_version - if set True will attempt to take spec_version from the\n            inventory itself instead of using the spec_version provided\n        \"\"\"\n        try:\n            with self.obj_fs.openbin(inv_file, 'r') as fh:\n                inventory = json.load(fh)\n        except json.decoder.JSONDecodeError as e:\n            self.log.error('E033', where=where, explanation=str(e))\n            raise ValidatorAbortException\n        inv_validator = InventoryValidator(log=self.log, where=where,\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return inventory, inv_validator\n\n    def validate_inventory_digest(self, inv_file, digest_algorithm, where=\"root\"):\n        \"\"\"Validate the appropriate inventory digest file in path.\"\"\"\n        inv_digest_file = inv_file + '.' + digest_algorithm\n        if not self.obj_fs.exists(inv_digest_file):\n            self.log.error('E058a', where=where, path=inv_digest_file)\n        else:\n            self.validate_inventory_digest_match(inv_file, inv_digest_file)\n\n    def validate_inventory_digest_match(self, inv_file, inv_digest_file):\n        \"\"\"Validate a given inventory digest for a given inventory file.\n\n        On error throws exception with debugging string intended to\n        be presented to a user.\n        \"\"\"\n        if not self.check_digests:\n            return\n        m = re.match(r'''.*\\.(\\w+)$''', inv_digest_file)\n        if m:\n            digest_algorithm = m.group(1)\n            try:\n                digest_recorded = self.read_inventory_digest(inv_digest_file)\n                digest_actual = file_digest(inv_file, digest_algorithm, pyfs=self.obj_fs)\n                if digest_actual != digest_recorded:\n                    self.log.error(\"E060\", inv_file=inv_file, actual=digest_actual, recorded=digest_recorded, inv_digest_file=inv_digest_file)\n            except Exception as e:  # pylint: disable=broad-except\n                self.log.error(\"E061\", description=str(e))\n        else:\n            self.log.error(\"E058b\", inv_digest_file=inv_digest_file)\n\n    def validate_object_root(self, version_dirs, already_checked):\n        \"\"\"Validate object root.\n\n        All expected_files must be present and no other files.\n        All expected_dirs must be present and no other dirs.\n        \"\"\"\n        expected_files = ['0=ocfl_object_' + self.spec_version, 'inventory.json',\n                          'inventory.json.' + self.digest_algorithm]\n        for entry in self.obj_fs.scandir(''):\n            if entry.is_file:\n                if entry.name not in expected_files and entry.name not in already_checked:\n                    self.log.error('E001a', file=entry.name)\n            elif entry.is_dir:\n                if entry.name in version_dirs:\n                    pass\n                elif entry.name == 'extensions':\n                    self.validate_extensions_dir()\n                elif re.match(r'''v\\d+$''', entry.name):\n                    # Looks like a version directory so give more specific error\n                    self.log.error('E046b', dir=entry.name)\n                else:\n                    # Simply an unexpected directory\n                    self.log.error('E001b', dir=entry.name)\n            else:\n                self.log.error('E001c', entry=entry.name)\n\n    def validate_extensions_dir(self):\n        \"\"\"Validate content of extensions directory inside object root.\n\n        Validate the extensions directory by checking that there aren't any\n        entries in the extensions directory that aren't directories themselves.\n        Where there are extension directories they SHOULD be registered and\n        this code relies up the registered_extensions property to list known\n        extensions.\n        \"\"\"\n        for entry in self.obj_fs.scandir('extensions'):\n            if entry.is_dir:\n                if entry.name not in self.registered_extensions:\n                    self.log.warning('W013', entry=entry.name)\n            else:\n                self.log.error('E067', entry=entry.name)\n\n    def validate_version_inventories(self, version_dirs):\n        \"\"\"Each version SHOULD have an inventory up to that point.\n\n        Also keep a record of any content digests different from those in the root inventory\n        so that we can also check them when validating the content.\n\n        version_dirs is an array of version directory names and is assumed to be in\n        version sequence (1, 2, 3...).\n        \"\"\"\n        prior_manifest_digests = {}  # file -> algorithm -> digest -> [versions]\n        prior_fixity_digests = {}  # file -> algorithm -> digest -> [versions]\n        if len(version_dirs) == 0:\n            return prior_manifest_digests, prior_fixity_digests\n        last_version = version_dirs[-1]\n        prev_version_dir = \"NONE\"  # will be set for first directory with inventory\n        prev_spec_version = '1.0'  # lowest version\n        for version_dir in version_dirs:\n            inv_file = fs.path.join(version_dir, 'inventory.json')\n            if not self.obj_fs.exists(inv_file):\n                self.log.warning('W010', where=version_dir)\n                continue\n            # There is an inventory file for this version directory, check it\n            if version_dir == last_version:\n                # Don't validate in this case. Per the spec the inventory in the last version\n                # MUST be identical to the copy in the object root, just check that\n                root_inv_file = 'inventory.json'\n                if not ocfl_files_identical(self.obj_fs, inv_file, root_inv_file):\n                    self.log.error('E064', root_inv_file=root_inv_file, inv_file=inv_file)\n                else:\n                    # We could also just compare digest files but this gives a more helpful error for\n                    # which file has the incorrect digest if they don't match\n                    self.validate_inventory_digest(inv_file, self.digest_algorithm, where=version_dir)\n                self.inventory_digest_files[version_dir] = 'inventory.json.' + self.digest_algorithm\n                this_spec_version = self.spec_version\n            else:\n                # Note that inventories in prior versions may use different digest algorithms\n                # from the current invenotory. Also,\n                # an may accord with the same or earlier versions of the specification\n                version_inventory, inv_validator = self.validate_inventory(inv_file, where=version_dir, extract_spec_version=True)\n                this_spec_version = inv_validator.spec_version\n                digest_algorithm = inv_validator.digest_algorithm\n                self.validate_inventory_digest(inv_file, digest_algorithm, where=version_dir)\n                self.inventory_digest_files[version_dir] = 'inventory.json.' + digest_algorithm\n                if self.id and 'id' in version_inventory:\n                    if version_inventory['id'] != self.id:\n                        self.log.error('E037b', where=version_dir, root_id=self.id, version_id=version_inventory['id'])\n                if 'manifest' in version_inventory:\n                    # Check that all files listed in prior inventories are in manifest\n                    not_seen = set(prior_manifest_digests.keys())\n                    for digest in version_inventory['manifest']:\n                        for filepath in version_inventory['manifest'][digest]:\n                            # We rely on the validation to check that anything present is OK\n                            if filepath in not_seen:\n                                not_seen.remove(filepath)\n                    if len(not_seen) > 0:\n                        self.log.error('E023b', where=version_dir, missing_filepaths=', '.join(sorted(not_seen)))\n                    # Record all prior digests\n                    for unnormalized_digest in version_inventory['manifest']:\n                        digest = normalized_digest(unnormalized_digest, digest_type=digest_algorithm)\n                        for filepath in version_inventory['manifest'][unnormalized_digest]:\n                            if filepath not in prior_manifest_digests:\n                                prior_manifest_digests[filepath] = {}\n                            if digest_algorithm not in prior_manifest_digests[filepath]:\n                                prior_manifest_digests[filepath][digest_algorithm] = {}\n                            if digest not in prior_manifest_digests[filepath][digest_algorithm]:\n                                prior_manifest_digests[filepath][digest_algorithm][digest] = []\n                            prior_manifest_digests[filepath][digest_algorithm][digest].append(version_dir)\n                # Is this inventory an appropriate prior version of the object root inventory?\n                if self.root_inv_validator is not None:\n                    self.root_inv_validator.validate_as_prior_version(inv_validator)\n                # Fixity blocks are independent in each version. Record all values and the versions\n                # they occur in for later checks against content\n                if 'fixity' in version_inventory:\n                    for digest_algorithm in version_inventory['fixity']:\n                        for unnormalized_digest in version_inventory['fixity'][digest_algorithm]:\n                            digest = normalized_digest(unnormalized_digest, digest_type=digest_algorithm)\n                            for filepath in version_inventory['fixity'][digest_algorithm][unnormalized_digest]:\n                                if filepath not in prior_fixity_digests:\n                                    prior_fixity_digests[filepath] = {}\n                                if digest_algorithm not in prior_fixity_digests[filepath]:\n                                    prior_fixity_digests[filepath][digest_algorithm] = {}\n                                if digest not in prior_fixity_digests[filepath][digest_algorithm]:\n                                    prior_fixity_digests[filepath][digest_algorithm][digest] = []\n                                prior_fixity_digests[filepath][digest_algorithm][digest].append(version_dir)\n            # We are validating the inventories in sequence and each new version must\n            # follow the same or later spec version to previous inventories\n            if prev_spec_version > this_spec_version:\n                self.log.error('E103', where=version_dir, this_spec_version=this_spec_version,\n                               prev_version_dir=prev_version_dir, prev_spec_version=prev_spec_version)\n            prev_version_dir = version_dir\n            prev_spec_version = this_spec_version\n        return prior_manifest_digests, prior_fixity_digests\n\n    def validate_content(self, inventory, version_dirs, prior_manifest_digests, prior_fixity_digests):\n        \"\"\"Validate file presence and content against inventory.\n\n        The root inventory in `inventory` is assumed to be valid and safe to use\n        for construction of file paths etc..\n        \"\"\"\n        files_seen = set()\n        # Check files in each version directory\n        for version_dir in version_dirs:\n            try:\n                # Check contents of version directory except content_directory\n                for entry in self.obj_fs.listdir(version_dir):\n                    if ((entry == 'inventory.json')\n                            or (version_dir in self.inventory_digest_files and entry == self.inventory_digest_files[version_dir])):\n                        pass\n                    elif entry == self.content_directory:\n                        # Check content_directory\n                        content_path = fs.path.join(version_dir, self.content_directory)\n                        num_content_files_in_version = 0\n                        for dirpath, dirs, files in ocfl_walk(self.obj_fs, content_path):\n                            if dirpath != '/' + content_path and (len(dirs) + len(files)) == 0:\n                                self.log.error(\"E024\", where=version_dir, path=dirpath)\n                            for file in files:\n                                files_seen.add(fs.path.join(dirpath, file).lstrip('/'))\n                                num_content_files_in_version += 1\n                        if num_content_files_in_version == 0:\n                            self.log.warning(\"W003\", where=version_dir)\n                    elif self.obj_fs.isdir(fs.path.join(version_dir, entry)):\n                        self.log.warning(\"W002\", where=version_dir, entry=entry)\n                    else:\n                        self.log.error(\"E015\", where=version_dir, entry=entry)\n            except (fs.errors.ResourceNotFound, fs.errors.DirectoryExpected):\n                self.log.error('E046a', version_dir=version_dir)\n        # Extract any digests in fixity and organize by filepath\n        fixity_digests = {}\n        if 'fixity' in inventory:\n            for digest_algorithm in inventory['fixity']:\n                for digest in inventory['fixity'][digest_algorithm]:\n                    for filepath in inventory['fixity'][digest_algorithm][digest]:\n                        if filepath in files_seen:\n                            if filepath not in fixity_digests:\n                                fixity_digests[filepath] = {}\n                            if digest_algorithm not in fixity_digests[filepath]:\n                                fixity_digests[filepath][digest_algorithm] = {}\n                            if digest not in fixity_digests[filepath][digest_algorithm]:\n                                fixity_digests[filepath][digest_algorithm][digest] = ['root']\n                        else:\n                            self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, digest=digest, content_path=filepath)\n        # Check all files in root manifest\n        if 'manifest' in inventory:\n            for digest in inventory['manifest']:\n                for filepath in inventory['manifest'][digest]:\n                    if filepath not in files_seen:\n                        self.log.error('E092b', where='root', content_path=filepath)\n                    else:\n                        if self.check_digests:\n                            content_digest = file_digest(filepath, digest_type=self.digest_algorithm, pyfs=self.obj_fs)\n                            if content_digest != normalized_digest(digest, digest_type=self.digest_algorithm):\n                                self.log.error('E092a', where='root', digest_algorithm=self.digest_algorithm, digest=digest, content_path=filepath, content_digest=content_digest)\n                            known_digests = {self.digest_algorithm: content_digest}\n                            # Are there digest values in the fixity block?\n                            self.check_additional_digests(filepath, known_digests, fixity_digests, 'E093a')\n                            # Are there other digests for this same file from other inventories?\n                            self.check_additional_digests(filepath, known_digests, prior_manifest_digests, 'E092a')\n                            self.check_additional_digests(filepath, known_digests, prior_fixity_digests, 'E093a')\n                        files_seen.discard(filepath)\n        # Anything left in files_seen is not mentioned in the inventory\n        if len(files_seen) > 0:\n            self.log.error('E023a', where='root', extra_files=', '.join(sorted(files_seen)))\n\n    def check_additional_digests(self, filepath, known_digests, additional_digests, error_code):\n        \"\"\"Check all the additional digests for filepath.\n\n        This method is intended to be used both for manifest digests in prior versions and\n        for fixity digests. The digests_seen dict is used to store any values calculated\n        so that we don't recalculate digests that might appear multiple times. It is added to\n        with any additional values calculated.\n\n        Parameters:\n            filepath - path of file in object (`v1/content/something` etc.)\n            known_digests - dict of algorithm->digest that we have calculated\n            additional_digests - dict: filepath -> algorithm -> digest -> [versions appears in]\n            error_code - error code to log on mismatch (E092a for manifest, E093a for fixity)\n        \"\"\"\n        if filepath in additional_digests:\n            for digest_algorithm in additional_digests[filepath]:\n                if digest_algorithm in known_digests:\n                    # Don't recompute anything, just use it if we've seen it before\n                    content_digest = known_digests[digest_algorithm]\n                else:\n                    content_digest = file_digest(filepath, digest_type=digest_algorithm, pyfs=self.obj_fs)\n                    known_digests[digest_algorithm] = content_digest\n                for digest in additional_digests[filepath][digest_algorithm]:\n                    if content_digest != normalized_digest(digest, digest_type=digest_algorithm):\n                        where = ','.join(additional_digests[filepath][digest_algorithm][digest])\n                        self.log.error(error_code, where=where, digest_algorithm=digest_algorithm, digest=digest, content_path=filepath, content_digest=content_digest)\n\n    def read_inventory_digest(self, inv_digest_file):\n        \"\"\"Read inventory digest from sidecar file.\n\n        Raise exception if there is an error, else return digest.\n        \"\"\"\n        with self.obj_fs.open(inv_digest_file, 'r') as fh:\n            line = fh.readline()\n            # we ignore any following lines, could raise exception\n        m = re.match(r'''(\\w+)\\s+(\\S+)\\s*$''', line)\n        if not m:\n            raise Exception(\"Bad inventory digest file %s, wrong format\" % (inv_digest_file))\n        if m.group(2) != 'inventory.json':\n            raise Exception(\"Bad inventory name in inventory digest file %s\" % (inv_digest_file))\n        return m.group(1)\n",
            "file_path": "ocfl/validator.py",
            "human_label": "Return string representation with self.log.status_str, with optional prefix.",
            "level": "class_runnable",
            "lineno": "65",
            "name": "status_str",
            "oracle_context": "{ \"apis\" : \"[]\", \"classes\" : \"[]\", \"vars\" : \"['status_str', 'log']\" }",
            "package": "validator",
            "project": "zimeon/ocfl-py",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b45e145108cfac7f210a07",
            "all_context": "{ \"import\" : \"re digest \", \"file\" : \"\", \"class\" : \"self.validate_manifest ; self.check_digests_present_and_used(self,manifest_files,digests_used) ; self.where ; self.check_digests_present_and_used ; self.validate_state_block(self,state,version,unnormalized_digests) ; self.manifest_files ; self.digest_regex(self) ; self.error ; self.validate_version_sequence ; self.check_content_paths_map_to_versions ; self.check_content_path ; self.check_logical_path(self,path,version,logical_paths,logical_directories) ; self.check_logical_path ; self.spec_versions_supported ; self.check_content_paths_map_to_versions(self,manifest_files,all_versions) ; self.head ; self.check_content_path(self,path,content_paths,content_directories) ; self.validate(self,inventory,extract_spec_version) ; self.validate_fixity(self,fixity,manifest_files) ; self.__init__(self,log,where,lax_digests,spec_version) ; self.validate_as_prior_version(self,prior) ; self.compare_states_for_version(self,prior,version) ; self.content_directory ; self.warning ; self.validate_state_block ; self.warning(self,code) ; self.error(self,code) ; self.unnormalized_digests ; self.id ; self.lax_digests ; self.log ; self.digest_algorithm ; self.validate_fixity ; self.digest_regex ; self.spec_version ; self.validate_versions ; self.validate_versions(self,versions,all_versions,unnormalized_digests) ; self.inventory ; self.compare_states_for_version ; self.all_versions ; self.validate_manifest(self,manifest) ; self.validate_version_sequence(self,versions) ; \" }",
            "code": "    def validate(self, inventory, extract_spec_version=False):\n        \"\"\"Validate a given inventory.\n\n        If extract_spec_version is True then will look at the type value to determine\n        the specification version. In the case that there is no type value or it isn't\n        valid, then other tests will be based on the version given in self.spec_version.\n        \"\"\"\n        # Basic structure\n        self.inventory = inventory\n        if 'id' in inventory:\n            iid = inventory['id']\n            if not isinstance(iid, str) or iid == '':\n                self.error(\"E037a\")\n            else:\n                # URI syntax https://www.rfc-editor.org/rfc/rfc3986.html#section-3.1 :\n                # scheme = ALPHA *( ALPHA / DIGIT / \"+\" / \"-\" / \".\" )\n                if not re.match(r'''[a-z][a-z\\d\\+\\-\\.]*:.+''', iid, re.IGNORECASE):\n                    self.warning(\"W005\", id=iid)\n                self.id = iid\n        else:\n            self.error(\"E036a\")\n        if 'type' not in inventory:\n            self.error(\"E036b\")\n        elif not isinstance(inventory['type'], str):\n            self.error(\"E999\")\n        elif extract_spec_version:\n            m = re.match(r'''https://ocfl.io/(\\d+.\\d)/spec/#inventory''', inventory['type'])\n            if not m:\n                self.error('E038b', got=inventory['type'], assumed_spec_version=self.spec_version)\n            elif m.group(1) in self.spec_versions_supported:\n                self.spec_version = m.group(1)\n            else:\n                self.error(\"E038c\", got=m.group(1), assumed_spec_version=self.spec_version)\n        elif inventory['type'] != 'https://ocfl.io/' + self.spec_version + '/spec/#inventory':\n            self.error(\"E038a\", expected='https://ocfl.io/' + self.spec_version + '/spec/#inventory', got=inventory['type'])\n        if 'digestAlgorithm' not in inventory:\n            self.error(\"E036c\")\n        elif inventory['digestAlgorithm'] == 'sha512':\n            pass\n        elif self.lax_digests:\n            self.digest_algorithm = inventory['digestAlgorithm']\n        elif inventory['digestAlgorithm'] == 'sha256':\n            self.warning(\"W004\")\n            self.digest_algorithm = inventory['digestAlgorithm']\n        else:\n            self.error(\"E039\", digest_algorithm=inventory['digestAlgorithm'])\n        if 'contentDirectory' in inventory:\n            # Careful only to set self.content_directory if value is safe\n            cd = inventory['contentDirectory']\n            if not isinstance(cd, str) or '/' in cd:\n                self.error(\"E017\")\n            elif cd in ('.', '..'):\n                self.error(\"E018\")\n            else:\n                self.content_directory = cd\n        manifest_files_correct_format = None\n        if 'manifest' not in inventory:\n            self.error(\"E041a\")\n        else:\n            (self.manifest_files, manifest_files_correct_format, self.unnormalized_digests) = self.validate_manifest(inventory['manifest'])\n        digests_used = []\n        if 'versions' not in inventory:\n            self.error(\"E041b\")\n        else:\n            self.all_versions = self.validate_version_sequence(inventory['versions'])\n            digests_used = self.validate_versions(inventory['versions'], self.all_versions, self.unnormalized_digests)\n        if 'head' not in inventory:\n            self.error(\"E036d\")\n        elif len(self.all_versions) > 0:\n            self.head = self.all_versions[-1]\n            if inventory['head'] != self.head:\n                self.error(\"E040\", got=inventory['head'], expected=self.head)\n        if len(self.all_versions) == 0:\n            # Abort tests is we don't have a valid version sequence, otherwise\n            # there will likely be spurious subsequent error reports\n            return\n        if len(self.all_versions) > 0:\n            if manifest_files_correct_format is not None:\n                self.check_content_paths_map_to_versions(manifest_files_correct_format, self.all_versions)\n            if self.manifest_files is not None:\n                self.check_digests_present_and_used(self.manifest_files, digests_used)\n        if 'fixity' in inventory:\n            self.validate_fixity(inventory['fixity'], self.manifest_files)\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : true, \"public_lib\" : false, \"current_class\" : true, \"current_file\" : false, \"current_project\" : false, \"external\" : false }",
            "docstring": "Validate a given inventory.\n\nIf extract_spec_version is True then will look at the type value to determine\nthe specification version. In the case that there is no type value or it isn't\nvalid, then other tests will be based on the version given in self.spec_version.",
            "end_lineno": "144",
            "file_content": "\"\"\"OCFL Inventory Validator.\n\nCode to validate the Python representation of an OCFL Inventory\nas read with json.load(). Does not examine anything in storage.\n\"\"\"\nimport re\n\nfrom .digest import digest_regex, normalized_digest\nfrom .validation_logger import ValidationLogger\nfrom .w3c_datetime import str_to_datetime\n\n\ndef get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    manifest = inventory['manifest']\n    file_map = {}\n    for digest in state:\n        if digest in manifest:\n            for file in state[digest]:\n                file_map[file] = set(manifest[digest])\n    return file_map\n\n\nclass InventoryValidator():\n    \"\"\"Class for OCFL Inventory Validator.\"\"\"\n\n    def __init__(self, log=None, where='???',\n                 lax_digests=False, spec_version='1.0'):\n        \"\"\"Initialize OCFL Inventory Validator.\"\"\"\n        self.log = ValidationLogger() if log is None else log\n        self.where = where\n        self.spec_version = spec_version\n        # Object state\n        self.inventory = None\n        self.id = None\n        self.digest_algorithm = 'sha512'\n        self.content_directory = 'content'\n        self.all_versions = []\n        self.manifest_files = None\n        self.unnormalized_digests = None\n        self.head = 'UNKNOWN'\n        # Validation control\n        self.lax_digests = lax_digests\n        # Configuration\n        self.spec_versions_supported = ('1.0', '1.1')\n\n    def error(self, code, **args):\n        \"\"\"Error with added context.\"\"\"\n        self.log.error(code, where=self.where, **args)\n\n    def warning(self, code, **args):\n        \"\"\"Warning with added context.\"\"\"\n        self.log.warning(code, where=self.where, **args)\n\n    def validate(self, inventory, extract_spec_version=False):\n        \"\"\"Validate a given inventory.\n\n        If extract_spec_version is True then will look at the type value to determine\n        the specification version. In the case that there is no type value or it isn't\n        valid, then other tests will be based on the version given in self.spec_version.\n        \"\"\"\n        # Basic structure\n        self.inventory = inventory\n        if 'id' in inventory:\n            iid = inventory['id']\n            if not isinstance(iid, str) or iid == '':\n                self.error(\"E037a\")\n            else:\n                # URI syntax https://www.rfc-editor.org/rfc/rfc3986.html#section-3.1 :\n                # scheme = ALPHA *( ALPHA / DIGIT / \"+\" / \"-\" / \".\" )\n                if not re.match(r'''[a-z][a-z\\d\\+\\-\\.]*:.+''', iid, re.IGNORECASE):\n                    self.warning(\"W005\", id=iid)\n                self.id = iid\n        else:\n            self.error(\"E036a\")\n        if 'type' not in inventory:\n            self.error(\"E036b\")\n        elif not isinstance(inventory['type'], str):\n            self.error(\"E999\")\n        elif extract_spec_version:\n            m = re.match(r'''https://ocfl.io/(\\d+.\\d)/spec/#inventory''', inventory['type'])\n            if not m:\n                self.error('E038b', got=inventory['type'], assumed_spec_version=self.spec_version)\n            elif m.group(1) in self.spec_versions_supported:\n                self.spec_version = m.group(1)\n            else:\n                self.error(\"E038c\", got=m.group(1), assumed_spec_version=self.spec_version)\n        elif inventory['type'] != 'https://ocfl.io/' + self.spec_version + '/spec/#inventory':\n            self.error(\"E038a\", expected='https://ocfl.io/' + self.spec_version + '/spec/#inventory', got=inventory['type'])\n        if 'digestAlgorithm' not in inventory:\n            self.error(\"E036c\")\n        elif inventory['digestAlgorithm'] == 'sha512':\n            pass\n        elif self.lax_digests:\n            self.digest_algorithm = inventory['digestAlgorithm']\n        elif inventory['digestAlgorithm'] == 'sha256':\n            self.warning(\"W004\")\n            self.digest_algorithm = inventory['digestAlgorithm']\n        else:\n            self.error(\"E039\", digest_algorithm=inventory['digestAlgorithm'])\n        if 'contentDirectory' in inventory:\n            # Careful only to set self.content_directory if value is safe\n            cd = inventory['contentDirectory']\n            if not isinstance(cd, str) or '/' in cd:\n                self.error(\"E017\")\n            elif cd in ('.', '..'):\n                self.error(\"E018\")\n            else:\n                self.content_directory = cd\n        manifest_files_correct_format = None\n        if 'manifest' not in inventory:\n            self.error(\"E041a\")\n        else:\n            (self.manifest_files, manifest_files_correct_format, self.unnormalized_digests) = self.validate_manifest(inventory['manifest'])\n        digests_used = []\n        if 'versions' not in inventory:\n            self.error(\"E041b\")\n        else:\n            self.all_versions = self.validate_version_sequence(inventory['versions'])\n            digests_used = self.validate_versions(inventory['versions'], self.all_versions, self.unnormalized_digests)\n        if 'head' not in inventory:\n            self.error(\"E036d\")\n        elif len(self.all_versions) > 0:\n            self.head = self.all_versions[-1]\n            if inventory['head'] != self.head:\n                self.error(\"E040\", got=inventory['head'], expected=self.head)\n        if len(self.all_versions) == 0:\n            # Abort tests is we don't have a valid version sequence, otherwise\n            # there will likely be spurious subsequent error reports\n            return\n        if len(self.all_versions) > 0:\n            if manifest_files_correct_format is not None:\n                self.check_content_paths_map_to_versions(manifest_files_correct_format, self.all_versions)\n            if self.manifest_files is not None:\n                self.check_digests_present_and_used(self.manifest_files, digests_used)\n        if 'fixity' in inventory:\n            self.validate_fixity(inventory['fixity'], self.manifest_files)\n\n    def validate_manifest(self, manifest):\n        \"\"\"Validate manifest block in inventory.\n\n        Returns:\n          * manifest_files - a mapping from file to digest for each file in\n              the manifest\n          * manifest_files_correct_format - a simple list of the manifest file\n              path that passed initial checks. They need to be checked for valid\n              version directories later, when we know what version directories\n              are valid\n          * unnormalized_digests - a set of the original digests in unnormalized\n              form that MUST match exactly the values used in state blocks\n        \"\"\"\n        manifest_files = {}\n        manifest_files_correct_format = []\n        unnormalized_digests = set()\n        manifest_digests = set()\n        if not isinstance(manifest, dict):\n            self.error('E041c')\n        else:\n            content_paths = set()\n            content_directories = set()\n            for digest in manifest:\n                m = re.match(self.digest_regex(), digest)\n                if not m:\n                    self.error('E025a', digest=digest, algorithm=self.digest_algorithm)  # wrong form of digest\n                elif not isinstance(manifest[digest], list):\n                    self.error('E092', digest=digest)  # must have path list value\n                else:\n                    unnormalized_digests.add(digest)\n                    norm_digest = normalized_digest(digest, self.digest_algorithm)\n                    if norm_digest in manifest_digests:\n                        # We have already seen this in different un-normalized form!\n                        self.error(\"E096\", digest=norm_digest)\n                    else:\n                        manifest_digests.add(norm_digest)\n                    for file in manifest[digest]:\n                        manifest_files[file] = norm_digest\n                        if self.check_content_path(file, content_paths, content_directories):\n                            manifest_files_correct_format.append(file)\n            # Check for conflicting content paths\n            for path in content_directories:\n                if path in content_paths:\n                    self.error(\"E101b\", path=path)\n        return manifest_files, manifest_files_correct_format, unnormalized_digests\n\n    def validate_fixity(self, fixity, manifest_files):\n        \"\"\"Validate fixity block in inventory.\n\n        Check the structure of the fixity block and makes sure that only files\n        listed in the manifest are referenced.\n        \"\"\"\n        if not isinstance(fixity, dict):\n            # The value of fixity must be a JSON object. In v1.0 I catch not an object\n            # as part of E056 but this was clarified as E111 in v1.1. The value may\n            # be an empty object in either case\n            self.error('E056a' if self.spec_version == '1.0' else 'E111')\n        else:\n            for digest_algorithm in fixity:\n                known_digest = True\n                try:\n                    regex = digest_regex(digest_algorithm)\n                except ValueError:\n                    if not self.lax_digests:\n                        self.error('E056b', algorithm=self.digest_algorithm)\n                        continue\n                    # Match anything\n                    regex = r'''^.*$'''\n                    known_digest = False\n                fixity_algoritm_block = fixity[digest_algorithm]\n                if not isinstance(fixity_algoritm_block, dict):\n                    self.error('E057a', algorithm=self.digest_algorithm)\n                else:\n                    digests_seen = set()\n                    for digest in fixity_algoritm_block:\n                        m = re.match(regex, digest)\n                        if not m:\n                            self.error('E057b', digest=digest, algorithm=digest_algorithm)  # wrong form of digest\n                        elif not isinstance(fixity_algoritm_block[digest], list):\n                            self.error('E057c', digest=digest, algorithm=digest_algorithm)  # must have path list value\n                        else:\n                            if known_digest:\n                                norm_digest = normalized_digest(digest, digest_algorithm)\n                            else:\n                                norm_digest = digest\n                            if norm_digest in digests_seen:\n                                # We have already seen this in different un-normalized form!\n                                self.error(\"E097\", digest=norm_digest, algorithm=digest_algorithm)\n                            else:\n                                digests_seen.add(norm_digest)\n                            for file in fixity_algoritm_block[digest]:\n                                if file not in manifest_files:\n                                    self.error(\"E057d\", digest=norm_digest, algorithm=digest_algorithm, path=file)\n\n    def validate_version_sequence(self, versions):\n        \"\"\"Validate sequence of version names in versions block in inventory.\n\n        Returns an array of in-sequence version directories that are part\n        of a valid sequences. May exclude other version directory names that are\n        not part of the valid sequence if an error is thrown.\n        \"\"\"\n        all_versions = []\n        if not isinstance(versions, dict):\n            self.error(\"E044\")\n            return all_versions\n        if len(versions) == 0:\n            self.error(\"E008\")\n            return all_versions\n        # Validate version sequence\n        # https://ocfl.io/draft/spec/#version-directories\n        zero_padded = None\n        max_version_num = 999999  # Excessive limit\n        if 'v1' in versions:\n            fmt = 'v%d'\n            zero_padded = False\n            all_versions.append('v1')\n        else:  # Find padding size\n            for n in range(2, 11):\n                fmt = 'v%0' + str(n) + 'd'\n                vkey = fmt % 1\n                if vkey in versions:\n                    all_versions.append(vkey)\n                    zero_padded = n\n                    max_version_num = (10 ** (n - 1)) - 1\n                    break\n            if not zero_padded:\n                self.error(\"E009\")\n                return all_versions\n        if zero_padded:\n            self.warning(\"W001\")\n        # Have v1 and know format, work through to check sequence\n        for n in range(2, max_version_num + 1):\n            v = (fmt % n)\n            if v in versions:\n                all_versions.append(v)\n            else:\n                if len(versions) != (n - 1):\n                    self.error(\"E010\")  # Extra version dirs outside sequence\n                return all_versions\n        # We have now included all possible versions up to the zero padding\n        # size, if there are more versions than this number then we must\n        # have extra that violate the zero-padding rule or are out of\n        # sequence\n        if len(versions) > max_version_num:\n            self.error(\"E011\")\n        return all_versions\n\n    def validate_versions(self, versions, all_versions, unnormalized_digests):\n        \"\"\"Validate versions blocks in inventory.\n\n        Requires as input two things which are assumed to be structurally correct\n        from prior basic validation:\n\n          * versions - which is the JSON object (dict) from the inventory\n          * all_versions - an ordered list of the versions to look at in versions\n                           (all other keys in versions will be ignored)\n\n        Returns a list of digests_used which can then be checked against the\n        manifest.\n        \"\"\"\n        digests_used = []\n        for v in all_versions:\n            version = versions[v]\n            if 'created' not in version:\n                self.error('E048', version=v)  # No created\n            elif not isinstance(versions[v]['created'], str):\n                self.error('E049d', version=v)  # Bad created\n            else:\n                created = versions[v]['created']\n                try:\n                    str_to_datetime(created)  # catch ValueError if fails\n                    if not re.search(r'''(Z|[+-]\\d\\d:\\d\\d)$''', created):  # FIXME - kludge\n                        self.error('E049a', version=v)\n                    if not re.search(r'''T\\d\\d:\\d\\d:\\d\\d''', created):  # FIXME - kludge\n                        self.error('E049b', version=v)\n                except ValueError as e:\n                    self.error('E049c', version=v, description=str(e))\n            if 'state' in version:\n                digests_used += self.validate_state_block(version['state'], version=v, unnormalized_digests=unnormalized_digests)\n            else:\n                self.error('E048c', version=v)\n            if 'message' not in version:\n                self.warning('W007a', version=v)\n            elif not isinstance(version['message'], str):\n                self.error('E094', version=v)\n            if 'user' not in version:\n                self.warning('W007b', version=v)\n            else:\n                user = version['user']\n                if not isinstance(user, dict):\n                    self.error('E054a', version=v)\n                else:\n                    if 'name' not in user or not isinstance(user['name'], str):\n                        self.error('E054b', version=v)\n                    if 'address' not in user:\n                        self.warning('W008', version=v)\n                    elif not isinstance(user['address'], str):\n                        self.error('E054c', version=v)\n                    elif not re.match(r'''\\w{3,6}:''', user['address']):\n                        self.warning('W009', version=v)\n        return digests_used\n\n    def validate_state_block(self, state, version, unnormalized_digests):\n        \"\"\"Validate state block in a version in an inventory.\n\n        The version is used only for error reporting.\n\n        Returns a list of content digests referenced in the state block.\n        \"\"\"\n        digests = []\n        logical_paths = set()\n        logical_directories = set()\n        if not isinstance(state, dict):\n            self.error('E050c', version=version)\n        else:\n            digest_re = re.compile(self.digest_regex())\n            for digest in state:\n                if not digest_re.match(digest):\n                    self.error('E050d', version=version, digest=digest)\n                elif not isinstance(state[digest], list):\n                    self.error('E050e', version=version, digest=digest)\n                else:\n                    for path in state[digest]:\n                        if path in logical_paths:\n                            self.error(\"E095a\", version=version, path=path)\n                        else:\n                            self.check_logical_path(path, version, logical_paths, logical_directories)\n                    if digest not in unnormalized_digests:\n                        # Exact string value must match, not just normalized\n                        self.error(\"E050f\", version=version, digest=digest)\n                    norm_digest = normalized_digest(digest, self.digest_algorithm)\n                    digests.append(norm_digest)\n            # Check for conflicting logical paths\n            for path in logical_directories:\n                if path in logical_paths:\n                    self.error(\"E095b\", version=version, path=path)\n        return digests\n\n    def check_content_paths_map_to_versions(self, manifest_files, all_versions):\n        \"\"\"Check that every content path starts with a valid version.\n\n        The content directory component has already been checked in\n        check_content_path(). We have already tested all paths enough\n        to know that they can be split into at least 2 components.\n        \"\"\"\n        for path in manifest_files:\n            version_dir, dummy_rest = path.split('/', 1)\n            if version_dir not in all_versions:\n                self.error('E042b', path=path)\n\n    def check_digests_present_and_used(self, manifest_files, digests_used):\n        \"\"\"Check all digests in manifest that are needed are present and used.\"\"\"\n        in_manifest = set(manifest_files.values())\n        in_state = set(digests_used)\n        not_in_manifest = in_state.difference(in_manifest)\n        if len(not_in_manifest) > 0:\n            self.error(\"E050a\", digests=\", \".join(sorted(not_in_manifest)))\n        not_in_state = in_manifest.difference(in_state)\n        if len(not_in_state) > 0:\n            self.error(\"E107\", digests=\", \".join(sorted(not_in_state)))\n\n    def digest_regex(self):\n        \"\"\"Return regex for validating un-normalized digest format.\"\"\"\n        try:\n            return digest_regex(self.digest_algorithm)\n        except ValueError:\n            if not self.lax_digests:\n                self.error('E026a', digest=self.digest_algorithm)\n        # Match anything\n        return r'''^.*$'''\n\n    def check_logical_path(self, path, version, logical_paths, logical_directories):\n        \"\"\"Check logical path and accumulate paths/directories for E095b check.\n\n        logical_paths and logical_directories are expected to be sets.\n\n        Only adds good paths to the accumulated paths/directories.\n        \"\"\"\n        if path.startswith('/') or path.endswith('/'):\n            self.error(\"E053\", version=version, path=path)\n        else:\n            elements = path.split('/')\n            for element in elements:\n                if element in ['.', '..', '']:\n                    self.error(\"E052\", version=version, path=path)\n                    return\n            # Accumulate paths and directories\n            logical_paths.add(path)\n            logical_directories.add('/'.join(elements[0:-1]))\n\n    def check_content_path(self, path, content_paths, content_directories):\n        \"\"\"Check logical path and accumulate paths/directories for E101 check.\n\n        Returns True if valid, else False. Only adds good paths to the\n        accumulated paths/directories. We don't yet know the set of valid\n        version directories so the check here is just for 'v' + digits.\n        \"\"\"\n        if path.startswith('/') or path.endswith('/'):\n            self.error(\"E100\", path=path)\n            return False\n        m = re.match(r'''^(v\\d+/''' + self.content_directory + r''')/(.+)''', path)\n        if not m:\n            self.error(\"E042a\", path=path)\n            return False\n        elements = m.group(2).split('/')\n        for element in elements:\n            if element in ('', '.', '..'):\n                self.error(\"E099\", path=path)\n                return False\n        # Accumulate paths and directories if not seen before\n        if path in content_paths:\n            self.error(\"E101a\", path=path)\n            return False\n        content_paths.add(path)\n        content_directories.add('/'.join([m.group(1)] + elements[0:-1]))\n        return True\n\n    def validate_as_prior_version(self, prior):\n        \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n        The input variable prior is also expected to be an InventoryValidator object\n        and both self and prior inventories are assumed to have been checked for\n        internal consistency.\n        \"\"\"\n        # Must have a subset of versions which also checks zero padding format etc.\n        if not set(prior.all_versions) < set(self.all_versions):\n            self.error('E066a', prior_head=prior.head)\n        else:\n            # Check references to files but realize that there might be different\n            # digest algorithms between versions\n            version = 'no-version'\n            for version in prior.all_versions:\n                # If the digest algorithm is the same then we can make a\n                # direct check on whether the state blocks match\n                if prior.digest_algorithm == self.digest_algorithm:\n                    self.compare_states_for_version(prior, version)\n                # Now check the mappings from state to logical path, which must\n                # be consistent even if the digestAlgorithm is different between\n                # versions. Get maps from logical paths to files on disk:\n                prior_map = get_logical_path_map(prior.inventory, version)\n                self_map = get_logical_path_map(self.inventory, version)\n                # Look first for differences in logical paths listed\n                only_in_prior = prior_map.keys() - self_map.keys()\n                only_in_self = self_map.keys() - prior_map.keys()\n                if only_in_prior or only_in_self:\n                    if only_in_prior:\n                        self.error('E066b', version=version, prior_head=prior.head, only_in=prior.head, logical_paths=','.join(only_in_prior))\n                    if only_in_self:\n                        self.error('E066b', version=version, prior_head=prior.head, only_in=self.where, logical_paths=','.join(only_in_self))\n                else:\n                    # Check them all in details - digests must match\n                    for logical_path, this_map in prior_map.items():\n                        if not this_map.issubset(self_map[logical_path]):\n                            self.error('E066c', version=version, prior_head=prior.head,\n                                       logical_path=logical_path, prior_content=','.join(this_map),\n                                       current_content=','.join(self_map[logical_path]))\n                # Check metadata\n                prior_version = prior.inventory['versions'][version]\n                self_version = self.inventory['versions'][version]\n                for key in ('created', 'message', 'user'):\n                    if prior_version.get(key) != self_version.get(key):\n                        self.warning('W011', version=version, prior_head=prior.head, key=key)\n\n    def compare_states_for_version(self, prior, version):\n        \"\"\"Compare state blocks for version between self and prior.\n\n        Assumes the same digest algorithm in both, do not call otherwise!\n\n        Looks only for digests that appear in one but not in the other, the code\n        in validate_as_prior_version(..) does a check for whether the same sets\n        of logical files appear and we don't want to duplicate an error message\n        about that.\n\n        While the mapping checks in validate_as_prior_version(..) do all that is\n        necessary to detect an error, the additional errors that may be generated\n        here provide more detailed diagnostics in the case that the digest\n        algorithm is the same across versions being compared.\n        \"\"\"\n        self_state = self.inventory['versions'][version]['state']\n        prior_state = prior.inventory['versions'][version]['state']\n        for digest in set(self_state.keys()).union(prior_state.keys()):\n            if digest not in prior_state:\n                self.error('E066d', version=version, prior_head=prior.head,\n                           digest=digest, logical_files=', '.join(self_state[digest]))\n            elif digest not in self_state:\n                self.error('E066e', version=version, prior_head=prior.head,\n                           digest=digest, logical_files=', '.join(prior_state[digest]))\n",
            "file_path": "ocfl/inventory_validator.py",
            "human_label": "Validate a given inventory. If extract_spec_version is True then will look at the type value to determine the specification version. In the case that there is no type value or it isn't valid, then other tests will be based on the version given in self.spec_version. (D)",
            "level": "class_runnable",
            "lineno": "62",
            "name": "validate",
            "oracle_context": "{ \"apis\" : \"['warning', 'group', 'isinstance', 'validate_versions', 'check_digests_present_and_used', 'error', 'check_content_paths_map_to_versions', 'len', 'validate_manifest', 'validate_version_sequence', 'validate_fixity', 'match']\", \"classes\" : \"['re']\", \"vars\" : \"['all_versions', 'unnormalized_digests', 'spec_versions_supported', 'id', 'manifest_files', 'digest_algorithm', 'spec_version', 'inventory', 'lax_digests', 'head', 'IGNORECASE', 'content_directory']\" }",
            "package": "inventory_validator",
            "project": "zimeon/ocfl-py",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b45e145108cfac7f210a09",
            "all_context": "{ \"import\" : \"re digest \", \"file\" : \"\", \"class\" : \"self.validate_manifest ; self.check_digests_present_and_used(self,manifest_files,digests_used) ; self.where ; self.check_digests_present_and_used ; self.validate_state_block(self,state,version,unnormalized_digests) ; self.manifest_files ; self.digest_regex(self) ; self.error ; self.validate_version_sequence ; self.check_content_paths_map_to_versions ; self.check_content_path ; self.check_logical_path(self,path,version,logical_paths,logical_directories) ; self.check_logical_path ; self.spec_versions_supported ; self.check_content_paths_map_to_versions(self,manifest_files,all_versions) ; self.head ; self.check_content_path(self,path,content_paths,content_directories) ; self.validate(self,inventory,extract_spec_version) ; self.validate_fixity(self,fixity,manifest_files) ; self.__init__(self,log,where,lax_digests,spec_version) ; self.validate_as_prior_version(self,prior) ; self.compare_states_for_version(self,prior,version) ; self.content_directory ; self.warning ; self.validate_state_block ; self.warning(self,code) ; self.error(self,code) ; self.unnormalized_digests ; self.id ; self.lax_digests ; self.log ; self.digest_algorithm ; self.validate_fixity ; self.digest_regex ; self.spec_version ; self.validate_versions ; self.validate_versions(self,versions,all_versions,unnormalized_digests) ; self.inventory ; self.compare_states_for_version ; self.all_versions ; self.validate_manifest(self,manifest) ; self.validate_version_sequence(self,versions) ; \" }",
            "code": "    def check_digests_present_and_used(self, manifest_files, digests_used):\n        \"\"\"Check all digests in manifest that are needed are present and used.\"\"\"\n        in_manifest = set(manifest_files.values())\n        in_state = set(digests_used)\n        not_in_manifest = in_state.difference(in_manifest)\n        if len(not_in_manifest) > 0:\n            self.error(\"E050a\", digests=\", \".join(sorted(not_in_manifest)))\n        not_in_state = in_manifest.difference(in_state)\n        if len(not_in_state) > 0:\n            self.error(\"E107\", digests=\", \".join(sorted(not_in_state)))\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : true, \"current_file\" : false, \"current_project\" : false, \"external\" : false }",
            "docstring": "Check all digests in manifest that are needed are present and used.",
            "end_lineno": "405",
            "file_content": "\"\"\"OCFL Inventory Validator.\n\nCode to validate the Python representation of an OCFL Inventory\nas read with json.load(). Does not examine anything in storage.\n\"\"\"\nimport re\n\nfrom .digest import digest_regex, normalized_digest\nfrom .validation_logger import ValidationLogger\nfrom .w3c_datetime import str_to_datetime\n\n\ndef get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    manifest = inventory['manifest']\n    file_map = {}\n    for digest in state:\n        if digest in manifest:\n            for file in state[digest]:\n                file_map[file] = set(manifest[digest])\n    return file_map\n\n\nclass InventoryValidator():\n    \"\"\"Class for OCFL Inventory Validator.\"\"\"\n\n    def __init__(self, log=None, where='???',\n                 lax_digests=False, spec_version='1.0'):\n        \"\"\"Initialize OCFL Inventory Validator.\"\"\"\n        self.log = ValidationLogger() if log is None else log\n        self.where = where\n        self.spec_version = spec_version\n        # Object state\n        self.inventory = None\n        self.id = None\n        self.digest_algorithm = 'sha512'\n        self.content_directory = 'content'\n        self.all_versions = []\n        self.manifest_files = None\n        self.unnormalized_digests = None\n        self.head = 'UNKNOWN'\n        # Validation control\n        self.lax_digests = lax_digests\n        # Configuration\n        self.spec_versions_supported = ('1.0', '1.1')\n\n    def error(self, code, **args):\n        \"\"\"Error with added context.\"\"\"\n        self.log.error(code, where=self.where, **args)\n\n    def warning(self, code, **args):\n        \"\"\"Warning with added context.\"\"\"\n        self.log.warning(code, where=self.where, **args)\n\n    def validate(self, inventory, extract_spec_version=False):\n        \"\"\"Validate a given inventory.\n\n        If extract_spec_version is True then will look at the type value to determine\n        the specification version. In the case that there is no type value or it isn't\n        valid, then other tests will be based on the version given in self.spec_version.\n        \"\"\"\n        # Basic structure\n        self.inventory = inventory\n        if 'id' in inventory:\n            iid = inventory['id']\n            if not isinstance(iid, str) or iid == '':\n                self.error(\"E037a\")\n            else:\n                # URI syntax https://www.rfc-editor.org/rfc/rfc3986.html#section-3.1 :\n                # scheme = ALPHA *( ALPHA / DIGIT / \"+\" / \"-\" / \".\" )\n                if not re.match(r'''[a-z][a-z\\d\\+\\-\\.]*:.+''', iid, re.IGNORECASE):\n                    self.warning(\"W005\", id=iid)\n                self.id = iid\n        else:\n            self.error(\"E036a\")\n        if 'type' not in inventory:\n            self.error(\"E036b\")\n        elif not isinstance(inventory['type'], str):\n            self.error(\"E999\")\n        elif extract_spec_version:\n            m = re.match(r'''https://ocfl.io/(\\d+.\\d)/spec/#inventory''', inventory['type'])\n            if not m:\n                self.error('E038b', got=inventory['type'], assumed_spec_version=self.spec_version)\n            elif m.group(1) in self.spec_versions_supported:\n                self.spec_version = m.group(1)\n            else:\n                self.error(\"E038c\", got=m.group(1), assumed_spec_version=self.spec_version)\n        elif inventory['type'] != 'https://ocfl.io/' + self.spec_version + '/spec/#inventory':\n            self.error(\"E038a\", expected='https://ocfl.io/' + self.spec_version + '/spec/#inventory', got=inventory['type'])\n        if 'digestAlgorithm' not in inventory:\n            self.error(\"E036c\")\n        elif inventory['digestAlgorithm'] == 'sha512':\n            pass\n        elif self.lax_digests:\n            self.digest_algorithm = inventory['digestAlgorithm']\n        elif inventory['digestAlgorithm'] == 'sha256':\n            self.warning(\"W004\")\n            self.digest_algorithm = inventory['digestAlgorithm']\n        else:\n            self.error(\"E039\", digest_algorithm=inventory['digestAlgorithm'])\n        if 'contentDirectory' in inventory:\n            # Careful only to set self.content_directory if value is safe\n            cd = inventory['contentDirectory']\n            if not isinstance(cd, str) or '/' in cd:\n                self.error(\"E017\")\n            elif cd in ('.', '..'):\n                self.error(\"E018\")\n            else:\n                self.content_directory = cd\n        manifest_files_correct_format = None\n        if 'manifest' not in inventory:\n            self.error(\"E041a\")\n        else:\n            (self.manifest_files, manifest_files_correct_format, self.unnormalized_digests) = self.validate_manifest(inventory['manifest'])\n        digests_used = []\n        if 'versions' not in inventory:\n            self.error(\"E041b\")\n        else:\n            self.all_versions = self.validate_version_sequence(inventory['versions'])\n            digests_used = self.validate_versions(inventory['versions'], self.all_versions, self.unnormalized_digests)\n        if 'head' not in inventory:\n            self.error(\"E036d\")\n        elif len(self.all_versions) > 0:\n            self.head = self.all_versions[-1]\n            if inventory['head'] != self.head:\n                self.error(\"E040\", got=inventory['head'], expected=self.head)\n        if len(self.all_versions) == 0:\n            # Abort tests is we don't have a valid version sequence, otherwise\n            # there will likely be spurious subsequent error reports\n            return\n        if len(self.all_versions) > 0:\n            if manifest_files_correct_format is not None:\n                self.check_content_paths_map_to_versions(manifest_files_correct_format, self.all_versions)\n            if self.manifest_files is not None:\n                self.check_digests_present_and_used(self.manifest_files, digests_used)\n        if 'fixity' in inventory:\n            self.validate_fixity(inventory['fixity'], self.manifest_files)\n\n    def validate_manifest(self, manifest):\n        \"\"\"Validate manifest block in inventory.\n\n        Returns:\n          * manifest_files - a mapping from file to digest for each file in\n              the manifest\n          * manifest_files_correct_format - a simple list of the manifest file\n              path that passed initial checks. They need to be checked for valid\n              version directories later, when we know what version directories\n              are valid\n          * unnormalized_digests - a set of the original digests in unnormalized\n              form that MUST match exactly the values used in state blocks\n        \"\"\"\n        manifest_files = {}\n        manifest_files_correct_format = []\n        unnormalized_digests = set()\n        manifest_digests = set()\n        if not isinstance(manifest, dict):\n            self.error('E041c')\n        else:\n            content_paths = set()\n            content_directories = set()\n            for digest in manifest:\n                m = re.match(self.digest_regex(), digest)\n                if not m:\n                    self.error('E025a', digest=digest, algorithm=self.digest_algorithm)  # wrong form of digest\n                elif not isinstance(manifest[digest], list):\n                    self.error('E092', digest=digest)  # must have path list value\n                else:\n                    unnormalized_digests.add(digest)\n                    norm_digest = normalized_digest(digest, self.digest_algorithm)\n                    if norm_digest in manifest_digests:\n                        # We have already seen this in different un-normalized form!\n                        self.error(\"E096\", digest=norm_digest)\n                    else:\n                        manifest_digests.add(norm_digest)\n                    for file in manifest[digest]:\n                        manifest_files[file] = norm_digest\n                        if self.check_content_path(file, content_paths, content_directories):\n                            manifest_files_correct_format.append(file)\n            # Check for conflicting content paths\n            for path in content_directories:\n                if path in content_paths:\n                    self.error(\"E101b\", path=path)\n        return manifest_files, manifest_files_correct_format, unnormalized_digests\n\n    def validate_fixity(self, fixity, manifest_files):\n        \"\"\"Validate fixity block in inventory.\n\n        Check the structure of the fixity block and makes sure that only files\n        listed in the manifest are referenced.\n        \"\"\"\n        if not isinstance(fixity, dict):\n            # The value of fixity must be a JSON object. In v1.0 I catch not an object\n            # as part of E056 but this was clarified as E111 in v1.1. The value may\n            # be an empty object in either case\n            self.error('E056a' if self.spec_version == '1.0' else 'E111')\n        else:\n            for digest_algorithm in fixity:\n                known_digest = True\n                try:\n                    regex = digest_regex(digest_algorithm)\n                except ValueError:\n                    if not self.lax_digests:\n                        self.error('E056b', algorithm=self.digest_algorithm)\n                        continue\n                    # Match anything\n                    regex = r'''^.*$'''\n                    known_digest = False\n                fixity_algoritm_block = fixity[digest_algorithm]\n                if not isinstance(fixity_algoritm_block, dict):\n                    self.error('E057a', algorithm=self.digest_algorithm)\n                else:\n                    digests_seen = set()\n                    for digest in fixity_algoritm_block:\n                        m = re.match(regex, digest)\n                        if not m:\n                            self.error('E057b', digest=digest, algorithm=digest_algorithm)  # wrong form of digest\n                        elif not isinstance(fixity_algoritm_block[digest], list):\n                            self.error('E057c', digest=digest, algorithm=digest_algorithm)  # must have path list value\n                        else:\n                            if known_digest:\n                                norm_digest = normalized_digest(digest, digest_algorithm)\n                            else:\n                                norm_digest = digest\n                            if norm_digest in digests_seen:\n                                # We have already seen this in different un-normalized form!\n                                self.error(\"E097\", digest=norm_digest, algorithm=digest_algorithm)\n                            else:\n                                digests_seen.add(norm_digest)\n                            for file in fixity_algoritm_block[digest]:\n                                if file not in manifest_files:\n                                    self.error(\"E057d\", digest=norm_digest, algorithm=digest_algorithm, path=file)\n\n    def validate_version_sequence(self, versions):\n        \"\"\"Validate sequence of version names in versions block in inventory.\n\n        Returns an array of in-sequence version directories that are part\n        of a valid sequences. May exclude other version directory names that are\n        not part of the valid sequence if an error is thrown.\n        \"\"\"\n        all_versions = []\n        if not isinstance(versions, dict):\n            self.error(\"E044\")\n            return all_versions\n        if len(versions) == 0:\n            self.error(\"E008\")\n            return all_versions\n        # Validate version sequence\n        # https://ocfl.io/draft/spec/#version-directories\n        zero_padded = None\n        max_version_num = 999999  # Excessive limit\n        if 'v1' in versions:\n            fmt = 'v%d'\n            zero_padded = False\n            all_versions.append('v1')\n        else:  # Find padding size\n            for n in range(2, 11):\n                fmt = 'v%0' + str(n) + 'd'\n                vkey = fmt % 1\n                if vkey in versions:\n                    all_versions.append(vkey)\n                    zero_padded = n\n                    max_version_num = (10 ** (n - 1)) - 1\n                    break\n            if not zero_padded:\n                self.error(\"E009\")\n                return all_versions\n        if zero_padded:\n            self.warning(\"W001\")\n        # Have v1 and know format, work through to check sequence\n        for n in range(2, max_version_num + 1):\n            v = (fmt % n)\n            if v in versions:\n                all_versions.append(v)\n            else:\n                if len(versions) != (n - 1):\n                    self.error(\"E010\")  # Extra version dirs outside sequence\n                return all_versions\n        # We have now included all possible versions up to the zero padding\n        # size, if there are more versions than this number then we must\n        # have extra that violate the zero-padding rule or are out of\n        # sequence\n        if len(versions) > max_version_num:\n            self.error(\"E011\")\n        return all_versions\n\n    def validate_versions(self, versions, all_versions, unnormalized_digests):\n        \"\"\"Validate versions blocks in inventory.\n\n        Requires as input two things which are assumed to be structurally correct\n        from prior basic validation:\n\n          * versions - which is the JSON object (dict) from the inventory\n          * all_versions - an ordered list of the versions to look at in versions\n                           (all other keys in versions will be ignored)\n\n        Returns a list of digests_used which can then be checked against the\n        manifest.\n        \"\"\"\n        digests_used = []\n        for v in all_versions:\n            version = versions[v]\n            if 'created' not in version:\n                self.error('E048', version=v)  # No created\n            elif not isinstance(versions[v]['created'], str):\n                self.error('E049d', version=v)  # Bad created\n            else:\n                created = versions[v]['created']\n                try:\n                    str_to_datetime(created)  # catch ValueError if fails\n                    if not re.search(r'''(Z|[+-]\\d\\d:\\d\\d)$''', created):  # FIXME - kludge\n                        self.error('E049a', version=v)\n                    if not re.search(r'''T\\d\\d:\\d\\d:\\d\\d''', created):  # FIXME - kludge\n                        self.error('E049b', version=v)\n                except ValueError as e:\n                    self.error('E049c', version=v, description=str(e))\n            if 'state' in version:\n                digests_used += self.validate_state_block(version['state'], version=v, unnormalized_digests=unnormalized_digests)\n            else:\n                self.error('E048c', version=v)\n            if 'message' not in version:\n                self.warning('W007a', version=v)\n            elif not isinstance(version['message'], str):\n                self.error('E094', version=v)\n            if 'user' not in version:\n                self.warning('W007b', version=v)\n            else:\n                user = version['user']\n                if not isinstance(user, dict):\n                    self.error('E054a', version=v)\n                else:\n                    if 'name' not in user or not isinstance(user['name'], str):\n                        self.error('E054b', version=v)\n                    if 'address' not in user:\n                        self.warning('W008', version=v)\n                    elif not isinstance(user['address'], str):\n                        self.error('E054c', version=v)\n                    elif not re.match(r'''\\w{3,6}:''', user['address']):\n                        self.warning('W009', version=v)\n        return digests_used\n\n    def validate_state_block(self, state, version, unnormalized_digests):\n        \"\"\"Validate state block in a version in an inventory.\n\n        The version is used only for error reporting.\n\n        Returns a list of content digests referenced in the state block.\n        \"\"\"\n        digests = []\n        logical_paths = set()\n        logical_directories = set()\n        if not isinstance(state, dict):\n            self.error('E050c', version=version)\n        else:\n            digest_re = re.compile(self.digest_regex())\n            for digest in state:\n                if not digest_re.match(digest):\n                    self.error('E050d', version=version, digest=digest)\n                elif not isinstance(state[digest], list):\n                    self.error('E050e', version=version, digest=digest)\n                else:\n                    for path in state[digest]:\n                        if path in logical_paths:\n                            self.error(\"E095a\", version=version, path=path)\n                        else:\n                            self.check_logical_path(path, version, logical_paths, logical_directories)\n                    if digest not in unnormalized_digests:\n                        # Exact string value must match, not just normalized\n                        self.error(\"E050f\", version=version, digest=digest)\n                    norm_digest = normalized_digest(digest, self.digest_algorithm)\n                    digests.append(norm_digest)\n            # Check for conflicting logical paths\n            for path in logical_directories:\n                if path in logical_paths:\n                    self.error(\"E095b\", version=version, path=path)\n        return digests\n\n    def check_content_paths_map_to_versions(self, manifest_files, all_versions):\n        \"\"\"Check that every content path starts with a valid version.\n\n        The content directory component has already been checked in\n        check_content_path(). We have already tested all paths enough\n        to know that they can be split into at least 2 components.\n        \"\"\"\n        for path in manifest_files:\n            version_dir, dummy_rest = path.split('/', 1)\n            if version_dir not in all_versions:\n                self.error('E042b', path=path)\n\n    def check_digests_present_and_used(self, manifest_files, digests_used):\n        \"\"\"Check all digests in manifest that are needed are present and used.\"\"\"\n        in_manifest = set(manifest_files.values())\n        in_state = set(digests_used)\n        not_in_manifest = in_state.difference(in_manifest)\n        if len(not_in_manifest) > 0:\n            self.error(\"E050a\", digests=\", \".join(sorted(not_in_manifest)))\n        not_in_state = in_manifest.difference(in_state)\n        if len(not_in_state) > 0:\n            self.error(\"E107\", digests=\", \".join(sorted(not_in_state)))\n\n    def digest_regex(self):\n        \"\"\"Return regex for validating un-normalized digest format.\"\"\"\n        try:\n            return digest_regex(self.digest_algorithm)\n        except ValueError:\n            if not self.lax_digests:\n                self.error('E026a', digest=self.digest_algorithm)\n        # Match anything\n        return r'''^.*$'''\n\n    def check_logical_path(self, path, version, logical_paths, logical_directories):\n        \"\"\"Check logical path and accumulate paths/directories for E095b check.\n\n        logical_paths and logical_directories are expected to be sets.\n\n        Only adds good paths to the accumulated paths/directories.\n        \"\"\"\n        if path.startswith('/') or path.endswith('/'):\n            self.error(\"E053\", version=version, path=path)\n        else:\n            elements = path.split('/')\n            for element in elements:\n                if element in ['.', '..', '']:\n                    self.error(\"E052\", version=version, path=path)\n                    return\n            # Accumulate paths and directories\n            logical_paths.add(path)\n            logical_directories.add('/'.join(elements[0:-1]))\n\n    def check_content_path(self, path, content_paths, content_directories):\n        \"\"\"Check logical path and accumulate paths/directories for E101 check.\n\n        Returns True if valid, else False. Only adds good paths to the\n        accumulated paths/directories. We don't yet know the set of valid\n        version directories so the check here is just for 'v' + digits.\n        \"\"\"\n        if path.startswith('/') or path.endswith('/'):\n            self.error(\"E100\", path=path)\n            return False\n        m = re.match(r'''^(v\\d+/''' + self.content_directory + r''')/(.+)''', path)\n        if not m:\n            self.error(\"E042a\", path=path)\n            return False\n        elements = m.group(2).split('/')\n        for element in elements:\n            if element in ('', '.', '..'):\n                self.error(\"E099\", path=path)\n                return False\n        # Accumulate paths and directories if not seen before\n        if path in content_paths:\n            self.error(\"E101a\", path=path)\n            return False\n        content_paths.add(path)\n        content_directories.add('/'.join([m.group(1)] + elements[0:-1]))\n        return True\n\n    def validate_as_prior_version(self, prior):\n        \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n        The input variable prior is also expected to be an InventoryValidator object\n        and both self and prior inventories are assumed to have been checked for\n        internal consistency.\n        \"\"\"\n        # Must have a subset of versions which also checks zero padding format etc.\n        if not set(prior.all_versions) < set(self.all_versions):\n            self.error('E066a', prior_head=prior.head)\n        else:\n            # Check references to files but realize that there might be different\n            # digest algorithms between versions\n            version = 'no-version'\n            for version in prior.all_versions:\n                # If the digest algorithm is the same then we can make a\n                # direct check on whether the state blocks match\n                if prior.digest_algorithm == self.digest_algorithm:\n                    self.compare_states_for_version(prior, version)\n                # Now check the mappings from state to logical path, which must\n                # be consistent even if the digestAlgorithm is different between\n                # versions. Get maps from logical paths to files on disk:\n                prior_map = get_logical_path_map(prior.inventory, version)\n                self_map = get_logical_path_map(self.inventory, version)\n                # Look first for differences in logical paths listed\n                only_in_prior = prior_map.keys() - self_map.keys()\n                only_in_self = self_map.keys() - prior_map.keys()\n                if only_in_prior or only_in_self:\n                    if only_in_prior:\n                        self.error('E066b', version=version, prior_head=prior.head, only_in=prior.head, logical_paths=','.join(only_in_prior))\n                    if only_in_self:\n                        self.error('E066b', version=version, prior_head=prior.head, only_in=self.where, logical_paths=','.join(only_in_self))\n                else:\n                    # Check them all in details - digests must match\n                    for logical_path, this_map in prior_map.items():\n                        if not this_map.issubset(self_map[logical_path]):\n                            self.error('E066c', version=version, prior_head=prior.head,\n                                       logical_path=logical_path, prior_content=','.join(this_map),\n                                       current_content=','.join(self_map[logical_path]))\n                # Check metadata\n                prior_version = prior.inventory['versions'][version]\n                self_version = self.inventory['versions'][version]\n                for key in ('created', 'message', 'user'):\n                    if prior_version.get(key) != self_version.get(key):\n                        self.warning('W011', version=version, prior_head=prior.head, key=key)\n\n    def compare_states_for_version(self, prior, version):\n        \"\"\"Compare state blocks for version between self and prior.\n\n        Assumes the same digest algorithm in both, do not call otherwise!\n\n        Looks only for digests that appear in one but not in the other, the code\n        in validate_as_prior_version(..) does a check for whether the same sets\n        of logical files appear and we don't want to duplicate an error message\n        about that.\n\n        While the mapping checks in validate_as_prior_version(..) do all that is\n        necessary to detect an error, the additional errors that may be generated\n        here provide more detailed diagnostics in the case that the digest\n        algorithm is the same across versions being compared.\n        \"\"\"\n        self_state = self.inventory['versions'][version]['state']\n        prior_state = prior.inventory['versions'][version]['state']\n        for digest in set(self_state.keys()).union(prior_state.keys()):\n            if digest not in prior_state:\n                self.error('E066d', version=version, prior_head=prior.head,\n                           digest=digest, logical_files=', '.join(self_state[digest]))\n            elif digest not in self_state:\n                self.error('E066e', version=version, prior_head=prior.head,\n                           digest=digest, logical_files=', '.join(prior_state[digest]))\n",
            "file_path": "ocfl/inventory_validator.py",
            "human_label": "Check all digests in manifest that are needed are present and used. Return error() in the class.",
            "level": "class_runnable",
            "lineno": "396",
            "name": "check_digests_present_and_used",
            "oracle_context": "{ \"apis\" : \"['set', 'difference', 'values', 'join', 'error', 'len', 'sorted']\", \"classes\" : \"[]\", \"vars\" : \"['Str']\" }",
            "package": "inventory_validator",
            "project": "zimeon/ocfl-py",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b45e165108cfac7f210a16",
            "all_context": "{ \"import\" : \"re digest \", \"file\" : \"get_logical_path_map(inventory,version) ; \", \"class\" : \"self.validate_manifest ; self.check_digests_present_and_used(self,manifest_files,digests_used) ; self.where ; self.check_digests_present_and_used ; self.validate_state_block(self,state,version,unnormalized_digests) ; self.manifest_files ; self.digest_regex(self) ; self.error ; self.validate_version_sequence ; self.check_content_paths_map_to_versions ; self.check_content_path ; self.check_logical_path(self,path,version,logical_paths,logical_directories) ; self.check_logical_path ; self.spec_versions_supported ; self.check_content_paths_map_to_versions(self,manifest_files,all_versions) ; self.head ; self.check_content_path(self,path,content_paths,content_directories) ; self.validate(self,inventory,extract_spec_version) ; self.validate_fixity(self,fixity,manifest_files) ; self.__init__(self,log,where,lax_digests,spec_version) ; self.validate_as_prior_version(self,prior) ; self.compare_states_for_version(self,prior,version) ; self.content_directory ; self.warning ; self.validate_state_block ; self.warning(self,code) ; self.error(self,code) ; self.unnormalized_digests ; self.id ; self.lax_digests ; self.log ; self.digest_algorithm ; self.validate_fixity ; self.digest_regex ; self.spec_version ; self.validate_versions ; self.validate_versions(self,versions,all_versions,unnormalized_digests) ; self.inventory ; self.compare_states_for_version ; self.all_versions ; self.validate_manifest(self,manifest) ; self.validate_version_sequence(self,versions) ; \" }",
            "code": "    def validate_as_prior_version(self, prior):\n        \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n        The input variable prior is also expected to be an InventoryValidator object\n        and both self and prior inventories are assumed to have been checked for\n        internal consistency.\n        \"\"\"\n        # Must have a subset of versions which also checks zero padding format etc.\n        if not set(prior.all_versions) < set(self.all_versions):\n            self.error('E066a', prior_head=prior.head)\n        else:\n            # Check references to files but realize that there might be different\n            # digest algorithms between versions\n            version = 'no-version'\n            for version in prior.all_versions:\n                # If the digest algorithm is the same then we can make a\n                # direct check on whether the state blocks match\n                if prior.digest_algorithm == self.digest_algorithm:\n                    self.compare_states_for_version(prior, version)\n                # Now check the mappings from state to logical path, which must\n                # be consistent even if the digestAlgorithm is different between\n                # versions. Get maps from logical paths to files on disk:\n                prior_map = get_logical_path_map(prior.inventory, version)\n                self_map = get_logical_path_map(self.inventory, version)\n                # Look first for differences in logical paths listed\n                only_in_prior = prior_map.keys() - self_map.keys()\n                only_in_self = self_map.keys() - prior_map.keys()\n                if only_in_prior or only_in_self:\n                    if only_in_prior:\n                        self.error('E066b', version=version, prior_head=prior.head, only_in=prior.head, logical_paths=','.join(only_in_prior))\n                    if only_in_self:\n                        self.error('E066b', version=version, prior_head=prior.head, only_in=self.where, logical_paths=','.join(only_in_self))\n                else:\n                    # Check them all in details - digests must match\n                    for logical_path, this_map in prior_map.items():\n                        if not this_map.issubset(self_map[logical_path]):\n                            self.error('E066c', version=version, prior_head=prior.head,\n                                       logical_path=logical_path, prior_content=','.join(this_map),\n                                       current_content=','.join(self_map[logical_path]))\n                # Check metadata\n                prior_version = prior.inventory['versions'][version]\n                self_version = self.inventory['versions'][version]\n                for key in ('created', 'message', 'user'):\n                    if prior_version.get(key) != self_version.get(key):\n                        self.warning('W011', version=version, prior_head=prior.head, key=key)\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : true, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Check that prior is a valid prior version of the current inventory object.\n\nThe input variable prior is also expected to be an InventoryValidator object\nand both self and prior inventories are assumed to have been checked for\ninternal consistency.",
            "end_lineno": "507",
            "file_content": "\"\"\"OCFL Inventory Validator.\n\nCode to validate the Python representation of an OCFL Inventory\nas read with json.load(). Does not examine anything in storage.\n\"\"\"\nimport re\n\nfrom .digest import digest_regex, normalized_digest\nfrom .validation_logger import ValidationLogger\nfrom .w3c_datetime import str_to_datetime\n\n\ndef get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    manifest = inventory['manifest']\n    file_map = {}\n    for digest in state:\n        if digest in manifest:\n            for file in state[digest]:\n                file_map[file] = set(manifest[digest])\n    return file_map\n\n\nclass InventoryValidator():\n    \"\"\"Class for OCFL Inventory Validator.\"\"\"\n\n    def __init__(self, log=None, where='???',\n                 lax_digests=False, spec_version='1.0'):\n        \"\"\"Initialize OCFL Inventory Validator.\"\"\"\n        self.log = ValidationLogger() if log is None else log\n        self.where = where\n        self.spec_version = spec_version\n        # Object state\n        self.inventory = None\n        self.id = None\n        self.digest_algorithm = 'sha512'\n        self.content_directory = 'content'\n        self.all_versions = []\n        self.manifest_files = None\n        self.unnormalized_digests = None\n        self.head = 'UNKNOWN'\n        # Validation control\n        self.lax_digests = lax_digests\n        # Configuration\n        self.spec_versions_supported = ('1.0', '1.1')\n\n    def error(self, code, **args):\n        \"\"\"Error with added context.\"\"\"\n        self.log.error(code, where=self.where, **args)\n\n    def warning(self, code, **args):\n        \"\"\"Warning with added context.\"\"\"\n        self.log.warning(code, where=self.where, **args)\n\n    def validate(self, inventory, extract_spec_version=False):\n        \"\"\"Validate a given inventory.\n\n        If extract_spec_version is True then will look at the type value to determine\n        the specification version. In the case that there is no type value or it isn't\n        valid, then other tests will be based on the version given in self.spec_version.\n        \"\"\"\n        # Basic structure\n        self.inventory = inventory\n        if 'id' in inventory:\n            iid = inventory['id']\n            if not isinstance(iid, str) or iid == '':\n                self.error(\"E037a\")\n            else:\n                # URI syntax https://www.rfc-editor.org/rfc/rfc3986.html#section-3.1 :\n                # scheme = ALPHA *( ALPHA / DIGIT / \"+\" / \"-\" / \".\" )\n                if not re.match(r'''[a-z][a-z\\d\\+\\-\\.]*:.+''', iid, re.IGNORECASE):\n                    self.warning(\"W005\", id=iid)\n                self.id = iid\n        else:\n            self.error(\"E036a\")\n        if 'type' not in inventory:\n            self.error(\"E036b\")\n        elif not isinstance(inventory['type'], str):\n            self.error(\"E999\")\n        elif extract_spec_version:\n            m = re.match(r'''https://ocfl.io/(\\d+.\\d)/spec/#inventory''', inventory['type'])\n            if not m:\n                self.error('E038b', got=inventory['type'], assumed_spec_version=self.spec_version)\n            elif m.group(1) in self.spec_versions_supported:\n                self.spec_version = m.group(1)\n            else:\n                self.error(\"E038c\", got=m.group(1), assumed_spec_version=self.spec_version)\n        elif inventory['type'] != 'https://ocfl.io/' + self.spec_version + '/spec/#inventory':\n            self.error(\"E038a\", expected='https://ocfl.io/' + self.spec_version + '/spec/#inventory', got=inventory['type'])\n        if 'digestAlgorithm' not in inventory:\n            self.error(\"E036c\")\n        elif inventory['digestAlgorithm'] == 'sha512':\n            pass\n        elif self.lax_digests:\n            self.digest_algorithm = inventory['digestAlgorithm']\n        elif inventory['digestAlgorithm'] == 'sha256':\n            self.warning(\"W004\")\n            self.digest_algorithm = inventory['digestAlgorithm']\n        else:\n            self.error(\"E039\", digest_algorithm=inventory['digestAlgorithm'])\n        if 'contentDirectory' in inventory:\n            # Careful only to set self.content_directory if value is safe\n            cd = inventory['contentDirectory']\n            if not isinstance(cd, str) or '/' in cd:\n                self.error(\"E017\")\n            elif cd in ('.', '..'):\n                self.error(\"E018\")\n            else:\n                self.content_directory = cd\n        manifest_files_correct_format = None\n        if 'manifest' not in inventory:\n            self.error(\"E041a\")\n        else:\n            (self.manifest_files, manifest_files_correct_format, self.unnormalized_digests) = self.validate_manifest(inventory['manifest'])\n        digests_used = []\n        if 'versions' not in inventory:\n            self.error(\"E041b\")\n        else:\n            self.all_versions = self.validate_version_sequence(inventory['versions'])\n            digests_used = self.validate_versions(inventory['versions'], self.all_versions, self.unnormalized_digests)\n        if 'head' not in inventory:\n            self.error(\"E036d\")\n        elif len(self.all_versions) > 0:\n            self.head = self.all_versions[-1]\n            if inventory['head'] != self.head:\n                self.error(\"E040\", got=inventory['head'], expected=self.head)\n        if len(self.all_versions) == 0:\n            # Abort tests is we don't have a valid version sequence, otherwise\n            # there will likely be spurious subsequent error reports\n            return\n        if len(self.all_versions) > 0:\n            if manifest_files_correct_format is not None:\n                self.check_content_paths_map_to_versions(manifest_files_correct_format, self.all_versions)\n            if self.manifest_files is not None:\n                self.check_digests_present_and_used(self.manifest_files, digests_used)\n        if 'fixity' in inventory:\n            self.validate_fixity(inventory['fixity'], self.manifest_files)\n\n    def validate_manifest(self, manifest):\n        \"\"\"Validate manifest block in inventory.\n\n        Returns:\n          * manifest_files - a mapping from file to digest for each file in\n              the manifest\n          * manifest_files_correct_format - a simple list of the manifest file\n              path that passed initial checks. They need to be checked for valid\n              version directories later, when we know what version directories\n              are valid\n          * unnormalized_digests - a set of the original digests in unnormalized\n              form that MUST match exactly the values used in state blocks\n        \"\"\"\n        manifest_files = {}\n        manifest_files_correct_format = []\n        unnormalized_digests = set()\n        manifest_digests = set()\n        if not isinstance(manifest, dict):\n            self.error('E041c')\n        else:\n            content_paths = set()\n            content_directories = set()\n            for digest in manifest:\n                m = re.match(self.digest_regex(), digest)\n                if not m:\n                    self.error('E025a', digest=digest, algorithm=self.digest_algorithm)  # wrong form of digest\n                elif not isinstance(manifest[digest], list):\n                    self.error('E092', digest=digest)  # must have path list value\n                else:\n                    unnormalized_digests.add(digest)\n                    norm_digest = normalized_digest(digest, self.digest_algorithm)\n                    if norm_digest in manifest_digests:\n                        # We have already seen this in different un-normalized form!\n                        self.error(\"E096\", digest=norm_digest)\n                    else:\n                        manifest_digests.add(norm_digest)\n                    for file in manifest[digest]:\n                        manifest_files[file] = norm_digest\n                        if self.check_content_path(file, content_paths, content_directories):\n                            manifest_files_correct_format.append(file)\n            # Check for conflicting content paths\n            for path in content_directories:\n                if path in content_paths:\n                    self.error(\"E101b\", path=path)\n        return manifest_files, manifest_files_correct_format, unnormalized_digests\n\n    def validate_fixity(self, fixity, manifest_files):\n        \"\"\"Validate fixity block in inventory.\n\n        Check the structure of the fixity block and makes sure that only files\n        listed in the manifest are referenced.\n        \"\"\"\n        if not isinstance(fixity, dict):\n            # The value of fixity must be a JSON object. In v1.0 I catch not an object\n            # as part of E056 but this was clarified as E111 in v1.1. The value may\n            # be an empty object in either case\n            self.error('E056a' if self.spec_version == '1.0' else 'E111')\n        else:\n            for digest_algorithm in fixity:\n                known_digest = True\n                try:\n                    regex = digest_regex(digest_algorithm)\n                except ValueError:\n                    if not self.lax_digests:\n                        self.error('E056b', algorithm=self.digest_algorithm)\n                        continue\n                    # Match anything\n                    regex = r'''^.*$'''\n                    known_digest = False\n                fixity_algoritm_block = fixity[digest_algorithm]\n                if not isinstance(fixity_algoritm_block, dict):\n                    self.error('E057a', algorithm=self.digest_algorithm)\n                else:\n                    digests_seen = set()\n                    for digest in fixity_algoritm_block:\n                        m = re.match(regex, digest)\n                        if not m:\n                            self.error('E057b', digest=digest, algorithm=digest_algorithm)  # wrong form of digest\n                        elif not isinstance(fixity_algoritm_block[digest], list):\n                            self.error('E057c', digest=digest, algorithm=digest_algorithm)  # must have path list value\n                        else:\n                            if known_digest:\n                                norm_digest = normalized_digest(digest, digest_algorithm)\n                            else:\n                                norm_digest = digest\n                            if norm_digest in digests_seen:\n                                # We have already seen this in different un-normalized form!\n                                self.error(\"E097\", digest=norm_digest, algorithm=digest_algorithm)\n                            else:\n                                digests_seen.add(norm_digest)\n                            for file in fixity_algoritm_block[digest]:\n                                if file not in manifest_files:\n                                    self.error(\"E057d\", digest=norm_digest, algorithm=digest_algorithm, path=file)\n\n    def validate_version_sequence(self, versions):\n        \"\"\"Validate sequence of version names in versions block in inventory.\n\n        Returns an array of in-sequence version directories that are part\n        of a valid sequences. May exclude other version directory names that are\n        not part of the valid sequence if an error is thrown.\n        \"\"\"\n        all_versions = []\n        if not isinstance(versions, dict):\n            self.error(\"E044\")\n            return all_versions\n        if len(versions) == 0:\n            self.error(\"E008\")\n            return all_versions\n        # Validate version sequence\n        # https://ocfl.io/draft/spec/#version-directories\n        zero_padded = None\n        max_version_num = 999999  # Excessive limit\n        if 'v1' in versions:\n            fmt = 'v%d'\n            zero_padded = False\n            all_versions.append('v1')\n        else:  # Find padding size\n            for n in range(2, 11):\n                fmt = 'v%0' + str(n) + 'd'\n                vkey = fmt % 1\n                if vkey in versions:\n                    all_versions.append(vkey)\n                    zero_padded = n\n                    max_version_num = (10 ** (n - 1)) - 1\n                    break\n            if not zero_padded:\n                self.error(\"E009\")\n                return all_versions\n        if zero_padded:\n            self.warning(\"W001\")\n        # Have v1 and know format, work through to check sequence\n        for n in range(2, max_version_num + 1):\n            v = (fmt % n)\n            if v in versions:\n                all_versions.append(v)\n            else:\n                if len(versions) != (n - 1):\n                    self.error(\"E010\")  # Extra version dirs outside sequence\n                return all_versions\n        # We have now included all possible versions up to the zero padding\n        # size, if there are more versions than this number then we must\n        # have extra that violate the zero-padding rule or are out of\n        # sequence\n        if len(versions) > max_version_num:\n            self.error(\"E011\")\n        return all_versions\n\n    def validate_versions(self, versions, all_versions, unnormalized_digests):\n        \"\"\"Validate versions blocks in inventory.\n\n        Requires as input two things which are assumed to be structurally correct\n        from prior basic validation:\n\n          * versions - which is the JSON object (dict) from the inventory\n          * all_versions - an ordered list of the versions to look at in versions\n                           (all other keys in versions will be ignored)\n\n        Returns a list of digests_used which can then be checked against the\n        manifest.\n        \"\"\"\n        digests_used = []\n        for v in all_versions:\n            version = versions[v]\n            if 'created' not in version:\n                self.error('E048', version=v)  # No created\n            elif not isinstance(versions[v]['created'], str):\n                self.error('E049d', version=v)  # Bad created\n            else:\n                created = versions[v]['created']\n                try:\n                    str_to_datetime(created)  # catch ValueError if fails\n                    if not re.search(r'''(Z|[+-]\\d\\d:\\d\\d)$''', created):  # FIXME - kludge\n                        self.error('E049a', version=v)\n                    if not re.search(r'''T\\d\\d:\\d\\d:\\d\\d''', created):  # FIXME - kludge\n                        self.error('E049b', version=v)\n                except ValueError as e:\n                    self.error('E049c', version=v, description=str(e))\n            if 'state' in version:\n                digests_used += self.validate_state_block(version['state'], version=v, unnormalized_digests=unnormalized_digests)\n            else:\n                self.error('E048c', version=v)\n            if 'message' not in version:\n                self.warning('W007a', version=v)\n            elif not isinstance(version['message'], str):\n                self.error('E094', version=v)\n            if 'user' not in version:\n                self.warning('W007b', version=v)\n            else:\n                user = version['user']\n                if not isinstance(user, dict):\n                    self.error('E054a', version=v)\n                else:\n                    if 'name' not in user or not isinstance(user['name'], str):\n                        self.error('E054b', version=v)\n                    if 'address' not in user:\n                        self.warning('W008', version=v)\n                    elif not isinstance(user['address'], str):\n                        self.error('E054c', version=v)\n                    elif not re.match(r'''\\w{3,6}:''', user['address']):\n                        self.warning('W009', version=v)\n        return digests_used\n\n    def validate_state_block(self, state, version, unnormalized_digests):\n        \"\"\"Validate state block in a version in an inventory.\n\n        The version is used only for error reporting.\n\n        Returns a list of content digests referenced in the state block.\n        \"\"\"\n        digests = []\n        logical_paths = set()\n        logical_directories = set()\n        if not isinstance(state, dict):\n            self.error('E050c', version=version)\n        else:\n            digest_re = re.compile(self.digest_regex())\n            for digest in state:\n                if not digest_re.match(digest):\n                    self.error('E050d', version=version, digest=digest)\n                elif not isinstance(state[digest], list):\n                    self.error('E050e', version=version, digest=digest)\n                else:\n                    for path in state[digest]:\n                        if path in logical_paths:\n                            self.error(\"E095a\", version=version, path=path)\n                        else:\n                            self.check_logical_path(path, version, logical_paths, logical_directories)\n                    if digest not in unnormalized_digests:\n                        # Exact string value must match, not just normalized\n                        self.error(\"E050f\", version=version, digest=digest)\n                    norm_digest = normalized_digest(digest, self.digest_algorithm)\n                    digests.append(norm_digest)\n            # Check for conflicting logical paths\n            for path in logical_directories:\n                if path in logical_paths:\n                    self.error(\"E095b\", version=version, path=path)\n        return digests\n\n    def check_content_paths_map_to_versions(self, manifest_files, all_versions):\n        \"\"\"Check that every content path starts with a valid version.\n\n        The content directory component has already been checked in\n        check_content_path(). We have already tested all paths enough\n        to know that they can be split into at least 2 components.\n        \"\"\"\n        for path in manifest_files:\n            version_dir, dummy_rest = path.split('/', 1)\n            if version_dir not in all_versions:\n                self.error('E042b', path=path)\n\n    def check_digests_present_and_used(self, manifest_files, digests_used):\n        \"\"\"Check all digests in manifest that are needed are present and used.\"\"\"\n        in_manifest = set(manifest_files.values())\n        in_state = set(digests_used)\n        not_in_manifest = in_state.difference(in_manifest)\n        if len(not_in_manifest) > 0:\n            self.error(\"E050a\", digests=\", \".join(sorted(not_in_manifest)))\n        not_in_state = in_manifest.difference(in_state)\n        if len(not_in_state) > 0:\n            self.error(\"E107\", digests=\", \".join(sorted(not_in_state)))\n\n    def digest_regex(self):\n        \"\"\"Return regex for validating un-normalized digest format.\"\"\"\n        try:\n            return digest_regex(self.digest_algorithm)\n        except ValueError:\n            if not self.lax_digests:\n                self.error('E026a', digest=self.digest_algorithm)\n        # Match anything\n        return r'''^.*$'''\n\n    def check_logical_path(self, path, version, logical_paths, logical_directories):\n        \"\"\"Check logical path and accumulate paths/directories for E095b check.\n\n        logical_paths and logical_directories are expected to be sets.\n\n        Only adds good paths to the accumulated paths/directories.\n        \"\"\"\n        if path.startswith('/') or path.endswith('/'):\n            self.error(\"E053\", version=version, path=path)\n        else:\n            elements = path.split('/')\n            for element in elements:\n                if element in ['.', '..', '']:\n                    self.error(\"E052\", version=version, path=path)\n                    return\n            # Accumulate paths and directories\n            logical_paths.add(path)\n            logical_directories.add('/'.join(elements[0:-1]))\n\n    def check_content_path(self, path, content_paths, content_directories):\n        \"\"\"Check logical path and accumulate paths/directories for E101 check.\n\n        Returns True if valid, else False. Only adds good paths to the\n        accumulated paths/directories. We don't yet know the set of valid\n        version directories so the check here is just for 'v' + digits.\n        \"\"\"\n        if path.startswith('/') or path.endswith('/'):\n            self.error(\"E100\", path=path)\n            return False\n        m = re.match(r'''^(v\\d+/''' + self.content_directory + r''')/(.+)''', path)\n        if not m:\n            self.error(\"E042a\", path=path)\n            return False\n        elements = m.group(2).split('/')\n        for element in elements:\n            if element in ('', '.', '..'):\n                self.error(\"E099\", path=path)\n                return False\n        # Accumulate paths and directories if not seen before\n        if path in content_paths:\n            self.error(\"E101a\", path=path)\n            return False\n        content_paths.add(path)\n        content_directories.add('/'.join([m.group(1)] + elements[0:-1]))\n        return True\n\n    def validate_as_prior_version(self, prior):\n        \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n        The input variable prior is also expected to be an InventoryValidator object\n        and both self and prior inventories are assumed to have been checked for\n        internal consistency.\n        \"\"\"\n        # Must have a subset of versions which also checks zero padding format etc.\n        if not set(prior.all_versions) < set(self.all_versions):\n            self.error('E066a', prior_head=prior.head)\n        else:\n            # Check references to files but realize that there might be different\n            # digest algorithms between versions\n            version = 'no-version'\n            for version in prior.all_versions:\n                # If the digest algorithm is the same then we can make a\n                # direct check on whether the state blocks match\n                if prior.digest_algorithm == self.digest_algorithm:\n                    self.compare_states_for_version(prior, version)\n                # Now check the mappings from state to logical path, which must\n                # be consistent even if the digestAlgorithm is different between\n                # versions. Get maps from logical paths to files on disk:\n                prior_map = get_logical_path_map(prior.inventory, version)\n                self_map = get_logical_path_map(self.inventory, version)\n                # Look first for differences in logical paths listed\n                only_in_prior = prior_map.keys() - self_map.keys()\n                only_in_self = self_map.keys() - prior_map.keys()\n                if only_in_prior or only_in_self:\n                    if only_in_prior:\n                        self.error('E066b', version=version, prior_head=prior.head, only_in=prior.head, logical_paths=','.join(only_in_prior))\n                    if only_in_self:\n                        self.error('E066b', version=version, prior_head=prior.head, only_in=self.where, logical_paths=','.join(only_in_self))\n                else:\n                    # Check them all in details - digests must match\n                    for logical_path, this_map in prior_map.items():\n                        if not this_map.issubset(self_map[logical_path]):\n                            self.error('E066c', version=version, prior_head=prior.head,\n                                       logical_path=logical_path, prior_content=','.join(this_map),\n                                       current_content=','.join(self_map[logical_path]))\n                # Check metadata\n                prior_version = prior.inventory['versions'][version]\n                self_version = self.inventory['versions'][version]\n                for key in ('created', 'message', 'user'):\n                    if prior_version.get(key) != self_version.get(key):\n                        self.warning('W011', version=version, prior_head=prior.head, key=key)\n\n    def compare_states_for_version(self, prior, version):\n        \"\"\"Compare state blocks for version between self and prior.\n\n        Assumes the same digest algorithm in both, do not call otherwise!\n\n        Looks only for digests that appear in one but not in the other, the code\n        in validate_as_prior_version(..) does a check for whether the same sets\n        of logical files appear and we don't want to duplicate an error message\n        about that.\n\n        While the mapping checks in validate_as_prior_version(..) do all that is\n        necessary to detect an error, the additional errors that may be generated\n        here provide more detailed diagnostics in the case that the digest\n        algorithm is the same across versions being compared.\n        \"\"\"\n        self_state = self.inventory['versions'][version]['state']\n        prior_state = prior.inventory['versions'][version]['state']\n        for digest in set(self_state.keys()).union(prior_state.keys()):\n            if digest not in prior_state:\n                self.error('E066d', version=version, prior_head=prior.head,\n                           digest=digest, logical_files=', '.join(self_state[digest]))\n            elif digest not in self_state:\n                self.error('E066e', version=version, prior_head=prior.head,\n                           digest=digest, logical_files=', '.join(prior_state[digest]))\n",
            "file_path": "ocfl/inventory_validator.py",
            "human_label": "Check that prior is a valid prior version of the current inventory object. The input variable prior is also expected to be an InventoryValidator object and both self and prior inventories are assumed to have been checked for internal consistency. Return error() in the class.",
            "level": "file_runnable",
            "lineno": "463",
            "name": "validate_as_prior_version",
            "oracle_context": "{ \"apis\" : \"['warning', 'compare_states_for_version', 'set', 'items', 'join', 'error', 'get', 'keys', 'issubset', 'get_logical_path_map']\", \"classes\" : \"[]\", \"vars\" : \"['Str', 'all_versions', 'this_map', 'digest_algorithm', 'where', 'inventory', 'head']\" }",
            "package": "inventory_validator",
            "project": "zimeon/ocfl-py",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b45e175108cfac7f210a19",
            "all_context": "{ \"import\" : \"re digest \", \"file\" : \"\", \"class\" : \"self.validate_manifest ; self.check_digests_present_and_used(self,manifest_files,digests_used) ; self.where ; self.check_digests_present_and_used ; self.validate_state_block(self,state,version,unnormalized_digests) ; self.manifest_files ; self.digest_regex(self) ; self.error ; self.validate_version_sequence ; self.check_content_paths_map_to_versions ; self.check_content_path ; self.check_logical_path(self,path,version,logical_paths,logical_directories) ; self.check_logical_path ; self.spec_versions_supported ; self.check_content_paths_map_to_versions(self,manifest_files,all_versions) ; self.head ; self.check_content_path(self,path,content_paths,content_directories) ; self.validate(self,inventory,extract_spec_version) ; self.validate_fixity(self,fixity,manifest_files) ; self.__init__(self,log,where,lax_digests,spec_version) ; self.validate_as_prior_version(self,prior) ; self.compare_states_for_version(self,prior,version) ; self.content_directory ; self.warning ; self.validate_state_block ; self.warning(self,code) ; self.error(self,code) ; self.unnormalized_digests ; self.id ; self.lax_digests ; self.log ; self.digest_algorithm ; self.validate_fixity ; self.digest_regex ; self.spec_version ; self.validate_versions ; self.validate_versions(self,versions,all_versions,unnormalized_digests) ; self.inventory ; self.compare_states_for_version ; self.all_versions ; self.validate_manifest(self,manifest) ; self.validate_version_sequence(self,versions) ; \" }",
            "code": "    def validate_fixity(self, fixity, manifest_files):\n        \"\"\"Validate fixity block in inventory.\n\n        Check the structure of the fixity block and makes sure that only files\n        listed in the manifest are referenced.\n        \"\"\"\n        if not isinstance(fixity, dict):\n            # The value of fixity must be a JSON object. In v1.0 I catch not an object\n            # as part of E056 but this was clarified as E111 in v1.1. The value may\n            # be an empty object in either case\n            self.error('E056a' if self.spec_version == '1.0' else 'E111')\n        else:\n            for digest_algorithm in fixity:\n                known_digest = True\n                try:\n                    regex = digest_regex(digest_algorithm)\n                except ValueError:\n                    if not self.lax_digests:\n                        self.error('E056b', algorithm=self.digest_algorithm)\n                        continue\n                    # Match anything\n                    regex = r'''^.*$'''\n                    known_digest = False\n                fixity_algoritm_block = fixity[digest_algorithm]\n                if not isinstance(fixity_algoritm_block, dict):\n                    self.error('E057a', algorithm=self.digest_algorithm)\n                else:\n                    digests_seen = set()\n                    for digest in fixity_algoritm_block:\n                        m = re.match(regex, digest)\n                        if not m:\n                            self.error('E057b', digest=digest, algorithm=digest_algorithm)  # wrong form of digest\n                        elif not isinstance(fixity_algoritm_block[digest], list):\n                            self.error('E057c', digest=digest, algorithm=digest_algorithm)  # must have path list value\n                        else:\n                            if known_digest:\n                                norm_digest = normalized_digest(digest, digest_algorithm)\n                            else:\n                                norm_digest = digest\n                            if norm_digest in digests_seen:\n                                # We have already seen this in different un-normalized form!\n                                self.error(\"E097\", digest=norm_digest, algorithm=digest_algorithm)\n                            else:\n                                digests_seen.add(norm_digest)\n                            for file in fixity_algoritm_block[digest]:\n                                if file not in manifest_files:\n                                    self.error(\"E057d\", digest=norm_digest, algorithm=digest_algorithm, path=file)\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : true, \"public_lib\" : true, \"current_class\" : true, \"current_file\" : false, \"current_project\" : false, \"external\" : false }",
            "docstring": "Validate fixity block in inventory.\n\nCheck the structure of the fixity block and makes sure that only files\nlisted in the manifest are referenced.",
            "end_lineno": "238",
            "file_content": "\"\"\"OCFL Inventory Validator.\n\nCode to validate the Python representation of an OCFL Inventory\nas read with json.load(). Does not examine anything in storage.\n\"\"\"\nimport re\n\nfrom .digest import digest_regex, normalized_digest\nfrom .validation_logger import ValidationLogger\nfrom .w3c_datetime import str_to_datetime\n\n\ndef get_logical_path_map(inventory, version):\n    \"\"\"Get a map of logical paths in state to files on disk for version in inventory.\n\n    Returns a dictionary: logical_path_in_state -> set(content_files)\n\n    The set of content_files may includes references to duplicate files in\n    later versions than the version being described.\n    \"\"\"\n    state = inventory['versions'][version]['state']\n    manifest = inventory['manifest']\n    file_map = {}\n    for digest in state:\n        if digest in manifest:\n            for file in state[digest]:\n                file_map[file] = set(manifest[digest])\n    return file_map\n\n\nclass InventoryValidator():\n    \"\"\"Class for OCFL Inventory Validator.\"\"\"\n\n    def __init__(self, log=None, where='???',\n                 lax_digests=False, spec_version='1.0'):\n        \"\"\"Initialize OCFL Inventory Validator.\"\"\"\n        self.log = ValidationLogger() if log is None else log\n        self.where = where\n        self.spec_version = spec_version\n        # Object state\n        self.inventory = None\n        self.id = None\n        self.digest_algorithm = 'sha512'\n        self.content_directory = 'content'\n        self.all_versions = []\n        self.manifest_files = None\n        self.unnormalized_digests = None\n        self.head = 'UNKNOWN'\n        # Validation control\n        self.lax_digests = lax_digests\n        # Configuration\n        self.spec_versions_supported = ('1.0', '1.1')\n\n    def error(self, code, **args):\n        \"\"\"Error with added context.\"\"\"\n        self.log.error(code, where=self.where, **args)\n\n    def warning(self, code, **args):\n        \"\"\"Warning with added context.\"\"\"\n        self.log.warning(code, where=self.where, **args)\n\n    def validate(self, inventory, extract_spec_version=False):\n        \"\"\"Validate a given inventory.\n\n        If extract_spec_version is True then will look at the type value to determine\n        the specification version. In the case that there is no type value or it isn't\n        valid, then other tests will be based on the version given in self.spec_version.\n        \"\"\"\n        # Basic structure\n        self.inventory = inventory\n        if 'id' in inventory:\n            iid = inventory['id']\n            if not isinstance(iid, str) or iid == '':\n                self.error(\"E037a\")\n            else:\n                # URI syntax https://www.rfc-editor.org/rfc/rfc3986.html#section-3.1 :\n                # scheme = ALPHA *( ALPHA / DIGIT / \"+\" / \"-\" / \".\" )\n                if not re.match(r'''[a-z][a-z\\d\\+\\-\\.]*:.+''', iid, re.IGNORECASE):\n                    self.warning(\"W005\", id=iid)\n                self.id = iid\n        else:\n            self.error(\"E036a\")\n        if 'type' not in inventory:\n            self.error(\"E036b\")\n        elif not isinstance(inventory['type'], str):\n            self.error(\"E999\")\n        elif extract_spec_version:\n            m = re.match(r'''https://ocfl.io/(\\d+.\\d)/spec/#inventory''', inventory['type'])\n            if not m:\n                self.error('E038b', got=inventory['type'], assumed_spec_version=self.spec_version)\n            elif m.group(1) in self.spec_versions_supported:\n                self.spec_version = m.group(1)\n            else:\n                self.error(\"E038c\", got=m.group(1), assumed_spec_version=self.spec_version)\n        elif inventory['type'] != 'https://ocfl.io/' + self.spec_version + '/spec/#inventory':\n            self.error(\"E038a\", expected='https://ocfl.io/' + self.spec_version + '/spec/#inventory', got=inventory['type'])\n        if 'digestAlgorithm' not in inventory:\n            self.error(\"E036c\")\n        elif inventory['digestAlgorithm'] == 'sha512':\n            pass\n        elif self.lax_digests:\n            self.digest_algorithm = inventory['digestAlgorithm']\n        elif inventory['digestAlgorithm'] == 'sha256':\n            self.warning(\"W004\")\n            self.digest_algorithm = inventory['digestAlgorithm']\n        else:\n            self.error(\"E039\", digest_algorithm=inventory['digestAlgorithm'])\n        if 'contentDirectory' in inventory:\n            # Careful only to set self.content_directory if value is safe\n            cd = inventory['contentDirectory']\n            if not isinstance(cd, str) or '/' in cd:\n                self.error(\"E017\")\n            elif cd in ('.', '..'):\n                self.error(\"E018\")\n            else:\n                self.content_directory = cd\n        manifest_files_correct_format = None\n        if 'manifest' not in inventory:\n            self.error(\"E041a\")\n        else:\n            (self.manifest_files, manifest_files_correct_format, self.unnormalized_digests) = self.validate_manifest(inventory['manifest'])\n        digests_used = []\n        if 'versions' not in inventory:\n            self.error(\"E041b\")\n        else:\n            self.all_versions = self.validate_version_sequence(inventory['versions'])\n            digests_used = self.validate_versions(inventory['versions'], self.all_versions, self.unnormalized_digests)\n        if 'head' not in inventory:\n            self.error(\"E036d\")\n        elif len(self.all_versions) > 0:\n            self.head = self.all_versions[-1]\n            if inventory['head'] != self.head:\n                self.error(\"E040\", got=inventory['head'], expected=self.head)\n        if len(self.all_versions) == 0:\n            # Abort tests is we don't have a valid version sequence, otherwise\n            # there will likely be spurious subsequent error reports\n            return\n        if len(self.all_versions) > 0:\n            if manifest_files_correct_format is not None:\n                self.check_content_paths_map_to_versions(manifest_files_correct_format, self.all_versions)\n            if self.manifest_files is not None:\n                self.check_digests_present_and_used(self.manifest_files, digests_used)\n        if 'fixity' in inventory:\n            self.validate_fixity(inventory['fixity'], self.manifest_files)\n\n    def validate_manifest(self, manifest):\n        \"\"\"Validate manifest block in inventory.\n\n        Returns:\n          * manifest_files - a mapping from file to digest for each file in\n              the manifest\n          * manifest_files_correct_format - a simple list of the manifest file\n              path that passed initial checks. They need to be checked for valid\n              version directories later, when we know what version directories\n              are valid\n          * unnormalized_digests - a set of the original digests in unnormalized\n              form that MUST match exactly the values used in state blocks\n        \"\"\"\n        manifest_files = {}\n        manifest_files_correct_format = []\n        unnormalized_digests = set()\n        manifest_digests = set()\n        if not isinstance(manifest, dict):\n            self.error('E041c')\n        else:\n            content_paths = set()\n            content_directories = set()\n            for digest in manifest:\n                m = re.match(self.digest_regex(), digest)\n                if not m:\n                    self.error('E025a', digest=digest, algorithm=self.digest_algorithm)  # wrong form of digest\n                elif not isinstance(manifest[digest], list):\n                    self.error('E092', digest=digest)  # must have path list value\n                else:\n                    unnormalized_digests.add(digest)\n                    norm_digest = normalized_digest(digest, self.digest_algorithm)\n                    if norm_digest in manifest_digests:\n                        # We have already seen this in different un-normalized form!\n                        self.error(\"E096\", digest=norm_digest)\n                    else:\n                        manifest_digests.add(norm_digest)\n                    for file in manifest[digest]:\n                        manifest_files[file] = norm_digest\n                        if self.check_content_path(file, content_paths, content_directories):\n                            manifest_files_correct_format.append(file)\n            # Check for conflicting content paths\n            for path in content_directories:\n                if path in content_paths:\n                    self.error(\"E101b\", path=path)\n        return manifest_files, manifest_files_correct_format, unnormalized_digests\n\n    def validate_fixity(self, fixity, manifest_files):\n        \"\"\"Validate fixity block in inventory.\n\n        Check the structure of the fixity block and makes sure that only files\n        listed in the manifest are referenced.\n        \"\"\"\n        if not isinstance(fixity, dict):\n            # The value of fixity must be a JSON object. In v1.0 I catch not an object\n            # as part of E056 but this was clarified as E111 in v1.1. The value may\n            # be an empty object in either case\n            self.error('E056a' if self.spec_version == '1.0' else 'E111')\n        else:\n            for digest_algorithm in fixity:\n                known_digest = True\n                try:\n                    regex = digest_regex(digest_algorithm)\n                except ValueError:\n                    if not self.lax_digests:\n                        self.error('E056b', algorithm=self.digest_algorithm)\n                        continue\n                    # Match anything\n                    regex = r'''^.*$'''\n                    known_digest = False\n                fixity_algoritm_block = fixity[digest_algorithm]\n                if not isinstance(fixity_algoritm_block, dict):\n                    self.error('E057a', algorithm=self.digest_algorithm)\n                else:\n                    digests_seen = set()\n                    for digest in fixity_algoritm_block:\n                        m = re.match(regex, digest)\n                        if not m:\n                            self.error('E057b', digest=digest, algorithm=digest_algorithm)  # wrong form of digest\n                        elif not isinstance(fixity_algoritm_block[digest], list):\n                            self.error('E057c', digest=digest, algorithm=digest_algorithm)  # must have path list value\n                        else:\n                            if known_digest:\n                                norm_digest = normalized_digest(digest, digest_algorithm)\n                            else:\n                                norm_digest = digest\n                            if norm_digest in digests_seen:\n                                # We have already seen this in different un-normalized form!\n                                self.error(\"E097\", digest=norm_digest, algorithm=digest_algorithm)\n                            else:\n                                digests_seen.add(norm_digest)\n                            for file in fixity_algoritm_block[digest]:\n                                if file not in manifest_files:\n                                    self.error(\"E057d\", digest=norm_digest, algorithm=digest_algorithm, path=file)\n\n    def validate_version_sequence(self, versions):\n        \"\"\"Validate sequence of version names in versions block in inventory.\n\n        Returns an array of in-sequence version directories that are part\n        of a valid sequences. May exclude other version directory names that are\n        not part of the valid sequence if an error is thrown.\n        \"\"\"\n        all_versions = []\n        if not isinstance(versions, dict):\n            self.error(\"E044\")\n            return all_versions\n        if len(versions) == 0:\n            self.error(\"E008\")\n            return all_versions\n        # Validate version sequence\n        # https://ocfl.io/draft/spec/#version-directories\n        zero_padded = None\n        max_version_num = 999999  # Excessive limit\n        if 'v1' in versions:\n            fmt = 'v%d'\n            zero_padded = False\n            all_versions.append('v1')\n        else:  # Find padding size\n            for n in range(2, 11):\n                fmt = 'v%0' + str(n) + 'd'\n                vkey = fmt % 1\n                if vkey in versions:\n                    all_versions.append(vkey)\n                    zero_padded = n\n                    max_version_num = (10 ** (n - 1)) - 1\n                    break\n            if not zero_padded:\n                self.error(\"E009\")\n                return all_versions\n        if zero_padded:\n            self.warning(\"W001\")\n        # Have v1 and know format, work through to check sequence\n        for n in range(2, max_version_num + 1):\n            v = (fmt % n)\n            if v in versions:\n                all_versions.append(v)\n            else:\n                if len(versions) != (n - 1):\n                    self.error(\"E010\")  # Extra version dirs outside sequence\n                return all_versions\n        # We have now included all possible versions up to the zero padding\n        # size, if there are more versions than this number then we must\n        # have extra that violate the zero-padding rule or are out of\n        # sequence\n        if len(versions) > max_version_num:\n            self.error(\"E011\")\n        return all_versions\n\n    def validate_versions(self, versions, all_versions, unnormalized_digests):\n        \"\"\"Validate versions blocks in inventory.\n\n        Requires as input two things which are assumed to be structurally correct\n        from prior basic validation:\n\n          * versions - which is the JSON object (dict) from the inventory\n          * all_versions - an ordered list of the versions to look at in versions\n                           (all other keys in versions will be ignored)\n\n        Returns a list of digests_used which can then be checked against the\n        manifest.\n        \"\"\"\n        digests_used = []\n        for v in all_versions:\n            version = versions[v]\n            if 'created' not in version:\n                self.error('E048', version=v)  # No created\n            elif not isinstance(versions[v]['created'], str):\n                self.error('E049d', version=v)  # Bad created\n            else:\n                created = versions[v]['created']\n                try:\n                    str_to_datetime(created)  # catch ValueError if fails\n                    if not re.search(r'''(Z|[+-]\\d\\d:\\d\\d)$''', created):  # FIXME - kludge\n                        self.error('E049a', version=v)\n                    if not re.search(r'''T\\d\\d:\\d\\d:\\d\\d''', created):  # FIXME - kludge\n                        self.error('E049b', version=v)\n                except ValueError as e:\n                    self.error('E049c', version=v, description=str(e))\n            if 'state' in version:\n                digests_used += self.validate_state_block(version['state'], version=v, unnormalized_digests=unnormalized_digests)\n            else:\n                self.error('E048c', version=v)\n            if 'message' not in version:\n                self.warning('W007a', version=v)\n            elif not isinstance(version['message'], str):\n                self.error('E094', version=v)\n            if 'user' not in version:\n                self.warning('W007b', version=v)\n            else:\n                user = version['user']\n                if not isinstance(user, dict):\n                    self.error('E054a', version=v)\n                else:\n                    if 'name' not in user or not isinstance(user['name'], str):\n                        self.error('E054b', version=v)\n                    if 'address' not in user:\n                        self.warning('W008', version=v)\n                    elif not isinstance(user['address'], str):\n                        self.error('E054c', version=v)\n                    elif not re.match(r'''\\w{3,6}:''', user['address']):\n                        self.warning('W009', version=v)\n        return digests_used\n\n    def validate_state_block(self, state, version, unnormalized_digests):\n        \"\"\"Validate state block in a version in an inventory.\n\n        The version is used only for error reporting.\n\n        Returns a list of content digests referenced in the state block.\n        \"\"\"\n        digests = []\n        logical_paths = set()\n        logical_directories = set()\n        if not isinstance(state, dict):\n            self.error('E050c', version=version)\n        else:\n            digest_re = re.compile(self.digest_regex())\n            for digest in state:\n                if not digest_re.match(digest):\n                    self.error('E050d', version=version, digest=digest)\n                elif not isinstance(state[digest], list):\n                    self.error('E050e', version=version, digest=digest)\n                else:\n                    for path in state[digest]:\n                        if path in logical_paths:\n                            self.error(\"E095a\", version=version, path=path)\n                        else:\n                            self.check_logical_path(path, version, logical_paths, logical_directories)\n                    if digest not in unnormalized_digests:\n                        # Exact string value must match, not just normalized\n                        self.error(\"E050f\", version=version, digest=digest)\n                    norm_digest = normalized_digest(digest, self.digest_algorithm)\n                    digests.append(norm_digest)\n            # Check for conflicting logical paths\n            for path in logical_directories:\n                if path in logical_paths:\n                    self.error(\"E095b\", version=version, path=path)\n        return digests\n\n    def check_content_paths_map_to_versions(self, manifest_files, all_versions):\n        \"\"\"Check that every content path starts with a valid version.\n\n        The content directory component has already been checked in\n        check_content_path(). We have already tested all paths enough\n        to know that they can be split into at least 2 components.\n        \"\"\"\n        for path in manifest_files:\n            version_dir, dummy_rest = path.split('/', 1)\n            if version_dir not in all_versions:\n                self.error('E042b', path=path)\n\n    def check_digests_present_and_used(self, manifest_files, digests_used):\n        \"\"\"Check all digests in manifest that are needed are present and used.\"\"\"\n        in_manifest = set(manifest_files.values())\n        in_state = set(digests_used)\n        not_in_manifest = in_state.difference(in_manifest)\n        if len(not_in_manifest) > 0:\n            self.error(\"E050a\", digests=\", \".join(sorted(not_in_manifest)))\n        not_in_state = in_manifest.difference(in_state)\n        if len(not_in_state) > 0:\n            self.error(\"E107\", digests=\", \".join(sorted(not_in_state)))\n\n    def digest_regex(self):\n        \"\"\"Return regex for validating un-normalized digest format.\"\"\"\n        try:\n            return digest_regex(self.digest_algorithm)\n        except ValueError:\n            if not self.lax_digests:\n                self.error('E026a', digest=self.digest_algorithm)\n        # Match anything\n        return r'''^.*$'''\n\n    def check_logical_path(self, path, version, logical_paths, logical_directories):\n        \"\"\"Check logical path and accumulate paths/directories for E095b check.\n\n        logical_paths and logical_directories are expected to be sets.\n\n        Only adds good paths to the accumulated paths/directories.\n        \"\"\"\n        if path.startswith('/') or path.endswith('/'):\n            self.error(\"E053\", version=version, path=path)\n        else:\n            elements = path.split('/')\n            for element in elements:\n                if element in ['.', '..', '']:\n                    self.error(\"E052\", version=version, path=path)\n                    return\n            # Accumulate paths and directories\n            logical_paths.add(path)\n            logical_directories.add('/'.join(elements[0:-1]))\n\n    def check_content_path(self, path, content_paths, content_directories):\n        \"\"\"Check logical path and accumulate paths/directories for E101 check.\n\n        Returns True if valid, else False. Only adds good paths to the\n        accumulated paths/directories. We don't yet know the set of valid\n        version directories so the check here is just for 'v' + digits.\n        \"\"\"\n        if path.startswith('/') or path.endswith('/'):\n            self.error(\"E100\", path=path)\n            return False\n        m = re.match(r'''^(v\\d+/''' + self.content_directory + r''')/(.+)''', path)\n        if not m:\n            self.error(\"E042a\", path=path)\n            return False\n        elements = m.group(2).split('/')\n        for element in elements:\n            if element in ('', '.', '..'):\n                self.error(\"E099\", path=path)\n                return False\n        # Accumulate paths and directories if not seen before\n        if path in content_paths:\n            self.error(\"E101a\", path=path)\n            return False\n        content_paths.add(path)\n        content_directories.add('/'.join([m.group(1)] + elements[0:-1]))\n        return True\n\n    def validate_as_prior_version(self, prior):\n        \"\"\"Check that prior is a valid prior version of the current inventory object.\n\n        The input variable prior is also expected to be an InventoryValidator object\n        and both self and prior inventories are assumed to have been checked for\n        internal consistency.\n        \"\"\"\n        # Must have a subset of versions which also checks zero padding format etc.\n        if not set(prior.all_versions) < set(self.all_versions):\n            self.error('E066a', prior_head=prior.head)\n        else:\n            # Check references to files but realize that there might be different\n            # digest algorithms between versions\n            version = 'no-version'\n            for version in prior.all_versions:\n                # If the digest algorithm is the same then we can make a\n                # direct check on whether the state blocks match\n                if prior.digest_algorithm == self.digest_algorithm:\n                    self.compare_states_for_version(prior, version)\n                # Now check the mappings from state to logical path, which must\n                # be consistent even if the digestAlgorithm is different between\n                # versions. Get maps from logical paths to files on disk:\n                prior_map = get_logical_path_map(prior.inventory, version)\n                self_map = get_logical_path_map(self.inventory, version)\n                # Look first for differences in logical paths listed\n                only_in_prior = prior_map.keys() - self_map.keys()\n                only_in_self = self_map.keys() - prior_map.keys()\n                if only_in_prior or only_in_self:\n                    if only_in_prior:\n                        self.error('E066b', version=version, prior_head=prior.head, only_in=prior.head, logical_paths=','.join(only_in_prior))\n                    if only_in_self:\n                        self.error('E066b', version=version, prior_head=prior.head, only_in=self.where, logical_paths=','.join(only_in_self))\n                else:\n                    # Check them all in details - digests must match\n                    for logical_path, this_map in prior_map.items():\n                        if not this_map.issubset(self_map[logical_path]):\n                            self.error('E066c', version=version, prior_head=prior.head,\n                                       logical_path=logical_path, prior_content=','.join(this_map),\n                                       current_content=','.join(self_map[logical_path]))\n                # Check metadata\n                prior_version = prior.inventory['versions'][version]\n                self_version = self.inventory['versions'][version]\n                for key in ('created', 'message', 'user'):\n                    if prior_version.get(key) != self_version.get(key):\n                        self.warning('W011', version=version, prior_head=prior.head, key=key)\n\n    def compare_states_for_version(self, prior, version):\n        \"\"\"Compare state blocks for version between self and prior.\n\n        Assumes the same digest algorithm in both, do not call otherwise!\n\n        Looks only for digests that appear in one but not in the other, the code\n        in validate_as_prior_version(..) does a check for whether the same sets\n        of logical files appear and we don't want to duplicate an error message\n        about that.\n\n        While the mapping checks in validate_as_prior_version(..) do all that is\n        necessary to detect an error, the additional errors that may be generated\n        here provide more detailed diagnostics in the case that the digest\n        algorithm is the same across versions being compared.\n        \"\"\"\n        self_state = self.inventory['versions'][version]['state']\n        prior_state = prior.inventory['versions'][version]['state']\n        for digest in set(self_state.keys()).union(prior_state.keys()):\n            if digest not in prior_state:\n                self.error('E066d', version=version, prior_head=prior.head,\n                           digest=digest, logical_files=', '.join(self_state[digest]))\n            elif digest not in self_state:\n                self.error('E066e', version=version, prior_head=prior.head,\n                           digest=digest, logical_files=', '.join(prior_state[digest]))\n",
            "file_path": "ocfl/inventory_validator.py",
            "human_label": "Validate fixity block in inventory. Check the structure of the fixity block and makes sure that only files listed in the manifest are referenced. Return error() in the class.",
            "level": "class_runnable",
            "lineno": "192",
            "name": "validate_fixity",
            "oracle_context": "{ \"apis\" : \"['set', 'isinstance', 'error', 'digest_regex', 'add', 'normalized_digest', 'match']\", \"classes\" : \"['digest_regex', 'normalized_digest', 're']\", \"vars\" : \"['lax_digests', 'spec_version', 'digest_algorithm']\" }",
            "package": "inventory_validator",
            "project": "zimeon/ocfl-py",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b463153879012d1948149a",
            "all_context": "{ \"import\" : \"os logging zipfile logging packtools \", \"file\" : \"logger ; select_filenames_by_prefix(prefix,files) ; match_file_by_prefix(prefix,file_path) ; explore_source(source) ; _explore_folder(folder) ; _explore_zipfile(zip_path) ; _group_files_by_xml_filename(source,xmls,files) ; _eval_file(prefix,file_path) ; \", \"class\" : \"\" }",
            "code": "def _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    docs = {}\n    for xml in xmls:\n        basename = os.path.basename(xml)\n        prefix, ext = os.path.splitext(basename)\n\n        docs.setdefault(prefix, Package(source, prefix))\n\n        # XML\n        docs[prefix].xml = xml\n\n        for file in select_filenames_by_prefix(prefix, files):\n            # avalia arquivo do pacote, se é asset ou rendition\n            component = _eval_file(prefix, file)\n            if not component:\n                continue\n\n            # resultado do avaliação do pacote\n            ftype = component.get(\"ftype\")\n            file_path = component[\"file_path\"]\n            comp_id = component[\"component_id\"]\n\n            if ftype:\n                docs[prefix].add_asset(comp_id, file_path)\n            else:\n                docs[prefix].add_rendition(comp_id, file_path)\n            files.remove(file)\n    return docs\n",
            "dependency": "{ \"builtin\" : false, \"standard_lib\" : true, \"public_lib\" : false, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Group files by their XML basename\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nxml_filename : str\n    XML filenames\nfiles : list\n    list of files in the folder or zipfile\n\nReturns\n-------\ndict\n    key: name of the XML files\n    value: Package",
            "end_lineno": "239",
            "file_content": "import logging\nimport os\n\nfrom packtools import file_utils\nfrom zipfile import ZipFile\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Package:\n    def __init__(self, source, name):\n        self._source = source\n        self._xml = None\n        self._assets = {}\n        self._renditions = {}\n        self._name = name\n        self.zip_file_path = file_utils.is_zipfile(source) and source\n\n    @property\n    def assets(self):\n        return self._assets\n\n    @property\n    def name(self):\n        return self._name\n\n    def file_path(self, file_path):\n        if file_utils.is_folder(self._source):\n            return os.path.join(self._source, file_path)\n        return file_path\n\n    def add_asset(self, basename, file_path):\n        \"\"\"\n        \"{\n            \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n            \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n            \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n        }\n        \"\"\"\n        self._assets[basename] = self.file_path(file_path)\n\n    def get_asset(self, basename):\n        try:\n            return self._assets[basename]\n        except KeyError:\n            return\n\n    def add_rendition(self, lang, file_path):\n        \"\"\"\n        {\n            \"original\": \"artigo02.pdf\",\n            \"en\": \"artigo02-en.pdf\",\n        }\n        \"\"\"\n        self._renditions[lang] = self.file_path(file_path)\n\n    def get_rendition(self, lang):\n        try:\n            return self._renditions[lang]\n        except KeyError:\n            return\n\n    @property\n    def source(self):\n        return self._source\n\n    @property\n    def xml(self):\n        return self.file_path(self._xml)\n\n    @xml.setter\n    def xml(self, value):\n        self._xml = value\n\n    @property\n    def renditions(self):\n        return self._renditions\n\n    @property\n    def xml_content(self):\n        if file_utils.is_folder(self._source):\n            with open(self.xml, \"rb\") as fp:\n                return fp.read()\n        with ZipFile(self._source) as zf:\n            return zf.read(self.xml)\n\n\ndef select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [\n        item\n        for item in files\n        if match_file_by_prefix(prefix, item)\n    ]\n\n\ndef match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorna `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    basename = os.path.basename(file_path)\n    if basename.startswith(prefix + \"-\"):\n        return True\n    if basename.startswith(prefix + \".\"):\n        return True\n    return False\n\n\ndef explore_source(source):\n    packages = _explore_zipfile(source)\n    if not packages:\n        packages = _explore_folder(source)\n    if not packages:\n        raise ValueError(\"%s: Invalid value for `source`\" % source)\n    return packages\n\n\ndef _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    if file_utils.is_folder(folder):\n        data = _group_files_by_xml_filename(\n            folder,\n            file_utils.xml_files_list(folder),\n            file_utils.files_list(folder),\n        )\n        return data\n\n\ndef _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    if file_utils.is_zipfile(zip_path):\n        with ZipFile(zip_path, 'r'):\n            data = _group_files_by_xml_filename(\n                zip_path,\n                file_utils.xml_files_list_from_zipfile(zip_path),\n                file_utils.files_list_from_zipfile(zip_path),\n            )\n            return data\n\n\ndef _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    docs = {}\n    for xml in xmls:\n        basename = os.path.basename(xml)\n        prefix, ext = os.path.splitext(basename)\n\n        docs.setdefault(prefix, Package(source, prefix))\n\n        # XML\n        docs[prefix].xml = xml\n\n        for file in select_filenames_by_prefix(prefix, files):\n            # avalia arquivo do pacote, se é asset ou rendition\n            component = _eval_file(prefix, file)\n            if not component:\n                continue\n\n            # resultado do avaliação do pacote\n            ftype = component.get(\"ftype\")\n            file_path = component[\"file_path\"]\n            comp_id = component[\"component_id\"]\n\n            if ftype:\n                docs[prefix].add_asset(comp_id, file_path)\n            else:\n                docs[prefix].add_rendition(comp_id, file_path)\n            files.remove(file)\n    return docs\n\n\ndef _eval_file(prefix, file_path):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereço do arquivo em análise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extensão\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    if not match_file_by_prefix(prefix, file_path):\n        # ignore files which name does not match\n        return\n    if file_path.endswith(\".xml\"):\n        # ignore XML files\n        return\n\n    # it matches\n    filename = os.path.basename(file_path)\n    fname, ext = os.path.splitext(filename)\n\n    lang = None\n    if ext == \".pdf\":\n        suffix = fname.replace(prefix, \"\")\n        if fname == prefix:\n            lang = \"original\"\n        elif len(suffix) == 3 and suffix[0] == \"-\":\n            # it is a rendition\n            lang = suffix[1:]\n\n    if lang:\n        return dict(\n            component_id=lang,\n            file_path=file_path,\n        )\n    else:\n        return dict(\n            component_id=filename,\n            component_name=fname,\n            ftype=ext[1:],\n            file_path=file_path,\n        )\n\n",
            "file_path": "packtools/sps/models/packages.py",
            "human_label": "Groups files by xmls and returns data in dict format.",
            "level": "file_runnable",
            "lineno": "194",
            "name": "_group_files_by_xml_filename",
            "oracle_context": "{ \"apis\" : \"['add_asset', 'select_filenames_by_prefix', 'basename', 'get', 'splitext', 'setdefault', 'remove', '_eval_file', 'add_rendition']\", \"classes\" : \"['Package', 'os']\", \"vars\" : \"['path', 'xml']\" }",
            "package": "packages",
            "project": "scieloorg/packtools",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b463153879012d1948149c",
            "all_context": "{ \"import\" : \"os logging zipfile logging packtools \", \"file\" : \"logger ; select_filenames_by_prefix(prefix,files) ; match_file_by_prefix(prefix,file_path) ; explore_source(source) ; _explore_folder(folder) ; _explore_zipfile(zip_path) ; _group_files_by_xml_filename(source,xmls,files) ; _eval_file(prefix,file_path) ; \", \"class\" : \"\" }",
            "code": "def select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [\n        item\n        for item in files\n        if match_file_by_prefix(prefix, item)\n    ]\n",
            "dependency": "{ \"builtin\" : false, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Get files which belongs to a document package.\n\nRetorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\nParameters\n----------\nprefix : str\n    Filename prefix\nfiles : str list\n    Files paths\nReturns\n-------\nlist\n    files paths which basename files matches to prefix",
            "end_lineno": "110",
            "file_content": "import logging\nimport os\n\nfrom packtools import file_utils\nfrom zipfile import ZipFile\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Package:\n    def __init__(self, source, name):\n        self._source = source\n        self._xml = None\n        self._assets = {}\n        self._renditions = {}\n        self._name = name\n        self.zip_file_path = file_utils.is_zipfile(source) and source\n\n    @property\n    def assets(self):\n        return self._assets\n\n    @property\n    def name(self):\n        return self._name\n\n    def file_path(self, file_path):\n        if file_utils.is_folder(self._source):\n            return os.path.join(self._source, file_path)\n        return file_path\n\n    def add_asset(self, basename, file_path):\n        \"\"\"\n        \"{\n            \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n            \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n            \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n        }\n        \"\"\"\n        self._assets[basename] = self.file_path(file_path)\n\n    def get_asset(self, basename):\n        try:\n            return self._assets[basename]\n        except KeyError:\n            return\n\n    def add_rendition(self, lang, file_path):\n        \"\"\"\n        {\n            \"original\": \"artigo02.pdf\",\n            \"en\": \"artigo02-en.pdf\",\n        }\n        \"\"\"\n        self._renditions[lang] = self.file_path(file_path)\n\n    def get_rendition(self, lang):\n        try:\n            return self._renditions[lang]\n        except KeyError:\n            return\n\n    @property\n    def source(self):\n        return self._source\n\n    @property\n    def xml(self):\n        return self.file_path(self._xml)\n\n    @xml.setter\n    def xml(self, value):\n        self._xml = value\n\n    @property\n    def renditions(self):\n        return self._renditions\n\n    @property\n    def xml_content(self):\n        if file_utils.is_folder(self._source):\n            with open(self.xml, \"rb\") as fp:\n                return fp.read()\n        with ZipFile(self._source) as zf:\n            return zf.read(self.xml)\n\n\ndef select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [\n        item\n        for item in files\n        if match_file_by_prefix(prefix, item)\n    ]\n\n\ndef match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorna `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    basename = os.path.basename(file_path)\n    if basename.startswith(prefix + \"-\"):\n        return True\n    if basename.startswith(prefix + \".\"):\n        return True\n    return False\n\n\ndef explore_source(source):\n    packages = _explore_zipfile(source)\n    if not packages:\n        packages = _explore_folder(source)\n    if not packages:\n        raise ValueError(\"%s: Invalid value for `source`\" % source)\n    return packages\n\n\ndef _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    if file_utils.is_folder(folder):\n        data = _group_files_by_xml_filename(\n            folder,\n            file_utils.xml_files_list(folder),\n            file_utils.files_list(folder),\n        )\n        return data\n\n\ndef _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    if file_utils.is_zipfile(zip_path):\n        with ZipFile(zip_path, 'r'):\n            data = _group_files_by_xml_filename(\n                zip_path,\n                file_utils.xml_files_list_from_zipfile(zip_path),\n                file_utils.files_list_from_zipfile(zip_path),\n            )\n            return data\n\n\ndef _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    docs = {}\n    for xml in xmls:\n        basename = os.path.basename(xml)\n        prefix, ext = os.path.splitext(basename)\n\n        docs.setdefault(prefix, Package(source, prefix))\n\n        # XML\n        docs[prefix].xml = xml\n\n        for file in select_filenames_by_prefix(prefix, files):\n            # avalia arquivo do pacote, se é asset ou rendition\n            component = _eval_file(prefix, file)\n            if not component:\n                continue\n\n            # resultado do avaliação do pacote\n            ftype = component.get(\"ftype\")\n            file_path = component[\"file_path\"]\n            comp_id = component[\"component_id\"]\n\n            if ftype:\n                docs[prefix].add_asset(comp_id, file_path)\n            else:\n                docs[prefix].add_rendition(comp_id, file_path)\n            files.remove(file)\n    return docs\n\n\ndef _eval_file(prefix, file_path):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereço do arquivo em análise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extensão\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    if not match_file_by_prefix(prefix, file_path):\n        # ignore files which name does not match\n        return\n    if file_path.endswith(\".xml\"):\n        # ignore XML files\n        return\n\n    # it matches\n    filename = os.path.basename(file_path)\n    fname, ext = os.path.splitext(filename)\n\n    lang = None\n    if ext == \".pdf\":\n        suffix = fname.replace(prefix, \"\")\n        if fname == prefix:\n            lang = \"original\"\n        elif len(suffix) == 3 and suffix[0] == \"-\":\n            # it is a rendition\n            lang = suffix[1:]\n\n    if lang:\n        return dict(\n            component_id=lang,\n            file_path=file_path,\n        )\n    else:\n        return dict(\n            component_id=filename,\n            component_name=fname,\n            ftype=ext[1:],\n            file_path=file_path,\n        )\n\n",
            "file_path": "packtools/sps/models/packages.py",
            "human_label": "For each file in files, return all files taht match the given prefix",
            "level": "file_runnable",
            "lineno": "89",
            "name": "select_filenames_by_prefix",
            "oracle_context": "{ \"apis\" : \"['match_file_by_prefix']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }",
            "package": "packages",
            "project": "scieloorg/packtools",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b463153879012d1948149d",
            "all_context": "{ \"import\" : \"os logging zipfile logging packtools \", \"file\" : \"logger ; select_filenames_by_prefix(prefix,files) ; match_file_by_prefix(prefix,file_path) ; explore_source(source) ; _explore_folder(folder) ; _explore_zipfile(zip_path) ; _group_files_by_xml_filename(source,xmls,files) ; _eval_file(prefix,file_path) ; \", \"class\" : \"\" }",
            "code": "def _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    if file_utils.is_folder(folder):\n        data = _group_files_by_xml_filename(\n            folder,\n            file_utils.xml_files_list(folder),\n            file_utils.files_list(folder),\n        )\n        return data\n",
            "dependency": "{ \"builtin\" : false, \"standard_lib\" : false, \"public_lib\" : true, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Get packages' data from folder\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nfolder : str\n    Folder of the package\nReturns\n-------\ndict",
            "end_lineno": "167",
            "file_content": "import logging\nimport os\n\nfrom packtools import file_utils\nfrom zipfile import ZipFile\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Package:\n    def __init__(self, source, name):\n        self._source = source\n        self._xml = None\n        self._assets = {}\n        self._renditions = {}\n        self._name = name\n        self.zip_file_path = file_utils.is_zipfile(source) and source\n\n    @property\n    def assets(self):\n        return self._assets\n\n    @property\n    def name(self):\n        return self._name\n\n    def file_path(self, file_path):\n        if file_utils.is_folder(self._source):\n            return os.path.join(self._source, file_path)\n        return file_path\n\n    def add_asset(self, basename, file_path):\n        \"\"\"\n        \"{\n            \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n            \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n            \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n        }\n        \"\"\"\n        self._assets[basename] = self.file_path(file_path)\n\n    def get_asset(self, basename):\n        try:\n            return self._assets[basename]\n        except KeyError:\n            return\n\n    def add_rendition(self, lang, file_path):\n        \"\"\"\n        {\n            \"original\": \"artigo02.pdf\",\n            \"en\": \"artigo02-en.pdf\",\n        }\n        \"\"\"\n        self._renditions[lang] = self.file_path(file_path)\n\n    def get_rendition(self, lang):\n        try:\n            return self._renditions[lang]\n        except KeyError:\n            return\n\n    @property\n    def source(self):\n        return self._source\n\n    @property\n    def xml(self):\n        return self.file_path(self._xml)\n\n    @xml.setter\n    def xml(self, value):\n        self._xml = value\n\n    @property\n    def renditions(self):\n        return self._renditions\n\n    @property\n    def xml_content(self):\n        if file_utils.is_folder(self._source):\n            with open(self.xml, \"rb\") as fp:\n                return fp.read()\n        with ZipFile(self._source) as zf:\n            return zf.read(self.xml)\n\n\ndef select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [\n        item\n        for item in files\n        if match_file_by_prefix(prefix, item)\n    ]\n\n\ndef match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorna `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    basename = os.path.basename(file_path)\n    if basename.startswith(prefix + \"-\"):\n        return True\n    if basename.startswith(prefix + \".\"):\n        return True\n    return False\n\n\ndef explore_source(source):\n    packages = _explore_zipfile(source)\n    if not packages:\n        packages = _explore_folder(source)\n    if not packages:\n        raise ValueError(\"%s: Invalid value for `source`\" % source)\n    return packages\n\n\ndef _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    if file_utils.is_folder(folder):\n        data = _group_files_by_xml_filename(\n            folder,\n            file_utils.xml_files_list(folder),\n            file_utils.files_list(folder),\n        )\n        return data\n\n\ndef _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    if file_utils.is_zipfile(zip_path):\n        with ZipFile(zip_path, 'r'):\n            data = _group_files_by_xml_filename(\n                zip_path,\n                file_utils.xml_files_list_from_zipfile(zip_path),\n                file_utils.files_list_from_zipfile(zip_path),\n            )\n            return data\n\n\ndef _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    docs = {}\n    for xml in xmls:\n        basename = os.path.basename(xml)\n        prefix, ext = os.path.splitext(basename)\n\n        docs.setdefault(prefix, Package(source, prefix))\n\n        # XML\n        docs[prefix].xml = xml\n\n        for file in select_filenames_by_prefix(prefix, files):\n            # avalia arquivo do pacote, se é asset ou rendition\n            component = _eval_file(prefix, file)\n            if not component:\n                continue\n\n            # resultado do avaliação do pacote\n            ftype = component.get(\"ftype\")\n            file_path = component[\"file_path\"]\n            comp_id = component[\"component_id\"]\n\n            if ftype:\n                docs[prefix].add_asset(comp_id, file_path)\n            else:\n                docs[prefix].add_rendition(comp_id, file_path)\n            files.remove(file)\n    return docs\n\n\ndef _eval_file(prefix, file_path):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereço do arquivo em análise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extensão\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    if not match_file_by_prefix(prefix, file_path):\n        # ignore files which name does not match\n        return\n    if file_path.endswith(\".xml\"):\n        # ignore XML files\n        return\n\n    # it matches\n    filename = os.path.basename(file_path)\n    fname, ext = os.path.splitext(filename)\n\n    lang = None\n    if ext == \".pdf\":\n        suffix = fname.replace(prefix, \"\")\n        if fname == prefix:\n            lang = \"original\"\n        elif len(suffix) == 3 and suffix[0] == \"-\":\n            # it is a rendition\n            lang = suffix[1:]\n\n    if lang:\n        return dict(\n            component_id=lang,\n            file_path=file_path,\n        )\n    else:\n        return dict(\n            component_id=filename,\n            component_name=fname,\n            ftype=ext[1:],\n            file_path=file_path,\n        )\n\n",
            "file_path": "packtools/sps/models/packages.py",
            "human_label": "Groups files in the given group by using _group_files_by_xml_filename.",
            "level": "file_runnable",
            "lineno": "147",
            "name": "_explore_folder",
            "oracle_context": "{ \"apis\" : \"['files_list', 'xml_files_list', '_group_files_by_xml_filename', 'is_folder']\", \"classes\" : \"['file_utils']\", \"vars\" : \"[]\" }",
            "package": "packages",
            "project": "scieloorg/packtools",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b463153879012d1948149f",
            "all_context": "{ \"import\" : \"os logging zipfile logging packtools \", \"file\" : \"logger ; select_filenames_by_prefix(prefix,files) ; match_file_by_prefix(prefix,file_path) ; explore_source(source) ; _explore_folder(folder) ; _explore_zipfile(zip_path) ; _group_files_by_xml_filename(source,xmls,files) ; _eval_file(prefix,file_path) ; \", \"class\" : \"\" }",
            "code": "def _eval_file(prefix, file_path):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereço do arquivo em análise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extensão\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    if not match_file_by_prefix(prefix, file_path):\n        # ignore files which name does not match\n        return\n    if file_path.endswith(\".xml\"):\n        # ignore XML files\n        return\n\n    # it matches\n    filename = os.path.basename(file_path)\n    fname, ext = os.path.splitext(filename)\n\n    lang = None\n    if ext == \".pdf\":\n        suffix = fname.replace(prefix, \"\")\n        if fname == prefix:\n            lang = \"original\"\n        elif len(suffix) == 3 and suffix[0] == \"-\":\n            # it is a rendition\n            lang = suffix[1:]\n\n    if lang:\n        return dict(\n            component_id=lang,\n            file_path=file_path,\n        )\n    else:\n        return dict(\n            component_id=filename,\n            component_name=fname,\n            ftype=ext[1:],\n            file_path=file_path,\n        )\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : true, \"public_lib\" : false, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\nIdentifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\no endereço do arquivo em análise.\n\nParameters\n----------\nprefix : str\n    nome do arquivo XML sem extensão\nfilename : str\n    filename\nfile_folder : str\n    file folder\n\nReturns\n-------\ndict",
            "end_lineno": "293",
            "file_content": "import logging\nimport os\n\nfrom packtools import file_utils\nfrom zipfile import ZipFile\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Package:\n    def __init__(self, source, name):\n        self._source = source\n        self._xml = None\n        self._assets = {}\n        self._renditions = {}\n        self._name = name\n        self.zip_file_path = file_utils.is_zipfile(source) and source\n\n    @property\n    def assets(self):\n        return self._assets\n\n    @property\n    def name(self):\n        return self._name\n\n    def file_path(self, file_path):\n        if file_utils.is_folder(self._source):\n            return os.path.join(self._source, file_path)\n        return file_path\n\n    def add_asset(self, basename, file_path):\n        \"\"\"\n        \"{\n            \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n            \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n            \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n        }\n        \"\"\"\n        self._assets[basename] = self.file_path(file_path)\n\n    def get_asset(self, basename):\n        try:\n            return self._assets[basename]\n        except KeyError:\n            return\n\n    def add_rendition(self, lang, file_path):\n        \"\"\"\n        {\n            \"original\": \"artigo02.pdf\",\n            \"en\": \"artigo02-en.pdf\",\n        }\n        \"\"\"\n        self._renditions[lang] = self.file_path(file_path)\n\n    def get_rendition(self, lang):\n        try:\n            return self._renditions[lang]\n        except KeyError:\n            return\n\n    @property\n    def source(self):\n        return self._source\n\n    @property\n    def xml(self):\n        return self.file_path(self._xml)\n\n    @xml.setter\n    def xml(self, value):\n        self._xml = value\n\n    @property\n    def renditions(self):\n        return self._renditions\n\n    @property\n    def xml_content(self):\n        if file_utils.is_folder(self._source):\n            with open(self.xml, \"rb\") as fp:\n                return fp.read()\n        with ZipFile(self._source) as zf:\n            return zf.read(self.xml)\n\n\ndef select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [\n        item\n        for item in files\n        if match_file_by_prefix(prefix, item)\n    ]\n\n\ndef match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorna `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    basename = os.path.basename(file_path)\n    if basename.startswith(prefix + \"-\"):\n        return True\n    if basename.startswith(prefix + \".\"):\n        return True\n    return False\n\n\ndef explore_source(source):\n    packages = _explore_zipfile(source)\n    if not packages:\n        packages = _explore_folder(source)\n    if not packages:\n        raise ValueError(\"%s: Invalid value for `source`\" % source)\n    return packages\n\n\ndef _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    if file_utils.is_folder(folder):\n        data = _group_files_by_xml_filename(\n            folder,\n            file_utils.xml_files_list(folder),\n            file_utils.files_list(folder),\n        )\n        return data\n\n\ndef _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    if file_utils.is_zipfile(zip_path):\n        with ZipFile(zip_path, 'r'):\n            data = _group_files_by_xml_filename(\n                zip_path,\n                file_utils.xml_files_list_from_zipfile(zip_path),\n                file_utils.files_list_from_zipfile(zip_path),\n            )\n            return data\n\n\ndef _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    docs = {}\n    for xml in xmls:\n        basename = os.path.basename(xml)\n        prefix, ext = os.path.splitext(basename)\n\n        docs.setdefault(prefix, Package(source, prefix))\n\n        # XML\n        docs[prefix].xml = xml\n\n        for file in select_filenames_by_prefix(prefix, files):\n            # avalia arquivo do pacote, se é asset ou rendition\n            component = _eval_file(prefix, file)\n            if not component:\n                continue\n\n            # resultado do avaliação do pacote\n            ftype = component.get(\"ftype\")\n            file_path = component[\"file_path\"]\n            comp_id = component[\"component_id\"]\n\n            if ftype:\n                docs[prefix].add_asset(comp_id, file_path)\n            else:\n                docs[prefix].add_rendition(comp_id, file_path)\n            files.remove(file)\n    return docs\n\n\ndef _eval_file(prefix, file_path):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereço do arquivo em análise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extensão\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    if not match_file_by_prefix(prefix, file_path):\n        # ignore files which name does not match\n        return\n    if file_path.endswith(\".xml\"):\n        # ignore XML files\n        return\n\n    # it matches\n    filename = os.path.basename(file_path)\n    fname, ext = os.path.splitext(filename)\n\n    lang = None\n    if ext == \".pdf\":\n        suffix = fname.replace(prefix, \"\")\n        if fname == prefix:\n            lang = \"original\"\n        elif len(suffix) == 3 and suffix[0] == \"-\":\n            # it is a rendition\n            lang = suffix[1:]\n\n    if lang:\n        return dict(\n            component_id=lang,\n            file_path=file_path,\n        )\n    else:\n        return dict(\n            component_id=filename,\n            component_name=fname,\n            ftype=ext[1:],\n            file_path=file_path,\n        )\n\n",
            "file_path": "packtools/sps/models/packages.py",
            "human_label": "Identify the type of the given file. Return None if the file do not match the given prefix or the type of the file is xml. Return dict with the key of component_id, file_path if the type of the file is \"pdf\", return dict with the key of component_id, file_path, ftype, file_path if the type of the file is not \"pdf\".",
            "level": "file_runnable",
            "lineno": "242",
            "name": "_eval_file",
            "oracle_context": "{ \"apis\" : \"['dict', 'basename', 'endswith', 'splitext', 'len', 'match_file_by_prefix', 'replace']\", \"classes\" : \"['os']\", \"vars\" : \"['path']\" }",
            "package": "packages",
            "project": "scieloorg/packtools",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b463153879012d194814a1",
            "all_context": "{ \"import\" : \"os logging zipfile logging packtools \", \"file\" : \"\", \"class\" : \"self.get_rendition(self,lang) ; self.source(self) ; self.xml(self) ; self.name(self) ; self._renditions ; self._assets ; self.xml ; self.file_path ; self.add_rendition(self,lang,file_path) ; self.get_asset(self,basename) ; self.xml_content(self) ; self.__init__(self,source,name) ; self._source ; self.assets(self) ; self.zip_file_path ; self.xml(self,value) ; self._xml ; self.file_path(self,file_path) ; self._name ; self.renditions(self) ; self.add_asset(self,basename,file_path) ; \" }",
            "code": "    def add_rendition(self, lang, file_path):\n        \"\"\"\n        {\n            \"original\": \"artigo02.pdf\",\n            \"en\": \"artigo02-en.pdf\",\n        }\n        \"\"\"\n        self._renditions[lang] = self.file_path(file_path)\n",
            "dependency": "{ \"builtin\" : false, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : true, \"current_file\" : false, \"current_project\" : false, \"external\" : false }",
            "docstring": "{\n    \"original\": \"artigo02.pdf\",\n    \"en\": \"artigo02-en.pdf\",\n}",
            "end_lineno": "56",
            "file_content": "import logging\nimport os\n\nfrom packtools import file_utils\nfrom zipfile import ZipFile\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Package:\n    def __init__(self, source, name):\n        self._source = source\n        self._xml = None\n        self._assets = {}\n        self._renditions = {}\n        self._name = name\n        self.zip_file_path = file_utils.is_zipfile(source) and source\n\n    @property\n    def assets(self):\n        return self._assets\n\n    @property\n    def name(self):\n        return self._name\n\n    def file_path(self, file_path):\n        if file_utils.is_folder(self._source):\n            return os.path.join(self._source, file_path)\n        return file_path\n\n    def add_asset(self, basename, file_path):\n        \"\"\"\n        \"{\n            \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n            \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n            \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n        }\n        \"\"\"\n        self._assets[basename] = self.file_path(file_path)\n\n    def get_asset(self, basename):\n        try:\n            return self._assets[basename]\n        except KeyError:\n            return\n\n    def add_rendition(self, lang, file_path):\n        \"\"\"\n        {\n            \"original\": \"artigo02.pdf\",\n            \"en\": \"artigo02-en.pdf\",\n        }\n        \"\"\"\n        self._renditions[lang] = self.file_path(file_path)\n\n    def get_rendition(self, lang):\n        try:\n            return self._renditions[lang]\n        except KeyError:\n            return\n\n    @property\n    def source(self):\n        return self._source\n\n    @property\n    def xml(self):\n        return self.file_path(self._xml)\n\n    @xml.setter\n    def xml(self, value):\n        self._xml = value\n\n    @property\n    def renditions(self):\n        return self._renditions\n\n    @property\n    def xml_content(self):\n        if file_utils.is_folder(self._source):\n            with open(self.xml, \"rb\") as fp:\n                return fp.read()\n        with ZipFile(self._source) as zf:\n            return zf.read(self.xml)\n\n\ndef select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [\n        item\n        for item in files\n        if match_file_by_prefix(prefix, item)\n    ]\n\n\ndef match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorna `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    basename = os.path.basename(file_path)\n    if basename.startswith(prefix + \"-\"):\n        return True\n    if basename.startswith(prefix + \".\"):\n        return True\n    return False\n\n\ndef explore_source(source):\n    packages = _explore_zipfile(source)\n    if not packages:\n        packages = _explore_folder(source)\n    if not packages:\n        raise ValueError(\"%s: Invalid value for `source`\" % source)\n    return packages\n\n\ndef _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    if file_utils.is_folder(folder):\n        data = _group_files_by_xml_filename(\n            folder,\n            file_utils.xml_files_list(folder),\n            file_utils.files_list(folder),\n        )\n        return data\n\n\ndef _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    if file_utils.is_zipfile(zip_path):\n        with ZipFile(zip_path, 'r'):\n            data = _group_files_by_xml_filename(\n                zip_path,\n                file_utils.xml_files_list_from_zipfile(zip_path),\n                file_utils.files_list_from_zipfile(zip_path),\n            )\n            return data\n\n\ndef _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    docs = {}\n    for xml in xmls:\n        basename = os.path.basename(xml)\n        prefix, ext = os.path.splitext(basename)\n\n        docs.setdefault(prefix, Package(source, prefix))\n\n        # XML\n        docs[prefix].xml = xml\n\n        for file in select_filenames_by_prefix(prefix, files):\n            # avalia arquivo do pacote, se é asset ou rendition\n            component = _eval_file(prefix, file)\n            if not component:\n                continue\n\n            # resultado do avaliação do pacote\n            ftype = component.get(\"ftype\")\n            file_path = component[\"file_path\"]\n            comp_id = component[\"component_id\"]\n\n            if ftype:\n                docs[prefix].add_asset(comp_id, file_path)\n            else:\n                docs[prefix].add_rendition(comp_id, file_path)\n            files.remove(file)\n    return docs\n\n\ndef _eval_file(prefix, file_path):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereço do arquivo em análise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extensão\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    if not match_file_by_prefix(prefix, file_path):\n        # ignore files which name does not match\n        return\n    if file_path.endswith(\".xml\"):\n        # ignore XML files\n        return\n\n    # it matches\n    filename = os.path.basename(file_path)\n    fname, ext = os.path.splitext(filename)\n\n    lang = None\n    if ext == \".pdf\":\n        suffix = fname.replace(prefix, \"\")\n        if fname == prefix:\n            lang = \"original\"\n        elif len(suffix) == 3 and suffix[0] == \"-\":\n            # it is a rendition\n            lang = suffix[1:]\n\n    if lang:\n        return dict(\n            component_id=lang,\n            file_path=file_path,\n        )\n    else:\n        return dict(\n            component_id=filename,\n            component_name=fname,\n            ftype=ext[1:],\n            file_path=file_path,\n        )\n\n",
            "file_path": "packtools/sps/models/packages.py",
            "human_label": "Assign the filepath invoke by filepath() in the class to \"lang\" in _renditions in the class.",
            "level": "class_runnable",
            "lineno": "49",
            "name": "add_rendition",
            "oracle_context": "{ \"apis\" : \"['file_path']\", \"classes\" : \"[]\", \"vars\" : \"['_renditions']\" }",
            "package": "packages",
            "project": "scieloorg/packtools",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b463163879012d194814a2",
            "all_context": "{ \"import\" : \"os logging zipfile logging packtools \", \"file\" : \"\", \"class\" : \"self.get_rendition(self,lang) ; self.source(self) ; self.xml(self) ; self.name(self) ; self._renditions ; self._assets ; self.xml ; self.file_path ; self.add_rendition(self,lang,file_path) ; self.get_asset(self,basename) ; self.xml_content(self) ; self.__init__(self,source,name) ; self._source ; self.assets(self) ; self.zip_file_path ; self.xml(self,value) ; self._xml ; self.file_path(self,file_path) ; self._name ; self.renditions(self) ; self.add_asset(self,basename,file_path) ; \" }",
            "code": "    def add_asset(self, basename, file_path):\n        \"\"\"\n        \"{\n            \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n            \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n            \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n        }\n        \"\"\"\n        self._assets[basename] = self.file_path(file_path)\n",
            "dependency": "{ \"builtin\" : false, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : true, \"current_file\" : false, \"current_project\" : false, \"external\" : false }",
            "docstring": "\"{\n    \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n    \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n    \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n}",
            "end_lineno": "41",
            "file_content": "import logging\nimport os\n\nfrom packtools import file_utils\nfrom zipfile import ZipFile\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Package:\n    def __init__(self, source, name):\n        self._source = source\n        self._xml = None\n        self._assets = {}\n        self._renditions = {}\n        self._name = name\n        self.zip_file_path = file_utils.is_zipfile(source) and source\n\n    @property\n    def assets(self):\n        return self._assets\n\n    @property\n    def name(self):\n        return self._name\n\n    def file_path(self, file_path):\n        if file_utils.is_folder(self._source):\n            return os.path.join(self._source, file_path)\n        return file_path\n\n    def add_asset(self, basename, file_path):\n        \"\"\"\n        \"{\n            \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n            \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n            \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n        }\n        \"\"\"\n        self._assets[basename] = self.file_path(file_path)\n\n    def get_asset(self, basename):\n        try:\n            return self._assets[basename]\n        except KeyError:\n            return\n\n    def add_rendition(self, lang, file_path):\n        \"\"\"\n        {\n            \"original\": \"artigo02.pdf\",\n            \"en\": \"artigo02-en.pdf\",\n        }\n        \"\"\"\n        self._renditions[lang] = self.file_path(file_path)\n\n    def get_rendition(self, lang):\n        try:\n            return self._renditions[lang]\n        except KeyError:\n            return\n\n    @property\n    def source(self):\n        return self._source\n\n    @property\n    def xml(self):\n        return self.file_path(self._xml)\n\n    @xml.setter\n    def xml(self, value):\n        self._xml = value\n\n    @property\n    def renditions(self):\n        return self._renditions\n\n    @property\n    def xml_content(self):\n        if file_utils.is_folder(self._source):\n            with open(self.xml, \"rb\") as fp:\n                return fp.read()\n        with ZipFile(self._source) as zf:\n            return zf.read(self.xml)\n\n\ndef select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [\n        item\n        for item in files\n        if match_file_by_prefix(prefix, item)\n    ]\n\n\ndef match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorna `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    basename = os.path.basename(file_path)\n    if basename.startswith(prefix + \"-\"):\n        return True\n    if basename.startswith(prefix + \".\"):\n        return True\n    return False\n\n\ndef explore_source(source):\n    packages = _explore_zipfile(source)\n    if not packages:\n        packages = _explore_folder(source)\n    if not packages:\n        raise ValueError(\"%s: Invalid value for `source`\" % source)\n    return packages\n\n\ndef _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    if file_utils.is_folder(folder):\n        data = _group_files_by_xml_filename(\n            folder,\n            file_utils.xml_files_list(folder),\n            file_utils.files_list(folder),\n        )\n        return data\n\n\ndef _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    if file_utils.is_zipfile(zip_path):\n        with ZipFile(zip_path, 'r'):\n            data = _group_files_by_xml_filename(\n                zip_path,\n                file_utils.xml_files_list_from_zipfile(zip_path),\n                file_utils.files_list_from_zipfile(zip_path),\n            )\n            return data\n\n\ndef _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    docs = {}\n    for xml in xmls:\n        basename = os.path.basename(xml)\n        prefix, ext = os.path.splitext(basename)\n\n        docs.setdefault(prefix, Package(source, prefix))\n\n        # XML\n        docs[prefix].xml = xml\n\n        for file in select_filenames_by_prefix(prefix, files):\n            # avalia arquivo do pacote, se é asset ou rendition\n            component = _eval_file(prefix, file)\n            if not component:\n                continue\n\n            # resultado do avaliação do pacote\n            ftype = component.get(\"ftype\")\n            file_path = component[\"file_path\"]\n            comp_id = component[\"component_id\"]\n\n            if ftype:\n                docs[prefix].add_asset(comp_id, file_path)\n            else:\n                docs[prefix].add_rendition(comp_id, file_path)\n            files.remove(file)\n    return docs\n\n\ndef _eval_file(prefix, file_path):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereço do arquivo em análise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extensão\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    if not match_file_by_prefix(prefix, file_path):\n        # ignore files which name does not match\n        return\n    if file_path.endswith(\".xml\"):\n        # ignore XML files\n        return\n\n    # it matches\n    filename = os.path.basename(file_path)\n    fname, ext = os.path.splitext(filename)\n\n    lang = None\n    if ext == \".pdf\":\n        suffix = fname.replace(prefix, \"\")\n        if fname == prefix:\n            lang = \"original\"\n        elif len(suffix) == 3 and suffix[0] == \"-\":\n            # it is a rendition\n            lang = suffix[1:]\n\n    if lang:\n        return dict(\n            component_id=lang,\n            file_path=file_path,\n        )\n    else:\n        return dict(\n            component_id=filename,\n            component_name=fname,\n            ftype=ext[1:],\n            file_path=file_path,\n        )\n\n",
            "file_path": "packtools/sps/models/packages.py",
            "human_label": "Assign the filepath invoke by filepath() in the class to \"basename\" in _assets in the class.",
            "level": "class_runnable",
            "lineno": "33",
            "name": "add_asset",
            "oracle_context": "{ \"apis\" : \"['file_path']\", \"classes\" : \"[]\", \"vars\" : \"['_assets']\" }",
            "package": "packages",
            "project": "scieloorg/packtools",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b463163879012d194814a4",
            "all_context": "{ \"import\" : \"os logging zipfile logging packtools \", \"file\" : \"logger ; select_filenames_by_prefix(prefix,files) ; match_file_by_prefix(prefix,file_path) ; explore_source(source) ; _explore_folder(folder) ; _explore_zipfile(zip_path) ; _group_files_by_xml_filename(source,xmls,files) ; _eval_file(prefix,file_path) ; \", \"class\" : \"\" }",
            "code": "def _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    if file_utils.is_zipfile(zip_path):\n        with ZipFile(zip_path, 'r'):\n            data = _group_files_by_xml_filename(\n                zip_path,\n                file_utils.xml_files_list_from_zipfile(zip_path),\n                file_utils.files_list_from_zipfile(zip_path),\n            )\n            return data\n",
            "dependency": "{ \"builtin\" : false, \"standard_lib\" : true, \"public_lib\" : true, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Get packages' data from zip_path\n\nGroups files by their XML basename and returns data in dict format.\n\nParameters\n----------\nzip_path : str\n    zip file path\nReturns\n-------\ndict",
            "end_lineno": "191",
            "file_content": "import logging\nimport os\n\nfrom packtools import file_utils\nfrom zipfile import ZipFile\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass Package:\n    def __init__(self, source, name):\n        self._source = source\n        self._xml = None\n        self._assets = {}\n        self._renditions = {}\n        self._name = name\n        self.zip_file_path = file_utils.is_zipfile(source) and source\n\n    @property\n    def assets(self):\n        return self._assets\n\n    @property\n    def name(self):\n        return self._name\n\n    def file_path(self, file_path):\n        if file_utils.is_folder(self._source):\n            return os.path.join(self._source, file_path)\n        return file_path\n\n    def add_asset(self, basename, file_path):\n        \"\"\"\n        \"{\n            \"artigo02-gf03.tiff\": \"/path/artigo02-gf03.tiff\",\n            \"artigo02-gf03.jpg\": \"/path/artigo02-gf03.jpg\",\n            \"artigo02-gf03.png\": \"/path/artigo02-gf03.png\",\n        }\n        \"\"\"\n        self._assets[basename] = self.file_path(file_path)\n\n    def get_asset(self, basename):\n        try:\n            return self._assets[basename]\n        except KeyError:\n            return\n\n    def add_rendition(self, lang, file_path):\n        \"\"\"\n        {\n            \"original\": \"artigo02.pdf\",\n            \"en\": \"artigo02-en.pdf\",\n        }\n        \"\"\"\n        self._renditions[lang] = self.file_path(file_path)\n\n    def get_rendition(self, lang):\n        try:\n            return self._renditions[lang]\n        except KeyError:\n            return\n\n    @property\n    def source(self):\n        return self._source\n\n    @property\n    def xml(self):\n        return self.file_path(self._xml)\n\n    @xml.setter\n    def xml(self, value):\n        self._xml = value\n\n    @property\n    def renditions(self):\n        return self._renditions\n\n    @property\n    def xml_content(self):\n        if file_utils.is_folder(self._source):\n            with open(self.xml, \"rb\") as fp:\n                return fp.read()\n        with ZipFile(self._source) as zf:\n            return zf.read(self.xml)\n\n\ndef select_filenames_by_prefix(prefix, files):\n    \"\"\"\n    Get files which belongs to a document package.\n\n    Retorna os arquivos da lista `files` cujos nomes iniciam com `prefix`\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    files : str list\n        Files paths\n    Returns\n    -------\n    list\n        files paths which basename files matches to prefix\n    \"\"\"\n    return [\n        item\n        for item in files\n        if match_file_by_prefix(prefix, item)\n    ]\n\n\ndef match_file_by_prefix(prefix, file_path):\n    \"\"\"\n    Identify if a `file_path` belongs to a document package by a given `prefix`\n\n    Retorna `True` para documentos pertencentes a um pacote.\n\n    Parameters\n    ----------\n    prefix : str\n        Filename prefix\n    file_path : str\n        File path\n    Returns\n    -------\n    bool\n        True - file belongs to the package\n    \"\"\"\n    basename = os.path.basename(file_path)\n    if basename.startswith(prefix + \"-\"):\n        return True\n    if basename.startswith(prefix + \".\"):\n        return True\n    return False\n\n\ndef explore_source(source):\n    packages = _explore_zipfile(source)\n    if not packages:\n        packages = _explore_folder(source)\n    if not packages:\n        raise ValueError(\"%s: Invalid value for `source`\" % source)\n    return packages\n\n\ndef _explore_folder(folder):\n    \"\"\"\n    Get packages' data from folder\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    folder : str\n        Folder of the package\n    Returns\n    -------\n    dict\n    \"\"\"\n    if file_utils.is_folder(folder):\n        data = _group_files_by_xml_filename(\n            folder,\n            file_utils.xml_files_list(folder),\n            file_utils.files_list(folder),\n        )\n        return data\n\n\ndef _explore_zipfile(zip_path):\n    \"\"\"\n    Get packages' data from zip_path\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    zip_path : str\n        zip file path\n    Returns\n    -------\n    dict\n    \"\"\"\n    if file_utils.is_zipfile(zip_path):\n        with ZipFile(zip_path, 'r'):\n            data = _group_files_by_xml_filename(\n                zip_path,\n                file_utils.xml_files_list_from_zipfile(zip_path),\n                file_utils.files_list_from_zipfile(zip_path),\n            )\n            return data\n\n\ndef _group_files_by_xml_filename(source, xmls, files):\n    \"\"\"\n    Group files by their XML basename\n\n    Groups files by their XML basename and returns data in dict format.\n\n    Parameters\n    ----------\n    xml_filename : str\n        XML filenames\n    files : list\n        list of files in the folder or zipfile\n\n    Returns\n    -------\n    dict\n        key: name of the XML files\n        value: Package\n    \"\"\"\n    docs = {}\n    for xml in xmls:\n        basename = os.path.basename(xml)\n        prefix, ext = os.path.splitext(basename)\n\n        docs.setdefault(prefix, Package(source, prefix))\n\n        # XML\n        docs[prefix].xml = xml\n\n        for file in select_filenames_by_prefix(prefix, files):\n            # avalia arquivo do pacote, se é asset ou rendition\n            component = _eval_file(prefix, file)\n            if not component:\n                continue\n\n            # resultado do avaliação do pacote\n            ftype = component.get(\"ftype\")\n            file_path = component[\"file_path\"]\n            comp_id = component[\"component_id\"]\n\n            if ftype:\n                docs[prefix].add_asset(comp_id, file_path)\n            else:\n                docs[prefix].add_rendition(comp_id, file_path)\n            files.remove(file)\n    return docs\n\n\ndef _eval_file(prefix, file_path):\n    \"\"\"\n    Identifica o tipo de arquivo do pacote: `asset` ou `rendition`.\n\n    Identifica o tipo de arquivo do pacote e atualiza `packages` com o tipo e\n    o endereço do arquivo em análise.\n\n    Parameters\n    ----------\n    prefix : str\n        nome do arquivo XML sem extensão\n    filename : str\n        filename\n    file_folder : str\n        file folder\n\n    Returns\n    -------\n    dict\n    \"\"\"\n    if not match_file_by_prefix(prefix, file_path):\n        # ignore files which name does not match\n        return\n    if file_path.endswith(\".xml\"):\n        # ignore XML files\n        return\n\n    # it matches\n    filename = os.path.basename(file_path)\n    fname, ext = os.path.splitext(filename)\n\n    lang = None\n    if ext == \".pdf\":\n        suffix = fname.replace(prefix, \"\")\n        if fname == prefix:\n            lang = \"original\"\n        elif len(suffix) == 3 and suffix[0] == \"-\":\n            # it is a rendition\n            lang = suffix[1:]\n\n    if lang:\n        return dict(\n            component_id=lang,\n            file_path=file_path,\n        )\n    else:\n        return dict(\n            component_id=filename,\n            component_name=fname,\n            ftype=ext[1:],\n            file_path=file_path,\n        )\n\n",
            "file_path": "packtools/sps/models/packages.py",
            "human_label": "Groups the given zip path by using _group_files_by_xml_filename.",
            "level": "file_runnable",
            "lineno": "170",
            "name": "_explore_zipfile",
            "oracle_context": "{ \"apis\" : \"['is_zipfile', 'files_list_from_zipfile', 'xml_files_list_from_zipfile', '_group_files_by_xml_filename']\", \"classes\" : \"['file_utils', 'ZipFile']\", \"vars\" : \"[]\" }",
            "package": "packages",
            "project": "scieloorg/packtools",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b86707b4d922cb0e688c2a",
            "all_context": "{ \"import\" : \"inspect logging asyncio collections secrets operator base64 contextlib enum functools datetime typing random copy logging aiohttp asyncio secrets enum functools krake datetime typing yarl \", \"file\" : \"\", \"class\" : \"self.__init__(self) ; self.on(self,hook) ; self.registry ; \" }",
            "code": "    def on(self, hook):\n        \"\"\"Decorator function to add a new handler to the registry.\n\n        Args:\n            hook (HookType): Hook attribute for which to register the handler.\n\n        Returns:\n            callable: Decorator for registering listeners for the specified\n            hook.\n\n        \"\"\"\n\n        def decorator(handler):\n            self.registry[hook].append(handler)\n\n            return handler\n\n        return decorator\n",
            "dependency": "{ \"builtin\" : false, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : true, \"current_file\" : false, \"current_project\" : false, \"external\" : false }",
            "docstring": "Decorator function to add a new handler to the registry.\n\nArgs:\n    hook (HookType): Hook attribute for which to register the handler.\n\nReturns:\n    callable: Decorator for registering listeners for the specified\n    hook.",
            "end_lineno": "114",
            "file_content": "\"\"\"This module defines the Hook Dispatcher and listeners for registering and\nexecuting hooks. Hook Dispatcher emits hooks based on :class:`Hook` attributes which\ndefine when the hook will be executed.\n\n\"\"\"\nimport asyncio\nimport logging\nimport random\nfrom base64 import b64encode\nfrom collections import defaultdict\nfrom contextlib import suppress\nfrom copy import deepcopy\nfrom datetime import datetime\nfrom functools import reduce\nfrom operator import getitem\nfrom enum import Enum, auto\nfrom inspect import iscoroutinefunction\nfrom OpenSSL import crypto\nfrom typing import NamedTuple\n\nimport yarl\nfrom aiohttp import ClientConnectorError\n\nfrom krake.controller import Observer\nfrom krake.controller.kubernetes.client import KubernetesClient, InvalidManifestError\nfrom krake.utils import camel_to_snake_case, get_kubernetes_resource_idx\nfrom kubernetes_asyncio.client.rest import ApiException\nfrom kubernetes_asyncio.client.api_client import ApiClient\nfrom kubernetes_asyncio import client\nfrom krake.data.kubernetes import ClusterState, Application, Cluster\nfrom yarl import URL\nfrom secrets import token_urlsafe\n\nfrom kubernetes_asyncio.client import (\n    Configuration,\n    V1Secret,\n    V1EnvVar,\n    V1VolumeMount,\n    V1Volume,\n    V1SecretKeySelector,\n    V1EnvVarSource,\n)\nfrom kubernetes_asyncio.config.kube_config import KubeConfigLoader\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass HookType(Enum):\n    ResourcePreCreate = auto()\n    ResourcePostCreate = auto()\n    ResourcePreUpdate = auto()\n    ResourcePostUpdate = auto()\n    ResourcePreDelete = auto()\n    ResourcePostDelete = auto()\n    ApplicationMangling = auto()\n    ApplicationPreMigrate = auto()\n    ApplicationPostMigrate = auto()\n    ApplicationPreReconcile = auto()\n    ApplicationPostReconcile = auto()\n    ApplicationPreDelete = auto()\n    ApplicationPostDelete = auto()\n    ClusterCreation = auto()\n    ClusterDeletion = auto()\n\n\nclass HookDispatcher(object):\n    \"\"\"Simple wrapper around a registry of handlers associated to :class:`Hook`\n     attributes. Each :class:`Hook` attribute defines when the handler will be\n     executed.\n\n    Listeners for certain hooks can be registered via :meth:`on`. Registered\n    listeners are executed via :meth:`hook`.\n\n    Example:\n        .. code:: python\n\n        listen = HookDispatcher()\n\n        @listen.on(HookType.PreApply)\n        def to_perform_before_app_creation(app, cluster, resource, controller):\n            # Do Stuff\n\n        @listen.on(HookType.PostApply)\n        def another_to_perform_after_app_creation(app, cluster, resource, resp):\n            # Do Stuff\n\n        @listen.on(HookType.PostDelete)\n        def to_perform_after_app_deletion(app, cluster, resource, resp):\n            # Do Stuff\n\n    \"\"\"\n\n    def __init__(self):\n        self.registry = defaultdict(list)\n\n    def on(self, hook):\n        \"\"\"Decorator function to add a new handler to the registry.\n\n        Args:\n            hook (HookType): Hook attribute for which to register the handler.\n\n        Returns:\n            callable: Decorator for registering listeners for the specified\n            hook.\n\n        \"\"\"\n\n        def decorator(handler):\n            self.registry[hook].append(handler)\n\n            return handler\n\n        return decorator\n\n    async def hook(self, hook, **kwargs):\n        \"\"\"Execute the list of handlers associated to the provided :class:`Hook`\n        attribute.\n\n        Args:\n            hook (HookType): The hook attribute for which to execute handlers.\n\n        \"\"\"\n        try:\n            handlers = self.registry[hook]\n        except KeyError:\n            pass\n        else:\n            for handler in handlers:\n                if iscoroutinefunction(handler):\n                    await handler(**kwargs)\n                else:\n                    handler(**kwargs)\n\n\nlisten = HookDispatcher()\n\n\n@listen.on(HookType.ResourcePostCreate)\n@listen.on(HookType.ResourcePostUpdate)\nasync def register_service(app, cluster, resource, response):\n    \"\"\"Register endpoint of Kubernetes Service object on creation and update.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        cluster (krake.data.kubernetes.Cluster): The cluster on which the\n            application is running\n        resource (dict): Kubernetes object description as specified in the\n            specification of the application.\n        response (kubernetes_asyncio.client.V1Service): Response of the\n            Kubernetes API\n\n    \"\"\"\n    if resource[\"kind\"] != \"Service\":\n        return\n\n    service_name = resource[\"metadata\"][\"name\"]\n\n    if response.spec and response.spec.type == \"LoadBalancer\":\n        # For a \"LoadBalancer\" type of Service, an external IP is given in the cluster\n        # by a load balancer controller to the service. In this case, the \"port\"\n        # specified in the spec is reachable from the outside.\n        if (\n            not response.status.load_balancer\n            or not response.status.load_balancer.ingress\n        ):\n            # When a \"LoadBalancer\" type of service is created, the IP is given by an\n            # additional controller (e.g. a controller that requests a floating IP to an\n            # OpenStack infrastructure). This process can take some time, but the\n            # Service itself already exist before the IP is assigned. In the case of an\n            # error with the controller, the IP is also not given. This \"<pending>\" IP\n            # just expresses that the Service exists, but the IP is not ready yet.\n            external_ip = \"<pending>\"\n        else:\n            external_ip = response.status.load_balancer.ingress[0].ip\n\n        if not response.spec.ports:\n            external_port = \"<pending>\"\n        else:\n            external_port = response.spec.ports[0].port\n        app.status.services[service_name] = f\"{external_ip}:{external_port}\"\n        return\n\n    node_port = None\n    # Ensure that ports are specified\n    if response.spec and response.spec.ports:\n        node_port = response.spec.ports[0].node_port\n\n    # If the service does not have a node port, remove a potential reference\n    # and return.\n    if node_port is None:\n        try:\n            del app.status.services[service_name]\n        except KeyError:\n            pass\n        return\n\n    # Determine URL of Kubernetes cluster API\n    loader = KubeConfigLoader(cluster.spec.kubeconfig)\n    config = Configuration()\n    await loader.load_and_set(config)\n    cluster_url = yarl.URL(config.host)\n\n    app.status.services[service_name] = f\"{cluster_url.host}:{node_port}\"\n\n\n@listen.on(HookType.ResourcePostDelete)\nasync def unregister_service(app, resource, **kwargs):\n    \"\"\"Unregister endpoint of Kubernetes Service object on deletion.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        resource (dict): Kubernetes object description as specified in the\n            specification of the application.\n\n    \"\"\"\n    if resource[\"kind\"] != \"Service\":\n        return\n\n    service_name = resource[\"metadata\"][\"name\"]\n    try:\n        del app.status.services[service_name]\n    except KeyError:\n        pass\n\n\n@listen.on(HookType.ResourcePostDelete)\nasync def remove_resource_from_last_observed_manifest(app, resource, **kwargs):\n    \"\"\"Remove a given resource from the last_observed_manifest after its deletion\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        resource (dict): Kubernetes object description as specified in the\n            specification of the application.\n\n    \"\"\"\n    try:\n        idx = get_kubernetes_resource_idx(app.status.last_observed_manifest, resource)\n    except IndexError:\n        return\n\n    app.status.last_observed_manifest.pop(idx)\n\n\ndef update_last_applied_manifest_dict_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_list_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (dict): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (dict): partial ``observer_schema``\n        response (dict): partial response from the Kubernetes API.\n\n    Raises:\n        KeyError: If the observed field is not present in the Kubernetes response\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n\n    \"\"\"\n    for key, value in observer_schema.items():\n\n        # Keys in the response are in camelCase\n        camel_key = camel_to_snake_case(key)\n\n        if camel_key not in response:\n            # An observed key should always be present in the k8s response\n            raise KeyError(\n                f\"Observed key {camel_key} is not present in response {response}\"\n            )\n\n        if isinstance(value, dict):\n            if key not in last_applied_manifest:\n                # The dictionary is observed, but not present in\n                # last_applied_manifest\n                last_applied_manifest[key] = {}\n\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[key], observer_schema[key], response[camel_key]\n            )\n\n        elif isinstance(value, list):\n            if key not in last_applied_manifest:\n                # The list is observed, but not present in last_applied_manifest\n                last_applied_manifest[key] = []\n\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[key], observer_schema[key], response[camel_key]\n            )\n\n        elif key not in last_applied_manifest:\n            # If key not present in last_applied_manifest, and value is neither a\n            # dict nor a list, simply add it.\n            last_applied_manifest[key] = response[camel_key]\n\n\ndef update_last_applied_manifest_list_from_resp(\n    last_applied_manifest, observer_schema, response\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_dict_from_resp``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n    from a partial Kubernetes response\n\n    Args:\n        last_applied_manifest (list): partial ``last_applied_manifest`` being\n            updated\n        observer_schema (list): partial ``observer_schema``\n        response (list): partial response from the Kubernetes API.\n\n    This function go through all observed fields, and initialized their value in\n    last_applied_manifest if they are not yet present\n\n    \"\"\"\n    # Looping over the observed resource, except the last element which is the\n    # special control dictionary\n    for idx, val in enumerate(observer_schema[:-1]):\n\n        if idx >= len(response):\n            # Element is observed but not present in k8s response, so following\n            # elements will also not exist.\n            #\n            # This doesn't raise an Exception as observing the element of a list\n            # doesn't ensure its presence. The list length is controlled by the\n            # special control dictionary\n            return\n\n        if isinstance(val, dict):\n            if idx >= len(last_applied_manifest):\n                # The dict is observed, but not present in last_applied_manifest\n                last_applied_manifest.append({})\n\n            update_last_applied_manifest_dict_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n\n        elif isinstance(response[idx], list):\n            if idx >= len(last_applied_manifest):\n                # The list is observed, but not present in last_applied_manifest\n                last_applied_manifest.append([])\n\n            update_last_applied_manifest_list_from_resp(\n                last_applied_manifest[idx], observer_schema[idx], response[idx]\n            )\n\n        elif idx >= len(last_applied_manifest):\n            # Element is not yet present in last_applied_manifest. Adding it.\n            last_applied_manifest.append(response[idx])\n\n\n@listen.on(HookType.ResourcePostCreate)\n@listen.on(HookType.ResourcePostUpdate)\ndef update_last_applied_manifest_from_resp(app, response, **kwargs):\n    \"\"\"Hook run after the creation or update of an application in order to update the\n    `status.last_applied_manifest` using the k8s response.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        response (kubernetes_asyncio.client.V1Status): Response of the Kubernetes API\n\n    After a Kubernetes resource has been created/updated, the\n    `status.last_applied_manifest` has to be updated. All fields already initialized\n    (either from the mangling of `spec.manifest`, or by a previous call to this\n    function) should be left untouched. Only observed fields which are not present in\n    `status.last_applied_manifest` should be initialized.\n\n    \"\"\"\n\n    if isinstance(response, dict):\n        # The Kubernetes API couldn't deserialize the k8s response into an object\n        resp = response\n    else:\n        # The Kubernetes API deserialized the k8s response into an object\n        resp = response.to_dict()\n\n    idx_applied = get_kubernetes_resource_idx(app.status.last_applied_manifest, resp)\n\n    idx_observed = get_kubernetes_resource_idx(app.status.mangled_observer_schema, resp)\n\n    update_last_applied_manifest_dict_from_resp(\n        app.status.last_applied_manifest[idx_applied],\n        app.status.mangled_observer_schema[idx_observed],\n        resp,\n    )\n\n\n@listen.on(HookType.ResourcePostCreate)\n@listen.on(HookType.ResourcePostUpdate)\ndef update_last_observed_manifest_from_resp(app, response, **kwargs):\n    \"\"\"Handler to run after the creation or update of a Kubernetes resource to update\n    the last_observed_manifest from the response of the Kubernetes API.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application the service belongs to\n        response (kubernetes_asyncio.client.V1Service): Response of the\n            Kubernetes API\n\n    The target last_observed_manifest holds the value of all observed fields plus the\n    special control dictionaries for the list length\n\n    \"\"\"\n    if isinstance(response, dict):\n        # The Kubernetes API couldn't deserialize the k8s response into an object\n        resp = response\n    else:\n        # The Kubernetes API deserialized the k8s response into an object\n        resp = response.to_dict()\n\n    try:\n        idx_observed = get_kubernetes_resource_idx(\n            app.status.mangled_observer_schema,\n            resp,\n        )\n    except IndexError:\n        # All created resources should be observed\n        raise\n\n    try:\n        idx_last_observed = get_kubernetes_resource_idx(\n            app.status.last_observed_manifest,\n            resp,\n        )\n    except IndexError:\n        # If the resource is not yes present in last_observed_manifest, append it.\n        idx_last_observed = len(app.status.last_observed_manifest)\n        app.status.last_observed_manifest.append({})\n\n    # Overwrite the last_observed_manifest for this resource\n    app.status.last_observed_manifest[\n        idx_last_observed\n    ] = update_last_observed_manifest_dict(\n        app.status.mangled_observer_schema[idx_observed], resp\n    )\n\n\ndef update_last_observed_manifest_dict(observed_resource, response):\n    \"\"\"Together with :func:``update_last_observed_manifest_list``, recursively\n    crafts the ``last_observed_manifest`` from the Kubernetes :attr:``response``.\n\n    Args:\n        observed_resource (dict): The schema to observe for the partial given resource\n        response (dict): The partial Kubernetes response for this resource.\n\n    Raises:\n        KeyError: If an observed key is not present in the Kubernetes response\n\n    Returns:\n        dict: The dictionary of observed keys and their value\n\n    Get the value of all observed fields from the Kubernetes response\n    \"\"\"\n    res = {}\n    for key, value in observed_resource.items():\n\n        camel_key = camel_to_snake_case(key)\n        if camel_key not in response:\n            raise KeyError(\n                f\"Observed key {camel_key} is not present in response {response}\"\n            )\n\n        if isinstance(value, dict):\n            res[key] = update_last_observed_manifest_dict(value, response[camel_key])\n\n        elif isinstance(value, list):\n            res[key] = update_last_observed_manifest_list(value, response[camel_key])\n\n        else:\n            res[key] = response[camel_key]\n\n    return res\n\n\ndef update_last_observed_manifest_list(observed_resource, response):\n    \"\"\"Together with :func:``update_last_observed_manifest_dict``, recursively\n    crafts the ``last_observed_manifest`` from the Kubernetes :attr:``response``.\n\n    Args:\n        observed_resource (list): the schema to observe for the partial given resource\n        response (list): the partial Kubernetes response for this resource.\n\n    Returns:\n        list: The list of observed elements, plus the special list length control\n            dictionary\n\n    Get the value of all observed elements from the Kubernetes response\n    \"\"\"\n\n    if not response:\n        return [{\"observer_schema_list_current_length\": 0}]\n\n    res = []\n    # Looping over the observed resource, except the last element which is the special\n    # control dictionary\n    for idx, val in enumerate(observed_resource[:-1]):\n\n        if idx >= len(response):\n            # Element is not present in the Kubernetes response, nothing more to do\n            break\n\n        if type(response[idx]) == dict:\n            res.append(update_last_observed_manifest_dict(val, response[idx]))\n\n        elif type(response[idx]) == list:\n            res.append(update_last_observed_manifest_list(val, response[idx]))\n\n        else:\n            res.append(response[idx])\n\n    # Append the special control dictionary to the list\n    res.append({\"observer_schema_list_current_length\": len(response)})\n\n    return res\n\n\ndef update_last_applied_manifest_dict_from_spec(\n    resource_status_new, resource_status_old, resource_observed\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_list_from_spec``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n\n    Args:\n        resource_status_new (dict): partial ``last_applied_manifest`` being updated\n        resource_status_old (dict): partial of the current ``last_applied_manifest``\n        resource_observed (dict): partial observer_schema for the manifest file\n            being updated\n\n    \"\"\"\n    for key, value in resource_observed.items():\n\n        if key not in resource_status_old:\n            continue\n\n        if key in resource_status_new:\n\n            if isinstance(value, dict):\n                update_last_applied_manifest_dict_from_spec(\n                    resource_status_new[key],\n                    resource_status_old[key],\n                    resource_observed[key],\n                )\n\n            elif isinstance(value, list):\n                update_last_applied_manifest_list_from_spec(\n                    resource_status_new[key],\n                    resource_status_old[key],\n                    resource_observed[key],\n                )\n\n        else:\n            # If the key is not present the spec.manifest, we first need to\n            # initialize it\n\n            if isinstance(value, dict):\n                resource_status_new[key] = {}\n                update_last_applied_manifest_dict_from_spec(\n                    resource_status_new[key],\n                    resource_status_old[key],\n                    resource_observed[key],\n                )\n\n            elif isinstance(value, list):\n                resource_status_new[key] = []\n                update_last_applied_manifest_list_from_spec(\n                    resource_status_new[key],\n                    resource_status_old[key],\n                    resource_observed[key],\n                )\n\n            else:\n                resource_status_new[key] = resource_status_old[key]\n\n\ndef update_last_applied_manifest_list_from_spec(\n    resource_status_new, resource_status_old, resource_observed\n):\n    \"\"\"Together with :func:``update_last_applied_manifest_dict_from_spec``, this\n    function is called recursively to update a partial ``last_applied_manifest``\n\n    Args:\n        resource_status_new (list): partial ``last_applied_manifest`` being updated\n        resource_status_old (list): partial of the current ``last_applied_manifest``\n        resource_observed (list): partial observer_schema for the manifest file\n            being updated\n\n    \"\"\"\n\n    # Looping over the observed resource, except the last element which is the\n    # special control dictionary\n    for idx, val in enumerate(resource_observed[:-1]):\n\n        if idx >= len(resource_status_old):\n            # The element in not in the current last_applied_manifest, and neither\n            # is the rest of the list\n            break\n\n        if idx < len(resource_status_new):\n            # The element is present in spec.manifest and in the current\n            # last_applied_manifest. Updating observed fields\n\n            if isinstance(val, dict):\n                update_last_applied_manifest_dict_from_spec(\n                    resource_status_new[idx],\n                    resource_status_old[idx],\n                    resource_observed[idx],\n                )\n\n            elif isinstance(val, list):\n                update_last_applied_manifest_list_from_spec(\n                    resource_status_new[idx],\n                    resource_status_old[idx],\n                    resource_observed[idx],\n                )\n\n        else:\n            # If the element is not present in the spec.manifest, we first have to\n            # initialize it.\n\n            if isinstance(val, dict):\n                resource_status_new.append({})\n                update_last_applied_manifest_dict_from_spec(\n                    resource_status_new[idx],\n                    resource_status_old[idx],\n                    resource_observed[idx],\n                )\n\n            elif isinstance(val, list):\n                resource_status_new.append([])\n                update_last_applied_manifest_list_from_spec(\n                    resource_status_new[idx],\n                    resource_status_old[idx],\n                    resource_observed[idx],\n                )\n\n            else:\n                resource_status_new.append(resource_status_old[idx])\n\n\ndef update_last_applied_manifest_from_spec(app):\n    \"\"\"Update the status.last_applied_manifest of an application from spec.manifests\n\n    Args:\n        app (krake.data.kubernetes.Application): Application to update\n\n    This function is called on application creation and updates. The\n    last_applied_manifest of an application is initialized as a copy of spec.manifest,\n    and is augmented by all known observed fields not yet initialized (i.e. all observed\n    fields or resources which are present in the current last_applied_manifest but not\n    in the spec.manifest)\n\n    \"\"\"\n\n    # The new last_applied_manifest is initialized as a copy of the spec.manifest, and\n    # augmented by all observed fields which are present in the current\n    # last_applied_manifest but not in the original spec.manifest\n    new_last_applied_manifest = deepcopy(app.spec.manifest)\n\n    # Loop over observed resources and observed fields, and check if they should be\n    # added to the new last_applied_manifest (i.e. present in the current\n    # last_applied_manifest but not in spec.manifest)\n    for resource_observed in app.status.mangled_observer_schema:\n\n        # If the resource is not present in the current last_applied_manifest, there is\n        # nothing to do. Whether the resource was initialized by spec.manifest doesn't\n        # matter.\n        try:\n            idx_status_old = get_kubernetes_resource_idx(\n                app.status.last_applied_manifest, resource_observed\n            )\n        except IndexError:\n            continue\n\n        # As the resource is present in the current last_applied_manifest, we need to go\n        # through it to check if observed fields should be set to their current value\n        # (i.e. fields are present in the current last_applied_manifest, but not in\n        # spec.manifest)\n        try:\n            # Check if the observed resource is present in spec.manifest\n            idx_status_new = get_kubernetes_resource_idx(\n                new_last_applied_manifest, resource_observed\n            )\n        except IndexError:\n            # The resource is observed but is not present in the spec.manifest.\n            # Create an empty resource, which will be augmented in\n            # update_last_applied_manifest_dict_from_spec with the observed and known\n            # fields.\n            new_last_applied_manifest.append({})\n            idx_status_new = len(new_last_applied_manifest) - 1\n\n        update_last_applied_manifest_dict_from_spec(\n            new_last_applied_manifest[idx_status_new],\n            app.status.last_applied_manifest[idx_status_old],\n            resource_observed,\n        )\n\n    app.status.last_applied_manifest = new_last_applied_manifest\n\n\nclass KubernetesApplicationObserver(Observer):\n    \"\"\"Observer specific for Kubernetes Applications. One observer is created for each\n    Application managed by the Controller, but not one per Kubernetes resource\n    (Deployment, Service...). If several resources are defined by an Application, they\n    are all monitored by the same observer.\n\n    The observer gets the actual status of the resources on the cluster using the\n    Kubernetes API, and compare it to the status stored in the API.\n\n    The observer is:\n     * started at initial Krake resource creation;\n\n     * deleted when a resource needs to be updated, then started again when it is done;\n\n     * simply deleted on resource deletion.\n\n    Args:\n        cluster (krake.data.kubernetes.Cluster): the cluster on which the observed\n            Application is created.\n        resource (krake.data.kubernetes.Application): the application that will be\n            observed.\n        on_res_update (coroutine): a coroutine called when a resource's actual status\n            differs from the status sent by the database. Its signature is:\n            ``(resource) -> updated_resource``. ``updated_resource`` is the instance of\n            the resource that is up-to-date with the API. The Observer internal instance\n            of the resource to observe will be updated. If the API cannot be contacted,\n            ``None`` can be returned. In this case the internal instance of the Observer\n            will not be updated.\n        time_step (int, optional): how frequently the Observer should watch the actual\n            status of the resources.\n\n    \"\"\"\n\n    def __init__(self, cluster, resource, on_res_update, time_step=2):\n        super().__init__(resource, on_res_update, time_step)\n        self.cluster = cluster\n\n    async def poll_resource(self):\n        \"\"\"Fetch the current status of the Application monitored by the Observer.\n\n        Returns:\n            krake.data.core.Status: the status object created using information from the\n                real world Applications resource.\n\n        \"\"\"\n        app = self.resource\n\n        status = deepcopy(app.status)\n        status.last_observed_manifest = []\n        # For each observed kubernetes resource of the Application,\n        # get its current status on the cluster.\n        for desired_resource in app.status.last_applied_manifest:\n            kube = KubernetesClient(self.cluster.spec.kubeconfig)\n            idx_observed = get_kubernetes_resource_idx(\n                app.status.mangled_observer_schema, desired_resource\n            )\n            observed_resource = app.status.mangled_observer_schema[idx_observed]\n            async with kube:\n                try:\n                    group, version, kind, name, namespace = kube.get_immutables(\n                        desired_resource\n                    )\n                    resource_api = await kube.get_resource_api(group, version, kind)\n                    resp = await resource_api.read(kind, name, namespace)\n                except ApiException as err:\n                    if err.status == 404:\n                        # Resource does not exist\n                        continue\n                    # Otherwise, log the unexpected errors\n                    logger.error(err)\n\n            observed_manifest = update_last_observed_manifest_dict(\n                observed_resource, resp.to_dict()\n            )\n            status.last_observed_manifest.append(observed_manifest)\n\n        return status\n\n\nclass KubernetesClusterObserver(Observer):\n    \"\"\"Observer specific for Kubernetes Clusters. One observer is created for each\n    Cluster managed by the Controller.\n\n    The observer gets the actual status of the cluster using the\n    Kubernetes API, and compare it to the status stored in the API.\n\n    The observer is:\n     * started at initial Krake resource creation;\n\n     * deleted when a resource needs to be updated, then started again when it is done;\n\n     * simply deleted on resource deletion.\n\n    Args:\n        cluster (krake.data.kubernetes.Cluster): the cluster which will be observed.\n        on_res_update (coroutine): a coroutine called when a resource's actual status\n            differs from the status sent by the database. Its signature is:\n            ``(resource) -> updated_resource``. ``updated_resource`` is the instance of\n            the resource that is up-to-date with the API. The Observer internal instance\n            of the resource to observe will be updated. If the API cannot be contacted,\n            ``None`` can be returned. In this case the internal instance of the Observer\n            will not be updated.\n        time_step (int, optional): how frequently the Observer should watch the actual\n            status of the resources.\n\n    \"\"\"\n\n    def __init__(self, cluster, on_res_update, time_step=2):\n        super().__init__(cluster, on_res_update, time_step)\n        self.cluster = cluster\n\n    async def poll_resource(self):\n        \"\"\"Fetch the current status of the Cluster monitored by the Observer.\n\n        Returns:\n            krake.data.core.Status: the status object created using information from the\n                real world Cluster.\n\n        \"\"\"\n        status = deepcopy(self.cluster.status)\n        # For each observed kubernetes cluster registered in Krake,\n        # get its current node status.\n        loader = KubeConfigLoader(self.cluster.spec.kubeconfig)\n        config = Configuration()\n        await loader.load_and_set(config)\n        kube = ApiClient(config)\n\n        async with kube as api:\n            v1 = client.CoreV1Api(api)\n            try:\n                response = await v1.list_node()\n\n            except ClientConnectorError as err:\n                status.state = ClusterState.OFFLINE\n                self.cluster.status.state = ClusterState.OFFLINE\n                # Log the error\n                logger.debug(err)\n                return status\n\n            condition_dict = {\n                \"MemoryPressure\": [],\n                \"DiskPressure\": [],\n                \"PIDPressure\": [],\n                \"Ready\": [],\n            }\n\n            for item in response.items:\n                for condition in item.status.conditions:\n                    condition_dict[condition.type].append(condition.status)\n                if (\n                    condition_dict[\"MemoryPressure\"] == [\"True\"]\n                    or condition_dict[\"DiskPressure\"] == [\"True\"]\n                    or condition_dict[\"PIDPressure\"] == [\"True\"]\n                ):\n                    status.state = ClusterState.UNHEALTHY\n                    self.cluster.status.state = ClusterState.UNHEALTHY\n                    return status\n                elif (\n                    condition_dict[\"Ready\"] == [\"True\"]\n                    and status.state is ClusterState.OFFLINE\n                ):\n                    status.state = ClusterState.CONNECTING\n                    self.cluster.status.state = ClusterState.CONNECTING\n                    return status\n                elif condition_dict[\"Ready\"] == [\"True\"]:\n                    status.state = ClusterState.ONLINE\n                    self.cluster.status.state = ClusterState.ONLINE\n                    return status\n                else:\n                    status.state = ClusterState.NOTREADY\n                    self.cluster.status.state = ClusterState.NOTREADY\n                    return status\n\n\n@listen.on(HookType.ApplicationPostReconcile)\n@listen.on(HookType.ApplicationPostMigrate)\n@listen.on(HookType.ClusterCreation)\nasync def register_observer(controller, resource, start=True, **kwargs):\n    \"\"\"Create an observer for the given Application or Cluster, and start it as a\n    background task if wanted.\n\n    If an observer already existed for this Application or Cluster, it is stopped\n    and deleted.\n\n    Args:\n        controller (KubernetesController): the controller for which the observer will be\n            added in the list of working observers.\n        resource (krake.data.kubernetes.Application): the Application to observe or\n        resource (krake.data.kubernetes.Cluster): the Cluster to observe.\n        start (bool, optional): if False, does not start the observer as background\n            task.\n\n    \"\"\"\n    if resource.kind == Application.kind:\n        cluster = await controller.kubernetes_api.read_cluster(\n            namespace=resource.status.running_on.namespace,\n            name=resource.status.running_on.name,\n        )\n\n        observer = KubernetesApplicationObserver(\n            cluster,\n            resource,\n            controller.on_status_update,\n            time_step=controller.observer_time_step,\n        )\n\n    elif resource.kind == Cluster.kind:\n        observer = KubernetesClusterObserver(\n            resource,\n            controller.on_status_update,\n            time_step=controller.observer_time_step,\n        )\n    else:\n        logger.debug(\"Unknown resource kind. No observer was registered.\", resource)\n        return\n\n    logger.debug(f\"Start observer for {resource.kind} %r\", resource.metadata.name)\n    task = None\n    if start:\n        task = controller.loop.create_task(observer.run())\n\n    controller.observers[resource.metadata.uid] = (observer, task)\n\n\n@listen.on(HookType.ApplicationPreReconcile)\n@listen.on(HookType.ApplicationPreMigrate)\n@listen.on(HookType.ApplicationPreDelete)\n@listen.on(HookType.ClusterDeletion)\nasync def unregister_observer(controller, resource, **kwargs):\n    \"\"\"Stop and delete the observer for the given Application or Cluster. If no observer\n    is started, do nothing.\n\n    Args:\n        controller (KubernetesController): the controller for which the observer will be\n            removed from the list of working observers.\n        resource (krake.data.kubernetes.Application): the Application whose observer\n        will be stopped or\n        resource (krake.data.kubernetes.Cluster): the Cluster whose observer will be\n        stopped.\n\n    \"\"\"\n    if resource.metadata.uid not in controller.observers:\n        return\n\n    logger.debug(f\"Stop observer for {resource.kind} %r\", resource.metadata.name)\n    _, task = controller.observers.pop(resource.metadata.uid)\n    task.cancel()\n\n    with suppress(asyncio.CancelledError):\n        await task\n\n\ndef utc_difference():\n    \"\"\"Get the difference in seconds between the current time and the current UTC time.\n\n    Returns:\n        int: the time difference in seconds.\n\n    \"\"\"\n    delta = datetime.now() - datetime.utcnow()\n    return delta.seconds\n\n\ndef generate_certificate(config):\n    \"\"\"Create and sign a new certificate using the one defined in the complete hook\n    configuration as intermediate certificate.\n\n    Args:\n        config (krake.data.config.CompleteHookConfiguration): the configuration of the\n            complete hook.\n\n    Returns:\n        CertificatePair: the content of the certificate created and its corresponding\n            key.\n\n    \"\"\"\n    with open(config.intermediate_src, \"rb\") as f:\n        intermediate_src = crypto.load_certificate(crypto.FILETYPE_PEM, f.read())\n    with open(config.intermediate_key_src, \"rb\") as f:\n        intermediate_key_src = crypto.load_privatekey(crypto.FILETYPE_PEM, f.read())\n\n    client_cert = crypto.X509()\n\n    # Set general information\n    client_cert.set_version(3)\n    client_cert.set_serial_number(random.randint(50000000000000, 100000000000000))\n    # If not set before, TLS will not accept to use this certificate in UTC cases, as\n    # the server time may be earlier.\n    time_offset = utc_difference() * -1\n    client_cert.gmtime_adj_notBefore(time_offset)\n    client_cert.gmtime_adj_notAfter(1 * 365 * 24 * 60 * 60)\n\n    # Set issuer and subject\n    intermediate_subject = intermediate_src.get_subject()\n    client_cert.set_issuer(intermediate_subject)\n    client_subj = crypto.X509Name(intermediate_subject)\n    client_subj.CN = config.hook_user\n    client_cert.set_subject(client_subj)\n\n    # Create and set the private key\n    client_key = crypto.PKey()\n    client_key.generate_key(crypto.TYPE_RSA, 2048)\n    client_cert.set_pubkey(client_key)\n\n    client_cert.sign(intermediate_key_src, \"sha256\")\n\n    cert_dump = crypto.dump_certificate(crypto.FILETYPE_PEM, client_cert).decode()\n    key_dump = crypto.dump_privatekey(crypto.FILETYPE_PEM, client_key).decode()\n    return CertificatePair(cert=cert_dump, key=key_dump)\n\n\ndef generate_default_observer_schema(app):\n    \"\"\"Generate the default observer schema for each Kubernetes resource present in\n    ``spec.manifest`` for which a custom observer schema hasn't been specified.\n\n    Args:\n        app (krake.data.kubernetes.Application): The application for which to generate a\n            default observer schema\n    \"\"\"\n\n    app.status.mangled_observer_schema = deepcopy(app.spec.observer_schema)\n\n    for resource_manifest in app.spec.manifest:\n        try:\n            get_kubernetes_resource_idx(\n                app.status.mangled_observer_schema, resource_manifest\n            )\n\n        except IndexError:\n            # Only create a default observer schema, if a custom observer schema hasn't\n            # been set by the user.\n            app.status.mangled_observer_schema.append(\n                generate_default_observer_schema_dict(\n                    resource_manifest,\n                    first_level=True,\n                )\n            )\n\n\ndef generate_default_observer_schema_dict(manifest_dict, first_level=False):\n    \"\"\"Together with :func:``generate_default_observer_schema_list``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_dict`` or ``manifest_list``.\n\n    Args:\n        manifest_dict (dict): Partial Kubernetes resources\n        first_level (bool, optional): If True, indicates that the dictionary represents\n            the whole observer schema of a Kubernetes resource\n\n    Returns:\n        dict: Generated partial observer_schema\n\n    This function creates a new dictionary from ``manifest_dict`` and replaces all\n    non-list and non-dict values by ``None``.\n\n    In case of ``first_level`` dictionary (i.e. complete ``observer_schema`` for a\n    resource), the values of the identifying fields are copied from the manifest file.\n\n    \"\"\"\n    observer_schema_dict = {}\n\n    for key, value in manifest_dict.items():\n\n        if isinstance(value, dict):\n            observer_schema_dict[key] = generate_default_observer_schema_dict(value)\n\n        elif isinstance(value, list):\n            observer_schema_dict[key] = generate_default_observer_schema_list(value)\n\n        else:\n            observer_schema_dict[key] = None\n\n    if first_level:\n        observer_schema_dict[\"apiVersion\"] = manifest_dict[\"apiVersion\"]\n        observer_schema_dict[\"kind\"] = manifest_dict[\"kind\"]\n        observer_schema_dict[\"metadata\"][\"name\"] = manifest_dict[\"metadata\"][\"name\"]\n\n        if (\n            \"spec\" in manifest_dict\n            and \"type\" in manifest_dict[\"spec\"]\n            and manifest_dict[\"spec\"][\"type\"] == \"LoadBalancer\"\n        ):\n            observer_schema_dict[\"status\"] = {\"load_balancer\": {\"ingress\": None}}\n\n    return observer_schema_dict\n\n\ndef generate_default_observer_schema_list(manifest_list):\n    \"\"\"Together with :func:``generate_default_observer_schema_dict``, this function is\n    called recursively to generate part of a default ``observer_schema`` from part of a\n    Kubernetes resource, defined respectively by ``manifest_list`` or ``manifest_dict``.\n\n    Args:\n        manifest_list (list): Partial Kubernetes resources\n\n    Returns:\n        list: Generated partial observer_schema\n\n    This function creates a new list from ``manifest_list`` and replaces all non-list\n    and non-dict elements by ``None``.\n\n    Additionally, it generates the default list control dictionary, using the current\n    length of the list as default minimum and maximum values.\n\n    \"\"\"\n    observer_schema_list = []\n\n    for value in manifest_list:\n\n        if isinstance(value, dict):\n            observer_schema_list.append(generate_default_observer_schema_dict(value))\n\n        elif isinstance(value, list):\n            observer_schema_list.append(generate_default_observer_schema_list(value))\n\n        else:\n            observer_schema_list.append(None)\n\n    observer_schema_list.append(\n        {\n            \"observer_schema_list_min_length\": len(manifest_list),\n            \"observer_schema_list_max_length\": len(manifest_list),\n        }\n    )\n\n    return observer_schema_list\n\n\n@listen.on(HookType.ApplicationMangling)\nasync def complete(app, api_endpoint, ssl_context, config):\n    \"\"\"Execute application complete hook defined by :class:`Complete`.\n    Hook mangles given application and injects complete hooks variables.\n\n    Application complete hook is disabled by default.\n    User enables this hook by the --hook-complete argument in rok cli.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application object processed\n            when the hook is called\n        api_endpoint (str): the given API endpoint\n        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint\n        config (krake.data.config.HooksConfiguration): Complete hook\n            configuration.\n\n    \"\"\"\n    if \"complete\" not in app.spec.hooks:\n        return\n\n    # Use the endpoint of the API only if the external endpoint has not been set.\n    if config.complete.external_endpoint:\n        api_endpoint = config.complete.external_endpoint\n\n    app.status.complete_token = \\\n        app.status.complete_token if app.status.complete_token else token_urlsafe()\n\n    # Generate only once the certificate and key for a specific Application\n    generated_cert = CertificatePair(\n        cert=app.status.complete_cert, key=app.status.complete_key\n    )\n    if ssl_context and generated_cert == (None, None):\n        generated_cert = generate_certificate(config.complete)\n        app.status.complete_cert = generated_cert.cert\n        app.status.complete_key = generated_cert.key\n\n    hook = Complete(\n        api_endpoint,\n        ssl_context,\n        hook_user=config.complete.hook_user,\n        cert_dest=config.complete.cert_dest,\n        env_token=config.complete.env_token,\n        env_url=config.complete.env_url,\n    )\n    hook.mangle_app(\n        app.metadata.name,\n        app.metadata.namespace,\n        app.status.complete_token,\n        app.status.last_applied_manifest,\n        config.complete.intermediate_src,\n        generated_cert,\n        app.status.mangled_observer_schema,\n        \"complete\"\n    )\n\n\n@listen.on(HookType.ApplicationMangling)\nasync def shutdown(app, api_endpoint, ssl_context, config):\n    \"\"\"Executes an application shutdown hook defined by :class:`Shutdown`.\n    The hook mangles the given application and injects shutdown hooks variables.\n\n    Application shutdown hook is disabled by default.\n    User enables this hook by the --hook-shutdown argument in rok cli.\n\n    Args:\n        app (krake.data.kubernetes.Application): Application object processed\n            when the hook is called\n        api_endpoint (str): the given API endpoint\n        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint\n        config (krake.data.config.HooksConfiguration): Shutdown hook\n            configuration.\n\n    \"\"\"\n    if \"shutdown\" not in app.spec.hooks:\n        return\n\n    # Use the endpoint of the API only if the external endpoint has not been set.\n    if config.shutdown.external_endpoint:\n        api_endpoint = config.shutdown.external_endpoint\n\n    app.status.shutdown_token = \\\n        app.status.shutdown_token if app.status.shutdown_token else token_urlsafe()\n\n    # Generate only once the certificate and key for a specific Application\n    generated_cert = CertificatePair(\n        cert=app.status.shutdown_cert, key=app.status.shutdown_key\n    )\n    if ssl_context and generated_cert == (None, None):\n        generated_cert = generate_certificate(config.shutdown)\n        app.status.shutdown_cert = generated_cert.cert\n        app.status.shutdown_key = generated_cert.key\n\n    hook = Shutdown(\n        api_endpoint,\n        ssl_context,\n        hook_user=config.shutdown.hook_user,\n        cert_dest=config.shutdown.cert_dest,\n        env_token=config.shutdown.env_token,\n        env_url=config.shutdown.env_url,\n    )\n    hook.mangle_app(\n        app.metadata.name,\n        app.metadata.namespace,\n        app.status.shutdown_token,\n        app.status.last_applied_manifest,\n        config.shutdown.intermediate_src,\n        generated_cert,\n        app.status.mangled_observer_schema,\n        \"shutdown\"\n    )\n\n\n@listen.on(HookType.ResourcePreDelete)\nasync def pre_shutdown(controller, app, **kwargs):\n    \"\"\"\n\n    Args:\n        app (krake.data.kubernetes.Application): Application object processed\n            when the hook is called\n    \"\"\"\n    if \"shutdown\" not in app.spec.hooks:\n        return\n\n    return\n\n\nclass SubResource(NamedTuple):\n    group: str\n    name: str\n    body: dict\n    path: tuple\n\n\nclass CertificatePair(NamedTuple):\n    \"\"\"Tuple which contains a certificate and its corresponding key.\n\n    Attributes:\n        cert (str): content of a certificate.\n        key (str): content of the key that corresponds to the certificate.\n\n    \"\"\"\n\n    cert: str\n    key: str\n\n\nclass Hook(object):\n\n    hook_resources = ()\n\n    ca_name = \"ca-bundle.pem\"\n    cert_name = \"cert.pem\"\n    key_name = \"key.pem\"\n\n    def __init__(\n        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n    ):\n        self.api_endpoint = api_endpoint\n        self.ssl_context = ssl_context\n        self.hook_user = hook_user\n        self.cert_dest = cert_dest\n        self.env_token = env_token\n        self.env_url = env_url\n\n    def mangle_app(\n        self,\n        name,\n        namespace,\n        token,\n        last_applied_manifest,\n        intermediate_src,\n        generated_cert,\n        mangled_observer_schema,\n        hook_type=\"\",\n    ):\n        \"\"\"Mangle a given application and inject complete hook resources and\n        sub-resources into the :attr:`last_applied_manifest` object by :meth:`mangle`.\n        Also mangle the observer_schema as new resources and sub-resources should\n        be observed.\n\n        :attr:`last_applied_manifest` is created as a deep copy of the desired\n        application resources, as defined by user. It can be updated by custom hook\n        resources or modified by custom hook sub-resources. It is used as a desired\n        state for the Krake deployment process.\n\n        Args:\n            name (str): Application name\n            namespace (str): Application namespace\n            token (str): Complete hook authentication token\n            last_applied_manifest (list): Application resources\n            intermediate_src (str): content of the certificate that is used to sign new\n                certificates for the complete hook.\n            generated_cert (CertificatePair): tuple that contains the content of the\n                new signed certificate for the Application, and the content of its\n                corresponding key.\n            mangled_observer_schema (list): Observed fields\n            hook_type (str, optional): Name of the hook the app should be mangled for\n\n        \"\"\"\n\n        secret_certs_name = \"-\".join([name, \"krake\", hook_type, \"secret\", \"certs\"])\n        secret_token_name = \"-\".join([name, \"krake\", hook_type, \"secret\", \"token\"])\n        volume_name = \"-\".join([name, \"krake\", hook_type, \"volume\"])\n        ca_certs = (\n            self.ssl_context.get_ca_certs(binary_form=True)\n            if self.ssl_context\n            else None\n        )\n\n        # Extract all different namespaces\n        # FIXME: too many assumptions here: do we create one ConfigMap for each\n        #  namespace?\n        resource_namespaces = {\n            resource[\"metadata\"].get(\"namespace\", \"default\")\n            for resource in last_applied_manifest\n        }\n\n        hook_resources = []\n        hook_sub_resources = []\n        if ca_certs:\n            hook_resources.extend(\n                [\n                    self.secret_certs(\n                        secret_certs_name,\n                        resource_namespace,\n                        intermediate_src=intermediate_src,\n                        generated_cert=generated_cert,\n                        ca_certs=ca_certs,\n                    )\n                    for resource_namespace in resource_namespaces\n                ]\n            )\n            hook_sub_resources.extend(\n                [*self.volumes(secret_certs_name, volume_name, self.cert_dest)]\n            )\n\n        hook_resources.extend(\n            [\n                self.secret_token(\n                    secret_token_name,\n                    name,\n                    namespace,\n                    resource_namespace,\n                    self.api_endpoint,\n                    token,\n                )\n                for resource_namespace in resource_namespaces\n            ]\n        )\n        hook_sub_resources.extend(\n            [\n                *self.env_vars(secret_token_name),\n            ]\n        )\n\n        self.mangle(\n            hook_resources,\n            last_applied_manifest,\n            mangled_observer_schema,\n        )\n        self.mangle(\n            hook_sub_resources,\n            last_applied_manifest,\n            mangled_observer_schema,\n            is_sub_resource=True,\n        )\n\n    def mangle(\n        self,\n        items,\n        last_applied_manifest,\n        mangled_observer_schema,\n        is_sub_resource=False,\n    ):\n        \"\"\"Mangle applications desired state with custom hook resources or\n        sub-resources.\n\n        Example:\n            .. code:: python\n\n            last_applied_manifest = [\n                {\n                    'apiVersion': 'v1',\n                    'kind': 'Pod',\n                    'metadata': {'name': 'test', 'namespace': 'default'},\n                    'spec': {'containers': [{'name': 'test'}]}\n                }\n            ]\n            mangled_observer_schema = [\n                {\n                    'apiVersion': 'v1',\n                    'kind': 'Pod',\n                    'metadata': {'name': 'test', 'namespace': 'default'},\n                    'spec': {\n                        'containers': [\n                            {'name': None},\n                            {\n                                'observer_schema_list_max_length': 1,\n                                'observer_schema_list_min_length': 1,\n                            },\n                        ]\n                    },\n                }\n            ]\n            hook_resources = [\n                {\n                    'apiVersion': 'v1',\n                    'kind': 'Secret',\n                    'metadata': {'name': 'sct', 'namespace': 'default'}\n                }\n            ]\n            hook_sub_resources = [\n                SubResource(\n                    group='env', name='env', body={'name': 'test', 'value': 'test'},\n                    path=(('spec', 'containers'),)\n                )\n            ]\n\n            mangle(\n                hook_resources,\n                last_applied_manifest,\n                mangled_observer_schema,\n            )\n            mangle(\n                hook_sub_resources,\n                last_applied_manifest,\n                mangled_observer_schema,\n                is_sub_resource=True\n            )\n\n            assert last_applied_manifest == [\n                {\n                    \"apiVersion\": \"v1\",\n                    \"kind\": \"Pod\",\n                    \"metadata\": {\"name\": \"test\", 'namespace': 'default'},\n                    \"spec\": {\n                        \"containers\": [\n                            {\n                                \"name\": \"test\",\n                                \"env\": [{\"name\": \"test\", \"value\": \"test\"}]\n                            }\n                        ]\n                    },\n                },\n                {\"apiVersion\": \"v1\", \"kind\": \"Secret\", \"metadata\": {\"name\": \"sct\"}},\n            ]\n\n            assert mangled_observer_schema == [\n                {\n                    \"apiVersion\": \"v1\",\n                    \"kind\": \"Pod\",\n                    \"metadata\": {\"name\": \"test\", \"namespace\": None},\n                    \"spec\": {\n                        \"containers\": [\n                            {\n                                \"name\": None,\n                                \"env\": [\n                                    {\"name\": None, \"value\": None},\n                                    {\n                                        \"observer_schema_list_max_length\": 1,\n                                        \"observer_schema_list_min_length\": 1,\n                                    },\n                                ],\n                            },\n                            {\n                                \"observer_schema_list_max_length\": 1,\n                                \"observer_schema_list_min_length\": 1,\n                            },\n                        ]\n                    },\n                },\n                {\n                    \"apiVersion\": \"v1\",\n                    \"kind\": \"Secret\",\n                    \"metadata\": {\"name\": \"sct\", \"namespace\": None},\n                },\n            ]\n\n        Args:\n            items (list[SubResource]): Custom hook resources or sub-resources\n            last_applied_manifest (list): Application resources\n            mangled_observer_schema (list): Observed resources\n            is_sub_resource (bool, optional): if False, the function only extend the\n                list of Kubernetes resources defined in :attr:`last_applied_manifest`\n                with new hook resources. Otherwise, the function injects each new hook\n                sub-resource into the :attr:`last_applied_manifest` object\n                sub-resources. Defaults to False.\n\n        \"\"\"\n\n        if not items:\n            return\n\n        if not is_sub_resource:\n            last_applied_manifest.extend(items)\n            for sub_resource in items:\n                # Generate the default observer schema for each resource\n                mangled_observer_schema.append(\n                    generate_default_observer_schema_dict(\n                        sub_resource,\n                        first_level=True,\n                    )\n                )\n            return\n\n        def inject(sub_resource, sub_resource_to_mangle, observed_resource_to_mangle):\n            \"\"\"Inject a hooks defined sub-resource into a Kubernetes sub-resource.\n\n            Args:\n                sub_resource (SubResource): Hook sub-resource that needs to be injected\n                    into :attr:`last_applied_manifest`\n                sub_resource_to_mangle (object): Kubernetes sub-resources from\n                    :attr:`last_applied_manifest` which need to be processed\n                observed_resource_to_mangle (dict): partial mangled_observer_schema\n                    corresponding to the Kubernetes sub-resource.\n\n            Raises:\n                InvalidManifestError: if the sub-resource which will be mangled is not a\n                    list or a dict.\n\n            \"\"\"\n\n            # Create sub-resource group if not present in the Kubernetes sub-resource\n            if sub_resource.group not in sub_resource_to_mangle:\n                # FIXME: This assumes the subresource group contains a list\n                sub_resource_to_mangle.update({sub_resource.group: []})\n\n            # Create sub-resource group if not present in the observed fields\n            if sub_resource.group not in observed_resource_to_mangle:\n                observed_resource_to_mangle.update(\n                    {\n                        sub_resource.group: [\n                            {\n                                \"observer_schema_list_min_length\": 0,\n                                \"observer_schema_list_max_length\": 0,\n                            }\n                        ]\n                    }\n                )\n\n            # Inject sub-resource\n            # If sub-resource name is already there update it, if not, append it\n            if sub_resource.name in [\n                g[\"name\"] for g in sub_resource_to_mangle[sub_resource.group]\n            ]:\n                # FIXME: Assuming we are dealing with a list\n                for idx, item in enumerate(sub_resource_to_mangle[sub_resource.group]):\n                    if item[\"name\"]:\n                        if hasattr(item, \"body\"):\n                            sub_resource_to_mangle[item.group][idx] = item[\"body\"]\n            else:\n                sub_resource_to_mangle[sub_resource.group].append(sub_resource.body)\n\n            # Make sure the value is observed\n            if sub_resource.name not in [\n                g[\"name\"] for g in observed_resource_to_mangle[sub_resource.group][:-1]\n            ]:\n                observed_resource_to_mangle[sub_resource.group].insert(\n                    -1, generate_default_observer_schema_dict(sub_resource.body)\n                )\n                observed_resource_to_mangle[sub_resource.group][-1][\n                    \"observer_schema_list_min_length\"\n                ] += 1\n                observed_resource_to_mangle[sub_resource.group][-1][\n                    \"observer_schema_list_max_length\"\n                ] += 1\n\n        for resource in last_applied_manifest:\n            # Complete hook is applied only on defined Kubernetes resources\n            if resource[\"kind\"] not in self.hook_resources:\n                continue\n\n            for sub_resource in items:\n                sub_resources_to_mangle = None\n                idx_observed = get_kubernetes_resource_idx(\n                    mangled_observer_schema, resource\n                )\n                for keys in sub_resource.path:\n                    try:\n                        sub_resources_to_mangle = reduce(getitem, keys, resource)\n                    except KeyError:\n                        continue\n\n                    break\n\n                # Create the path to the observed sub-resource, if it doesn't yet exist\n                try:\n                    observed_sub_resources = reduce(\n                        getitem, keys, mangled_observer_schema[idx_observed]\n                    )\n                except KeyError:\n                    Complete.create_path(\n                        mangled_observer_schema[idx_observed], list(keys)\n                    )\n                    observed_sub_resources = reduce(\n                        getitem, keys, mangled_observer_schema[idx_observed]\n                    )\n\n                if isinstance(sub_resources_to_mangle, list):\n                    for idx, sub_resource_to_mangle in enumerate(\n                        sub_resources_to_mangle\n                    ):\n\n                        # Ensure that each element of the list is observed.\n                        idx_observed = idx\n                        if idx >= len(observed_sub_resources[:-1]):\n                            idx_observed = len(observed_sub_resources[:-1])\n                            # FIXME: Assuming each element of the list contains a\n                            # dictionary, therefore initializing new elements with an\n                            # empty dict\n                            observed_sub_resources.insert(-1, {})\n                        observed_sub_resource = observed_sub_resources[idx_observed]\n\n                        # FIXME: This is assuming a list always contains dict\n                        inject(\n                            sub_resource, sub_resource_to_mangle, observed_sub_resource\n                        )\n\n                elif isinstance(sub_resources_to_mangle, dict):\n                    inject(\n                        sub_resource, sub_resources_to_mangle, observed_sub_resources\n                    )\n\n                else:\n                    message = (\n                        f\"The sub-resource to mangle {sub_resources_to_mangle!r} has an\"\n                        \"invalid type, should be in '[dict, list]'\"\n                    )\n                    raise InvalidManifestError(message)\n\n    @staticmethod\n    def attribute_map(obj):\n        \"\"\"Convert a Kubernetes object to dict based on its attribute mapping\n\n        Example:\n            .. code:: python\n\n            from kubernetes_asyncio.client import V1VolumeMount\n\n            d = attribute_map(\n                    V1VolumeMount(name=\"name\", mount_path=\"path\")\n            )\n            assert d == {'mountPath': 'path', 'name': 'name'}\n\n        Args:\n            obj (object): Kubernetes object\n\n        Returns:\n            dict: Converted Kubernetes object\n\n        \"\"\"\n        return {\n            obj.attribute_map[attr]: getattr(obj, attr)\n            for attr, _ in obj.to_dict().items()\n            if getattr(obj, attr) is not None\n        }\n\n    @staticmethod\n    def create_path(mangled_observer_schema, keys):\n        \"\"\"Create the path to the observed field in the observer schema.\n\n        When a sub-resource is mangled, it should be observed. This function creates\n        the path to the subresource to observe.\n\n        Args:\n            mangled_observer_schema (dict): Partial observer schema of a resource\n            keys (list): list of keys forming the path to the sub-resource to\n                observe\n\n        FIXME: This assumes we are only adding keys to dict. We don't consider lists\n\n        \"\"\"\n\n        # Unpack the first key first, as it contains the base directory\n        key = keys.pop(0)\n\n        # If the key is the last of the list, we reached the end of the path.\n        if len(keys) == 0:\n            mangled_observer_schema[key] = None\n            return\n\n        if key not in mangled_observer_schema:\n            mangled_observer_schema[key] = {}\n        Hook.create_path(mangled_observer_schema[key], keys)\n\n    def secret_certs(\n        self,\n        secret_name,\n        namespace,\n        ca_certs=None,\n        intermediate_src=None,\n        generated_cert=None,\n    ):\n        \"\"\"Create a complete hooks secret resource.\n\n        Complete hook secret stores Krake CAs and client certificates to communicate\n        with the Krake API.\n\n        Args:\n            secret_name (str): Secret name\n            namespace (str): Kubernetes namespace where the Secret will be created.\n            ca_certs (list): Krake CA list\n            intermediate_src (str): content of the certificate that is used to sign new\n                certificates for the complete hook.\n            generated_cert (CertificatePair): tuple that contains the content of the\n                new signed certificate for the Application, and the content of its\n                corresponding key.\n\n        Returns:\n            dict: complete hook secret resource\n\n        \"\"\"\n        ca_certs_pem = \"\"\n        for ca_cert in ca_certs:\n            x509 = crypto.load_certificate(crypto.FILETYPE_ASN1, ca_cert)\n            ca_certs_pem += crypto.dump_certificate(crypto.FILETYPE_PEM, x509).decode()\n\n        # Add the intermediate certificate into the chain\n        with open(intermediate_src, \"r\") as f:\n            intermediate_src_content = f.read()\n        ca_certs_pem += intermediate_src_content\n\n        data = {\n            self.ca_name: self._encode_to_64(ca_certs_pem),\n            self.cert_name: self._encode_to_64(generated_cert.cert),\n            self.key_name: self._encode_to_64(generated_cert.key),\n        }\n        return self.secret(secret_name, data, namespace)\n\n    def secret_token(\n        self, secret_name, name, namespace, resource_namespace, api_endpoint, token\n    ):\n        \"\"\"Create a hooks secret resource.\n\n        The hook secret stores Krake authentication token\n        and hook URL for given application.\n\n        Args:\n            secret_name (str): Secret name\n            name (str): Application name\n            namespace (str): Application namespace\n            resource_namespace (str): Kubernetes namespace where the\n                Secret will be created.\n            api_endpoint (str): Krake API endpoint\n            token (str): Complete hook authentication token\n\n        Returns:\n            dict: complete hook secret resource\n\n        \"\"\"\n        pass\n\n    def volumes(self, secret_name, volume_name, mount_path):\n        \"\"\"Create complete hooks volume and volume mount sub-resources\n\n        Complete hook volume gives access to hook's secret, which stores\n        Krake CAs and client certificates to communicate with the Krake API.\n        Complete hook volume mount puts the volume into the application\n\n        Args:\n            secret_name (str): Secret name\n            volume_name (str): Volume name\n            mount_path (list): Volume mount path\n\n        Returns:\n            list: List of complete hook volume and volume mount sub-resources\n\n        \"\"\"\n        volume = V1Volume(name=volume_name, secret={\"secretName\": secret_name})\n        volume_mount = V1VolumeMount(name=volume_name, mount_path=mount_path)\n        return [\n            SubResource(\n                group=\"volumes\",\n                name=volume.name,\n                body=self.attribute_map(volume),\n                path=((\"spec\", \"template\", \"spec\"), (\"spec\",)),\n            ),\n            SubResource(\n                group=\"volumeMounts\",\n                name=volume_mount.name,\n                body=self.attribute_map(volume_mount),\n                path=(\n                    (\"spec\", \"template\", \"spec\", \"containers\"),\n                    (\"spec\", \"containers\"),  # kind: Pod\n                ),\n            ),\n        ]\n\n    @staticmethod\n    def _encode_to_64(string):\n        \"\"\"Compute the base 64 encoding of a string.\n\n        Args:\n            string (str): the string to encode.\n\n        Returns:\n            str: the result of the encoding.\n\n        \"\"\"\n        return b64encode(string.encode()).decode()\n\n    def secret(self, secret_name, secret_data, namespace, _type=\"Opaque\"):\n        \"\"\"Create a secret resource.\n\n        Args:\n            secret_name (str): Secret name\n            secret_data (dict): Secret data\n            namespace (str): Kubernetes namespace where the Secret will be created.\n            _type (str, optional): Secret type. Defaults to Opaque.\n\n        Returns:\n            dict: secret resource\n\n        \"\"\"\n        return self.attribute_map(\n            V1Secret(\n                api_version=\"v1\",\n                kind=\"Secret\",\n                data=secret_data,\n                metadata={\"name\": secret_name, \"namespace\": namespace},\n                type=_type,\n            )\n        )\n\n    @staticmethod\n    def create_hook_url(name, namespace, api_endpoint):\n        \"\"\"Create an applications' hook URL.\n        Function needs to be specified for each hook.\n\n        Args:\n            name (str): Application name\n            namespace (str): Application namespace\n            api_endpoint (str): Krake API endpoint\n\n        Returns:\n            str: Application shutdown url\n\n        \"\"\"\n        pass\n\n    def env_vars(self, secret_name):\n        \"\"\"Create the hooks' environment variables sub-resources.\n        Function needs to be specified for each hook.\n\n        Creates hook environment variables to store Krake authentication token\n        and a hook URL for the given applications.\n\n        Args:\n            secret_name (str): Secret name\n\n        Returns:\n            list: List of shutdown hook environment variables sub-resources\n\n        \"\"\"\n        pass\n\n\nclass Complete(Hook):\n    \"\"\"Mangle given application and inject complete hooks variables into it.\n\n    Hook injects a Kubernetes secret, which stores Krake authentication token\n    and the Krake complete hook URL for the given application. The variables\n    from Kubernetes secret are imported as environment variables\n    into the application resource definition. Only resources defined in\n    :args:`hook_resources` can be modified.\n\n    Names of environment variables are defined in the application controller\n    configuration file.\n\n    If TLS is enabled on the Krake API, the complete hook injects a Kubernetes secret,\n    and it's corresponding volume and volume mount definitions for the Krake CA,\n    the client certificate with the right CN, and its key. The directory where the\n    secret is mounted is defined in the configuration.\n\n    Args:\n        api_endpoint (str): the given API endpoint\n        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint\n        cert_dest (str, optional): Path of the directory where the CA, client\n            certificate and key to the Krake API will be stored.\n        env_token (str, optional): Name of the environment variable, which stores Krake\n            authentication token.\n        env_url (str, optional): Name of the environment variable,\n            which stores Krake complete hook URL.\n\n    \"\"\"\n\n    hook_resources = (\"Pod\", \"Deployment\", \"ReplicationController\")\n\n    def __init__(\n        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n    ):\n        super().__init__(\n            api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n        )\n        self.env_url = env_url\n\n    def secret_token(\n        self, secret_name, name, namespace, resource_namespace, api_endpoint, token\n    ):\n        \"\"\"Create complete hooks secret resource.\n\n        Complete hook secret stores Krake authentication token\n        and complete hook URL for given application.\n\n        Args:\n            secret_name (str): Secret name\n            name (str): Application name\n            namespace (str): Application namespace\n            resource_namespace (str): Kubernetes namespace where the\n                Secret will be created.\n            api_endpoint (str): Krake API endpoint\n            token (str): Complete hook authentication token\n\n        Returns:\n            dict: complete hook secret resource\n\n        \"\"\"\n        complete_url = self.create_hook_url(name, namespace, api_endpoint)\n        data = {\n            self.env_token.lower(): self._encode_to_64(token),\n            self.env_url.lower(): self._encode_to_64(complete_url),\n        }\n        return self.secret(secret_name, data, resource_namespace)\n\n    @staticmethod\n    def create_hook_url(name, namespace, api_endpoint):\n        \"\"\"Create an applications' complete URL.\n\n        Args:\n            name (str): Application name\n            namespace (str): Application namespace\n            api_endpoint (str): Krake API endpoint\n\n        Returns:\n            str: Application complete url\n\n        \"\"\"\n        api_url = URL(api_endpoint)\n        return str(\n            api_url.with_path(\n                f\"/kubernetes/namespaces/{namespace}/applications/{name}/complete\"\n            )\n        )\n\n    def env_vars(self, secret_name):\n        \"\"\"Create complete hooks environment variables sub-resources\n\n        Create complete hook environment variables store Krake authentication token\n        and complete hook URL for given application.\n\n        Args:\n            secret_name (str): Secret name\n\n        Returns:\n            list: List of complete hook environment variables sub-resources\n\n        \"\"\"\n        sub_resources = []\n\n        env_token = V1EnvVar(\n            name=self.env_token,\n            value_from=self.attribute_map(\n                V1EnvVarSource(\n                    secret_key_ref=self.attribute_map(\n                        V1SecretKeySelector(\n                            name=secret_name, key=self.env_token.lower()\n                        )\n                    )\n                )\n            ),\n        )\n        env_url = V1EnvVar(\n            name=self.env_url,\n            value_from=self.attribute_map(\n                V1EnvVarSource(\n                    secret_key_ref=self.attribute_map(\n                        V1SecretKeySelector(name=secret_name, key=self.env_url.lower())\n                    )\n                )\n            ),\n        )\n\n        for env in (env_token, env_url):\n            sub_resources.append(\n                SubResource(\n                    group=\"env\",\n                    name=env.name,\n                    body=self.attribute_map(env),\n                    path=(\n                        (\"spec\", \"template\", \"spec\", \"containers\"),\n                        (\"spec\", \"containers\"),  # kind: Pod\n                    ),\n                )\n            )\n        return sub_resources\n\n\nclass Shutdown(Hook):\n    \"\"\"Mangle given application and inject shutdown hooks variables into it.\n\n    Hook injects a Kubernetes secret, which stores Krake authentication token\n    and the Krake complete hook URL for the given application. The variables\n    from the Kubernetes secret are imported as environment variables\n    into the application resource definition. Only resources defined in\n    :args:`hook_resources` can be modified.\n\n    Names of environment variables are defined in the application controller\n    configuration file.\n\n    If TLS is enabled on the Krake API, the shutdown hook injects a Kubernetes secret,\n    and it's corresponding volume and volume mount definitions for the Krake CA,\n    the client certificate with the right CN, and its key. The directory where the\n    secret is mounted is defined in the configuration.\n\n    Args:\n        api_endpoint (str): the given API endpoint\n        ssl_context (ssl.SSLContext): SSL context to communicate with the API endpoint\n        cert_dest (str, optional): Path of the directory where the CA, client\n            certificate and key to the Krake API will be stored.\n        env_token (str, optional): Name of the environment variable, which stores Krake\n            authentication token.\n        env_url (str, optional): Name of the environment variable,\n            which stores Krake complete hook URL.\n\n    \"\"\"\n\n    hook_resources = (\"Pod\", \"Deployment\", \"ReplicationController\")\n\n    def __init__(\n        self, api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n    ):\n        super().__init__(\n            api_endpoint, ssl_context, hook_user, cert_dest, env_token, env_url\n        )\n        self.env_url = env_url\n\n    def secret_token(\n        self, secret_name, name, namespace, resource_namespace, api_endpoint, token\n    ):\n        \"\"\"Create shutdown hooks secret resource.\n\n        Shutdown hook secret stores Krake authentication token\n        and shutdown hook URL for given application.\n\n        Args:\n            secret_name (str): Secret name\n            name (str): Application name\n            namespace (str): Application namespace\n            resource_namespace (str): Kubernetes namespace where the\n                Secret will be created.\n            api_endpoint (str): Krake API endpoint\n            token (str): Shutdown hook authentication token\n\n        Returns:\n            dict: shutdown hook secret resource\n\n        \"\"\"\n        shutdown_url = self.create_hook_url(name, namespace, api_endpoint)\n        data = {\n            self.env_token.lower(): self._encode_to_64(token),\n            self.env_url.lower(): self._encode_to_64(shutdown_url),\n        }\n        return self.secret(secret_name, data, resource_namespace)\n\n    @staticmethod\n    def create_hook_url(name, namespace, api_endpoint):\n        \"\"\"Create an applications' shutdown URL.\n\n        Args:\n            name (str): Application name\n            namespace (str): Application namespace\n            api_endpoint (str): Krake API endpoint\n\n        Returns:\n            str: Application shutdown url\n\n        \"\"\"\n        api_url = URL(api_endpoint)\n        return str(\n            api_url.with_path(\n                f\"/kubernetes/namespaces/{namespace}/applications/{name}/shutdown\"\n            )\n        )\n\n    def env_vars(self, secret_name):\n        \"\"\"Create shutdown hooks environment variables sub-resources.\n\n        Creates shutdown hook environment variables to store Krake authentication token\n        and a shutdown hook URL for given applications.\n\n        Args:\n            secret_name (str): Secret name\n\n        Returns:\n            list: List of shutdown hook environment variables sub-resources\n\n        \"\"\"\n        sub_resources = []\n\n        env_resources = []\n\n        env_token = V1EnvVar(\n            name=self.env_token,\n            value_from=self.attribute_map(\n                V1EnvVarSource(\n                    secret_key_ref=self.attribute_map(\n                        V1SecretKeySelector(\n                            name=secret_name,\n                            key=self.env_token.lower()\n                        )\n                    )\n                )\n            )\n        )\n        env_resources.append(env_token)\n\n        env_url = V1EnvVar(\n            name=self.env_url,\n            value_from=self.attribute_map(\n                V1EnvVarSource(\n                    secret_key_ref=self.attribute_map(\n                        V1SecretKeySelector(name=secret_name, key=self.env_url.lower())\n                    )\n                )\n            ),\n        )\n        env_resources.append(env_url)\n\n        for env in env_resources:\n            sub_resources.append(\n                SubResource(\n                    group=\"env\",\n                    name=env.name,\n                    body=self.attribute_map(env),\n                    path=(\n                        (\"spec\", \"template\", \"spec\", \"containers\"),\n                        (\"spec\", \"containers\"),  # kind: Pod\n                    ),\n                )\n            )\n        return sub_resources\n",
            "file_path": "krake/krake/controller/kubernetes/hooks.py",
            "human_label": "Return a decorator function to add a new handler to the \"hook\" in the registry in the class.",
            "level": "class_runnable",
            "lineno": "97",
            "name": "on",
            "oracle_context": "{ \"apis\" : \"['append']\", \"classes\" : \"[]\", \"vars\" : \"['registry']\" }",
            "package": "hooks",
            "project": "rak-n-rok/Krake",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b87d24d292efb640a55670",
            "all_context": "{ \"import\" : \"typing errno sys os functools subprocess re typing functools \", \"file\" : \"get_keywords() ; get_config() ; register_vcs_handler(vcs,method) ; run_command(commands,args,cwd,verbose,hide_stderr,env) ; versions_from_parentdir(parentdir_prefix,root,verbose) ; git_get_keywords(versionfile_abs) ; git_versions_from_keywords(keywords,tag_prefix,verbose) ; git_pieces_from_vcs(tag_prefix,root,verbose,runner) ; plus_or_dot(pieces) ; render_pep440(pieces) ; render_pep440_branch(pieces) ; pep440_split_post(ver) ; render_pep440_pre(pieces) ; render_pep440_post(pieces) ; render_pep440_post_branch(pieces) ; render_pep440_old(pieces) ; render_git_describe(pieces) ; render_git_describe_long(pieces) ; render(pieces,style) ; get_versions() ; \", \"class\" : \"\" }",
            "code": "def get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for _ in cfg.versionfile_source.split(\"/\"):\n            root = os.path.dirname(root)\n    except NameError:\n        return {\n            \"version\": \"0+unknown\",\n            \"full-revisionid\": None,\n            \"dirty\": None,\n            \"error\": \"unable to find root of source tree\",\n            \"date\": None,\n        }\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {\n        \"version\": \"0+unknown\",\n        \"full-revisionid\": None,\n        \"dirty\": None,\n        \"error\": \"unable to compute version\",\n        \"date\": None,\n    }\n",
            "dependency": "{ \"builtin\" : false, \"standard_lib\" : true, \"public_lib\" : false, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Get version information or return default if unable to do so.",
            "end_lineno": "686",
            "file_content": "# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.22 (https://github.com/python-versioneer/python-versioneer)\n\n\"\"\"Git implementation of _version.py.\"\"\"\n\nimport errno\nimport functools\nimport os\nimport re\nimport subprocess\nimport sys\nfrom typing import Callable, Dict\n\n\ndef get_keywords():\n    \"\"\"Get the keywords needed to look up the version information.\"\"\"\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = \"$Format:%d$\"\n    git_full = \"$Format:%H$\"\n    git_date = \"$Format:%ci$\"\n    keywords = {\"refnames\": git_refnames, \"full\": git_full, \"date\": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n\n\ndef get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    # these strings are filled in when 'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"src/prestoplot/_version.py\"\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n\nLONG_VERSION_PY: Dict[str, str] = {}\nHANDLERS: Dict[str, Dict[str, Callable]] = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"Call the given command(s).\"\"\"\n    assert isinstance(commands, list)\n    process = None\n\n    popen_kwargs = {}\n    if sys.platform == \"win32\":\n        # This hides the console window if pythonw.exe is used\n        startupinfo = subprocess.STARTUPINFO()\n        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n        popen_kwargs[\"startupinfo\"] = startupinfo\n\n    for command in commands:\n        try:\n            dispcmd = str([command] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            process = subprocess.Popen(\n                [command] + args,\n                cwd=cwd,\n                env=env,\n                stdout=subprocess.PIPE,\n                stderr=(subprocess.PIPE if hide_stderr else None),\n                **popen_kwargs\n            )\n            break\n        except OSError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(\"unable to find command, tried %s\" % (commands,))\n        return None, None\n    stdout = process.communicate()[0].strip().decode()\n    if process.returncode != 0:\n        if verbose:\n            print(\"unable to run %s (error)\" % dispcmd)\n            print(\"stdout was %s\" % stdout)\n        return None, process.returncode\n    return stdout, process.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    \"\"\"Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    \"\"\"\n    rootdirs = []\n\n    for _ in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\n                \"version\": dirname[len(parentdir_prefix) :],\n                \"full-revisionid\": None,\n                \"dirty\": False,\n                \"error\": None,\n                \"date\": None,\n            }\n        rootdirs.append(root)\n        root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\n            \"Tried directories %s but none started with prefix %s\"\n            % (str(rootdirs), parentdir_prefix)\n        )\n    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n\n\n@register_vcs_handler(\"git\", \"get_keywords\")\ndef git_get_keywords(versionfile_abs):\n    \"\"\"Extract version information from the given file.\"\"\"\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don't want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        with open(versionfile_abs, \"r\") as fobj:\n            for line in fobj:\n                if line.strip().startswith(\"git_refnames =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"refnames\"] = mo.group(1)\n                if line.strip().startswith(\"git_full =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"full\"] = mo.group(1)\n                if line.strip().startswith(\"git_date =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"date\"] = mo.group(1)\n    except OSError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(\"git\", \"keywords\")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    \"\"\"Get version information from git keywords.\"\"\"\n    if \"refnames\" not in keywords:\n        raise NotThisMethod(\"Short version file found\")\n    date = keywords.get(\"date\")\n    if date is not None:\n        # Use only the last line.  Previous lines may contain GPG signature\n        # information.\n        date = date.splitlines()[-1]\n\n        # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601\n        # -like\" string, which we must then edit to make compliant), because\n        # it's been around since git-1.5.3, and it's too difficult to\n        # discover which version we're using, or to work around using an\n        # older one.\n        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n    refnames = keywords[\"refnames\"].strip()\n    if refnames.startswith(\"$Format\"):\n        if verbose:\n            print(\"keywords are unexpanded, not using\")\n        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n    TAG = \"tag: \"\n    tags = {r[len(TAG) :] for r in refs if r.startswith(TAG)}\n    if not tags:\n        # Either we're using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like \"release\" and\n        # \"stabilization\", as well as \"HEAD\" and \"master\".\n        tags = {r for r in refs if re.search(r\"\\d\", r)}\n        if verbose:\n            print(\"discarding '%s', no digits\" % \",\".join(refs - tags))\n    if verbose:\n        print(\"likely tags: %s\" % \",\".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix) :]\n            # Filter out refs that exactly match prefix or that don't start\n            # with a number once the prefix is stripped (mostly a concern\n            # when prefix is '')\n            if not re.match(r\"\\d\", r):\n                continue\n            if verbose:\n                print(\"picking %s\" % r)\n            return {\n                \"version\": r,\n                \"full-revisionid\": keywords[\"full\"].strip(),\n                \"dirty\": False,\n                \"error\": None,\n                \"date\": date,\n            }\n    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n    if verbose:\n        print(\"no suitable tags, using unknown + full revision id\")\n    return {\n        \"version\": \"0+unknown\",\n        \"full-revisionid\": keywords[\"full\"].strip(),\n        \"dirty\": False,\n        \"error\": \"no suitable tags\",\n        \"date\": None,\n    }\n\n\n@register_vcs_handler(\"git\", \"pieces_from_vcs\")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command):\n    \"\"\"Get version from 'git describe' in the root of the source tree.\n\n    This only gets called if the git-archive 'subst' keywords were *not*\n    expanded, and _version.py hasn't already been rewritten with a short\n    version string, meaning we're inside a checked out source tree.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n\n    # GIT_DIR can interfere with correct operation of Versioneer.\n    # It may be intended to be passed to the Versioneer-versioned project,\n    # but that should not change where we get our version from.\n    env = os.environ.copy()\n    env.pop(\"GIT_DIR\", None)\n    runner = functools.partial(runner, env=env)\n\n    _, rc = runner(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root, hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(\"Directory %s not under git control\" % root)\n        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n\n    MATCH_ARGS = [\"--match\", \"%s*\" % tag_prefix] if tag_prefix else []\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn't one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = runner(\n        GITS,\n        [\"describe\", \"--tags\", \"--dirty\", \"--always\", \"--long\", *MATCH_ARGS],\n        cwd=root,\n    )\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(\"'git describe' failed\")\n    describe_out = describe_out.strip()\n    full_out, rc = runner(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(\"'git rev-parse' failed\")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[\"long\"] = full_out\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\n    pieces[\"error\"] = None\n\n    branch_name, rc = runner(GITS, [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"], cwd=root)\n    # --abbrev-ref was added in git-1.6.3\n    if rc != 0 or branch_name is None:\n        raise NotThisMethod(\"'git rev-parse --abbrev-ref' returned error\")\n    branch_name = branch_name.strip()\n\n    if branch_name == \"HEAD\":\n        # If we aren't exactly on a branch, pick a branch which represents\n        # the current commit. If all else fails, we are on a branchless\n        # commit.\n        branches, rc = runner(GITS, [\"branch\", \"--contains\"], cwd=root)\n        # --contains was added in git-1.5.4\n        if rc != 0 or branches is None:\n            raise NotThisMethod(\"'git branch --contains' returned error\")\n        branches = branches.split(\"\\n\")\n\n        # Remove the first line if we're running detached\n        if \"(\" in branches[0]:\n            branches.pop(0)\n\n        # Strip off the leading \"* \" from the list of branches.\n        branches = [branch[2:] for branch in branches]\n        if \"master\" in branches:\n            branch_name = \"master\"\n        elif not branches:\n            branch_name = None\n        else:\n            # Pick the first branch that is returned. Good or bad.\n            branch_name = branches[0]\n\n    pieces[\"branch\"] = branch_name\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(\"-dirty\")\n    pieces[\"dirty\"] = dirty\n    if dirty:\n        git_describe = git_describe[: git_describe.rindex(\"-dirty\")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if \"-\" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\"^(.+)-(\\d+)-g([0-9a-f]+)$\", git_describe)\n        if not mo:\n            # unparsable. Maybe git-describe is misbehaving?\n            pieces[\"error\"] = \"unable to parse git-describe output: '%s'\" % describe_out\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = \"tag '%s' doesn't start with prefix '%s'\"\n                print(fmt % (full_tag, tag_prefix))\n            pieces[\"error\"] = \"tag '%s' doesn't start with prefix '%s'\" % (\n                full_tag,\n                tag_prefix,\n            )\n            return pieces\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix) :]\n\n        # distance: number of commits since tag\n        pieces[\"distance\"] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[\"short\"] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[\"closest-tag\"] = None\n        count_out, rc = runner(GITS, [\"rev-list\", \"HEAD\", \"--count\"], cwd=root)\n        pieces[\"distance\"] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = runner(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"], cwd=root)[0].strip()\n    # Use only the last line.  Previous lines may contain GPG signature\n    # information.\n    date = date.splitlines()[-1]\n    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n\n\ndef render_pep440(pieces):\n    \"\"\"Build up version string, with post-release \"local version identifier\".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += plus_or_dot(pieces)\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_branch(pieces):\n    \"\"\"TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch. Note that .dev0 sorts backwards\n    (a feature branch will appear \"older\" than the master branch).\n\n    Exceptions:\n    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0\"\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+untagged.%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef pep440_split_post(ver):\n    \"\"\"Split pep440 version string at the post-release segment.\n\n    Returns the release segments before the post-release and the\n    post-release version number (or -1 if no post-release segment is present).\n    \"\"\"\n    vc = str.split(ver, \".post\")\n    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None\n\n\ndef render_pep440_pre(pieces):\n    \"\"\"TAG[.postN.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post0.devDISTANCE\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"]:\n            # update the post release segment\n            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])\n            rendered = tag_version\n            if post_version is not None:\n                rendered += \".post%d.dev%d\" % (post_version + 1, pieces[\"distance\"])\n            else:\n                rendered += \".post0.dev%d\" % (pieces[\"distance\"])\n        else:\n            # no commits, use the tag as the version\n            rendered = pieces[\"closest-tag\"]\n    else:\n        # exception #1\n        rendered = \"0.post0.dev%d\" % pieces[\"distance\"]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear \"older\" than the corresponding clean one),\n    but you shouldn't be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%s\" % pieces[\"short\"]\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n        rendered += \"+g%s\" % pieces[\"short\"]\n    return rendered\n\n\ndef render_pep440_post_branch(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%s\" % pieces[\"short\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+g%s\" % pieces[\"short\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]] .\n\n    The \".dev0\" means dirty.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n    return rendered\n\n\ndef render_git_describe(pieces):\n    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n\n    Like 'git describe --tags --dirty --always'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n\n    Like 'git describe --tags --dirty --always -long'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if pieces[\"error\"]:\n        return {\n            \"version\": \"unknown\",\n            \"full-revisionid\": pieces.get(\"long\"),\n            \"dirty\": None,\n            \"error\": pieces[\"error\"],\n            \"date\": None,\n        }\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%s'\" % style)\n\n    return {\n        \"version\": rendered,\n        \"full-revisionid\": pieces[\"long\"],\n        \"dirty\": pieces[\"dirty\"],\n        \"error\": None,\n        \"date\": pieces.get(\"date\"),\n    }\n\n\ndef get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for _ in cfg.versionfile_source.split(\"/\"):\n            root = os.path.dirname(root)\n    except NameError:\n        return {\n            \"version\": \"0+unknown\",\n            \"full-revisionid\": None,\n            \"dirty\": None,\n            \"error\": \"unable to find root of source tree\",\n            \"date\": None,\n        }\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {\n        \"version\": \"0+unknown\",\n        \"full-revisionid\": None,\n        \"dirty\": None,\n        \"error\": \"unable to compute version\",\n        \"date\": None,\n    }\n",
            "file_path": "src/prestoplot/_version.py",
            "human_label": "Obtains the version information. If the version information cannot be obtained, the default value is returned.",
            "level": "file_runnable",
            "lineno": "637",
            "name": "get_versions",
            "oracle_context": "{ \"apis\" : \"['versions_from_parentdir', 'get_config', 'dirname', 'get_keywords', 'split', 'realpath', 'git_versions_from_keywords', 'git_pieces_from_vcs', 'render']\", \"classes\" : \"['NotThisMethod', 'os']\", \"vars\" : \"['tag_prefix', 'versionfile_source', 'path', 'verbose', 'parentdir_prefix', 'style']\" }",
            "package": "_version",
            "project": "eykd/prestoplot",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b87d24d292efb640a5566f",
            "all_context": "{ \"import\" : \"typing errno sys os functools subprocess re typing functools \", \"file\" : \"get_keywords() ; get_config() ; register_vcs_handler(vcs,method) ; run_command(commands,args,cwd,verbose,hide_stderr,env) ; versions_from_parentdir(parentdir_prefix,root,verbose) ; git_get_keywords(versionfile_abs) ; git_versions_from_keywords(keywords,tag_prefix,verbose) ; git_pieces_from_vcs(tag_prefix,root,verbose,runner) ; plus_or_dot(pieces) ; render_pep440(pieces) ; render_pep440_branch(pieces) ; pep440_split_post(ver) ; render_pep440_pre(pieces) ; render_pep440_post(pieces) ; render_pep440_post_branch(pieces) ; render_pep440_old(pieces) ; render_git_describe(pieces) ; render_git_describe_long(pieces) ; render(pieces,style) ; get_versions() ; \", \"class\" : \"\" }",
            "code": "def render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if pieces[\"error\"]:\n        return {\n            \"version\": \"unknown\",\n            \"full-revisionid\": pieces.get(\"long\"),\n            \"dirty\": None,\n            \"error\": pieces[\"error\"],\n            \"date\": None,\n        }\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%s'\" % style)\n\n    return {\n        \"version\": rendered,\n        \"full-revisionid\": pieces[\"long\"],\n        \"dirty\": pieces[\"dirty\"],\n        \"error\": None,\n        \"date\": pieces.get(\"date\"),\n    }\n",
            "dependency": "{ \"builtin\" : false, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Render the given version pieces into the requested style.",
            "end_lineno": "634",
            "file_content": "# This file helps to compute a version number in source trees obtained from\n# git-archive tarball (such as those provided by githubs download-from-tag\n# feature). Distribution tarballs (built by setup.py sdist) and build\n# directories (produced by setup.py build) will contain a much shorter file\n# that just contains the computed version number.\n\n# This file is released into the public domain. Generated by\n# versioneer-0.22 (https://github.com/python-versioneer/python-versioneer)\n\n\"\"\"Git implementation of _version.py.\"\"\"\n\nimport errno\nimport functools\nimport os\nimport re\nimport subprocess\nimport sys\nfrom typing import Callable, Dict\n\n\ndef get_keywords():\n    \"\"\"Get the keywords needed to look up the version information.\"\"\"\n    # these strings will be replaced by git during git-archive.\n    # setup.py/versioneer.py will grep for the variable names, so they must\n    # each be defined on a line of their own. _version.py will just call\n    # get_keywords().\n    git_refnames = \"$Format:%d$\"\n    git_full = \"$Format:%H$\"\n    git_date = \"$Format:%ci$\"\n    keywords = {\"refnames\": git_refnames, \"full\": git_full, \"date\": git_date}\n    return keywords\n\n\nclass VersioneerConfig:\n    \"\"\"Container for Versioneer configuration parameters.\"\"\"\n\n\ndef get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    # these strings are filled in when 'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"src/prestoplot/_version.py\"\n    cfg.verbose = False\n    return cfg\n\n\nclass NotThisMethod(Exception):\n    \"\"\"Exception raised if a method is not valid for the current scenario.\"\"\"\n\n\nLONG_VERSION_PY: Dict[str, str] = {}\nHANDLERS: Dict[str, Dict[str, Callable]] = {}\n\n\ndef register_vcs_handler(vcs, method):  # decorator\n    \"\"\"Create decorator to mark a method as the handler of a VCS.\"\"\"\n\n    def decorate(f):\n        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n\n    return decorate\n\n\ndef run_command(commands, args, cwd=None, verbose=False, hide_stderr=False, env=None):\n    \"\"\"Call the given command(s).\"\"\"\n    assert isinstance(commands, list)\n    process = None\n\n    popen_kwargs = {}\n    if sys.platform == \"win32\":\n        # This hides the console window if pythonw.exe is used\n        startupinfo = subprocess.STARTUPINFO()\n        startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n        popen_kwargs[\"startupinfo\"] = startupinfo\n\n    for command in commands:\n        try:\n            dispcmd = str([command] + args)\n            # remember shell=False, so use git.cmd on windows, not just git\n            process = subprocess.Popen(\n                [command] + args,\n                cwd=cwd,\n                env=env,\n                stdout=subprocess.PIPE,\n                stderr=(subprocess.PIPE if hide_stderr else None),\n                **popen_kwargs\n            )\n            break\n        except OSError:\n            e = sys.exc_info()[1]\n            if e.errno == errno.ENOENT:\n                continue\n            if verbose:\n                print(\"unable to run %s\" % dispcmd)\n                print(e)\n            return None, None\n    else:\n        if verbose:\n            print(\"unable to find command, tried %s\" % (commands,))\n        return None, None\n    stdout = process.communicate()[0].strip().decode()\n    if process.returncode != 0:\n        if verbose:\n            print(\"unable to run %s (error)\" % dispcmd)\n            print(\"stdout was %s\" % stdout)\n        return None, process.returncode\n    return stdout, process.returncode\n\n\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    \"\"\"Try to determine the version from the parent directory name.\n\n    Source tarballs conventionally unpack into a directory that includes both\n    the project name and a version string. We will also support searching up\n    two directory levels for an appropriately named parent directory\n    \"\"\"\n    rootdirs = []\n\n    for _ in range(3):\n        dirname = os.path.basename(root)\n        if dirname.startswith(parentdir_prefix):\n            return {\n                \"version\": dirname[len(parentdir_prefix) :],\n                \"full-revisionid\": None,\n                \"dirty\": False,\n                \"error\": None,\n                \"date\": None,\n            }\n        rootdirs.append(root)\n        root = os.path.dirname(root)  # up a level\n\n    if verbose:\n        print(\n            \"Tried directories %s but none started with prefix %s\"\n            % (str(rootdirs), parentdir_prefix)\n        )\n    raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n\n\n@register_vcs_handler(\"git\", \"get_keywords\")\ndef git_get_keywords(versionfile_abs):\n    \"\"\"Extract version information from the given file.\"\"\"\n    # the code embedded in _version.py can just fetch the value of these\n    # keywords. When used from setup.py, we don't want to import _version.py,\n    # so we do it with a regexp instead. This function is not used from\n    # _version.py.\n    keywords = {}\n    try:\n        with open(versionfile_abs, \"r\") as fobj:\n            for line in fobj:\n                if line.strip().startswith(\"git_refnames =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"refnames\"] = mo.group(1)\n                if line.strip().startswith(\"git_full =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"full\"] = mo.group(1)\n                if line.strip().startswith(\"git_date =\"):\n                    mo = re.search(r'=\\s*\"(.*)\"', line)\n                    if mo:\n                        keywords[\"date\"] = mo.group(1)\n    except OSError:\n        pass\n    return keywords\n\n\n@register_vcs_handler(\"git\", \"keywords\")\ndef git_versions_from_keywords(keywords, tag_prefix, verbose):\n    \"\"\"Get version information from git keywords.\"\"\"\n    if \"refnames\" not in keywords:\n        raise NotThisMethod(\"Short version file found\")\n    date = keywords.get(\"date\")\n    if date is not None:\n        # Use only the last line.  Previous lines may contain GPG signature\n        # information.\n        date = date.splitlines()[-1]\n\n        # git-2.2.0 added \"%cI\", which expands to an ISO-8601 -compliant\n        # datestamp. However we prefer \"%ci\" (which expands to an \"ISO-8601\n        # -like\" string, which we must then edit to make compliant), because\n        # it's been around since git-1.5.3, and it's too difficult to\n        # discover which version we're using, or to work around using an\n        # older one.\n        date = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n    refnames = keywords[\"refnames\"].strip()\n    if refnames.startswith(\"$Format\"):\n        if verbose:\n            print(\"keywords are unexpanded, not using\")\n        raise NotThisMethod(\"unexpanded keywords, not a git-archive tarball\")\n    refs = {r.strip() for r in refnames.strip(\"()\").split(\",\")}\n    # starting in git-1.8.3, tags are listed as \"tag: foo-1.0\" instead of\n    # just \"foo-1.0\". If we see a \"tag: \" prefix, prefer those.\n    TAG = \"tag: \"\n    tags = {r[len(TAG) :] for r in refs if r.startswith(TAG)}\n    if not tags:\n        # Either we're using git < 1.8.3, or there really are no tags. We use\n        # a heuristic: assume all version tags have a digit. The old git %d\n        # expansion behaves like git log --decorate=short and strips out the\n        # refs/heads/ and refs/tags/ prefixes that would let us distinguish\n        # between branches and tags. By ignoring refnames without digits, we\n        # filter out many common branch names like \"release\" and\n        # \"stabilization\", as well as \"HEAD\" and \"master\".\n        tags = {r for r in refs if re.search(r\"\\d\", r)}\n        if verbose:\n            print(\"discarding '%s', no digits\" % \",\".join(refs - tags))\n    if verbose:\n        print(\"likely tags: %s\" % \",\".join(sorted(tags)))\n    for ref in sorted(tags):\n        # sorting will prefer e.g. \"2.0\" over \"2.0rc1\"\n        if ref.startswith(tag_prefix):\n            r = ref[len(tag_prefix) :]\n            # Filter out refs that exactly match prefix or that don't start\n            # with a number once the prefix is stripped (mostly a concern\n            # when prefix is '')\n            if not re.match(r\"\\d\", r):\n                continue\n            if verbose:\n                print(\"picking %s\" % r)\n            return {\n                \"version\": r,\n                \"full-revisionid\": keywords[\"full\"].strip(),\n                \"dirty\": False,\n                \"error\": None,\n                \"date\": date,\n            }\n    # no suitable tags, so version is \"0+unknown\", but full hex is still there\n    if verbose:\n        print(\"no suitable tags, using unknown + full revision id\")\n    return {\n        \"version\": \"0+unknown\",\n        \"full-revisionid\": keywords[\"full\"].strip(),\n        \"dirty\": False,\n        \"error\": \"no suitable tags\",\n        \"date\": None,\n    }\n\n\n@register_vcs_handler(\"git\", \"pieces_from_vcs\")\ndef git_pieces_from_vcs(tag_prefix, root, verbose, runner=run_command):\n    \"\"\"Get version from 'git describe' in the root of the source tree.\n\n    This only gets called if the git-archive 'subst' keywords were *not*\n    expanded, and _version.py hasn't already been rewritten with a short\n    version string, meaning we're inside a checked out source tree.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n\n    # GIT_DIR can interfere with correct operation of Versioneer.\n    # It may be intended to be passed to the Versioneer-versioned project,\n    # but that should not change where we get our version from.\n    env = os.environ.copy()\n    env.pop(\"GIT_DIR\", None)\n    runner = functools.partial(runner, env=env)\n\n    _, rc = runner(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root, hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(\"Directory %s not under git control\" % root)\n        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n\n    MATCH_ARGS = [\"--match\", \"%s*\" % tag_prefix] if tag_prefix else []\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn't one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = runner(\n        GITS,\n        [\"describe\", \"--tags\", \"--dirty\", \"--always\", \"--long\", *MATCH_ARGS],\n        cwd=root,\n    )\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(\"'git describe' failed\")\n    describe_out = describe_out.strip()\n    full_out, rc = runner(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(\"'git rev-parse' failed\")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[\"long\"] = full_out\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\n    pieces[\"error\"] = None\n\n    branch_name, rc = runner(GITS, [\"rev-parse\", \"--abbrev-ref\", \"HEAD\"], cwd=root)\n    # --abbrev-ref was added in git-1.6.3\n    if rc != 0 or branch_name is None:\n        raise NotThisMethod(\"'git rev-parse --abbrev-ref' returned error\")\n    branch_name = branch_name.strip()\n\n    if branch_name == \"HEAD\":\n        # If we aren't exactly on a branch, pick a branch which represents\n        # the current commit. If all else fails, we are on a branchless\n        # commit.\n        branches, rc = runner(GITS, [\"branch\", \"--contains\"], cwd=root)\n        # --contains was added in git-1.5.4\n        if rc != 0 or branches is None:\n            raise NotThisMethod(\"'git branch --contains' returned error\")\n        branches = branches.split(\"\\n\")\n\n        # Remove the first line if we're running detached\n        if \"(\" in branches[0]:\n            branches.pop(0)\n\n        # Strip off the leading \"* \" from the list of branches.\n        branches = [branch[2:] for branch in branches]\n        if \"master\" in branches:\n            branch_name = \"master\"\n        elif not branches:\n            branch_name = None\n        else:\n            # Pick the first branch that is returned. Good or bad.\n            branch_name = branches[0]\n\n    pieces[\"branch\"] = branch_name\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(\"-dirty\")\n    pieces[\"dirty\"] = dirty\n    if dirty:\n        git_describe = git_describe[: git_describe.rindex(\"-dirty\")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if \"-\" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r\"^(.+)-(\\d+)-g([0-9a-f]+)$\", git_describe)\n        if not mo:\n            # unparsable. Maybe git-describe is misbehaving?\n            pieces[\"error\"] = \"unable to parse git-describe output: '%s'\" % describe_out\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = \"tag '%s' doesn't start with prefix '%s'\"\n                print(fmt % (full_tag, tag_prefix))\n            pieces[\"error\"] = \"tag '%s' doesn't start with prefix '%s'\" % (\n                full_tag,\n                tag_prefix,\n            )\n            return pieces\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix) :]\n\n        # distance: number of commits since tag\n        pieces[\"distance\"] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[\"short\"] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[\"closest-tag\"] = None\n        count_out, rc = runner(GITS, [\"rev-list\", \"HEAD\", \"--count\"], cwd=root)\n        pieces[\"distance\"] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = runner(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"], cwd=root)[0].strip()\n    # Use only the last line.  Previous lines may contain GPG signature\n    # information.\n    date = date.splitlines()[-1]\n    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n\n    return pieces\n\n\ndef plus_or_dot(pieces):\n    \"\"\"Return a + if we don't already have one, else return a .\"\"\"\n    if \"+\" in pieces.get(\"closest-tag\", \"\"):\n        return \".\"\n    return \"+\"\n\n\ndef render_pep440(pieces):\n    \"\"\"Build up version string, with post-release \"local version identifier\".\n\n    Our goal: TAG[+DISTANCE.gHEX[.dirty]] . Note that if you\n    get a tagged build and then dirty it, you'll get TAG+0.gHEX.dirty\n\n    Exceptions:\n    1: no tags. git_describe was just HEX. 0+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += plus_or_dot(pieces)\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0+untagged.%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_branch(pieces):\n    \"\"\"TAG[[.dev0]+DISTANCE.gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch. Note that .dev0 sorts backwards\n    (a feature branch will appear \"older\" than the master branch).\n\n    Exceptions:\n    1: no tags. 0[.dev0]+untagged.DISTANCE.gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0\"\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+untagged.%d.g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef pep440_split_post(ver):\n    \"\"\"Split pep440 version string at the post-release segment.\n\n    Returns the release segments before the post-release and the\n    post-release version number (or -1 if no post-release segment is present).\n    \"\"\"\n    vc = str.split(ver, \".post\")\n    return vc[0], int(vc[1] or 0) if len(vc) == 2 else None\n\n\ndef render_pep440_pre(pieces):\n    \"\"\"TAG[.postN.devDISTANCE] -- No -dirty.\n\n    Exceptions:\n    1: no tags. 0.post0.devDISTANCE\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        if pieces[\"distance\"]:\n            # update the post release segment\n            tag_version, post_version = pep440_split_post(pieces[\"closest-tag\"])\n            rendered = tag_version\n            if post_version is not None:\n                rendered += \".post%d.dev%d\" % (post_version + 1, pieces[\"distance\"])\n            else:\n                rendered += \".post0.dev%d\" % (pieces[\"distance\"])\n        else:\n            # no commits, use the tag as the version\n            rendered = pieces[\"closest-tag\"]\n    else:\n        # exception #1\n        rendered = \"0.post0.dev%d\" % pieces[\"distance\"]\n    return rendered\n\n\ndef render_pep440_post(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX] .\n\n    The \".dev0\" means dirty. Note that .dev0 sorts backwards\n    (a dirty tree will appear \"older\" than the corresponding clean one),\n    but you shouldn't be releasing software with -dirty anyways.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%s\" % pieces[\"short\"]\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n        rendered += \"+g%s\" % pieces[\"short\"]\n    return rendered\n\n\ndef render_pep440_post_branch(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]+gHEX[.dirty]] .\n\n    The \".dev0\" means not master branch.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]+gHEX[.dirty]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"branch\"] != \"master\":\n                rendered += \".dev0\"\n            rendered += plus_or_dot(pieces)\n            rendered += \"g%s\" % pieces[\"short\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dirty\"\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"branch\"] != \"master\":\n            rendered += \".dev0\"\n        rendered += \"+g%s\" % pieces[\"short\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dirty\"\n    return rendered\n\n\ndef render_pep440_old(pieces):\n    \"\"\"TAG[.postDISTANCE[.dev0]] .\n\n    The \".dev0\" means dirty.\n\n    Exceptions:\n    1: no tags. 0.postDISTANCE[.dev0]\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"] or pieces[\"dirty\"]:\n            rendered += \".post%d\" % pieces[\"distance\"]\n            if pieces[\"dirty\"]:\n                rendered += \".dev0\"\n    else:\n        # exception #1\n        rendered = \"0.post%d\" % pieces[\"distance\"]\n        if pieces[\"dirty\"]:\n            rendered += \".dev0\"\n    return rendered\n\n\ndef render_git_describe(pieces):\n    \"\"\"TAG[-DISTANCE-gHEX][-dirty].\n\n    Like 'git describe --tags --dirty --always'.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        if pieces[\"distance\"]:\n            rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render_git_describe_long(pieces):\n    \"\"\"TAG-DISTANCE-gHEX[-dirty].\n\n    Like 'git describe --tags --dirty --always -long'.\n    The distance/hash is unconditional.\n\n    Exceptions:\n    1: no tags. HEX[-dirty]  (note: no 'g' prefix)\n    \"\"\"\n    if pieces[\"closest-tag\"]:\n        rendered = pieces[\"closest-tag\"]\n        rendered += \"-%d-g%s\" % (pieces[\"distance\"], pieces[\"short\"])\n    else:\n        # exception #1\n        rendered = pieces[\"short\"]\n    if pieces[\"dirty\"]:\n        rendered += \"-dirty\"\n    return rendered\n\n\ndef render(pieces, style):\n    \"\"\"Render the given version pieces into the requested style.\"\"\"\n    if pieces[\"error\"]:\n        return {\n            \"version\": \"unknown\",\n            \"full-revisionid\": pieces.get(\"long\"),\n            \"dirty\": None,\n            \"error\": pieces[\"error\"],\n            \"date\": None,\n        }\n\n    if not style or style == \"default\":\n        style = \"pep440\"  # the default\n\n    if style == \"pep440\":\n        rendered = render_pep440(pieces)\n    elif style == \"pep440-branch\":\n        rendered = render_pep440_branch(pieces)\n    elif style == \"pep440-pre\":\n        rendered = render_pep440_pre(pieces)\n    elif style == \"pep440-post\":\n        rendered = render_pep440_post(pieces)\n    elif style == \"pep440-post-branch\":\n        rendered = render_pep440_post_branch(pieces)\n    elif style == \"pep440-old\":\n        rendered = render_pep440_old(pieces)\n    elif style == \"git-describe\":\n        rendered = render_git_describe(pieces)\n    elif style == \"git-describe-long\":\n        rendered = render_git_describe_long(pieces)\n    else:\n        raise ValueError(\"unknown style '%s'\" % style)\n\n    return {\n        \"version\": rendered,\n        \"full-revisionid\": pieces[\"long\"],\n        \"dirty\": pieces[\"dirty\"],\n        \"error\": None,\n        \"date\": pieces.get(\"date\"),\n    }\n\n\ndef get_versions():\n    \"\"\"Get version information or return default if unable to do so.\"\"\"\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix, verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for _ in cfg.versionfile_source.split(\"/\"):\n            root = os.path.dirname(root)\n    except NameError:\n        return {\n            \"version\": \"0+unknown\",\n            \"full-revisionid\": None,\n            \"dirty\": None,\n            \"error\": \"unable to find root of source tree\",\n            \"date\": None,\n        }\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {\n        \"version\": \"0+unknown\",\n        \"full-revisionid\": None,\n        \"dirty\": None,\n        \"error\": \"unable to compute version\",\n        \"date\": None,\n    }\n",
            "file_path": "src/prestoplot/_version.py",
            "human_label": "Input pieces and a style, render the pieces to the corresponding style.",
            "level": "file_runnable",
            "lineno": "595",
            "name": "render",
            "oracle_context": "{ \"apis\" : \"['render_pep440_pre', 'get', 'render_pep440_old', 'render_pep440_post', 'render_pep440', 'render_pep440_branch', 'render_git_describe', 'render_git_describe_long', 'render_pep440_post_branch']\", \"classes\" : \"['ValueError']\", \"vars\" : \"[]\" }",
            "package": "_version",
            "project": "eykd/prestoplot",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b45e945108cfac7f210a4a",
            "all_context": "{ \"import\" : \"logging json re namaste logging pyfs validator fs \", \"file\" : \"\", \"class\" : \"self.check_root_structure(self) ; self.declaration_tvalue ; self.parse_layout_file ; self.__init__(self,root,disposition,lax_digests) ; self.registered_extensions ; self.description ; self.dispositor(self) ; self.object_path(self,identifier) ; self.extension ; self.initialize(self) ; self.object_paths(self) ; self.validate_extensions_dir(self) ; self.validate_hierarchy(self,validate_objects,check_digests,show_warnings) ; self.layout_file ; self.validate_hierarchy ; self.parse_layout_file(self) ; self.validate(self,validate_objects,check_digests,show_warnings,show_errors,lang) ; self.traversal_error(self,code) ; self.validate_extensions_dir ; self.check_root_structure ; self.open_root_fs ; self.list(self) ; self.add(self,object_path) ; self.root ; self.lax_digests ; self.num_traversal_errors ; self.dispositor ; self.log ; self.disposition ; self._dispositor ; self.good_objects ; self.num_objects ; self.root_fs ; self.object_path ; self.object_paths ; self.traversal_error ; self.spec_file ; self.open_root_fs(self,create) ; \" }",
            "code": "    def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n        \"\"\"Validate storage root hierarchy.\n\n        Returns:\n            num_objects - number of objects checked\n            good_objects - number of objects checked that were found to be valid\n        \"\"\"\n        num_objects = 0\n        good_objects = 0\n        for dirpath in self.object_paths():\n            if validate_objects:\n                validator = Validator(check_digests=check_digests,\n                                      lax_digests=self.lax_digests,\n                                      show_warnings=show_warnings)\n                if validator.validate(ocfl_opendir(self.root_fs, dirpath)):\n                    good_objects += 1\n                else:\n                    logging.info(\"Object at %s in INVALID\", dirpath)\n                messages = validator.status_str(prefix='[[' + dirpath + ']]')\n                if messages != '':\n                    print(messages)\n                num_objects += 1\n        return num_objects, good_objects\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : true, \"public_lib\" : true, \"current_class\" : true, \"current_file\" : false, \"current_project\" : false, \"external\" : false }",
            "docstring": "Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid",
            "end_lineno": "233",
            "file_content": "\"\"\"OCFL Storage Root library.\n\nThis code uses PyFilesystem (import fs) exclusively for access to files. This\nshould enable application beyond the operating system filesystem.\n\"\"\"\nimport json\nimport logging\nimport re\nimport fs\nfrom fs.copy import copy_dir\n\nfrom .disposition import get_dispositor\nfrom .namaste import find_namastes, Namaste\nfrom .object import Object\nfrom .pyfs import open_fs, ocfl_walk, ocfl_opendir\nfrom .validator import Validator\nfrom .validation_logger import ValidationLogger\n\n\nclass StoreException(Exception):\n    \"\"\"Exception class for OCFL Storage Root.\"\"\"\n\n\nclass Store():\n    \"\"\"Class for handling OCFL Storage Root and include OCFL Objects.\"\"\"\n\n    def __init__(self, root=None, disposition=None, lax_digests=False):\n        \"\"\"Initialize OCFL Storage Root.\"\"\"\n        self.root = root\n        self.disposition = disposition\n        self.lax_digests = lax_digests\n        self._dispositor = None\n        #\n        self.declaration_tvalue = 'ocfl_1.0'\n        self.spec_file = 'ocfl_1.0.txt'\n        self.layout_file = 'ocfl_layout.json'\n        self.registered_extensions = [\n            # '0002-flat-direct-storage-layout',  # not included because doesn't have config\n            '0003-hash-and-id-n-tuple-storage-layout',\n            '0004-hashed-n-tuple-storage-layout'\n        ]\n        #\n        self.root_fs = None\n        self.num_traversal_errors = 0\n        self.extension = None\n        self.description = None\n        self.log = None\n        self.num_objects = 0\n        self.good_objects = 0\n\n    def open_root_fs(self, create=False):\n        \"\"\"Open pyfs filesystem for this OCFL storage root.\"\"\"\n        try:\n            self.root_fs = open_fs(self.root, create=create)\n        except (fs.opener.errors.OpenerError, fs.errors.CreateFailed) as e:\n            raise StoreException(\"Failed to open OCFL storage root filesystem '%s' (%s)\" % (self.root, str(e)))\n\n    @property\n    def dispositor(self):\n        \"\"\"Instance of dispositor class.\n\n        Lazily initialized.\n        \"\"\"\n        if not self._dispositor:\n            self._dispositor = get_dispositor(disposition=self.disposition)\n        return self._dispositor\n\n    def traversal_error(self, code, **kwargs):\n        \"\"\"Record error traversing OCFL storage root.\"\"\"\n        self.num_traversal_errors += 1\n        if self.log is None:  # FIXME - What to do in non-validator context?\n            args = ', '.join('{0}={1!r}'.format(k, v) for k, v in kwargs.items())\n            logging.error(\"Traversal error %s - %s\", code, args)\n        else:\n            self.log.error(code, **kwargs)\n\n    def object_path(self, identifier):\n        \"\"\"Path to OCFL object with given identifier relative to the OCFL storage root.\"\"\"\n        return self.dispositor.identifier_to_path(identifier)\n\n    def initialize(self):\n        \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n        (parent, root_dir) = fs.path.split(self.root)\n        parent_fs = open_fs(parent)\n        if parent_fs.exists(root_dir):\n            raise StoreException(\"OCFL storage root %s already exists, aborting!\" % (self.root))\n        self.root_fs = parent_fs.makedir(root_dir)\n        logging.debug(\"Created OCFL storage root at %s\", self.root)\n        # Create root declaration\n        Namaste(d=0, content=self.declaration_tvalue).write(pyfs=self.root_fs)\n        # Create a layout declaration\n        if self.disposition is not None:\n            with self.root_fs.open(self.layout_file, 'w') as fh:\n                layout = {'extension': self.disposition,\n                          'description': \"Non-standard layout from ocfl-py disposition -- FIXME\"}\n                json.dump(layout, fh, sort_keys=True, indent=2)\n        logging.info(\"Created OCFL storage root %s\", self.root)\n\n    def check_root_structure(self):\n        \"\"\"Check the OCFL storage root structure.\n\n        Assumed that self.root_fs filesystem is available. Raises\n        StoreException if there is an error.\n        \"\"\"\n        # Storage root declaration\n        namastes = find_namastes(0, pyfs=self.root_fs)\n        if len(namastes) == 0:\n            raise StoreException(\"Storage root %s lacks required 0= declaration file\" % (self.root))\n        if len(namastes) > 1:\n            raise StoreException(\"Storage root %s has more than one 0= style declaration file\" % (self.root))\n        if namastes[0].tvalue != self.declaration_tvalue:\n            raise StoreException(\"Storage root %s declaration file not as expected, got %s\" % (self.root, namastes[0].filename))\n        if not namastes[0].content_ok(pyfs=self.root_fs):\n            raise StoreException(\"Storage root %s required declaration file %s has invalid content\" % (self.root, namastes[0].filename))\n        # Specification file and layout file\n        if self.root_fs.exists(self.spec_file) and not self.root_fs.isfile(self.spec_file):\n            raise StoreException(\"Storage root %s includes a specification entry that isn't a file\" % (self.root))\n        self.extension, self.description = self.parse_layout_file()\n        # Other files are allowed...\n        return True\n\n    def parse_layout_file(self):\n        \"\"\"Read and parse layout file in OCFL storage root.\n\n        Returns:\n          - (extension, description) strings on success,\n          - (None, None) if there is now layout file (it is optional)\n          - otherwise raises a StoreException.\n        \"\"\"\n        if self.root_fs.exists(self.layout_file):\n            try:\n                with self.root_fs.open(self.layout_file) as fh:\n                    layout = json.load(fh)\n                if not isinstance(layout, dict):\n                    raise StoreException(\"Storage root %s has layout file that isn't a JSON object\" % (self.root))\n                if ('extension' not in layout or not isinstance(layout['extension'], str)\n                        or 'description' not in layout or not isinstance(layout['description'], str)):\n                    raise StoreException(\"Storage root %s has layout file doesn't have required extension and description string entries\" % (self.root))\n                return layout['extension'], layout['description']\n            except Exception as e:  # FIXME - more specific?\n                raise StoreException(\"OCFL storage root %s has layout file that can't be read (%s)\" % (self.root, str(e)))\n        else:\n            return None, None\n\n    def object_paths(self):\n        \"\"\"Generate object paths for every obect in the OCFL storage root.\n\n        Yields (dirpath) that is the path to the directory for each object\n        located, relative to the OCFL storage root and without a preceding /.\n\n        Will log any errors seen while traversing the directory tree under the\n        storage root.\n        \"\"\"\n        for (dirpath, dirs, files) in ocfl_walk(self.root_fs, is_storage_root=True):\n            if dirpath == '/':\n                if 'extensions' in dirs:\n                    self.validate_extensions_dir()\n                    dirs.remove('extensions')\n                # Ignore any other files in storage root\n            elif (len(dirs) + len(files)) == 0:\n                self.traversal_error(\"E073\", path=dirpath)\n            elif len(files) == 0:\n                pass  # Just an intermediate directory\n            else:\n                # Is this directory an OCFL object? Look for any 0= file.\n                zero_eqs = [file for file in files if file.startswith('0=')]\n                if len(zero_eqs) > 1:\n                    self.traversal_error(\"E003d\", path=dirpath)\n                elif len(zero_eqs) == 1:\n                    declaration = zero_eqs[0]\n                    match = re.match(r'''0=ocfl_object_(\\d+\\.\\d+)''', declaration)\n                    if match and match.group(1) == '1.0':\n                        yield dirpath.lstrip('/')\n                    elif match:\n                        self.traversal_error(\"E004a\", path=dirpath, version=match.group(1))\n                    else:\n                        self.traversal_error(\"E004b\", path=dirpath, declaration=declaration)\n                else:\n                    self.traversal_error(\"E072\", path=dirpath)\n\n    def validate_extensions_dir(self):\n        \"\"\"Validate content of extensions directory inside storage root.\n\n        Validate the extensions directory by checking that there aren't any\n        entries in the extensions directory that aren't directories themselves.\n        Where there are extension directories they SHOULD be registered and\n        this code relies up the registered_extensions property to list known\n        storage root extensions.\n        \"\"\"\n        for entry in self.root_fs.scandir('extensions'):\n            if entry.is_dir:\n                if entry.name not in self.registered_extensions:\n                    self.log.warning('W901', entry=entry.name)  # FIXME - No good warning code in spec\n            else:\n                self.traversal_error('E086', entry=entry.name)\n\n    def list(self):\n        \"\"\"List contents of this OCFL storage root.\"\"\"\n        self.open_root_fs()\n        self.check_root_structure()\n        self.num_objects = 0\n        for dirpath in self.object_paths():\n            with ocfl_opendir(self.root_fs, dirpath) as obj_fs:\n                # Parse inventory to extract id\n                identifier = Object(obj_fs=obj_fs).id_from_inventory()\n                print(\"%s -- id=%s\" % (dirpath, identifier))\n                self.num_objects += 1\n                # FIXME - maybe do some more stuff in here\n        logging.info(\"Found %d OCFL Objects under root %s\", self.num_objects, self.root)\n\n    def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n        \"\"\"Validate storage root hierarchy.\n\n        Returns:\n            num_objects - number of objects checked\n            good_objects - number of objects checked that were found to be valid\n        \"\"\"\n        num_objects = 0\n        good_objects = 0\n        for dirpath in self.object_paths():\n            if validate_objects:\n                validator = Validator(check_digests=check_digests,\n                                      lax_digests=self.lax_digests,\n                                      show_warnings=show_warnings)\n                if validator.validate(ocfl_opendir(self.root_fs, dirpath)):\n                    good_objects += 1\n                else:\n                    logging.info(\"Object at %s in INVALID\", dirpath)\n                messages = validator.status_str(prefix='[[' + dirpath + ']]')\n                if messages != '':\n                    print(messages)\n                num_objects += 1\n        return num_objects, good_objects\n\n    def validate(self, validate_objects=True, check_digests=True, show_warnings=False, show_errors=True, lang='en'):\n        \"\"\"Validate OCFL storage root and optionally all objects.\"\"\"\n        valid = True\n        self.log = ValidationLogger(show_warnings=show_warnings, show_errors=show_errors, lang=lang)\n        self.open_root_fs()\n        try:\n            self.check_root_structure()\n            logging.info(\"Storage root structure is VALID\")\n        except StoreException as e:\n            valid = False\n            logging.info(\"Storage root structure is INVALID (%s)\", str(e))\n        self.num_objects, self.good_objects = self.validate_hierarchy(validate_objects=validate_objects, check_digests=check_digests, show_warnings=show_warnings)\n        if validate_objects:\n            if self.good_objects == self.num_objects:\n                logging.info(\"Objects checked: %d / %d are VALID\", self.good_objects, self.num_objects)\n            else:\n                valid = False\n                logging.info(\"Objects checked: %d / %d are INVALID\", self.num_objects - self.good_objects, self.num_objects)\n        else:\n            logging.info(\"Not checking OCFL objects\")\n        print(str(self.log))\n        if self.num_traversal_errors > 0:\n            valid = False\n            logging.info(\"Encountered %d errors traversing storage root\", self.num_traversal_errors)\n        # FIXME - do some stuff in here\n        if valid:\n            logging.info(\"Storage root %s is VALID\", self.root)\n        else:\n            logging.info(\"Storage root %s is INVALID\", self.root)\n        return valid\n\n    def add(self, object_path):\n        \"\"\"Add pre-constructed object from object_path.\"\"\"\n        self.open_root_fs()\n        self.check_root_structure()\n        # Sanity check\n        o = Object()\n        o.open_fs(object_path)\n        inventory = o.parse_inventory()\n        identifier = inventory['id']\n        # Now copy\n        path = self.object_path(identifier)\n        logging.info(\"Copying from %s to %s\", object_path, fs.path.join(self.root, path))\n        try:\n            copy_dir(o.obj_fs, '/', self.root_fs, path)\n            logging.info(\"Copied\")\n        except Exception as e:\n            logging.error(\"Copy failed: %s\", str(e))\n            raise StoreException(\"Add object failed!\")\n",
            "file_path": "ocfl/store.py",
            "human_label": "Validate storage root hierarchy.\n\nReturns:\n    num_objects - number of objects checked\n    good_objects - number of objects checked that were found to be valid",
            "level": "class_runnable",
            "lineno": "211",
            "name": "validate_hierarchy",
            "oracle_context": "{ \"apis\" : \"['object_paths', 'print', 'info', 'status_str', 'validate', 'ocfl_opendir']\", \"classes\" : \"['logging', 'ocfl_opendir', 'Validator']\", \"vars\" : \"['lax_digests', 'root_fs']\" }",
            "package": "store",
            "project": "zimeon/ocfl-py",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b45e515108cfac7f210a3c",
            "all_context": "{ \"import\" : \"logging json re namaste logging pyfs validator fs \", \"file\" : \"\", \"class\" : \"self.check_root_structure(self) ; self.declaration_tvalue ; self.parse_layout_file ; self.__init__(self,root,disposition,lax_digests) ; self.registered_extensions ; self.description ; self.dispositor(self) ; self.object_path(self,identifier) ; self.extension ; self.initialize(self) ; self.object_paths(self) ; self.validate_extensions_dir(self) ; self.validate_hierarchy(self,validate_objects,check_digests,show_warnings) ; self.layout_file ; self.validate_hierarchy ; self.parse_layout_file(self) ; self.validate(self,validate_objects,check_digests,show_warnings,show_errors,lang) ; self.traversal_error(self,code) ; self.validate_extensions_dir ; self.check_root_structure ; self.open_root_fs ; self.list(self) ; self.add(self,object_path) ; self.root ; self.lax_digests ; self.num_traversal_errors ; self.dispositor ; self.log ; self.disposition ; self._dispositor ; self.good_objects ; self.num_objects ; self.root_fs ; self.object_path ; self.object_paths ; self.traversal_error ; self.spec_file ; self.open_root_fs(self,create) ; \" }",
            "code": "    def initialize(self):\n        \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n        (parent, root_dir) = fs.path.split(self.root)\n        parent_fs = open_fs(parent)\n        if parent_fs.exists(root_dir):\n            raise StoreException(\"OCFL storage root %s already exists, aborting!\" % (self.root))\n        self.root_fs = parent_fs.makedir(root_dir)\n        logging.debug(\"Created OCFL storage root at %s\", self.root)\n        # Create root declaration\n        Namaste(d=0, content=self.declaration_tvalue).write(pyfs=self.root_fs)\n        # Create a layout declaration\n        if self.disposition is not None:\n            with self.root_fs.open(self.layout_file, 'w') as fh:\n                layout = {'extension': self.disposition,\n                          'description': \"Non-standard layout from ocfl-py disposition -- FIXME\"}\n                json.dump(layout, fh, sort_keys=True, indent=2)\n        logging.info(\"Created OCFL storage root %s\", self.root)\n",
            "dependency": "{ \"builtin\" : false, \"standard_lib\" : true, \"public_lib\" : true, \"current_class\" : true, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Create and initialize a new OCFL storage root.",
            "end_lineno": "97",
            "file_content": "\"\"\"OCFL Storage Root library.\n\nThis code uses PyFilesystem (import fs) exclusively for access to files. This\nshould enable application beyond the operating system filesystem.\n\"\"\"\nimport json\nimport logging\nimport re\nimport fs\nfrom fs.copy import copy_dir\n\nfrom .disposition import get_dispositor\nfrom .namaste import find_namastes, Namaste\nfrom .object import Object\nfrom .pyfs import open_fs, ocfl_walk, ocfl_opendir\nfrom .validator import Validator\nfrom .validation_logger import ValidationLogger\n\n\nclass StoreException(Exception):\n    \"\"\"Exception class for OCFL Storage Root.\"\"\"\n\n\nclass Store():\n    \"\"\"Class for handling OCFL Storage Root and include OCFL Objects.\"\"\"\n\n    def __init__(self, root=None, disposition=None, lax_digests=False):\n        \"\"\"Initialize OCFL Storage Root.\"\"\"\n        self.root = root\n        self.disposition = disposition\n        self.lax_digests = lax_digests\n        self._dispositor = None\n        #\n        self.declaration_tvalue = 'ocfl_1.0'\n        self.spec_file = 'ocfl_1.0.txt'\n        self.layout_file = 'ocfl_layout.json'\n        self.registered_extensions = [\n            # '0002-flat-direct-storage-layout',  # not included because doesn't have config\n            '0003-hash-and-id-n-tuple-storage-layout',\n            '0004-hashed-n-tuple-storage-layout'\n        ]\n        #\n        self.root_fs = None\n        self.num_traversal_errors = 0\n        self.extension = None\n        self.description = None\n        self.log = None\n        self.num_objects = 0\n        self.good_objects = 0\n\n    def open_root_fs(self, create=False):\n        \"\"\"Open pyfs filesystem for this OCFL storage root.\"\"\"\n        try:\n            self.root_fs = open_fs(self.root, create=create)\n        except (fs.opener.errors.OpenerError, fs.errors.CreateFailed) as e:\n            raise StoreException(\"Failed to open OCFL storage root filesystem '%s' (%s)\" % (self.root, str(e)))\n\n    @property\n    def dispositor(self):\n        \"\"\"Instance of dispositor class.\n\n        Lazily initialized.\n        \"\"\"\n        if not self._dispositor:\n            self._dispositor = get_dispositor(disposition=self.disposition)\n        return self._dispositor\n\n    def traversal_error(self, code, **kwargs):\n        \"\"\"Record error traversing OCFL storage root.\"\"\"\n        self.num_traversal_errors += 1\n        if self.log is None:  # FIXME - What to do in non-validator context?\n            args = ', '.join('{0}={1!r}'.format(k, v) for k, v in kwargs.items())\n            logging.error(\"Traversal error %s - %s\", code, args)\n        else:\n            self.log.error(code, **kwargs)\n\n    def object_path(self, identifier):\n        \"\"\"Path to OCFL object with given identifier relative to the OCFL storage root.\"\"\"\n        return self.dispositor.identifier_to_path(identifier)\n\n    def initialize(self):\n        \"\"\"Create and initialize a new OCFL storage root.\"\"\"\n        (parent, root_dir) = fs.path.split(self.root)\n        parent_fs = open_fs(parent)\n        if parent_fs.exists(root_dir):\n            raise StoreException(\"OCFL storage root %s already exists, aborting!\" % (self.root))\n        self.root_fs = parent_fs.makedir(root_dir)\n        logging.debug(\"Created OCFL storage root at %s\", self.root)\n        # Create root declaration\n        Namaste(d=0, content=self.declaration_tvalue).write(pyfs=self.root_fs)\n        # Create a layout declaration\n        if self.disposition is not None:\n            with self.root_fs.open(self.layout_file, 'w') as fh:\n                layout = {'extension': self.disposition,\n                          'description': \"Non-standard layout from ocfl-py disposition -- FIXME\"}\n                json.dump(layout, fh, sort_keys=True, indent=2)\n        logging.info(\"Created OCFL storage root %s\", self.root)\n\n    def check_root_structure(self):\n        \"\"\"Check the OCFL storage root structure.\n\n        Assumed that self.root_fs filesystem is available. Raises\n        StoreException if there is an error.\n        \"\"\"\n        # Storage root declaration\n        namastes = find_namastes(0, pyfs=self.root_fs)\n        if len(namastes) == 0:\n            raise StoreException(\"Storage root %s lacks required 0= declaration file\" % (self.root))\n        if len(namastes) > 1:\n            raise StoreException(\"Storage root %s has more than one 0= style declaration file\" % (self.root))\n        if namastes[0].tvalue != self.declaration_tvalue:\n            raise StoreException(\"Storage root %s declaration file not as expected, got %s\" % (self.root, namastes[0].filename))\n        if not namastes[0].content_ok(pyfs=self.root_fs):\n            raise StoreException(\"Storage root %s required declaration file %s has invalid content\" % (self.root, namastes[0].filename))\n        # Specification file and layout file\n        if self.root_fs.exists(self.spec_file) and not self.root_fs.isfile(self.spec_file):\n            raise StoreException(\"Storage root %s includes a specification entry that isn't a file\" % (self.root))\n        self.extension, self.description = self.parse_layout_file()\n        # Other files are allowed...\n        return True\n\n    def parse_layout_file(self):\n        \"\"\"Read and parse layout file in OCFL storage root.\n\n        Returns:\n          - (extension, description) strings on success,\n          - (None, None) if there is now layout file (it is optional)\n          - otherwise raises a StoreException.\n        \"\"\"\n        if self.root_fs.exists(self.layout_file):\n            try:\n                with self.root_fs.open(self.layout_file) as fh:\n                    layout = json.load(fh)\n                if not isinstance(layout, dict):\n                    raise StoreException(\"Storage root %s has layout file that isn't a JSON object\" % (self.root))\n                if ('extension' not in layout or not isinstance(layout['extension'], str)\n                        or 'description' not in layout or not isinstance(layout['description'], str)):\n                    raise StoreException(\"Storage root %s has layout file doesn't have required extension and description string entries\" % (self.root))\n                return layout['extension'], layout['description']\n            except Exception as e:  # FIXME - more specific?\n                raise StoreException(\"OCFL storage root %s has layout file that can't be read (%s)\" % (self.root, str(e)))\n        else:\n            return None, None\n\n    def object_paths(self):\n        \"\"\"Generate object paths for every obect in the OCFL storage root.\n\n        Yields (dirpath) that is the path to the directory for each object\n        located, relative to the OCFL storage root and without a preceding /.\n\n        Will log any errors seen while traversing the directory tree under the\n        storage root.\n        \"\"\"\n        for (dirpath, dirs, files) in ocfl_walk(self.root_fs, is_storage_root=True):\n            if dirpath == '/':\n                if 'extensions' in dirs:\n                    self.validate_extensions_dir()\n                    dirs.remove('extensions')\n                # Ignore any other files in storage root\n            elif (len(dirs) + len(files)) == 0:\n                self.traversal_error(\"E073\", path=dirpath)\n            elif len(files) == 0:\n                pass  # Just an intermediate directory\n            else:\n                # Is this directory an OCFL object? Look for any 0= file.\n                zero_eqs = [file for file in files if file.startswith('0=')]\n                if len(zero_eqs) > 1:\n                    self.traversal_error(\"E003d\", path=dirpath)\n                elif len(zero_eqs) == 1:\n                    declaration = zero_eqs[0]\n                    match = re.match(r'''0=ocfl_object_(\\d+\\.\\d+)''', declaration)\n                    if match and match.group(1) == '1.0':\n                        yield dirpath.lstrip('/')\n                    elif match:\n                        self.traversal_error(\"E004a\", path=dirpath, version=match.group(1))\n                    else:\n                        self.traversal_error(\"E004b\", path=dirpath, declaration=declaration)\n                else:\n                    self.traversal_error(\"E072\", path=dirpath)\n\n    def validate_extensions_dir(self):\n        \"\"\"Validate content of extensions directory inside storage root.\n\n        Validate the extensions directory by checking that there aren't any\n        entries in the extensions directory that aren't directories themselves.\n        Where there are extension directories they SHOULD be registered and\n        this code relies up the registered_extensions property to list known\n        storage root extensions.\n        \"\"\"\n        for entry in self.root_fs.scandir('extensions'):\n            if entry.is_dir:\n                if entry.name not in self.registered_extensions:\n                    self.log.warning('W901', entry=entry.name)  # FIXME - No good warning code in spec\n            else:\n                self.traversal_error('E086', entry=entry.name)\n\n    def list(self):\n        \"\"\"List contents of this OCFL storage root.\"\"\"\n        self.open_root_fs()\n        self.check_root_structure()\n        self.num_objects = 0\n        for dirpath in self.object_paths():\n            with ocfl_opendir(self.root_fs, dirpath) as obj_fs:\n                # Parse inventory to extract id\n                identifier = Object(obj_fs=obj_fs).id_from_inventory()\n                print(\"%s -- id=%s\" % (dirpath, identifier))\n                self.num_objects += 1\n                # FIXME - maybe do some more stuff in here\n        logging.info(\"Found %d OCFL Objects under root %s\", self.num_objects, self.root)\n\n    def validate_hierarchy(self, validate_objects=True, check_digests=True, show_warnings=False):\n        \"\"\"Validate storage root hierarchy.\n\n        Returns:\n            num_objects - number of objects checked\n            good_objects - number of objects checked that were found to be valid\n        \"\"\"\n        num_objects = 0\n        good_objects = 0\n        for dirpath in self.object_paths():\n            if validate_objects:\n                validator = Validator(check_digests=check_digests,\n                                      lax_digests=self.lax_digests,\n                                      show_warnings=show_warnings)\n                if validator.validate(ocfl_opendir(self.root_fs, dirpath)):\n                    good_objects += 1\n                else:\n                    logging.info(\"Object at %s in INVALID\", dirpath)\n                messages = validator.status_str(prefix='[[' + dirpath + ']]')\n                if messages != '':\n                    print(messages)\n                num_objects += 1\n        return num_objects, good_objects\n\n    def validate(self, validate_objects=True, check_digests=True, show_warnings=False, show_errors=True, lang='en'):\n        \"\"\"Validate OCFL storage root and optionally all objects.\"\"\"\n        valid = True\n        self.log = ValidationLogger(show_warnings=show_warnings, show_errors=show_errors, lang=lang)\n        self.open_root_fs()\n        try:\n            self.check_root_structure()\n            logging.info(\"Storage root structure is VALID\")\n        except StoreException as e:\n            valid = False\n            logging.info(\"Storage root structure is INVALID (%s)\", str(e))\n        self.num_objects, self.good_objects = self.validate_hierarchy(validate_objects=validate_objects, check_digests=check_digests, show_warnings=show_warnings)\n        if validate_objects:\n            if self.good_objects == self.num_objects:\n                logging.info(\"Objects checked: %d / %d are VALID\", self.good_objects, self.num_objects)\n            else:\n                valid = False\n                logging.info(\"Objects checked: %d / %d are INVALID\", self.num_objects - self.good_objects, self.num_objects)\n        else:\n            logging.info(\"Not checking OCFL objects\")\n        print(str(self.log))\n        if self.num_traversal_errors > 0:\n            valid = False\n            logging.info(\"Encountered %d errors traversing storage root\", self.num_traversal_errors)\n        # FIXME - do some stuff in here\n        if valid:\n            logging.info(\"Storage root %s is VALID\", self.root)\n        else:\n            logging.info(\"Storage root %s is INVALID\", self.root)\n        return valid\n\n    def add(self, object_path):\n        \"\"\"Add pre-constructed object from object_path.\"\"\"\n        self.open_root_fs()\n        self.check_root_structure()\n        # Sanity check\n        o = Object()\n        o.open_fs(object_path)\n        inventory = o.parse_inventory()\n        identifier = inventory['id']\n        # Now copy\n        path = self.object_path(identifier)\n        logging.info(\"Copying from %s to %s\", object_path, fs.path.join(self.root, path))\n        try:\n            copy_dir(o.obj_fs, '/', self.root_fs, path)\n            logging.info(\"Copied\")\n        except Exception as e:\n            logging.error(\"Copy failed: %s\", str(e))\n            raise StoreException(\"Add object failed!\")\n",
            "file_path": "ocfl/store.py",
            "human_label": "Create and initialize a new OCFL storage root.",
            "level": "file_runnable",
            "lineno": "81",
            "name": "initialize",
            "oracle_context": "{ \"apis\" : \"['open_fs', 'debug', 'makedir', 'exists', 'write', 'info', 'split', 'open', 'dump']\", \"classes\" : \"['open_fs', 'fs', 'json', 'logging', 'Namaste', 'StoreException']\", \"vars\" : \"['root', 'path', 'declaration_tvalue', 'root_fs', 'layout_file', 'disposition']\" }",
            "package": "store",
            "project": "zimeon/ocfl-py",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b45e23e0d4551b0392c90a",
            "all_context": "{ \"import\" : \"json re namaste pyfs fs digest \", \"file\" : \"\", \"class\" : \"self.validate_inventory_digest_match(self,inv_file,inv_digest_file) ; self.status_str ; self.validate_inventory(self,inv_file,where,extract_spec_version) ; self.registered_extensions ; self.read_inventory_digest ; self.obj_fs ; self.root_inv_validator ; self.validate_inventory_digest ; self.initialize(self) ; self.check_additional_digests(self,filepath,known_digests,additional_digests,error_code) ; self.validate_object_root(self,version_dirs,already_checked) ; self.validate_extensions_dir(self) ; self.inventory_digest_files ; self.validate_inventory ; self.check_additional_digests ; self.content_directory ; self.validate_version_inventories ; self.validate_extensions_dir ; self.status_str(self,prefix) ; self.validate_content(self,inventory,version_dirs,prior_manifest_digests,prior_fixity_digests) ; self.id ; self.__str__(self) ; self.lax_digests ; self.read_inventory_digest(self,inv_digest_file) ; self.validate_inventory_digest(self,inv_file,digest_algorithm,where) ; self.log ; self.digest_algorithm ; self.validate_content ; self.validate_inventory_digest_match ; self.spec_version ; self.check_digests ; self.validate_object_root ; self.initialize ; self.validate(self,path) ; self.__init__(self,log,show_warnings,show_errors,check_digests,lax_digests,lang) ; self.validate_version_inventories(self,version_dirs) ; \" }",
            "code": "    def validate_version_inventories(self, version_dirs):\n        \"\"\"Each version SHOULD have an inventory up to that point.\n\n        Also keep a record of any content digests different from those in the root inventory\n        so that we can also check them when validating the content.\n\n        version_dirs is an array of version directory names and is assumed to be in\n        version sequence (1, 2, 3...).\n        \"\"\"\n        prior_manifest_digests = {}  # file -> algorithm -> digest -> [versions]\n        prior_fixity_digests = {}  # file -> algorithm -> digest -> [versions]\n        if len(version_dirs) == 0:\n            return prior_manifest_digests, prior_fixity_digests\n        last_version = version_dirs[-1]\n        prev_version_dir = \"NONE\"  # will be set for first directory with inventory\n        prev_spec_version = '1.0'  # lowest version\n        for version_dir in version_dirs:\n            inv_file = fs.path.join(version_dir, 'inventory.json')\n            if not self.obj_fs.exists(inv_file):\n                self.log.warning('W010', where=version_dir)\n                continue\n            # There is an inventory file for this version directory, check it\n            if version_dir == last_version:\n                # Don't validate in this case. Per the spec the inventory in the last version\n                # MUST be identical to the copy in the object root, just check that\n                root_inv_file = 'inventory.json'\n                if not ocfl_files_identical(self.obj_fs, inv_file, root_inv_file):\n                    self.log.error('E064', root_inv_file=root_inv_file, inv_file=inv_file)\n                else:\n                    # We could also just compare digest files but this gives a more helpful error for\n                    # which file has the incorrect digest if they don't match\n                    self.validate_inventory_digest(inv_file, self.digest_algorithm, where=version_dir)\n                self.inventory_digest_files[version_dir] = 'inventory.json.' + self.digest_algorithm\n                this_spec_version = self.spec_version\n            else:\n                # Note that inventories in prior versions may use different digest algorithms\n                # from the current invenotory. Also,\n                # an may accord with the same or earlier versions of the specification\n                version_inventory, inv_validator = self.validate_inventory(inv_file, where=version_dir, extract_spec_version=True)\n                this_spec_version = inv_validator.spec_version\n                digest_algorithm = inv_validator.digest_algorithm\n                self.validate_inventory_digest(inv_file, digest_algorithm, where=version_dir)\n                self.inventory_digest_files[version_dir] = 'inventory.json.' + digest_algorithm\n                if self.id and 'id' in version_inventory:\n                    if version_inventory['id'] != self.id:\n                        self.log.error('E037b', where=version_dir, root_id=self.id, version_id=version_inventory['id'])\n                if 'manifest' in version_inventory:\n                    # Check that all files listed in prior inventories are in manifest\n                    not_seen = set(prior_manifest_digests.keys())\n                    for digest in version_inventory['manifest']:\n                        for filepath in version_inventory['manifest'][digest]:\n                            # We rely on the validation to check that anything present is OK\n                            if filepath in not_seen:\n                                not_seen.remove(filepath)\n                    if len(not_seen) > 0:\n                        self.log.error('E023b', where=version_dir, missing_filepaths=', '.join(sorted(not_seen)))\n                    # Record all prior digests\n                    for unnormalized_digest in version_inventory['manifest']:\n                        digest = normalized_digest(unnormalized_digest, digest_type=digest_algorithm)\n                        for filepath in version_inventory['manifest'][unnormalized_digest]:\n                            if filepath not in prior_manifest_digests:\n                                prior_manifest_digests[filepath] = {}\n                            if digest_algorithm not in prior_manifest_digests[filepath]:\n                                prior_manifest_digests[filepath][digest_algorithm] = {}\n                            if digest not in prior_manifest_digests[filepath][digest_algorithm]:\n                                prior_manifest_digests[filepath][digest_algorithm][digest] = []\n                            prior_manifest_digests[filepath][digest_algorithm][digest].append(version_dir)\n                # Is this inventory an appropriate prior version of the object root inventory?\n                if self.root_inv_validator is not None:\n                    self.root_inv_validator.validate_as_prior_version(inv_validator)\n                # Fixity blocks are independent in each version. Record all values and the versions\n                # they occur in for later checks against content\n                if 'fixity' in version_inventory:\n                    for digest_algorithm in version_inventory['fixity']:\n                        for unnormalized_digest in version_inventory['fixity'][digest_algorithm]:\n                            digest = normalized_digest(unnormalized_digest, digest_type=digest_algorithm)\n                            for filepath in version_inventory['fixity'][digest_algorithm][unnormalized_digest]:\n                                if filepath not in prior_fixity_digests:\n                                    prior_fixity_digests[filepath] = {}\n                                if digest_algorithm not in prior_fixity_digests[filepath]:\n                                    prior_fixity_digests[filepath][digest_algorithm] = {}\n                                if digest not in prior_fixity_digests[filepath][digest_algorithm]:\n                                    prior_fixity_digests[filepath][digest_algorithm][digest] = []\n                                prior_fixity_digests[filepath][digest_algorithm][digest].append(version_dir)\n            # We are validating the inventories in sequence and each new version must\n            # follow the same or later spec version to previous inventories\n            if prev_spec_version > this_spec_version:\n                self.log.error('E103', where=version_dir, this_spec_version=this_spec_version,\n                               prev_version_dir=prev_version_dir, prev_spec_version=prev_spec_version)\n            prev_version_dir = version_dir\n            prev_spec_version = this_spec_version\n        return prior_manifest_digests, prior_fixity_digests\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : false, \"public_lib\" : true, \"current_class\" : true, \"current_file\" : false, \"current_project\" : false, \"external\" : false }",
            "docstring": "Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).",
            "end_lineno": "325",
            "file_content": "\"\"\"OCFL Validator.\n\nPhilosophy of this code is to keep it separate from the implementations\nof Store, Object and Version used to build and manipulate OCFL data, but\nto leverage lower level functions such as digest creation etc.. Code style\nis plain/verbose with detailed and specific validation errors that might\nhelp someone debug an implementation.\n\nThis code uses PyFilesystem (import fs) exclusively for access to files. This\nshould enable application beyond the operating system filesystem.\n\"\"\"\nimport json\nimport re\nimport fs\n\nfrom .digest import file_digest, normalized_digest\nfrom .inventory_validator import InventoryValidator\nfrom .namaste import find_namastes\nfrom .pyfs import open_fs, ocfl_walk, ocfl_files_identical\nfrom .validation_logger import ValidationLogger\n\n\nclass ValidatorAbortException(Exception):\n    \"\"\"Exception class to bail out of validation.\"\"\"\n\n\nclass Validator():\n    \"\"\"Class for OCFL Validator.\"\"\"\n\n    def __init__(self, log=None, show_warnings=False, show_errors=True, check_digests=True, lax_digests=False, lang='en'):\n        \"\"\"Initialize OCFL validator.\"\"\"\n        self.log = log\n        self.check_digests = check_digests\n        self.lax_digests = lax_digests\n        if self.log is None:\n            self.log = ValidationLogger(show_warnings=show_warnings, show_errors=show_errors, lang=lang)\n        self.registered_extensions = [\n            '0001-digest-algorithms', '0002-flat-direct-storage-layout',\n            '0003-hash-and-id-n-tuple-storage-layout', '0004-hashed-n-tuple-storage-layout',\n            '0005-mutable-head'\n        ]\n        # The following actually initialized in initialize() method\n        self.id = None\n        self.spec_version = None\n        self.digest_algorithm = None\n        self.content_directory = None\n        self.inventory_digest_files = None\n        self.root_inv_validator = None\n        self.obj_fs = None\n        self.initialize()\n\n    def initialize(self):\n        \"\"\"Initialize object state.\n\n        Must be called between attempts to validate objects.\n        \"\"\"\n        self.id = None\n        self.spec_version = '1.0'  # default to latest published version\n        self.digest_algorithm = 'sha512'\n        self.content_directory = 'content'\n        self.inventory_digest_files = {}  # index by version_dir, algorithms may differ\n        self.root_inv_validator = None\n        self.obj_fs = None\n\n    def status_str(self, prefix=''):\n        \"\"\"Return string representation of validation log, with optional prefix.\"\"\"\n        return self.log.status_str(prefix=prefix)\n\n    def __str__(self):\n        \"\"\"Return string representation of validation log.\"\"\"\n        return self.status_str()\n\n    def validate(self, path):\n        \"\"\"Validate OCFL object at path or pyfs root.\n\n        Returns True if valid (warnings permitted), False otherwise.\n        \"\"\"\n        self.initialize()\n        try:\n            if isinstance(path, str):\n                self.obj_fs = open_fs(path)\n            else:\n                self.obj_fs = path\n                path = self.obj_fs.desc('')\n        except fs.errors.CreateFailed:\n            self.log.error('E003e', path=path)\n            return False\n        # Object declaration, set spec version number. If there are multiple declarations,\n        # look for the lastest object version then report any others as errors\n        namastes = find_namastes(0, pyfs=self.obj_fs)\n        if len(namastes) == 0:\n            self.log.error('E003a', assumed_version=self.spec_version)\n        else:\n            spec_version = None\n            for namaste in namastes:\n                # Extract and check spec version number\n                this_file_version = None\n                for version in ('1.1', '1.0'):\n                    if namaste.filename == '0=ocfl_object_' + version:\n                        this_file_version = version\n                        break\n                if this_file_version is None:\n                    self.log.error('E006', filename=namaste.filename)\n                elif spec_version is None or this_file_version > spec_version:\n                    spec_version = this_file_version\n                    if not namaste.content_ok(pyfs=self.obj_fs):\n                        self.log.error('E007', filename=namaste.filename)\n            if spec_version is None:\n                self.log.error('E003c', assumed_version=self.spec_version)\n            else:\n                self.spec_version = spec_version\n                if len(namastes) > 1:\n                    self.log.error('E003b', files=len(namastes), using_version=self.spec_version)\n        # Object root inventory file\n        inv_file = 'inventory.json'\n        if not self.obj_fs.exists(inv_file):\n            self.log.error('E063')\n            return False\n        try:\n            inventory, inv_validator = self.validate_inventory(inv_file)\n            inventory_is_valid = self.log.num_errors == 0\n            self.root_inv_validator = inv_validator\n            all_versions = inv_validator.all_versions\n            self.id = inv_validator.id\n            self.content_directory = inv_validator.content_directory\n            self.digest_algorithm = inv_validator.digest_algorithm\n            self.validate_inventory_digest(inv_file, self.digest_algorithm)\n            # Object root\n            self.validate_object_root(all_versions, already_checked=[namaste.filename for namaste in namastes])\n            # Version inventory files\n            (prior_manifest_digests, prior_fixity_digests) = self.validate_version_inventories(all_versions)\n            if inventory_is_valid:\n                # Object content\n                self.validate_content(inventory, all_versions, prior_manifest_digests, prior_fixity_digests)\n        except ValidatorAbortException:\n            pass\n        return self.log.num_errors == 0\n\n    def validate_inventory(self, inv_file, where='root', extract_spec_version=False):\n        \"\"\"Validate a given inventory file, record errors with self.log.error().\n\n        Returns inventory object for use in later validation\n        of object content. Does not look at anything else in the\n        object itself.\n\n        where - used for reporting messages of where inventory is in object\n\n        extract_spec_version - if set True will attempt to take spec_version from the\n            inventory itself instead of using the spec_version provided\n        \"\"\"\n        try:\n            with self.obj_fs.openbin(inv_file, 'r') as fh:\n                inventory = json.load(fh)\n        except json.decoder.JSONDecodeError as e:\n            self.log.error('E033', where=where, explanation=str(e))\n            raise ValidatorAbortException\n        inv_validator = InventoryValidator(log=self.log, where=where,\n                                           lax_digests=self.lax_digests,\n                                           spec_version=self.spec_version)\n        inv_validator.validate(inventory, extract_spec_version=extract_spec_version)\n        return inventory, inv_validator\n\n    def validate_inventory_digest(self, inv_file, digest_algorithm, where=\"root\"):\n        \"\"\"Validate the appropriate inventory digest file in path.\"\"\"\n        inv_digest_file = inv_file + '.' + digest_algorithm\n        if not self.obj_fs.exists(inv_digest_file):\n            self.log.error('E058a', where=where, path=inv_digest_file)\n        else:\n            self.validate_inventory_digest_match(inv_file, inv_digest_file)\n\n    def validate_inventory_digest_match(self, inv_file, inv_digest_file):\n        \"\"\"Validate a given inventory digest for a given inventory file.\n\n        On error throws exception with debugging string intended to\n        be presented to a user.\n        \"\"\"\n        if not self.check_digests:\n            return\n        m = re.match(r'''.*\\.(\\w+)$''', inv_digest_file)\n        if m:\n            digest_algorithm = m.group(1)\n            try:\n                digest_recorded = self.read_inventory_digest(inv_digest_file)\n                digest_actual = file_digest(inv_file, digest_algorithm, pyfs=self.obj_fs)\n                if digest_actual != digest_recorded:\n                    self.log.error(\"E060\", inv_file=inv_file, actual=digest_actual, recorded=digest_recorded, inv_digest_file=inv_digest_file)\n            except Exception as e:  # pylint: disable=broad-except\n                self.log.error(\"E061\", description=str(e))\n        else:\n            self.log.error(\"E058b\", inv_digest_file=inv_digest_file)\n\n    def validate_object_root(self, version_dirs, already_checked):\n        \"\"\"Validate object root.\n\n        All expected_files must be present and no other files.\n        All expected_dirs must be present and no other dirs.\n        \"\"\"\n        expected_files = ['0=ocfl_object_' + self.spec_version, 'inventory.json',\n                          'inventory.json.' + self.digest_algorithm]\n        for entry in self.obj_fs.scandir(''):\n            if entry.is_file:\n                if entry.name not in expected_files and entry.name not in already_checked:\n                    self.log.error('E001a', file=entry.name)\n            elif entry.is_dir:\n                if entry.name in version_dirs:\n                    pass\n                elif entry.name == 'extensions':\n                    self.validate_extensions_dir()\n                elif re.match(r'''v\\d+$''', entry.name):\n                    # Looks like a version directory so give more specific error\n                    self.log.error('E046b', dir=entry.name)\n                else:\n                    # Simply an unexpected directory\n                    self.log.error('E001b', dir=entry.name)\n            else:\n                self.log.error('E001c', entry=entry.name)\n\n    def validate_extensions_dir(self):\n        \"\"\"Validate content of extensions directory inside object root.\n\n        Validate the extensions directory by checking that there aren't any\n        entries in the extensions directory that aren't directories themselves.\n        Where there are extension directories they SHOULD be registered and\n        this code relies up the registered_extensions property to list known\n        extensions.\n        \"\"\"\n        for entry in self.obj_fs.scandir('extensions'):\n            if entry.is_dir:\n                if entry.name not in self.registered_extensions:\n                    self.log.warning('W013', entry=entry.name)\n            else:\n                self.log.error('E067', entry=entry.name)\n\n    def validate_version_inventories(self, version_dirs):\n        \"\"\"Each version SHOULD have an inventory up to that point.\n\n        Also keep a record of any content digests different from those in the root inventory\n        so that we can also check them when validating the content.\n\n        version_dirs is an array of version directory names and is assumed to be in\n        version sequence (1, 2, 3...).\n        \"\"\"\n        prior_manifest_digests = {}  # file -> algorithm -> digest -> [versions]\n        prior_fixity_digests = {}  # file -> algorithm -> digest -> [versions]\n        if len(version_dirs) == 0:\n            return prior_manifest_digests, prior_fixity_digests\n        last_version = version_dirs[-1]\n        prev_version_dir = \"NONE\"  # will be set for first directory with inventory\n        prev_spec_version = '1.0'  # lowest version\n        for version_dir in version_dirs:\n            inv_file = fs.path.join(version_dir, 'inventory.json')\n            if not self.obj_fs.exists(inv_file):\n                self.log.warning('W010', where=version_dir)\n                continue\n            # There is an inventory file for this version directory, check it\n            if version_dir == last_version:\n                # Don't validate in this case. Per the spec the inventory in the last version\n                # MUST be identical to the copy in the object root, just check that\n                root_inv_file = 'inventory.json'\n                if not ocfl_files_identical(self.obj_fs, inv_file, root_inv_file):\n                    self.log.error('E064', root_inv_file=root_inv_file, inv_file=inv_file)\n                else:\n                    # We could also just compare digest files but this gives a more helpful error for\n                    # which file has the incorrect digest if they don't match\n                    self.validate_inventory_digest(inv_file, self.digest_algorithm, where=version_dir)\n                self.inventory_digest_files[version_dir] = 'inventory.json.' + self.digest_algorithm\n                this_spec_version = self.spec_version\n            else:\n                # Note that inventories in prior versions may use different digest algorithms\n                # from the current invenotory. Also,\n                # an may accord with the same or earlier versions of the specification\n                version_inventory, inv_validator = self.validate_inventory(inv_file, where=version_dir, extract_spec_version=True)\n                this_spec_version = inv_validator.spec_version\n                digest_algorithm = inv_validator.digest_algorithm\n                self.validate_inventory_digest(inv_file, digest_algorithm, where=version_dir)\n                self.inventory_digest_files[version_dir] = 'inventory.json.' + digest_algorithm\n                if self.id and 'id' in version_inventory:\n                    if version_inventory['id'] != self.id:\n                        self.log.error('E037b', where=version_dir, root_id=self.id, version_id=version_inventory['id'])\n                if 'manifest' in version_inventory:\n                    # Check that all files listed in prior inventories are in manifest\n                    not_seen = set(prior_manifest_digests.keys())\n                    for digest in version_inventory['manifest']:\n                        for filepath in version_inventory['manifest'][digest]:\n                            # We rely on the validation to check that anything present is OK\n                            if filepath in not_seen:\n                                not_seen.remove(filepath)\n                    if len(not_seen) > 0:\n                        self.log.error('E023b', where=version_dir, missing_filepaths=', '.join(sorted(not_seen)))\n                    # Record all prior digests\n                    for unnormalized_digest in version_inventory['manifest']:\n                        digest = normalized_digest(unnormalized_digest, digest_type=digest_algorithm)\n                        for filepath in version_inventory['manifest'][unnormalized_digest]:\n                            if filepath not in prior_manifest_digests:\n                                prior_manifest_digests[filepath] = {}\n                            if digest_algorithm not in prior_manifest_digests[filepath]:\n                                prior_manifest_digests[filepath][digest_algorithm] = {}\n                            if digest not in prior_manifest_digests[filepath][digest_algorithm]:\n                                prior_manifest_digests[filepath][digest_algorithm][digest] = []\n                            prior_manifest_digests[filepath][digest_algorithm][digest].append(version_dir)\n                # Is this inventory an appropriate prior version of the object root inventory?\n                if self.root_inv_validator is not None:\n                    self.root_inv_validator.validate_as_prior_version(inv_validator)\n                # Fixity blocks are independent in each version. Record all values and the versions\n                # they occur in for later checks against content\n                if 'fixity' in version_inventory:\n                    for digest_algorithm in version_inventory['fixity']:\n                        for unnormalized_digest in version_inventory['fixity'][digest_algorithm]:\n                            digest = normalized_digest(unnormalized_digest, digest_type=digest_algorithm)\n                            for filepath in version_inventory['fixity'][digest_algorithm][unnormalized_digest]:\n                                if filepath not in prior_fixity_digests:\n                                    prior_fixity_digests[filepath] = {}\n                                if digest_algorithm not in prior_fixity_digests[filepath]:\n                                    prior_fixity_digests[filepath][digest_algorithm] = {}\n                                if digest not in prior_fixity_digests[filepath][digest_algorithm]:\n                                    prior_fixity_digests[filepath][digest_algorithm][digest] = []\n                                prior_fixity_digests[filepath][digest_algorithm][digest].append(version_dir)\n            # We are validating the inventories in sequence and each new version must\n            # follow the same or later spec version to previous inventories\n            if prev_spec_version > this_spec_version:\n                self.log.error('E103', where=version_dir, this_spec_version=this_spec_version,\n                               prev_version_dir=prev_version_dir, prev_spec_version=prev_spec_version)\n            prev_version_dir = version_dir\n            prev_spec_version = this_spec_version\n        return prior_manifest_digests, prior_fixity_digests\n\n    def validate_content(self, inventory, version_dirs, prior_manifest_digests, prior_fixity_digests):\n        \"\"\"Validate file presence and content against inventory.\n\n        The root inventory in `inventory` is assumed to be valid and safe to use\n        for construction of file paths etc..\n        \"\"\"\n        files_seen = set()\n        # Check files in each version directory\n        for version_dir in version_dirs:\n            try:\n                # Check contents of version directory except content_directory\n                for entry in self.obj_fs.listdir(version_dir):\n                    if ((entry == 'inventory.json')\n                            or (version_dir in self.inventory_digest_files and entry == self.inventory_digest_files[version_dir])):\n                        pass\n                    elif entry == self.content_directory:\n                        # Check content_directory\n                        content_path = fs.path.join(version_dir, self.content_directory)\n                        num_content_files_in_version = 0\n                        for dirpath, dirs, files in ocfl_walk(self.obj_fs, content_path):\n                            if dirpath != '/' + content_path and (len(dirs) + len(files)) == 0:\n                                self.log.error(\"E024\", where=version_dir, path=dirpath)\n                            for file in files:\n                                files_seen.add(fs.path.join(dirpath, file).lstrip('/'))\n                                num_content_files_in_version += 1\n                        if num_content_files_in_version == 0:\n                            self.log.warning(\"W003\", where=version_dir)\n                    elif self.obj_fs.isdir(fs.path.join(version_dir, entry)):\n                        self.log.warning(\"W002\", where=version_dir, entry=entry)\n                    else:\n                        self.log.error(\"E015\", where=version_dir, entry=entry)\n            except (fs.errors.ResourceNotFound, fs.errors.DirectoryExpected):\n                self.log.error('E046a', version_dir=version_dir)\n        # Extract any digests in fixity and organize by filepath\n        fixity_digests = {}\n        if 'fixity' in inventory:\n            for digest_algorithm in inventory['fixity']:\n                for digest in inventory['fixity'][digest_algorithm]:\n                    for filepath in inventory['fixity'][digest_algorithm][digest]:\n                        if filepath in files_seen:\n                            if filepath not in fixity_digests:\n                                fixity_digests[filepath] = {}\n                            if digest_algorithm not in fixity_digests[filepath]:\n                                fixity_digests[filepath][digest_algorithm] = {}\n                            if digest not in fixity_digests[filepath][digest_algorithm]:\n                                fixity_digests[filepath][digest_algorithm][digest] = ['root']\n                        else:\n                            self.log.error('E093b', where='root', digest_algorithm=digest_algorithm, digest=digest, content_path=filepath)\n        # Check all files in root manifest\n        if 'manifest' in inventory:\n            for digest in inventory['manifest']:\n                for filepath in inventory['manifest'][digest]:\n                    if filepath not in files_seen:\n                        self.log.error('E092b', where='root', content_path=filepath)\n                    else:\n                        if self.check_digests:\n                            content_digest = file_digest(filepath, digest_type=self.digest_algorithm, pyfs=self.obj_fs)\n                            if content_digest != normalized_digest(digest, digest_type=self.digest_algorithm):\n                                self.log.error('E092a', where='root', digest_algorithm=self.digest_algorithm, digest=digest, content_path=filepath, content_digest=content_digest)\n                            known_digests = {self.digest_algorithm: content_digest}\n                            # Are there digest values in the fixity block?\n                            self.check_additional_digests(filepath, known_digests, fixity_digests, 'E093a')\n                            # Are there other digests for this same file from other inventories?\n                            self.check_additional_digests(filepath, known_digests, prior_manifest_digests, 'E092a')\n                            self.check_additional_digests(filepath, known_digests, prior_fixity_digests, 'E093a')\n                        files_seen.discard(filepath)\n        # Anything left in files_seen is not mentioned in the inventory\n        if len(files_seen) > 0:\n            self.log.error('E023a', where='root', extra_files=', '.join(sorted(files_seen)))\n\n    def check_additional_digests(self, filepath, known_digests, additional_digests, error_code):\n        \"\"\"Check all the additional digests for filepath.\n\n        This method is intended to be used both for manifest digests in prior versions and\n        for fixity digests. The digests_seen dict is used to store any values calculated\n        so that we don't recalculate digests that might appear multiple times. It is added to\n        with any additional values calculated.\n\n        Parameters:\n            filepath - path of file in object (`v1/content/something` etc.)\n            known_digests - dict of algorithm->digest that we have calculated\n            additional_digests - dict: filepath -> algorithm -> digest -> [versions appears in]\n            error_code - error code to log on mismatch (E092a for manifest, E093a for fixity)\n        \"\"\"\n        if filepath in additional_digests:\n            for digest_algorithm in additional_digests[filepath]:\n                if digest_algorithm in known_digests:\n                    # Don't recompute anything, just use it if we've seen it before\n                    content_digest = known_digests[digest_algorithm]\n                else:\n                    content_digest = file_digest(filepath, digest_type=digest_algorithm, pyfs=self.obj_fs)\n                    known_digests[digest_algorithm] = content_digest\n                for digest in additional_digests[filepath][digest_algorithm]:\n                    if content_digest != normalized_digest(digest, digest_type=digest_algorithm):\n                        where = ','.join(additional_digests[filepath][digest_algorithm][digest])\n                        self.log.error(error_code, where=where, digest_algorithm=digest_algorithm, digest=digest, content_path=filepath, content_digest=content_digest)\n\n    def read_inventory_digest(self, inv_digest_file):\n        \"\"\"Read inventory digest from sidecar file.\n\n        Raise exception if there is an error, else return digest.\n        \"\"\"\n        with self.obj_fs.open(inv_digest_file, 'r') as fh:\n            line = fh.readline()\n            # we ignore any following lines, could raise exception\n        m = re.match(r'''(\\w+)\\s+(\\S+)\\s*$''', line)\n        if not m:\n            raise Exception(\"Bad inventory digest file %s, wrong format\" % (inv_digest_file))\n        if m.group(2) != 'inventory.json':\n            raise Exception(\"Bad inventory name in inventory digest file %s\" % (inv_digest_file))\n        return m.group(1)\n",
            "file_path": "ocfl/validator.py",
            "human_label": "Each version SHOULD have an inventory up to that point.\n\nAlso keep a record of any content digests different from those in the root inventory\nso that we can also check them when validating the content.\n\nversion_dirs is an array of version directory names and is assumed to be in\nversion sequence (1, 2, 3...).",
            "level": "class_runnable",
            "lineno": "234",
            "name": "validate_version_inventories",
            "oracle_context": "{ \"apis\" : \"['warning', 'set', 'exists', 'ocfl_files_identical', 'join', 'append', 'error', 'validate_inventory_digest', 'validate_as_prior_version', 'validate_inventory', 'len', 'remove', 'sorted', 'keys', 'normalized_digest']\", \"classes\" : \"['normalized_digest', 'ocfl_files_identical', 'fs']\", \"vars\" : \"['Str', 'inventory_digest_files', 'root_inv_validator', 'path', 'id', 'digest_algorithm', 'obj_fs', 'log', 'spec_version']\" }",
            "package": "validator",
            "project": "zimeon/ocfl-py",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b45e21e0d4551b0392c8ed",
            "all_context": "{ \"import\" : \"re sys pyfs fs namaste \", \"file\" : \"\", \"class\" : \"\" }",
            "code": "def find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        pyfs = open_fs(path, create=False)\n    except (fs.opener.errors.OpenerError, fs.errors.CreateFailed):\n        # Failed to open path as a filesystem, try enclosing directory\n        # in case path is a file\n        (parent, filename) = fs.path.split(path)\n        try:\n            pyfs = open_fs(parent, create=False)\n        except (fs.opener.errors.OpenerError, fs.errors.CreateFailed) as e:\n            return \"path cannot be opened, and nor can parent (\" + str(e) + \")\"\n        # Can open parent, is filename a file there?\n        try:\n            info = pyfs.getinfo(filename)\n        except fs.errors.ResourceNotFound:\n            return \"path does not exist\"\n        if info.is_dir:\n            return \"directory that could not be opened as a filesystem, this should not happen\"  # pragma: no cover\n        return 'file'\n    namastes = find_namastes(0, pyfs=pyfs)\n    if len(namastes) == 0:\n        return \"no 0= declaration file\"\n    # Look at the first 0= Namaste file that is of OCFL form to determine type, if there are\n    # multiple declarations this will be caught later\n    for namaste in namastes:\n        m = re.match(r'''ocfl(_object)?_(\\d+\\.\\d+)$''', namaste.tvalue)\n        if m:\n            return 'root' if m.group(1) is None else 'object'\n    return \"unrecognized 0= declaration file or files (first is %s)\" % (namastes[0].tvalue)\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : true, \"public_lib\" : true, \"current_class\" : false, \"current_file\" : false, \"current_project\" : false, \"external\" : false }",
            "docstring": "Return a string indicating the type of thing at the given path.\n\nReturn values:\n    'root' - looks like an OCFL Storage Root\n    'object' - looks like an OCFL Object\n    'file' - a file, might be an inventory\n    other string explains error description\n\nLooks only at \"0=*\" Namaste files to determine the directory type.",
            "end_lineno": "143",
            "file_content": "# -*- coding: utf-8 -*-\n\"\"\"Utility functions to support the OCFL Object library.\"\"\"\nimport re\nimport sys\n\nimport fs\nimport fs.path\n\nfrom ._version import __version__\nfrom .namaste import find_namastes\nfrom .pyfs import open_fs\n\n\nNORMALIZATIONS = ['uri', 'md5']  # Must match possibilities in map_filepaths()\n\n\nclass ObjectException(Exception):\n    \"\"\"Exception class for OCFL Object.\"\"\"\n\n\ndef add_object_args(parser):\n    \"\"\"Add Object settings to argparse or argument group instance parser.\"\"\"\n    # Disk scanning\n    parser.add_argument('--skip', action='append', default=['README.md', '.DS_Store'],\n                        help='directories and files to ignore')\n    parser.add_argument('--normalization', '--norm', default=None,\n                        help='filepath normalization strategy (None, %s)' %\n                        (', '.join(NORMALIZATIONS)))\n    # Versioning strategy settings\n    parser.add_argument('--no-forward-delta', action='store_true',\n                        help='do not use forward deltas')\n    parser.add_argument('--no-dedupe', '--no-dedup', action='store_true',\n                        help='do not use deduplicate files within a version')\n    # Validation settings\n    parser.add_argument('--lax-digests', action='store_true',\n                        help='allow use of any known digest')\n    # Object files\n    parser.add_argument('--objdir', '--obj',\n                        help='read from or write to OCFL object directory objdir')\n\n\ndef add_shared_args(parser):\n    \"\"\"Add arguments to be shared by any ocfl-py scripts.\"\"\"\n    parser.add_argument('--verbose', '-v', action='store_true',\n                        help=\"be more verbose\")\n    parser.add_argument('--version', action='store_true',\n                        help='Show version number and exit')\n\n\ndef check_shared_args(args):\n    \"\"\"Check arguments set with add_shared_args.\"\"\"\n    if args.version:\n        print(\"%s is part of ocfl-py version %s\" % (fs.path.basename(sys.argv[0]), __version__))\n        sys.exit(0)\n\n\ndef next_version(version):\n    \"\"\"Next version identifier following existing pattern.\n\n    Must deal with both zero-prefixed and non-zero prefixed versions.\n    \"\"\"\n    m = re.match(r'''v((\\d)\\d*)$''', version)\n    if not m:\n        raise ObjectException(\"Bad version '%s'\" % version)\n    next_n = int(m.group(1)) + 1\n    if m.group(2) == '0':\n        # Zero-padded version\n        next_v = ('v0%0' + str(len(version) - 2) + 'd') % next_n\n        if len(next_v) != len(version):\n            raise ObjectException(\"Version number overflow for zero-padded version %d to %d\" % (version, next_v))\n        return next_v\n    # Not zero-padded\n    return 'v' + str(next_n)\n\n\ndef remove_first_directory(path):\n    \"\"\"Remove first directory from input path.\n\n    The return value will not have a trailing parh separator, even if\n    the input path does. Will return an empty string if the input path\n    has just one path segment.\n    \"\"\"\n    # FIXME - how to do this efficiently? Current code does complete\n    # split and rejoins, excluding the first directory\n    rpath = ''\n    while True:\n        (head, tail) = fs.path.split(path)\n        if path in (head, tail):\n            break\n        path = head\n        rpath = tail if rpath == '' else fs.path.join(tail, rpath)\n    return rpath\n\n\ndef make_unused_filepath(filepath, used, separator='__'):\n    \"\"\"Find filepath with string appended that makes it disjoint from those in used.\"\"\"\n    n = 1\n    while True:\n        n += 1\n        f = filepath + separator + str(n)\n        if f not in used:\n            return f\n\n\ndef find_path_type(path):\n    \"\"\"Return a string indicating the type of thing at the given path.\n\n    Return values:\n        'root' - looks like an OCFL Storage Root\n        'object' - looks like an OCFL Object\n        'file' - a file, might be an inventory\n        other string explains error description\n\n    Looks only at \"0=*\" Namaste files to determine the directory type.\n    \"\"\"\n    try:\n        pyfs = open_fs(path, create=False)\n    except (fs.opener.errors.OpenerError, fs.errors.CreateFailed):\n        # Failed to open path as a filesystem, try enclosing directory\n        # in case path is a file\n        (parent, filename) = fs.path.split(path)\n        try:\n            pyfs = open_fs(parent, create=False)\n        except (fs.opener.errors.OpenerError, fs.errors.CreateFailed) as e:\n            return \"path cannot be opened, and nor can parent (\" + str(e) + \")\"\n        # Can open parent, is filename a file there?\n        try:\n            info = pyfs.getinfo(filename)\n        except fs.errors.ResourceNotFound:\n            return \"path does not exist\"\n        if info.is_dir:\n            return \"directory that could not be opened as a filesystem, this should not happen\"  # pragma: no cover\n        return 'file'\n    namastes = find_namastes(0, pyfs=pyfs)\n    if len(namastes) == 0:\n        return \"no 0= declaration file\"\n    # Look at the first 0= Namaste file that is of OCFL form to determine type, if there are\n    # multiple declarations this will be caught later\n    for namaste in namastes:\n        m = re.match(r'''ocfl(_object)?_(\\d+\\.\\d+)$''', namaste.tvalue)\n        if m:\n            return 'root' if m.group(1) is None else 'object'\n    return \"unrecognized 0= declaration file or files (first is %s)\" % (namastes[0].tvalue)\n",
            "file_path": "ocfl/object_utils.py",
            "human_label": "Return a string indicating the type of thing at the given path",
            "level": "plib_runnable",
            "lineno": "105",
            "name": "find_path_type",
            "oracle_context": "{ \"apis\" : \"['getinfo', 'open_fs', 'group', 'len', 'split', 'find_namastes', 'match', 'str']\", \"classes\" : \"['find_namastes', 'fs', 're', 'open_fs']\", \"vars\" : \"['path', 'tvalue', 'CreateFailed', 'opener', 'ResourceNotFound', 'OpenerError', 'errors', 'is_dir']\" }",
            "package": "object_utils",
            "project": "zimeon/ocfl-py",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b45b396decaeff903e1001",
            "all_context": "{ \"import\" : \"datetime sys os collections argparse time libmozdata datetime jinja2 argparse \", \"file\" : \"\", \"class\" : \"self.get_list_bugs ; self.set_needinfo(self) ; self.get_comments ; self.has_flags ; self.get_email_subject ; self.query_params ; self.subject ; self.get_extra_for_needinfo_template ; self.__class__ ; self.__tool_path__ ; self.get_products(self) ; self.add_to_cache(self,bugs) ; self.cache ; self.name ; self.ignore_meta ; self.set_needinfo ; self.amend_bzparams(self,params,bug_ids) ; self.filter_no_nag_keyword(self) ; self.all_include_fields(self) ; self.has_bot_set_ni ; self.run(self) ; self.get_mail_to_auto_ni ; self.get_list_bugs(self,bugs) ; self.get_data(self) ; self.get_dates(self,date) ; self.get_bug_sort_key ; self.get_documentation ; self.__tool_name__ ; self.get_documentation(self) ; self.has_default_products(self) ; self.name(self) ; self.bughandler ; self.dryrun ; self.prod_comp ; self.preamble(self) ; self.handle_bug ; self.get_max_actions(self) ; self.terminate(self) ; self.autofix ; self.get_bugs(self,date,bug_ids,chunk_size) ; self.get_args_parser ; self.get_bz_params(self,date) ; self.get_data ; self.get_autofix_change ; self.get_mail_to_auto_ni(self,bug) ; self.get_auto_ni_skiplist(self) ; self._commenthandler ; self.parse_custom_arguments ; self.has_enough_data(self) ; self._populate_prioritized_actions ; self.ignore_meta(self) ; self.init_versions(self) ; self.has_product_component ; self.get_email_subject(self,date) ; self.columns(self) ; self.get_extra_for_template(self) ; self.has_product_component(self) ; self.has_individual_autofix ; self.__init__(self) ; self._set_tool_name(self) ; self.has_last_comment_time ; self.get_max_years ; self.needinfo_template(self) ; self.commenthandler(self,bug,bugid,data) ; self.auto_needinfo ; self.get_email(self,date,bug_ids) ; self.commenthandler ; self.template(self) ; self.get_summary(self,bug) ; self.quota_actions ; self.extra_ni ; self.versions ; self.add_no_manager(self,bugid) ; self.add_prioritized_action(self,bug,quota_name,needinfo,autofix) ; self.failure_callback(self,bugid) ; self.template ; self.get_extra_for_needinfo_template(self) ; self.autofix_changes ; self.has_access_to_sec_bugs ; self.get_summary ; self.get_bz_params ; self.get_autofix_change(self) ; self.needinfo_template ; self.has_needinfo(self) ; self.send_email ; self.has_enough_data ; self.get_max_years(self) ; self.subject(self) ; self.get_auto_ni_skiplist ; self.ignore_date(self) ; self.max_days_in_cache ; self.has_assignee(self) ; self.get_tool_path(self) ; self.must_run ; self.test_mode ; self.parse_custom_arguments(self,args) ; self.get_comments(self,bugs) ; self.exclude_no_action_bugs(self) ; self.has_needinfo ; self.sort_columns(self) ; self.handle_bug(self,bug,data) ; self.amend_bzparams ; self.add_auto_ni ; self.no_manager ; self.get_bugs ; self.description(self) ; self.ignore_date ; self.bughandler(self,bug,data) ; self.query_url ; self.has_assignee ; self.organize(self,bugs) ; self.sort_columns ; self.get_args_parser(self) ; self._commenthandler(self,bug,bugid,data) ; self.failure_callback ; self.get_extra_for_template ; self.must_run(self,date) ; self.get_email ; self.add_custom_arguments ; self.send_email(self,date) ; self.organize ; self.filter_no_nag_keyword ; self._populate_prioritized_actions(self,bugs) ; self.has_bot_set_ni(self,bug) ; self.columns ; self.get_tool_path ; self.get_db_extra ; self.max_days_in_cache(self) ; self.get_product_component(self) ; self.nag_date ; self.get_max_ni ; self.get_config ; self.has_last_comment_time(self) ; self.send_mails ; self.set_people_to_nag ; self.add_auto_ni(self,bugid,data) ; self.autofix(self,bugs) ; self.get_bug_sort_key(self,bug) ; self.get_email_data(self,date,bug_ids) ; self.get_config(self,entry,default) ; self.get_max_ni(self) ; self.has_default_products ; self.get_products ; self.has_individual_autofix(self,changes) ; self.has_autofix ; self.terminate ; self.get_email_data ; self.get_max_actions ; self.exclude_no_action_bugs ; self.preamble ; self.get_db_extra(self) ; self.add_to_cache ; self.has_access_to_sec_bugs(self) ; self.add_custom_arguments(self,parser) ; self._set_tool_name ; self.all_include_fields ; self.description ; \" }",
            "code": "    def amend_bzparams(self, params, bug_ids):\n        \"\"\"Amend the Bugzilla params\"\"\"\n        if not self.all_include_fields():\n            if \"include_fields\" in params:\n                fields = params[\"include_fields\"]\n                if isinstance(fields, list):\n                    if \"id\" not in fields:\n                        fields.append(\"id\")\n                elif isinstance(fields, str):\n                    if fields != \"id\":\n                        params[\"include_fields\"] = [fields, \"id\"]\n                else:\n                    params[\"include_fields\"] = [fields, \"id\"]\n            else:\n                params[\"include_fields\"] = [\"id\"]\n\n            params[\"include_fields\"] += [\"summary\", \"groups\"]\n\n            if self.has_assignee() and \"assigned_to\" not in params[\"include_fields\"]:\n                params[\"include_fields\"].append(\"assigned_to\")\n\n            if self.has_product_component():\n                if \"product\" not in params[\"include_fields\"]:\n                    params[\"include_fields\"].append(\"product\")\n                if \"component\" not in params[\"include_fields\"]:\n                    params[\"include_fields\"].append(\"component\")\n\n            if self.has_needinfo() and \"flags\" not in params[\"include_fields\"]:\n                params[\"include_fields\"].append(\"flags\")\n\n        if bug_ids:\n            params[\"bug_id\"] = bug_ids\n\n        if self.filter_no_nag_keyword():\n            n = utils.get_last_field_num(params)\n            params.update(\n                {\n                    \"f\" + n: \"status_whiteboard\",\n                    \"o\" + n: \"notsubstring\",\n                    \"v\" + n: \"[no-nag]\",\n                }\n            )\n\n        if self.ignore_meta():\n            n = utils.get_last_field_num(params)\n            params.update({\"f\" + n: \"keywords\", \"o\" + n: \"nowords\", \"v\" + n: \"meta\"})\n\n        # Limit the checkers to X years. Unlimited if max_years = -1\n        max_years = self.get_max_years()\n        if max_years > 0:\n            n = utils.get_last_field_num(params)\n            params.update(\n                {\n                    f\"f{n}\": \"creation_ts\",\n                    f\"o{n}\": \"greaterthan\",\n                    f\"v{n}\": f\"-{max_years}y\",\n                }\n            )\n\n        if self.has_default_products():\n            params[\"product\"] = self.get_products()\n\n        if not self.has_access_to_sec_bugs():\n            n = utils.get_last_field_num(params)\n            params.update({\"f\" + n: \"bug_group\", \"o\" + n: \"isempty\"})\n\n        self.has_flags = \"flags\" in params.get(\"include_fields\", [])\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : false, \"public_lib\" : true, \"current_class\" : true, \"current_file\" : false, \"current_project\" : true, \"external\" : false }",
            "docstring": "Amend the Bugzilla params",
            "end_lineno": "421",
            "file_content": "# This Source Code Form is subject to the terms of the Mozilla Public\n# License, v. 2.0. If a copy of the MPL was not distributed with this file,\n# You can obtain one at http://mozilla.org/MPL/2.0/.\n\nimport argparse\nimport os\nimport sys\nimport time\nfrom collections import defaultdict\nfrom datetime import datetime\n\nfrom dateutil.relativedelta import relativedelta\nfrom jinja2 import Environment, FileSystemLoader\nfrom libmozdata import utils as lmdutils\nfrom libmozdata.bugzilla import Bugzilla\n\nfrom auto_nag import db, logger, mail, utils\nfrom auto_nag.cache import Cache\nfrom auto_nag.nag_me import Nag\n\n\nclass BzCleaner(object):\n    def __init__(self):\n        super(BzCleaner, self).__init__()\n        self._set_tool_name()\n        self.has_autofix = False\n        self.autofix_changes = {}\n        self.quota_actions = defaultdict(list)\n        self.no_manager = set()\n        self.auto_needinfo = {}\n        self.has_flags = False\n        self.cache = Cache(self.name(), self.max_days_in_cache())\n        self.test_mode = utils.get_config(\"common\", \"test\", False)\n        self.versions = None\n        logger.info(\"Run tool {}\".format(self.get_tool_path()))\n\n    def _set_tool_name(self):\n        module = sys.modules[self.__class__.__module__]\n        base = os.path.dirname(__file__)\n        scripts = os.path.join(base, \"scripts\")\n        self.__tool_path__ = os.path.relpath(module.__file__, scripts)\n        name = os.path.basename(module.__file__)\n        name = os.path.splitext(name)[0]\n        self.__tool_name__ = name\n\n    def init_versions(self):\n        self.versions = utils.get_checked_versions()\n        return bool(self.versions)\n\n    def max_days_in_cache(self):\n        \"\"\"Get the max number of days the data must be kept in cache\"\"\"\n        return self.get_config(\"max_days_in_cache\", -1)\n\n    def preamble(self):\n        return None\n\n    def description(self):\n        \"\"\"Get the description for the help\"\"\"\n        return \"\"\n\n    def name(self):\n        \"\"\"Get the tool name\"\"\"\n        return self.__tool_name__\n\n    def get_tool_path(self):\n        \"\"\"Get the tool path\"\"\"\n        return self.__tool_path__\n\n    def needinfo_template(self):\n        \"\"\"Get the txt template filename\"\"\"\n        return self.name() + \"_needinfo.txt\"\n\n    def template(self):\n        \"\"\"Get the html template filename\"\"\"\n        return self.name() + \".html\"\n\n    def subject(self):\n        \"\"\"Get the partial email subject\"\"\"\n        return self.description()\n\n    def get_email_subject(self, date):\n        \"\"\"Get the email subject with a date or not\"\"\"\n        af = \"[autofix]\" if self.has_autofix else \"\"\n        if date:\n            return \"[autonag]{} {} for the {}\".format(af, self.subject(), date)\n        return \"[autonag]{} {}\".format(af, self.subject())\n\n    def ignore_date(self):\n        \"\"\"Should we ignore the date ?\"\"\"\n        return False\n\n    def must_run(self, date):\n        \"\"\"Check if the tool must run for this date\"\"\"\n        days = self.get_config(\"must_run\", None)\n        if not days:\n            return True\n        weekday = date.weekday()\n        week = utils.get_weekdays()\n        for day in days:\n            if week[day] == weekday:\n                return True\n        return False\n\n    def has_enough_data(self):\n        \"\"\"Check if the tool has enough data to run\"\"\"\n        if self.versions is None:\n            # init_versions() has never been called\n            return True\n        return bool(self.versions)\n\n    def filter_no_nag_keyword(self):\n        \"\"\"If True, then remove the bugs with [no-nag] in whiteboard from the bug list\"\"\"\n        return True\n\n    def add_no_manager(self, bugid):\n        self.no_manager.add(str(bugid))\n\n    def has_assignee(self):\n        return False\n\n    def has_needinfo(self):\n        return False\n\n    def get_mail_to_auto_ni(self, bug):\n        return None\n\n    def all_include_fields(self):\n        return False\n\n    def get_max_ni(self):\n        return -1\n\n    def get_max_actions(self):\n        return -1\n\n    def exclude_no_action_bugs(self):\n        \"\"\"\n        If `True`, then remove bugs that have no actions from the email (e.g.,\n        needinfo got ignored due to exceeding the limit). This is applied only\n        when using the `add_prioritized_action()` method.\n\n        Returning `False` could be useful if we want to list all actions the tool\n        would do if it had no limits.\n        \"\"\"\n        return True\n\n    def ignore_meta(self):\n        return False\n\n    def columns(self):\n        \"\"\"The fields to get for the columns in email report\"\"\"\n        return [\"id\", \"summary\"]\n\n    def sort_columns(self):\n        \"\"\"Returns the key to sort columns\"\"\"\n        return None\n\n    def get_dates(self, date):\n        \"\"\"Get the dates for the bugzilla query (changedafter and changedbefore fields)\"\"\"\n        date = lmdutils.get_date_ymd(date)\n        lookup = self.get_config(\"days_lookup\", 7)\n        start_date = date - relativedelta(days=lookup)\n        end_date = date + relativedelta(days=1)\n\n        return start_date, end_date\n\n    def get_extra_for_template(self):\n        \"\"\"Get extra data to put in the template\"\"\"\n        return {}\n\n    def get_extra_for_needinfo_template(self):\n        \"\"\"Get extra data to put in the needinfo template\"\"\"\n        return {}\n\n    def get_config(self, entry, default=None):\n        return utils.get_config(self.name(), entry, default=default)\n\n    def get_bz_params(self, date):\n        \"\"\"Get the Bugzilla parameters for the search query\"\"\"\n        return {}\n\n    def get_data(self):\n        \"\"\"Get the data structure to use in the bughandler\"\"\"\n        return {}\n\n    def get_summary(self, bug):\n        return \"...\" if bug[\"groups\"] else bug[\"summary\"]\n\n    def has_default_products(self):\n        return True\n\n    def has_product_component(self):\n        return False\n\n    def get_product_component(self):\n        return self.prod_comp\n\n    def get_max_years(self):\n        return self.get_config(\"max-years\", -1)\n\n    def has_access_to_sec_bugs(self):\n        return self.get_config(\"sec\", True)\n\n    def handle_bug(self, bug, data):\n        \"\"\"Implement this function to get all the bugs from the query\"\"\"\n        return bug\n\n    def get_db_extra(self):\n        \"\"\"Get extra information required for db insertion\"\"\"\n        return {\n            bugid: ni_mail\n            for ni_mail, v in self.auto_needinfo.items()\n            for bugid in v[\"bugids\"]\n        }\n\n    def get_auto_ni_skiplist(self):\n        \"\"\"Return a set of email addresses that should never be needinfoed\"\"\"\n        return set(self.get_config(\"needinfo_skiplist\", default=[]))\n\n    def add_auto_ni(self, bugid, data):\n        if not data:\n            return False\n\n        ni_mail = data[\"mail\"]\n        if ni_mail in self.get_auto_ni_skiplist() or utils.is_no_assignee(ni_mail):\n            return False\n        if ni_mail in self.auto_needinfo:\n            max_ni = self.get_max_ni()\n            info = self.auto_needinfo[ni_mail]\n            if max_ni > 0 and len(info[\"bugids\"]) >= max_ni:\n                return False\n            info[\"bugids\"].append(str(bugid))\n        else:\n            self.auto_needinfo[ni_mail] = {\n                \"nickname\": data[\"nickname\"],\n                \"bugids\": [str(bugid)],\n            }\n        return True\n\n    def add_prioritized_action(self, bug, quota_name, needinfo=None, autofix=None):\n        \"\"\"\n        - `quota_name` is the key used to apply the limits, e.g., triage owner, team, or component\n        \"\"\"\n        assert needinfo or autofix\n\n        # Avoid having more than one ni from our bot\n        if needinfo and self.has_bot_set_ni(bug):\n            needinfo = autofix = None\n\n        action = {\n            \"bug\": bug,\n            \"needinfo\": needinfo,\n            \"autofix\": autofix,\n        }\n\n        self.quota_actions[quota_name].append(action)\n\n    def get_bug_sort_key(self, bug):\n        return None\n\n    def _populate_prioritized_actions(self, bugs):\n        max_actions = self.get_max_actions()\n        max_ni = self.get_max_ni()\n        exclude_no_action_bugs = (\n            len(self.quota_actions) > 0 and self.exclude_no_action_bugs()\n        )\n        bugs_with_action = set()\n\n        for actions in self.quota_actions.values():\n            if len(actions) > max_ni or len(actions) > max_actions:\n                actions.sort(\n                    key=lambda action: (\n                        not action[\"needinfo\"],\n                        self.get_bug_sort_key(action[\"bug\"]),\n                    )\n                )\n\n            ni_count = 0\n            actions_count = 0\n            for action in actions:\n                bugid = str(action[\"bug\"][\"id\"])\n                if max_actions > 0 and actions_count >= max_actions:\n                    break\n\n                if action[\"needinfo\"]:\n                    if max_ni > 0 and ni_count >= max_ni:\n                        continue\n\n                    ok = self.add_auto_ni(bugid, action[\"needinfo\"])\n                    if not ok:\n                        # If we can't needinfo, we do not add the autofix\n                        continue\n\n                    if \"extra\" in action[\"needinfo\"]:\n                        self.extra_ni[bugid] = action[\"needinfo\"][\"extra\"]\n\n                    bugs_with_action.add(bugid)\n                    ni_count += 1\n\n                if action[\"autofix\"]:\n                    assert bugid not in self.autofix_changes\n                    self.autofix_changes[bugid] = action[\"autofix\"]\n                    bugs_with_action.add(bugid)\n\n                if action[\"autofix\"] or action[\"needinfo\"]:\n                    actions_count += 1\n\n        if exclude_no_action_bugs:\n            bugs = {id: bug for id, bug in bugs.items() if id in bugs_with_action}\n\n        return bugs\n\n    def bughandler(self, bug, data):\n        \"\"\"bug handler for the Bugzilla query\"\"\"\n        if bug[\"id\"] in self.cache:\n            return\n\n        if self.handle_bug(bug, data) is None:\n            return\n\n        bugid = str(bug[\"id\"])\n        res = {\"id\": bugid}\n\n        auto_ni = self.get_mail_to_auto_ni(bug)\n        self.add_auto_ni(bugid, auto_ni)\n\n        res[\"summary\"] = self.get_summary(bug)\n\n        if self.has_assignee():\n            res[\"assignee\"] = utils.get_name_from_user_detail(bug[\"assigned_to_detail\"])\n\n        if self.has_needinfo():\n            s = set()\n            for flag in utils.get_needinfo(bug):\n                s.add(flag[\"requestee\"])\n            res[\"needinfos\"] = sorted(s)\n\n        if self.has_product_component():\n            for k in [\"product\", \"component\"]:\n                res[k] = bug[k]\n\n        if isinstance(self, Nag):\n            bug = self.set_people_to_nag(bug, res)\n            if not bug:\n                return\n\n        if bugid in data:\n            data[bugid].update(res)\n        else:\n            data[bugid] = res\n\n    def get_products(self):\n        return self.get_config(\"products\") + self.get_config(\"additional_products\", [])\n\n    def amend_bzparams(self, params, bug_ids):\n        \"\"\"Amend the Bugzilla params\"\"\"\n        if not self.all_include_fields():\n            if \"include_fields\" in params:\n                fields = params[\"include_fields\"]\n                if isinstance(fields, list):\n                    if \"id\" not in fields:\n                        fields.append(\"id\")\n                elif isinstance(fields, str):\n                    if fields != \"id\":\n                        params[\"include_fields\"] = [fields, \"id\"]\n                else:\n                    params[\"include_fields\"] = [fields, \"id\"]\n            else:\n                params[\"include_fields\"] = [\"id\"]\n\n            params[\"include_fields\"] += [\"summary\", \"groups\"]\n\n            if self.has_assignee() and \"assigned_to\" not in params[\"include_fields\"]:\n                params[\"include_fields\"].append(\"assigned_to\")\n\n            if self.has_product_component():\n                if \"product\" not in params[\"include_fields\"]:\n                    params[\"include_fields\"].append(\"product\")\n                if \"component\" not in params[\"include_fields\"]:\n                    params[\"include_fields\"].append(\"component\")\n\n            if self.has_needinfo() and \"flags\" not in params[\"include_fields\"]:\n                params[\"include_fields\"].append(\"flags\")\n\n        if bug_ids:\n            params[\"bug_id\"] = bug_ids\n\n        if self.filter_no_nag_keyword():\n            n = utils.get_last_field_num(params)\n            params.update(\n                {\n                    \"f\" + n: \"status_whiteboard\",\n                    \"o\" + n: \"notsubstring\",\n                    \"v\" + n: \"[no-nag]\",\n                }\n            )\n\n        if self.ignore_meta():\n            n = utils.get_last_field_num(params)\n            params.update({\"f\" + n: \"keywords\", \"o\" + n: \"nowords\", \"v\" + n: \"meta\"})\n\n        # Limit the checkers to X years. Unlimited if max_years = -1\n        max_years = self.get_max_years()\n        if max_years > 0:\n            n = utils.get_last_field_num(params)\n            params.update(\n                {\n                    f\"f{n}\": \"creation_ts\",\n                    f\"o{n}\": \"greaterthan\",\n                    f\"v{n}\": f\"-{max_years}y\",\n                }\n            )\n\n        if self.has_default_products():\n            params[\"product\"] = self.get_products()\n\n        if not self.has_access_to_sec_bugs():\n            n = utils.get_last_field_num(params)\n            params.update({\"f\" + n: \"bug_group\", \"o\" + n: \"isempty\"})\n\n        self.has_flags = \"flags\" in params.get(\"include_fields\", [])\n\n    def get_bugs(self, date=\"today\", bug_ids=[], chunk_size=None):\n        \"\"\"Get the bugs\"\"\"\n        bugs = self.get_data()\n        params = self.get_bz_params(date)\n        self.amend_bzparams(params, bug_ids)\n        self.query_url = utils.get_bz_search_url(params)\n\n        if isinstance(self, Nag):\n            self.query_params: dict = params\n\n        old_CHUNK_SIZE = Bugzilla.BUGZILLA_CHUNK_SIZE\n        try:\n            if chunk_size:\n                Bugzilla.BUGZILLA_CHUNK_SIZE = chunk_size\n\n            Bugzilla(\n                params,\n                bughandler=self.bughandler,\n                bugdata=bugs,\n                timeout=self.get_config(\"bz_query_timeout\"),\n            ).get_data().wait()\n        finally:\n            Bugzilla.BUGZILLA_CHUNK_SIZE = old_CHUNK_SIZE\n\n        self.get_comments(bugs)\n\n        return bugs\n\n    def commenthandler(self, bug, bugid, data):\n        return\n\n    def _commenthandler(self, bug, bugid, data):\n        comments = bug[\"comments\"]\n        bugid = str(bugid)\n        if self.has_last_comment_time():\n            if comments:\n                data[bugid][\"last_comment\"] = utils.get_human_lag(comments[-1][\"time\"])\n            else:\n                data[bugid][\"last_comment\"] = \"\"\n\n        self.commenthandler(bug, bugid, data)\n\n    def get_comments(self, bugs):\n        \"\"\"Get the bugs comments\"\"\"\n        if self.has_last_comment_time():\n            bugids = self.get_list_bugs(bugs)\n            Bugzilla(\n                bugids=bugids, commenthandler=self._commenthandler, commentdata=bugs\n            ).get_data().wait()\n        return bugs\n\n    def has_last_comment_time(self):\n        return False\n\n    def get_list_bugs(self, bugs):\n        return [x[\"id\"] for x in bugs.values()]\n\n    def get_documentation(self):\n        return \"For more information, please visit [auto_nag documentation](https://wiki.mozilla.org/Release_Management/autonag#{}).\".format(\n            self.get_tool_path().replace(\"/\", \".2F\")\n        )\n\n    def has_bot_set_ni(self, bug):\n        if not self.has_flags:\n            raise Exception\n        return utils.has_bot_set_ni(bug)\n\n    def set_needinfo(self):\n        if not self.auto_needinfo:\n            return {}\n\n        template_name = self.needinfo_template()\n        assert bool(template_name)\n        env = Environment(loader=FileSystemLoader(\"templates\"))\n        template = env.get_template(template_name)\n        res = {}\n\n        doc = self.get_documentation()\n\n        for ni_mail, info in self.auto_needinfo.items():\n            nick = info[\"nickname\"]\n            for bugid in info[\"bugids\"]:\n                data = {\n                    \"comment\": {\"body\": \"\"},\n                    \"flags\": [\n                        {\n                            \"name\": \"needinfo\",\n                            \"requestee\": ni_mail,\n                            \"status\": \"?\",\n                            \"new\": \"true\",\n                        }\n                    ],\n                }\n\n                comment = None\n                if nick:\n                    comment = template.render(\n                        nickname=nick,\n                        extra=self.get_extra_for_needinfo_template(),\n                        plural=utils.plural,\n                        bugid=bugid,\n                        documentation=doc,\n                    )\n                    comment = comment.strip() + \"\\n\"\n                    data[\"comment\"][\"body\"] = comment\n\n                if bugid not in res:\n                    res[bugid] = data\n                else:\n                    res[bugid][\"flags\"] += data[\"flags\"]\n                    if comment:\n                        res[bugid][\"comment\"][\"body\"] = comment\n\n        return res\n\n    def has_individual_autofix(self, changes):\n        # check if we have a dictionary with bug numbers as keys\n        # return True if all the keys are bug number\n        # (which means that each bug has its own autofix)\n        return changes and all(\n            isinstance(bugid, int) or bugid.isdigit() for bugid in changes\n        )\n\n    def get_autofix_change(self):\n        \"\"\"Get the change to do to autofix the bugs\"\"\"\n        return self.autofix_changes\n\n    def autofix(self, bugs):\n        \"\"\"Autofix the bugs according to what is returned by get_autofix_change\"\"\"\n        ni_changes = self.set_needinfo()\n        change = self.get_autofix_change()\n\n        if not ni_changes and not change:\n            return bugs\n\n        self.has_autofix = True\n        new_changes = {}\n        if not self.has_individual_autofix(change):\n            bugids = self.get_list_bugs(bugs)\n            for bugid in bugids:\n                new_changes[bugid] = utils.merge_bz_changes(\n                    change, ni_changes.get(bugid, {})\n                )\n        else:\n            change = {str(k): v for k, v in change.items()}\n            bugids = set(change.keys()) | set(ni_changes.keys())\n            for bugid in bugids:\n                mrg = utils.merge_bz_changes(\n                    change.get(bugid, {}), ni_changes.get(bugid, {})\n                )\n                if mrg:\n                    new_changes[bugid] = mrg\n\n        if self.dryrun or self.test_mode:\n            for bugid, ch in new_changes.items():\n                logger.info(\n                    \"The bugs: {}\\n will be autofixed with:\\n{}\".format(bugid, ch)\n                )\n        else:\n            extra = self.get_db_extra()\n            max_retries = utils.get_config(\"common\", \"bugzilla_max_retries\", 3)\n            for bugid, ch in new_changes.items():\n                added = False\n                for _ in range(max_retries):\n                    failures = Bugzilla([str(bugid)]).put(ch)\n                    if failures:\n                        time.sleep(1)\n                    else:\n                        added = True\n                        db.BugChange.add(self.name(), bugid, extra=extra.get(bugid, \"\"))\n                        break\n                if not added:\n                    self.failure_callback(bugid)\n                    logger.error(\n                        \"{}: Cannot put data for bug {} (change => {}).\".format(\n                            self.name(), bugid, ch\n                        )\n                    )\n\n        return bugs\n\n    def failure_callback(self, bugid):\n        \"\"\"Called on Bugzilla.put failures\"\"\"\n        return\n\n    def terminate(self):\n        \"\"\"Called when everything is done\"\"\"\n        return\n\n    def organize(self, bugs):\n        return utils.organize(bugs, self.columns(), key=self.sort_columns())\n\n    def add_to_cache(self, bugs):\n        \"\"\"Add the bug keys to cache\"\"\"\n        if isinstance(bugs, dict):\n            self.cache.add(bugs.keys())\n        else:\n            self.cache.add(bugs)\n\n    def get_email_data(self, date, bug_ids):\n        bugs = self.get_bugs(date=date, bug_ids=bug_ids)\n        bugs = self._populate_prioritized_actions(bugs)\n        bugs = self.autofix(bugs)\n        self.add_to_cache(bugs)\n        if bugs:\n            return self.organize(bugs)\n\n    def get_email(self, date, bug_ids=[]):\n        \"\"\"Get title and body for the email\"\"\"\n        data = self.get_email_data(date, bug_ids)\n        if data:\n            extra = self.get_extra_for_template()\n            env = Environment(loader=FileSystemLoader(\"templates\"))\n            template = env.get_template(self.template())\n            message = template.render(\n                date=date,\n                data=data,\n                extra=extra,\n                str=str,\n                enumerate=enumerate,\n                plural=utils.plural,\n                no_manager=self.no_manager,\n                table_attrs=self.get_config(\"table_attrs\"),\n                preamble=self.preamble(),\n            )\n            common = env.get_template(\"common.html\")\n            body = common.render(\n                message=message, query_url=utils.shorten_long_bz_url(self.query_url)\n            )\n            return self.get_email_subject(date), body\n        return None, None\n\n    def send_email(self, date=\"today\"):\n        \"\"\"Send the email\"\"\"\n        if date:\n            date = lmdutils.get_date(date)\n            d = lmdutils.get_date_ymd(date)\n            if isinstance(self, Nag):\n                self.nag_date: datetime = d\n\n            if not self.must_run(d):\n                return\n\n        if not self.has_enough_data():\n            logger.info(\"The tool {} hasn't enough data to run\".format(self.name()))\n            return\n\n        login_info = utils.get_login_info()\n        title, body = self.get_email(date)\n        if title:\n            receivers = utils.get_receivers(self.name())\n            status = \"Success\"\n            try:\n                mail.send(\n                    login_info[\"ldap_username\"],\n                    receivers,\n                    title,\n                    body,\n                    html=True,\n                    login=login_info,\n                    dryrun=self.dryrun,\n                )\n            except Exception:\n                logger.exception(\"Tool {}\".format(self.name()))\n                status = \"Failure\"\n\n            db.Email.add(self.name(), receivers, \"global\", status)\n            if isinstance(self, Nag):\n                self.send_mails(title, dryrun=self.dryrun)\n        else:\n            name = self.name().upper()\n            if date:\n                logger.info(\"{}: No data for {}\".format(name, date))\n            else:\n                logger.info(\"{}: No data\".format(name))\n            logger.info(\"Query: {}\".format(self.query_url))\n\n    def add_custom_arguments(self, parser):\n        pass\n\n    def parse_custom_arguments(self, args):\n        pass\n\n    def get_args_parser(self):\n        \"\"\"Get the argumends from the command line\"\"\"\n        parser = argparse.ArgumentParser(description=self.description())\n        parser.add_argument(\n            \"--production\",\n            dest=\"dryrun\",\n            action=\"store_false\",\n            help=\"If the flag is not passed, just do the query, and print emails to console without emailing anyone\",\n        )\n\n        if not self.ignore_date():\n            parser.add_argument(\n                \"-D\",\n                \"--date\",\n                dest=\"date\",\n                action=\"store\",\n                default=\"today\",\n                help=\"Date for the query\",\n            )\n\n        self.add_custom_arguments(parser)\n\n        return parser\n\n    def run(self):\n        \"\"\"Run the tool\"\"\"\n        args = self.get_args_parser().parse_args()\n        self.parse_custom_arguments(args)\n        date = \"\" if self.ignore_date() else args.date\n        self.dryrun = args.dryrun\n        self.cache.set_dry_run(self.dryrun)\n        try:\n            self.send_email(date=date)\n            self.terminate()\n            logger.info(\"Tool {} has finished.\".format(self.get_tool_path()))\n        except Exception:\n            logger.exception(\"Tool {}\".format(self.name()))\n",
            "file_path": "auto_nag/bzcleaner.py",
            "human_label": "Amend the Bugzilla params",
            "level": "class_runnable",
            "lineno": "355",
            "name": "amend_bzparams",
            "oracle_context": "{ \"apis\" : \"['isinstance', 'append', 'get', 'has_needinfo', 'get_last_field_num', 'update', 'has_assignee', 'ignore_meta', 'has_access_to_sec_bugs', 'all_include_fields', 'get_products', 'get_max_years', 'has_product_component', 'filter_no_nag_keyword', 'has_default_products']\", \"classes\" : \"['utils']\", \"vars\" : \"['has_flags']\" }",
            "package": "bzcleaner",
            "project": "mozilla/relman-auto-nag",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b45665d7d32e5b55cc8365",
            "all_context": "{ \"import\" : \"collections argparse borgmatic argparse \", \"file\" : \"SUBPARSER_ALIASES ; parse_subparser_arguments(unparsed_arguments,subparsers) ; make_parsers() ; parse_arguments() ; \", \"class\" : \"\" }",
            "code": "def parse_arguments(*unparsed_arguments):\n    '''\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    '''\n    top_level_parser, subparsers = make_parsers()\n\n    arguments, remaining_arguments = parse_subparser_arguments(\n        unparsed_arguments, subparsers.choices\n    )\n    arguments['global'] = top_level_parser.parse_args(remaining_arguments)\n\n    if arguments['global'].excludes_filename:\n        raise ValueError(\n            'The --excludes option has been replaced with exclude_patterns in configuration'\n        )\n\n    if 'init' in arguments and arguments['global'].dry_run:\n        raise ValueError('The init action cannot be used with the --dry-run option')\n\n    if (\n        'list' in arguments\n        and 'info' in arguments\n        and arguments['list'].json\n        and arguments['info'].json\n    ):\n        raise ValueError('With the --json option, list and info actions cannot be used together')\n\n    return arguments\n",
            "dependency": "{ \"builtin\" : false, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Given command-line arguments with which this script was invoked, parse the arguments and return\nthem as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.",
            "end_lineno": "705",
            "file_content": "import collections\nfrom argparse import Action, ArgumentParser\n\nfrom borgmatic.config import collect\n\nSUBPARSER_ALIASES = {\n    'init': ['--init', '-I'],\n    'prune': ['--prune', '-p'],\n    'compact': [],\n    'create': ['--create', '-C'],\n    'check': ['--check', '-k'],\n    'extract': ['--extract', '-x'],\n    'export-tar': ['--export-tar'],\n    'mount': ['--mount', '-m'],\n    'umount': ['--umount', '-u'],\n    'restore': ['--restore', '-r'],\n    'list': ['--list', '-l'],\n    'info': ['--info', '-i'],\n    'borg': [],\n}\n\n\ndef parse_subparser_arguments(unparsed_arguments, subparsers):\n    '''\n    Given a sequence of arguments and a dict from subparser name to argparse.ArgumentParser\n    instance, give each requested action's subparser a shot at parsing all arguments. This allows\n    common arguments like \"--repository\" to be shared across multiple subparsers.\n\n    Return the result as a tuple of (a dict mapping from subparser name to a parsed namespace of\n    arguments, a list of remaining arguments not claimed by any subparser).\n    '''\n    arguments = collections.OrderedDict()\n    remaining_arguments = list(unparsed_arguments)\n    alias_to_subparser_name = {\n        alias: subparser_name\n        for subparser_name, aliases in SUBPARSER_ALIASES.items()\n        for alias in aliases\n    }\n\n    # If the \"borg\" action is used, skip all other subparsers. This avoids confusion like\n    # \"borg list\" triggering borgmatic's own list action.\n    if 'borg' in unparsed_arguments:\n        subparsers = {'borg': subparsers['borg']}\n\n    for subparser_name, subparser in subparsers.items():\n        if subparser_name not in remaining_arguments:\n            continue\n\n        canonical_name = alias_to_subparser_name.get(subparser_name, subparser_name)\n\n        # If a parsed value happens to be the same as the name of a subparser, remove it from the\n        # remaining arguments. This prevents, for instance, \"check --only extract\" from triggering\n        # the \"extract\" subparser.\n        parsed, unused_remaining = subparser.parse_known_args(unparsed_arguments)\n        for value in vars(parsed).values():\n            if isinstance(value, str):\n                if value in subparsers:\n                    remaining_arguments.remove(value)\n            elif isinstance(value, list):\n                for item in value:\n                    if item in subparsers:\n                        remaining_arguments.remove(item)\n\n        arguments[canonical_name] = parsed\n\n    # If no actions are explicitly requested, assume defaults: prune, compact, create, and check.\n    if not arguments and '--help' not in unparsed_arguments and '-h' not in unparsed_arguments:\n        for subparser_name in ('prune', 'compact', 'create', 'check'):\n            subparser = subparsers[subparser_name]\n            parsed, unused_remaining = subparser.parse_known_args(unparsed_arguments)\n            arguments[subparser_name] = parsed\n\n    remaining_arguments = list(unparsed_arguments)\n\n    # Now ask each subparser, one by one, to greedily consume arguments.\n    for subparser_name, subparser in subparsers.items():\n        if subparser_name not in arguments.keys():\n            continue\n\n        subparser = subparsers[subparser_name]\n        unused_parsed, remaining_arguments = subparser.parse_known_args(remaining_arguments)\n\n    # Special case: If \"borg\" is present in the arguments, consume all arguments after (+1) the\n    # \"borg\" action.\n    if 'borg' in arguments:\n        borg_options_index = remaining_arguments.index('borg') + 1\n        arguments['borg'].options = remaining_arguments[borg_options_index:]\n        remaining_arguments = remaining_arguments[:borg_options_index]\n\n    # Remove the subparser names themselves.\n    for subparser_name, subparser in subparsers.items():\n        if subparser_name in remaining_arguments:\n            remaining_arguments.remove(subparser_name)\n\n    return (arguments, remaining_arguments)\n\n\nclass Extend_action(Action):\n    '''\n    An argparse action to support Python 3.8's \"extend\" action in older versions of Python.\n    '''\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        items = getattr(namespace, self.dest, None)\n\n        if items:\n            items.extend(values)\n        else:\n            setattr(namespace, self.dest, list(values))\n\n\ndef make_parsers():\n    '''\n    Build a top-level parser and its subparsers and return them as a tuple.\n    '''\n    config_paths = collect.get_default_config_paths(expand_home=True)\n    unexpanded_config_paths = collect.get_default_config_paths(expand_home=False)\n\n    global_parser = ArgumentParser(add_help=False)\n    global_parser.register('action', 'extend', Extend_action)\n    global_group = global_parser.add_argument_group('global arguments')\n\n    global_group.add_argument(\n        '-c',\n        '--config',\n        nargs='*',\n        dest='config_paths',\n        default=config_paths,\n        help='Configuration filenames or directories, defaults to: {}'.format(\n            ' '.join(unexpanded_config_paths)\n        ),\n    )\n    global_group.add_argument(\n        '--excludes',\n        dest='excludes_filename',\n        help='Deprecated in favor of exclude_patterns within configuration',\n    )\n    global_group.add_argument(\n        '-n',\n        '--dry-run',\n        dest='dry_run',\n        action='store_true',\n        help='Go through the motions, but do not actually write to any repositories',\n    )\n    global_group.add_argument(\n        '-nc', '--no-color', dest='no_color', action='store_true', help='Disable colored output'\n    )\n    global_group.add_argument(\n        '-v',\n        '--verbosity',\n        type=int,\n        choices=range(-1, 3),\n        default=0,\n        help='Display verbose progress to the console (from only errors to very verbose: -1, 0, 1, or 2)',\n    )\n    global_group.add_argument(\n        '--syslog-verbosity',\n        type=int,\n        choices=range(-1, 3),\n        default=0,\n        help='Log verbose progress to syslog (from only errors to very verbose: -1, 0, 1, or 2). Ignored when console is interactive or --log-file is given',\n    )\n    global_group.add_argument(\n        '--log-file-verbosity',\n        type=int,\n        choices=range(-1, 3),\n        default=0,\n        help='Log verbose progress to log file (from only errors to very verbose: -1, 0, 1, or 2). Only used when --log-file is given',\n    )\n    global_group.add_argument(\n        '--monitoring-verbosity',\n        type=int,\n        choices=range(-1, 3),\n        default=0,\n        help='Log verbose progress to monitoring integrations that support logging (from only errors to very verbose: -1, 0, 1, or 2)',\n    )\n    global_group.add_argument(\n        '--log-file',\n        type=str,\n        default=None,\n        help='Write log messages to this file instead of syslog',\n    )\n    global_group.add_argument(\n        '--override',\n        metavar='SECTION.OPTION=VALUE',\n        nargs='+',\n        dest='overrides',\n        action='extend',\n        help='One or more configuration file options to override with specified values',\n    )\n    global_group.add_argument(\n        '--no-environment-interpolation',\n        dest='resolve_env',\n        action='store_false',\n        help='Do not resolve environment variables in configuration file',\n    )\n    global_group.add_argument(\n        '--bash-completion',\n        default=False,\n        action='store_true',\n        help='Show bash completion script and exit',\n    )\n    global_group.add_argument(\n        '--version',\n        dest='version',\n        default=False,\n        action='store_true',\n        help='Display installed version number of borgmatic and exit',\n    )\n\n    top_level_parser = ArgumentParser(\n        description='''\n            Simple, configuration-driven backup software for servers and workstations. If none of\n            the action options are given, then borgmatic defaults to: prune, compact, create, and\n            check.\n            ''',\n        parents=[global_parser],\n    )\n\n    subparsers = top_level_parser.add_subparsers(\n        title='actions',\n        metavar='',\n        help='Specify zero or more actions. Defaults to prune, compact, create, and check. Use --help with action for details:',\n    )\n    init_parser = subparsers.add_parser(\n        'init',\n        aliases=SUBPARSER_ALIASES['init'],\n        help='Initialize an empty Borg repository',\n        description='Initialize an empty Borg repository',\n        add_help=False,\n    )\n    init_group = init_parser.add_argument_group('init arguments')\n    init_group.add_argument(\n        '-e',\n        '--encryption',\n        dest='encryption_mode',\n        help='Borg repository encryption mode',\n        required=True,\n    )\n    init_group.add_argument(\n        '--append-only',\n        dest='append_only',\n        action='store_true',\n        help='Create an append-only repository',\n    )\n    init_group.add_argument(\n        '--storage-quota',\n        dest='storage_quota',\n        help='Create a repository with a fixed storage quota',\n    )\n    init_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')\n\n    prune_parser = subparsers.add_parser(\n        'prune',\n        aliases=SUBPARSER_ALIASES['prune'],\n        help='Prune archives according to the retention policy (with Borg 1.2+, run compact afterwards to actually free space)',\n        description='Prune archives according to the retention policy (with Borg 1.2+, run compact afterwards to actually free space)',\n        add_help=False,\n    )\n    prune_group = prune_parser.add_argument_group('prune arguments')\n    prune_group.add_argument(\n        '--stats',\n        dest='stats',\n        default=False,\n        action='store_true',\n        help='Display statistics of archive',\n    )\n    prune_group.add_argument(\n        '--files', dest='files', default=False, action='store_true', help='Show per-file details'\n    )\n    prune_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')\n\n    compact_parser = subparsers.add_parser(\n        'compact',\n        aliases=SUBPARSER_ALIASES['compact'],\n        help='Compact segments to free space (Borg 1.2+ only)',\n        description='Compact segments to free space (Borg 1.2+ only)',\n        add_help=False,\n    )\n    compact_group = compact_parser.add_argument_group('compact arguments')\n    compact_group.add_argument(\n        '--progress',\n        dest='progress',\n        default=False,\n        action='store_true',\n        help='Display progress as each segment is compacted',\n    )\n    compact_group.add_argument(\n        '--cleanup-commits',\n        dest='cleanup_commits',\n        default=False,\n        action='store_true',\n        help='Cleanup commit-only 17-byte segment files left behind by Borg 1.1',\n    )\n    compact_group.add_argument(\n        '--threshold',\n        type=int,\n        dest='threshold',\n        help='Minimum saved space percentage threshold for compacting a segment, defaults to 10',\n    )\n    compact_group.add_argument(\n        '-h', '--help', action='help', help='Show this help message and exit'\n    )\n\n    create_parser = subparsers.add_parser(\n        'create',\n        aliases=SUBPARSER_ALIASES['create'],\n        help='Create archives (actually perform backups)',\n        description='Create archives (actually perform backups)',\n        add_help=False,\n    )\n    create_group = create_parser.add_argument_group('create arguments')\n    create_group.add_argument(\n        '--progress',\n        dest='progress',\n        default=False,\n        action='store_true',\n        help='Display progress for each file as it is backed up',\n    )\n    create_group.add_argument(\n        '--stats',\n        dest='stats',\n        default=False,\n        action='store_true',\n        help='Display statistics of archive',\n    )\n    create_group.add_argument(\n        '--files', dest='files', default=False, action='store_true', help='Show per-file details'\n    )\n    create_group.add_argument(\n        '--json', dest='json', default=False, action='store_true', help='Output results as JSON'\n    )\n    create_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')\n\n    check_parser = subparsers.add_parser(\n        'check',\n        aliases=SUBPARSER_ALIASES['check'],\n        help='Check archives for consistency',\n        description='Check archives for consistency',\n        add_help=False,\n    )\n    check_group = check_parser.add_argument_group('check arguments')\n    check_group.add_argument(\n        '--progress',\n        dest='progress',\n        default=False,\n        action='store_true',\n        help='Display progress for each file as it is checked',\n    )\n    check_group.add_argument(\n        '--repair',\n        dest='repair',\n        default=False,\n        action='store_true',\n        help='Attempt to repair any inconsistencies found (for interactive use)',\n    )\n    check_group.add_argument(\n        '--only',\n        metavar='CHECK',\n        choices=('repository', 'archives', 'data', 'extract'),\n        dest='only',\n        action='append',\n        help='Run a particular consistency check (repository, archives, data, or extract) instead of configured checks (subject to configured frequency, can specify flag multiple times)',\n    )\n    check_group.add_argument(\n        '--force',\n        default=False,\n        action='store_true',\n        help='Ignore configured check frequencies and run checks unconditionally',\n    )\n    check_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')\n\n    extract_parser = subparsers.add_parser(\n        'extract',\n        aliases=SUBPARSER_ALIASES['extract'],\n        help='Extract files from a named archive to the current directory',\n        description='Extract a named archive to the current directory',\n        add_help=False,\n    )\n    extract_group = extract_parser.add_argument_group('extract arguments')\n    extract_group.add_argument(\n        '--repository',\n        help='Path of repository to extract, defaults to the configured repository if there is only one',\n    )\n    extract_group.add_argument(\n        '--archive', help='Name of archive to extract (or \"latest\")', required=True\n    )\n    extract_group.add_argument(\n        '--path',\n        '--restore-path',\n        metavar='PATH',\n        nargs='+',\n        dest='paths',\n        help='Paths to extract from archive, defaults to the entire archive',\n    )\n    extract_group.add_argument(\n        '--destination',\n        metavar='PATH',\n        dest='destination',\n        help='Directory to extract files into, defaults to the current directory',\n    )\n    extract_group.add_argument(\n        '--strip-components',\n        type=int,\n        metavar='NUMBER',\n        dest='strip_components',\n        help='Number of leading path components to remove from each extracted path. Skip paths with fewer elements',\n    )\n    extract_group.add_argument(\n        '--progress',\n        dest='progress',\n        default=False,\n        action='store_true',\n        help='Display progress for each file as it is extracted',\n    )\n    extract_group.add_argument(\n        '-h', '--help', action='help', help='Show this help message and exit'\n    )\n\n    export_tar_parser = subparsers.add_parser(\n        'export-tar',\n        aliases=SUBPARSER_ALIASES['export-tar'],\n        help='Export an archive to a tar-formatted file or stream',\n        description='Export an archive to a tar-formatted file or stream',\n        add_help=False,\n    )\n    export_tar_group = export_tar_parser.add_argument_group('export-tar arguments')\n    export_tar_group.add_argument(\n        '--repository',\n        help='Path of repository to export from, defaults to the configured repository if there is only one',\n    )\n    export_tar_group.add_argument(\n        '--archive', help='Name of archive to export (or \"latest\")', required=True\n    )\n    export_tar_group.add_argument(\n        '--path',\n        metavar='PATH',\n        nargs='+',\n        dest='paths',\n        help='Paths to export from archive, defaults to the entire archive',\n    )\n    export_tar_group.add_argument(\n        '--destination',\n        metavar='PATH',\n        dest='destination',\n        help='Path to destination export tar file, or \"-\" for stdout (but be careful about dirtying output with --verbosity or --files)',\n        required=True,\n    )\n    export_tar_group.add_argument(\n        '--tar-filter', help='Name of filter program to pipe data through'\n    )\n    export_tar_group.add_argument(\n        '--files', default=False, action='store_true', help='Show per-file details'\n    )\n    export_tar_group.add_argument(\n        '--strip-components',\n        type=int,\n        metavar='NUMBER',\n        dest='strip_components',\n        help='Number of leading path components to remove from each exported path. Skip paths with fewer elements',\n    )\n    export_tar_group.add_argument(\n        '-h', '--help', action='help', help='Show this help message and exit'\n    )\n\n    mount_parser = subparsers.add_parser(\n        'mount',\n        aliases=SUBPARSER_ALIASES['mount'],\n        help='Mount files from a named archive as a FUSE filesystem',\n        description='Mount a named archive as a FUSE filesystem',\n        add_help=False,\n    )\n    mount_group = mount_parser.add_argument_group('mount arguments')\n    mount_group.add_argument(\n        '--repository',\n        help='Path of repository to use, defaults to the configured repository if there is only one',\n    )\n    mount_group.add_argument('--archive', help='Name of archive to mount (or \"latest\")')\n    mount_group.add_argument(\n        '--mount-point',\n        metavar='PATH',\n        dest='mount_point',\n        help='Path where filesystem is to be mounted',\n        required=True,\n    )\n    mount_group.add_argument(\n        '--path',\n        metavar='PATH',\n        nargs='+',\n        dest='paths',\n        help='Paths to mount from archive, defaults to the entire archive',\n    )\n    mount_group.add_argument(\n        '--foreground',\n        dest='foreground',\n        default=False,\n        action='store_true',\n        help='Stay in foreground until ctrl-C is pressed',\n    )\n    mount_group.add_argument('--options', dest='options', help='Extra Borg mount options')\n    mount_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')\n\n    umount_parser = subparsers.add_parser(\n        'umount',\n        aliases=SUBPARSER_ALIASES['umount'],\n        help='Unmount a FUSE filesystem that was mounted with \"borgmatic mount\"',\n        description='Unmount a mounted FUSE filesystem',\n        add_help=False,\n    )\n    umount_group = umount_parser.add_argument_group('umount arguments')\n    umount_group.add_argument(\n        '--mount-point',\n        metavar='PATH',\n        dest='mount_point',\n        help='Path of filesystem to unmount',\n        required=True,\n    )\n    umount_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')\n\n    restore_parser = subparsers.add_parser(\n        'restore',\n        aliases=SUBPARSER_ALIASES['restore'],\n        help='Restore database dumps from a named archive',\n        description='Restore database dumps from a named archive. (To extract files instead, use \"borgmatic extract\".)',\n        add_help=False,\n    )\n    restore_group = restore_parser.add_argument_group('restore arguments')\n    restore_group.add_argument(\n        '--repository',\n        help='Path of repository to restore from, defaults to the configured repository if there is only one',\n    )\n    restore_group.add_argument(\n        '--archive', help='Name of archive to restore from (or \"latest\")', required=True\n    )\n    restore_group.add_argument(\n        '--database',\n        metavar='NAME',\n        nargs='+',\n        dest='databases',\n        help='Names of databases to restore from archive, defaults to all databases. Note that any databases to restore must be defined in borgmatic\\'s configuration',\n    )\n    restore_group.add_argument(\n        '-h', '--help', action='help', help='Show this help message and exit'\n    )\n\n    list_parser = subparsers.add_parser(\n        'list',\n        aliases=SUBPARSER_ALIASES['list'],\n        help='List archives',\n        description='List archives or the contents of an archive',\n        add_help=False,\n    )\n    list_group = list_parser.add_argument_group('list arguments')\n    list_group.add_argument(\n        '--repository', help='Path of repository to list, defaults to the configured repositories',\n    )\n    list_group.add_argument('--archive', help='Name of archive to list (or \"latest\")')\n    list_group.add_argument(\n        '--path',\n        metavar='PATH',\n        nargs='+',\n        dest='paths',\n        help='Paths or patterns to list from a single selected archive (via \"--archive\"), defaults to listing the entire archive',\n    )\n    list_group.add_argument(\n        '--find',\n        metavar='PATH',\n        nargs='+',\n        dest='find_paths',\n        help='Partial paths or patterns to search for and list across multiple archives',\n    )\n    list_group.add_argument(\n        '--short', default=False, action='store_true', help='Output only archive or path names'\n    )\n    list_group.add_argument('--format', help='Format for file listing')\n    list_group.add_argument(\n        '--json', default=False, action='store_true', help='Output results as JSON'\n    )\n    list_group.add_argument(\n        '-P', '--prefix', help='Only list archive names starting with this prefix'\n    )\n    list_group.add_argument(\n        '-a', '--glob-archives', metavar='GLOB', help='Only list archive names matching this glob'\n    )\n    list_group.add_argument(\n        '--successful',\n        default=True,\n        action='store_true',\n        help='Deprecated in favor of listing successful (non-checkpoint) backups by default in newer versions of Borg',\n    )\n    list_group.add_argument(\n        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'\n    )\n    list_group.add_argument(\n        '--first', metavar='N', help='List first N archives after other filters are applied'\n    )\n    list_group.add_argument(\n        '--last', metavar='N', help='List last N archives after other filters are applied'\n    )\n    list_group.add_argument(\n        '-e', '--exclude', metavar='PATTERN', help='Exclude paths matching the pattern'\n    )\n    list_group.add_argument(\n        '--exclude-from', metavar='FILENAME', help='Exclude paths from exclude file, one per line'\n    )\n    list_group.add_argument('--pattern', help='Include or exclude paths matching a pattern')\n    list_group.add_argument(\n        '--patterns-from',\n        metavar='FILENAME',\n        help='Include or exclude paths matching patterns from pattern file, one per line',\n    )\n    list_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')\n\n    info_parser = subparsers.add_parser(\n        'info',\n        aliases=SUBPARSER_ALIASES['info'],\n        help='Display summary information on archives',\n        description='Display summary information on archives',\n        add_help=False,\n    )\n    info_group = info_parser.add_argument_group('info arguments')\n    info_group.add_argument(\n        '--repository',\n        help='Path of repository to show info for, defaults to the configured repository if there is only one',\n    )\n    info_group.add_argument('--archive', help='Name of archive to show info for (or \"latest\")')\n    info_group.add_argument(\n        '--json', dest='json', default=False, action='store_true', help='Output results as JSON'\n    )\n    info_group.add_argument(\n        '-P', '--prefix', help='Only show info for archive names starting with this prefix'\n    )\n    info_group.add_argument(\n        '-a',\n        '--glob-archives',\n        metavar='GLOB',\n        help='Only show info for archive names matching this glob',\n    )\n    info_group.add_argument(\n        '--sort-by', metavar='KEYS', help='Comma-separated list of sorting keys'\n    )\n    info_group.add_argument(\n        '--first',\n        metavar='N',\n        help='Show info for first N archives after other filters are applied',\n    )\n    info_group.add_argument(\n        '--last', metavar='N', help='Show info for last N archives after other filters are applied'\n    )\n    info_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')\n\n    borg_parser = subparsers.add_parser(\n        'borg',\n        aliases=SUBPARSER_ALIASES['borg'],\n        help='Run an arbitrary Borg command',\n        description='Run an arbitrary Borg command based on borgmatic\\'s configuration',\n        add_help=False,\n    )\n    borg_group = borg_parser.add_argument_group('borg arguments')\n    borg_group.add_argument(\n        '--repository',\n        help='Path of repository to pass to Borg, defaults to the configured repositories',\n    )\n    borg_group.add_argument('--archive', help='Name of archive to pass to Borg (or \"latest\")')\n    borg_group.add_argument(\n        '--',\n        metavar='OPTION',\n        dest='options',\n        nargs='+',\n        help='Options to pass to Borg, command first (\"create\", \"list\", etc). \"--\" is optional. To specify the repository or the archive, you must use --repository or --archive instead of providing them here.',\n    )\n    borg_group.add_argument('-h', '--help', action='help', help='Show this help message and exit')\n\n    return top_level_parser, subparsers\n\n\ndef parse_arguments(*unparsed_arguments):\n    '''\n    Given command-line arguments with which this script was invoked, parse the arguments and return\n    them as a dict mapping from subparser name (or \"global\") to an argparse.Namespace instance.\n    '''\n    top_level_parser, subparsers = make_parsers()\n\n    arguments, remaining_arguments = parse_subparser_arguments(\n        unparsed_arguments, subparsers.choices\n    )\n    arguments['global'] = top_level_parser.parse_args(remaining_arguments)\n\n    if arguments['global'].excludes_filename:\n        raise ValueError(\n            'The --excludes option has been replaced with exclude_patterns in configuration'\n        )\n\n    if 'init' in arguments and arguments['global'].dry_run:\n        raise ValueError('The init action cannot be used with the --dry-run option')\n\n    if (\n        'list' in arguments\n        and 'info' in arguments\n        and arguments['list'].json\n        and arguments['info'].json\n    ):\n        raise ValueError('With the --json option, list and info actions cannot be used together')\n\n    return arguments\n",
            "file_path": "borgmatic/commands/arguments.py",
            "human_label": "Parses parameters and returns them as dict maps",
            "level": "file_runnable",
            "lineno": "677",
            "name": "parse_arguments",
            "oracle_context": "{ \"apis\" : \"['parse_args', 'parse_subparser_arguments', 'make_parsers']\", \"classes\" : \"['ValueError']\", \"vars\" : \"['json', 'excludes_filename', 'dry_run', 'choices']\" }",
            "package": "arguments",
            "project": "witten/atticmatic",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b438ba66fea644fe22cca2",
            "all_context": "{ \"import\" : \"os logging logging ruamel \", \"file\" : \"logger ; load_configuration(filename) ; include_configuration(loader,filename_node) ; DELETED_NODE ; deep_merge_nodes(nodes) ; \", \"class\" : \"\" }",
            "code": "def deep_merge_nodes(nodes):\n    '''\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n        (\n            ruamel.yaml.nodes.ScalarNode as a key,\n            ruamel.yaml.nodes.MappingNode or other Node as a value,\n        ),\n\n    ... deep merge any node values corresponding to duplicate keys and return the result. If\n    there are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n    of the values wins.\n\n    For instance, given node values of:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                    ),\n                ]),\n            ),\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    ... the returned result would be:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    The purpose of deep merging like this is to support, for instance, merging one borgmatic\n    configuration file into another for reuse, such that a configuration section (\"retention\",\n    etc.) does not completely replace the corresponding section in a merged file.\n    '''\n    # Map from original node key/value to the replacement merged node. DELETED_NODE as a replacement\n    # node indications deletion.\n    replaced_nodes = {}\n\n    # To find nodes that require merging, compare each node with each other node.\n    for a_key, a_value in nodes:\n        for b_key, b_value in nodes:\n            # If we've already considered one of the nodes for merging, skip it.\n            if (a_key, a_value) in replaced_nodes or (b_key, b_value) in replaced_nodes:\n                continue\n\n            # If the keys match and the values are different, we need to merge these two A and B nodes.\n            if a_key.tag == b_key.tag and a_key.value == b_key.value and a_value != b_value:\n                # Since we're merging into the B node, consider the A node a duplicate and remove it.\n                replaced_nodes[(a_key, a_value)] = DELETED_NODE\n\n                # If we're dealing with MappingNodes, recurse and merge its values as well.\n                if isinstance(b_value, ruamel.yaml.nodes.MappingNode):\n                    replaced_nodes[(b_key, b_value)] = (\n                        b_key,\n                        ruamel.yaml.nodes.MappingNode(\n                            tag=b_value.tag,\n                            value=deep_merge_nodes(a_value.value + b_value.value),\n                            start_mark=b_value.start_mark,\n                            end_mark=b_value.end_mark,\n                            flow_style=b_value.flow_style,\n                            comment=b_value.comment,\n                            anchor=b_value.anchor,\n                        ),\n                    )\n                # If we're dealing with SequenceNodes, merge by appending one sequence to the other.\n                elif isinstance(b_value, ruamel.yaml.nodes.SequenceNode):\n                    replaced_nodes[(b_key, b_value)] = (\n                        b_key,\n                        ruamel.yaml.nodes.SequenceNode(\n                            tag=b_value.tag,\n                            value=a_value.value + b_value.value,\n                            start_mark=b_value.start_mark,\n                            end_mark=b_value.end_mark,\n                            flow_style=b_value.flow_style,\n                            comment=b_value.comment,\n                            anchor=b_value.anchor,\n                        ),\n                    )\n\n    return [\n        replaced_nodes.get(node, node) for node in nodes if replaced_nodes.get(node) != DELETED_NODE\n    ]\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n    (\n        ruamel.yaml.nodes.ScalarNode as a key,\n        ruamel.yaml.nodes.MappingNode or other Node as a value,\n    ),\n\n... deep merge any node values corresponding to duplicate keys and return the result. If\nthere are colliding keys with non-MappingNode values (e.g., integers or strings), the last\nof the values wins.\n\nFor instance, given node values of:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                ),\n            ]),\n        ),\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\n... the returned result would be:\n\n    [\n        (\n            ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n            MappingNode(tag='tag:yaml.org,2002:map', value=[\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                ),\n                (\n                    ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                    ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                ),\n            ]),\n        ),\n    ]\n\nThe purpose of deep merging like this is to support, for instance, merging one borgmatic\nconfiguration file into another for reuse, such that a configuration section (\"retention\",\netc.) does not completely replace the corresponding section in a merged file.",
            "end_lineno": "177",
            "file_content": "import logging\nimport os\n\nimport ruamel.yaml\n\nlogger = logging.getLogger(__name__)\n\n\nclass Yaml_with_loader_stream(ruamel.yaml.YAML):\n    '''\n    A derived class of ruamel.yaml.YAML that simply tacks the loaded stream (file object) onto the\n    loader class so that it's available anywhere that's passed a loader (in this case,\n    include_configuration() below).\n    '''\n\n    def get_constructor_parser(self, stream):\n        constructor, parser = super(Yaml_with_loader_stream, self).get_constructor_parser(stream)\n        constructor.loader.stream = stream\n        return constructor, parser\n\n\ndef load_configuration(filename):\n    '''\n    Load the given configuration file and return its contents as a data structure of nested dicts\n    and lists.\n\n    Raise ruamel.yaml.error.YAMLError if something goes wrong parsing the YAML, or RecursionError\n    if there are too many recursive includes.\n    '''\n    yaml = Yaml_with_loader_stream(typ='safe')\n    yaml.Constructor = Include_constructor\n\n    return yaml.load(open(filename))\n\n\ndef include_configuration(loader, filename_node):\n    '''\n    Load the given YAML filename (ignoring the given loader so we can use our own) and return its\n    contents as a data structure of nested dicts and lists. If the filename is relative, probe for\n    it within 1. the current working directory and 2. the directory containing the YAML file doing\n    the including.\n\n    Raise FileNotFoundError if an included file was not found.\n    '''\n    include_directories = [os.getcwd(), os.path.abspath(os.path.dirname(loader.stream.name))]\n    include_filename = os.path.expanduser(filename_node.value)\n\n    if not os.path.isabs(include_filename):\n        candidate_filenames = [\n            os.path.join(directory, include_filename) for directory in include_directories\n        ]\n\n        for candidate_filename in candidate_filenames:\n            if os.path.exists(candidate_filename):\n                include_filename = candidate_filename\n                break\n        else:\n            raise FileNotFoundError(\n                f'Could not find include {filename_node.value} at {\" or \".join(candidate_filenames)}'\n            )\n\n    return load_configuration(include_filename)\n\n\nDELETED_NODE = object()\n\n\ndef deep_merge_nodes(nodes):\n    '''\n    Given a nested borgmatic configuration data structure as a list of tuples in the form of:\n\n        (\n            ruamel.yaml.nodes.ScalarNode as a key,\n            ruamel.yaml.nodes.MappingNode or other Node as a value,\n        ),\n\n    ... deep merge any node values corresponding to duplicate keys and return the result. If\n    there are colliding keys with non-MappingNode values (e.g., integers or strings), the last\n    of the values wins.\n\n    For instance, given node values of:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='7')\n                    ),\n                ]),\n            ),\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    ... the returned result would be:\n\n        [\n            (\n                ScalarNode(tag='tag:yaml.org,2002:str', value='retention'),\n                MappingNode(tag='tag:yaml.org,2002:map', value=[\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_hourly'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='24')\n                    ),\n                    (\n                        ScalarNode(tag='tag:yaml.org,2002:str', value='keep_daily'),\n                        ScalarNode(tag='tag:yaml.org,2002:int', value='5')\n                    ),\n                ]),\n            ),\n        ]\n\n    The purpose of deep merging like this is to support, for instance, merging one borgmatic\n    configuration file into another for reuse, such that a configuration section (\"retention\",\n    etc.) does not completely replace the corresponding section in a merged file.\n    '''\n    # Map from original node key/value to the replacement merged node. DELETED_NODE as a replacement\n    # node indications deletion.\n    replaced_nodes = {}\n\n    # To find nodes that require merging, compare each node with each other node.\n    for a_key, a_value in nodes:\n        for b_key, b_value in nodes:\n            # If we've already considered one of the nodes for merging, skip it.\n            if (a_key, a_value) in replaced_nodes or (b_key, b_value) in replaced_nodes:\n                continue\n\n            # If the keys match and the values are different, we need to merge these two A and B nodes.\n            if a_key.tag == b_key.tag and a_key.value == b_key.value and a_value != b_value:\n                # Since we're merging into the B node, consider the A node a duplicate and remove it.\n                replaced_nodes[(a_key, a_value)] = DELETED_NODE\n\n                # If we're dealing with MappingNodes, recurse and merge its values as well.\n                if isinstance(b_value, ruamel.yaml.nodes.MappingNode):\n                    replaced_nodes[(b_key, b_value)] = (\n                        b_key,\n                        ruamel.yaml.nodes.MappingNode(\n                            tag=b_value.tag,\n                            value=deep_merge_nodes(a_value.value + b_value.value),\n                            start_mark=b_value.start_mark,\n                            end_mark=b_value.end_mark,\n                            flow_style=b_value.flow_style,\n                            comment=b_value.comment,\n                            anchor=b_value.anchor,\n                        ),\n                    )\n                # If we're dealing with SequenceNodes, merge by appending one sequence to the other.\n                elif isinstance(b_value, ruamel.yaml.nodes.SequenceNode):\n                    replaced_nodes[(b_key, b_value)] = (\n                        b_key,\n                        ruamel.yaml.nodes.SequenceNode(\n                            tag=b_value.tag,\n                            value=a_value.value + b_value.value,\n                            start_mark=b_value.start_mark,\n                            end_mark=b_value.end_mark,\n                            flow_style=b_value.flow_style,\n                            comment=b_value.comment,\n                            anchor=b_value.anchor,\n                        ),\n                    )\n\n    return [\n        replaced_nodes.get(node, node) for node in nodes if replaced_nodes.get(node) != DELETED_NODE\n    ]\n\n\nclass Include_constructor(ruamel.yaml.SafeConstructor):\n    '''\n    A YAML \"constructor\" (a ruamel.yaml concept) that supports a custom \"!include\" tag for including\n    separate YAML configuration files. Example syntax: `retention: !include common.yaml`\n    '''\n\n    def __init__(self, preserve_quotes=None, loader=None):\n        super(Include_constructor, self).__init__(preserve_quotes, loader)\n        self.add_constructor('!include', include_configuration)\n\n    def flatten_mapping(self, node):\n        '''\n        Support the special case of deep merging included configuration into an existing mapping\n        using the YAML '<<' merge key. Example syntax:\n\n        ```\n        retention:\n            keep_daily: 1\n\n        <<: !include common.yaml\n        ```\n\n        These includes are deep merged into the current configuration file. For instance, in this\n        example, any \"retention\" options in common.yaml will get merged into the \"retention\" section\n        in the example configuration file.\n        '''\n        representer = ruamel.yaml.representer.SafeRepresenter()\n\n        for index, (key_node, value_node) in enumerate(node.value):\n            if key_node.tag == u'tag:yaml.org,2002:merge' and value_node.tag == '!include':\n                included_value = representer.represent_data(self.construct_object(value_node))\n                node.value[index] = (key_node, included_value)\n\n        super(Include_constructor, self).flatten_mapping(node)\n\n        node.value = deep_merge_nodes(node.value)\n",
            "file_path": "borgmatic/config/load.py",
            "human_label": "merge any node values corresponding to duplicate keys and return the result. If there are colliding keys with non-MappingNode values, the last of the values remains.",
            "level": "file_runnable",
            "lineno": "68",
            "name": "deep_merge_nodes",
            "oracle_context": "{ \"apis\" : \"['get', 'isinstance', 'SequenceNode', 'MappingNode']\", \"classes\" : \"[]\", \"vars\" : \"['anchor', 'ruamel', 'tag', 'nodes', 'end_mark', 'yaml', 'start_mark', 'comment', 'DELETED_NODE', 'flow_style', 'value']\" }",
            "package": "load",
            "project": "witten/borgmatic",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62ece4982e6aefcf4aabbd6f",
            "all_context": "{ \"import\" : \"requests \", \"file\" : \"\", \"class\" : \"\" }",
            "code": "import requests\ndef get_repo_archive(url: str, destination_path: Path) -> Path:\n    \"\"\"\n    Given an url and a destination path, retrieve and extract .tar.gz archive\n    which contains 'desc' file for each package.\n    Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n    Args:\n        url: url of the .tar.gz archive to download\n        destination_path: the path on disk where to extract archive\n\n    Returns:\n        a directory Path where the archive has been extracted to.\n    \"\"\"\n    res = requests.get(url)\n    destination_path.parent.mkdir(parents=True, exist_ok=True)\n    destination_path.write_bytes(res.content)\n\n    extract_to = Path(str(destination_path).split(\".tar.gz\")[0])\n    tar = tarfile.open(destination_path)\n    tar.extractall(path=extract_to)\n    tar.close()\n\n    return extract_to\n",
            "dependency": "",
            "docstring": "Given an url and a destination path, retrieve and extract .tar.gz archive\nwhich contains 'desc' file for each package.\nEach .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\nArgs:\n    url: url of the .tar.gz archive to download\n    destination_path: the path on disk where to extract archive\n\nReturns:\n    a directory Path where the archive has been extracted to.",
            "end_lineno": "247",
            "file_content": "# Copyright (C) 2022  The Software Heritage developers\n# See the AUTHORS file at the top-level directory of this distribution\n# License: GNU General Public License version 3, or any later version\n# See top-level LICENSE file for more information\nimport datetime\nimport logging\nfrom pathlib import Path\nimport re\nimport tarfile\nfrom typing import Any, Dict, Iterator, List, Optional\nfrom urllib.parse import unquote, urljoin\n\nfrom bs4 import BeautifulSoup\nimport requests\n\nfrom swh.model.hashutil import hash_to_hex\nfrom swh.scheduler.interface import SchedulerInterface\nfrom swh.scheduler.model import ListedOrigin\n\nfrom ..pattern import CredentialsType, StatelessLister\n\nlogger = logging.getLogger(__name__)\n\n# Aliasing the page results returned by `get_pages` method from the lister.\nArchListerPage = List[Dict[str, Any]]\n\n\ndef size_to_bytes(size: str) -> int:\n    \"\"\"Convert human readable file size to bytes.\n\n    Resulting value is an approximation as input value is in most case rounded.\n\n    Args:\n        size: A string representing a human readable file size (eg: '500K')\n\n    Returns:\n        A decimal representation of file size\n\n        Examples::\n\n            >>> size_to_bytes(\"500\")\n            500\n            >>> size_to_bytes(\"1K\")\n            1000\n    \"\"\"\n    units = {\n        \"K\": 1000,\n        \"M\": 1000**2,\n        \"G\": 1000**3,\n        \"T\": 1000**4,\n        \"P\": 1000**5,\n        \"E\": 1000**6,\n        \"Z\": 1000**7,\n        \"Y\": 1000**8,\n    }\n    if size.endswith(tuple(units)):\n        v, u = (size[:-1], size[-1])\n        return int(v) * units[u]\n    else:\n        return int(size)\n\n\nclass ArchLister(StatelessLister[ArchListerPage]):\n    \"\"\"List Arch linux origins from 'core', 'extra', and 'community' repositories\n\n    For 'official' Arch Linux it downloads core.tar.gz, extra.tar.gz and community.tar.gz\n    from https://archive.archlinux.org/repos/last/ extract to a temp directory and\n    then walks through each 'desc' files.\n\n    Each 'desc' file describe the latest released version of a package and helps\n    to build an origin url from where scrapping artifacts metadata.\n\n    For 'arm' Arch Linux it follow the same discovery process parsing 'desc' files.\n    The main difference is that we can't get existing versions of an arm package\n    because https://archlinuxarm.org does not have an 'archive' website or api.\n    \"\"\"\n\n    LISTER_NAME = \"arch\"\n    VISIT_TYPE = \"arch\"\n    INSTANCE = \"arch\"\n\n    DESTINATION_PATH = Path(\"/tmp/archlinux_archive\")\n\n    ARCH_PACKAGE_URL_PATTERN = \"{base_url}/packages/{repo}/{arch}/{pkgname}\"\n    ARCH_PACKAGE_VERSIONS_URL_PATTERN = \"{base_url}/packages/{pkgname[0]}/{pkgname}\"\n    ARCH_PACKAGE_DOWNLOAD_URL_PATTERN = (\n        \"{base_url}/packages/{pkgname[0]}/{pkgname}/{filename}\"\n    )\n    ARCH_API_URL_PATTERN = \"{base_url}/packages/{repo}/{arch}/{pkgname}/json\"\n\n    ARM_PACKAGE_URL_PATTERN = \"{base_url}/packages/{arch}/{pkgname}\"\n    ARM_PACKAGE_DOWNLOAD_URL_PATTERN = \"{base_url}/{arch}/{repo}/{filename}\"\n\n    def __init__(\n        self,\n        scheduler: SchedulerInterface,\n        credentials: Optional[CredentialsType] = None,\n        flavours: Dict[str, Any] = {\n            \"official\": {\n                \"archs\": [\"x86_64\"],\n                \"repos\": [\"core\", \"extra\", \"community\"],\n                \"base_info_url\": \"https://archlinux.org\",\n                \"base_archive_url\": \"https://archive.archlinux.org\",\n                \"base_mirror_url\": \"\",\n                \"base_api_url\": \"https://archlinux.org\",\n            },\n            \"arm\": {\n                \"archs\": [\"armv7h\", \"aarch64\"],\n                \"repos\": [\"core\", \"extra\", \"community\"],\n                \"base_info_url\": \"https://archlinuxarm.org\",\n                \"base_archive_url\": \"\",\n                \"base_mirror_url\": \"https://uk.mirror.archlinuxarm.org\",\n                \"base_api_url\": \"\",\n            },\n        },\n    ):\n        super().__init__(\n            scheduler=scheduler,\n            credentials=credentials,\n            url=flavours[\"official\"][\"base_info_url\"],\n            instance=self.INSTANCE,\n        )\n\n        self.flavours = flavours\n\n    def scrap_package_versions(\n        self, name: str, repo: str, base_url: str\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Given a package 'name' and 'repo', make an http call to origin url and parse its content\n        to get package versions artifacts data.\n        That method is suitable only for 'official' Arch Linux, not 'arm'.\n\n        Args:\n            name: Package name\n            repo: The repository the package belongs to (one of self.repos)\n\n        Returns:\n            A list of dict of version\n\n            Example::\n\n                [\n                    {\"url\": \"https://archive.archlinux.org/packages/d/dialog/dialog-1:1.3_20190211-1-x86_64.pkg.tar.xz\",  # noqa: B950\n                    \"arch\": \"x86_64\",\n                    \"repo\": \"core\",\n                    \"name\": \"dialog\",\n                    \"version\": \"1:1.3_20190211-1\",\n                    \"length\": 180000,\n                    \"filename\": \"dialog-1:1.3_20190211-1-x86_64.pkg.tar.xz\",\n                    \"last_modified\": \"2019-02-13T08:36:00\"},\n                ]\n        \"\"\"\n        url = self.ARCH_PACKAGE_VERSIONS_URL_PATTERN.format(\n            pkgname=name, base_url=base_url\n        )\n        soup = BeautifulSoup(requests.get(url).text, \"html.parser\")\n        links = soup.find_all(\"a\", href=True)\n\n        # drop the first line (used to go to up directory)\n        if links[0].attrs[\"href\"] == \"../\":\n            links.pop(0)\n\n        versions = []\n\n        for link in links:\n            # filename displayed can be cropped if name is too long, get it from href instead\n            filename = unquote(link.attrs[\"href\"])\n\n            if filename.endswith((\".tar.xz\", \".tar.zst\")):\n                # Extract arch from filename\n                arch_rex = re.compile(\n                    rf\"^{re.escape(name)}-(?P<version>.*)-(?P<arch>any|i686|x86_64)\"\n                    rf\"(.pkg.tar.(?:zst|xz))$\"\n                )\n                m = arch_rex.match(filename)\n                if m is None:\n                    logger.error(\n                        \"Can not find a match for architecture in %(filename)s\"\n                        % dict(filename=filename)\n                    )\n                else:\n                    arch = m.group(\"arch\")\n                    version = m.group(\"version\")\n\n                # Extract last_modified and an approximate file size\n                raw_text = link.next_sibling\n                raw_text_rex = re.compile(\n                    r\"^(?P<last_modified>\\d+-\\w+-\\d+ \\d\\d:\\d\\d)\\s+(?P<size>\\w+)$\"\n                )\n                s = raw_text_rex.search(raw_text.strip())\n                if s is None:\n                    logger.error(\n                        \"Can not find a match for 'last_modified' and/or \"\n                        \"'size' in '%(raw_text)s'\" % dict(raw_text=raw_text)\n                    )\n                else:\n                    assert s.groups()\n                    assert len(s.groups()) == 2\n                    last_modified_str, size = s.groups()\n\n                # format as expected\n                last_modified = datetime.datetime.strptime(\n                    last_modified_str, \"%d-%b-%Y %H:%M\"\n                ).isoformat()\n\n                length = size_to_bytes(size)  # we want bytes\n\n                # link url is relative, format a canonical one\n                url = self.ARCH_PACKAGE_DOWNLOAD_URL_PATTERN.format(\n                    base_url=base_url, pkgname=name, filename=filename\n                )\n                versions.append(\n                    dict(\n                        name=name,\n                        version=version,\n                        repo=repo,\n                        arch=arch,\n                        filename=filename,\n                        url=url,\n                        last_modified=last_modified,\n                        length=length,\n                    )\n                )\n        return versions\n\n    def get_repo_archive(self, url: str, destination_path: Path) -> Path:\n        \"\"\"Given an url and a destination path, retrieve and extract .tar.gz archive\n        which contains 'desc' file for each package.\n        Each .tar.gz archive corresponds to an Arch Linux repo ('core', 'extra', 'community').\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n\n        Returns:\n            a directory Path where the archive has been extracted to.\n        \"\"\"\n        res = requests.get(url)\n        destination_path.parent.mkdir(parents=True, exist_ok=True)\n        destination_path.write_bytes(res.content)\n\n        extract_to = Path(str(destination_path).split(\".tar.gz\")[0])\n        tar = tarfile.open(destination_path)\n        tar.extractall(path=extract_to)\n        tar.close()\n\n        return extract_to\n\n    def parse_desc_file(\n        self,\n        path: Path,\n        repo: str,\n        base_url: str,\n        dl_url_fmt: str,\n    ) -> Dict[str, Any]:\n        \"\"\"Extract package information from a 'desc' file.\n        There are subtle differences between parsing 'official' and 'arm' des files\n\n        Args:\n            path: A path to a 'desc' file on disk\n            repo: The repo the package belongs to\n\n        Returns:\n            A dict of metadata\n\n            Example::\n\n                {'api_url': 'https://archlinux.org/packages/core/x86_64/dialog/json',\n                 'arch': 'x86_64',\n                 'base': 'dialog',\n                 'builddate': '1650081535',\n                 'csize': '203028',\n                 'desc': 'A tool to display dialog boxes from shell scripts',\n                 'filename': 'dialog-1:1.3_20220414-1-x86_64.pkg.tar.zst',\n                 'isize': '483988',\n                 'license': 'LGPL2.1',\n                 'md5sum': '06407c0cb11c50d7bf83d600f2e8107c',\n                 'name': 'dialog',\n                 'packager': 'Evangelos Foutras <foutrelis@archlinux.org>',\n                 'pgpsig': 'pgpsig content xxx',\n                 'project_url': 'https://invisible-island.net/dialog/',\n                 'provides': 'libdialog.so=15-64',\n                 'repo': 'core',\n                 'sha256sum': 'ef8c8971f591de7db0f455970ef5d81d5aced1ddf139f963f16f6730b1851fa7',\n                 'url': 'https://archive.archlinux.org/packages/.all/dialog-1:1.3_20220414-1-x86_64.pkg.tar.zst',  # noqa: B950\n                 'version': '1:1.3_20220414-1'}\n        \"\"\"\n        rex = re.compile(r\"^\\%(?P<k>\\w+)\\%\\n(?P<v>.*)\\n$\", re.M)\n        with path.open(\"rb\") as content:\n            parsed = rex.findall(content.read().decode())\n            data = {entry[0].lower(): entry[1] for entry in parsed}\n\n            if \"url\" in data.keys():\n                data[\"project_url\"] = data[\"url\"]\n\n            assert data[\"name\"]\n            assert data[\"filename\"]\n            assert data[\"arch\"]\n\n            data[\"repo\"] = repo\n            data[\"url\"] = urljoin(\n                base_url,\n                dl_url_fmt.format(\n                    base_url=base_url,\n                    pkgname=data[\"name\"],\n                    filename=data[\"filename\"],\n                    arch=data[\"arch\"],\n                    repo=repo,\n                ),\n            )\n\n            assert data[\"md5sum\"]\n            assert data[\"sha256sum\"]\n            data[\"checksums\"] = {\n                \"md5sum\": hash_to_hex(data[\"md5sum\"]),\n                \"sha256sum\": hash_to_hex(data[\"sha256sum\"]),\n            }\n        return data\n\n    def get_pages(self) -> Iterator[ArchListerPage]:\n        \"\"\"Yield an iterator sorted by name in ascending order of pages.\n\n        Each page is a list of package belonging to a flavour ('official', 'arm'),\n        and a repo ('core', 'extra', 'community')\n        \"\"\"\n\n        for name, flavour in self.flavours.items():\n            for arch in flavour[\"archs\"]:\n                for repo in flavour[\"repos\"]:\n                    page = []\n                    if name == \"official\":\n                        prefix = urljoin(flavour[\"base_archive_url\"], \"/repos/last/\")\n                        filename = f\"{repo}.files.tar.gz\"\n                        archive_url = urljoin(prefix, f\"{repo}/os/{arch}/{filename}\")\n                        destination_path = Path(self.DESTINATION_PATH, arch, filename)\n                        base_url = flavour[\"base_archive_url\"]\n                        dl_url_fmt = self.ARCH_PACKAGE_DOWNLOAD_URL_PATTERN\n                        base_info_url = flavour[\"base_info_url\"]\n                        info_url_fmt = self.ARCH_PACKAGE_URL_PATTERN\n                    elif name == \"arm\":\n                        filename = f\"{repo}.files.tar.gz\"\n                        archive_url = urljoin(\n                            flavour[\"base_mirror_url\"], f\"{arch}/{repo}/{filename}\"\n                        )\n                        destination_path = Path(self.DESTINATION_PATH, arch, filename)\n                        base_url = flavour[\"base_mirror_url\"]\n                        dl_url_fmt = self.ARM_PACKAGE_DOWNLOAD_URL_PATTERN\n                        base_info_url = flavour[\"base_info_url\"]\n                        info_url_fmt = self.ARM_PACKAGE_URL_PATTERN\n\n                    archive = self.get_repo_archive(\n                        url=archive_url, destination_path=destination_path\n                    )\n\n                    assert archive\n\n                    packages_desc = list(archive.glob(\"**/desc\"))\n                    logger.debug(\n                        \"Processing %(instance)s source packages info from \"\n                        \"%(flavour)s %(arch)s %(repo)s repository, \"\n                        \"(%(qty)s packages).\"\n                        % dict(\n                            instance=self.instance,\n                            flavour=name,\n                            arch=arch,\n                            repo=repo,\n                            qty=len(packages_desc),\n                        )\n                    )\n\n                    for package_desc in packages_desc:\n                        data = self.parse_desc_file(\n                            path=package_desc,\n                            repo=repo,\n                            base_url=base_url,\n                            dl_url_fmt=dl_url_fmt,\n                        )\n\n                        assert data[\"builddate\"]\n                        last_modified = datetime.datetime.fromtimestamp(\n                            float(data[\"builddate\"]), tz=datetime.timezone.utc\n                        )\n\n                        assert data[\"name\"]\n                        assert data[\"filename\"]\n                        assert data[\"arch\"]\n                        url = info_url_fmt.format(\n                            base_url=base_info_url,\n                            pkgname=data[\"name\"],\n                            filename=data[\"filename\"],\n                            repo=repo,\n                            arch=data[\"arch\"],\n                        )\n\n                        assert data[\"version\"]\n                        if name == \"official\":\n                            # find all versions of a package scrapping archive\n                            versions = self.scrap_package_versions(\n                                name=data[\"name\"],\n                                repo=repo,\n                                base_url=base_url,\n                            )\n                        elif name == \"arm\":\n                            # There is no way to get related versions of a package,\n                            # but 'data' represents the latest released version,\n                            # use it in this case\n                            assert data[\"builddate\"]\n                            assert data[\"csize\"]\n                            assert data[\"url\"]\n                            versions = [\n                                dict(\n                                    name=data[\"name\"],\n                                    version=data[\"version\"],\n                                    repo=repo,\n                                    arch=data[\"arch\"],\n                                    filename=data[\"filename\"],\n                                    url=data[\"url\"],\n                                    last_modified=last_modified.replace(\n                                        tzinfo=None\n                                    ).isoformat(timespec=\"seconds\"),\n                                    length=int(data[\"csize\"]),\n                                )\n                            ]\n\n                        package = {\n                            \"name\": data[\"name\"],\n                            \"version\": data[\"version\"],\n                            \"last_modified\": last_modified,\n                            \"url\": url,\n                            \"versions\": versions,\n                            \"data\": data,\n                        }\n                        page.append(package)\n                    yield page\n\n    def get_origins_from_page(self, page: ArchListerPage) -> Iterator[ListedOrigin]:\n        \"\"\"Iterate on all arch pages and yield ListedOrigin instances.\"\"\"\n        assert self.lister_obj.id is not None\n        for origin in page:\n            yield ListedOrigin(\n                lister_id=self.lister_obj.id,\n                visit_type=self.VISIT_TYPE,\n                url=origin[\"url\"],\n                last_update=origin[\"last_modified\"],\n                extra_loader_arguments={\n                    \"artifacts\": origin[\"versions\"],\n                },\n            )\n",
            "file_path": "swh/lister/arch/lister.py",
            "human_label": "Given an url and a destination path, retrieve and extract .tar.gz archive which contains 'desc' file for each package\n\n        Args:\n            url: url of the .tar.gz archive to download\n            destination_path: the path on disk where to extract archive\n        Returns:\n                path where the archive is extracted to",
            "level": "slib_runnable",
            "lineno": "226",
            "name": "get_repo_archive",
            "oracle_context": "{ \"apis\" : \"['write_bytes', 'get', 'extractall', 'close', 'split', 'open', 'str', 'mkdir']\", \"classes\" : \"['requests', 'Path', 'tarfile']\", \"vars\" : \"['content', 'parent']\" }",
            "package": "",
            "project": "SoftwareHeritage/swh-lister",
            "test_lineno": "42",
            "test_name": "test_get_repo_archive"
        },
        {
            "_id": "62ece4982e6aefcf4aabbd76",
            "all_context": "{ \"import\" : \"os \", \"file\" : \"\", \"class\" : \"\" }",
            "code": "import os\ndef match(filename):\n    \"\"\"\n    Check if the filename is a type that this module supports\n\n    Args:\n        filename: Filename to match\n    Returns:\n        False if not a match, True if supported\n    \"\"\"\n\n    base_name = os.path.basename(filename)\n    base_name_lower = base_name.lower()\n    return base_name_lower == 'doxyfile'\n",
            "dependency": "",
            "docstring": "Check if the filename is a type that this module supports\n\nArgs:\n    filename: Filename to match\nReturns:\n    False if not a match, True if supported",
            "end_lineno": "212",
            "file_content": "\"\"\"Docopt is a Pythonic command-line interface parser that will make you smile.\n\nNow: with spellcheck, flag extension (de-abbreviation), and capitalization fixes.\n(but only when unambiguous)\n\n * Licensed under terms of MIT license (see LICENSE-MIT)\n\nContributors (roughly in chronological order):\n\n * Copyright (c) 2012 Andrew Kassen <atkassen@ucdavis.edu>\n * Copyright (c) 2012 jeffrimko <jeffrimko@gmail.com>\n * Copyright (c) 2012 Andrew Sutton <met48@met48.com>\n * Copyright (c) 2012 Andrew Sutton <met48@met48.com>\n * Copyright (c) 2012 Nima Johari <nimajohari@gmail.com>\n * Copyright (c) 2012-2013 Vladimir Keleshev, vladimir@keleshev.com\n * Copyright (c) 2014-2018 Matt Boersma <matt@sprout.org>\n * Copyright (c) 2016 amir <ladsgroup@gmail.com>\n * Copyright (c) 2015 Benjamin Bach <benjaoming@gmail.com>\n * Copyright (c) 2017 Oleg Bulkin <o.bulkin@gmail.com>\n * Copyright (c) 2018 Iain Barnett <iainspeed@gmail.com>\n * Copyright (c) 2019 itdaniher, itdaniher@gmail.com\n\n\"\"\"\nfrom __future__ import annotations\n\nimport sys\nimport re\nimport inspect\n\nfrom typing import Any, Type, Union, Callable, cast\n\n__all__ = [\"docopt\", \"magic_docopt\", \"magic\", \"DocoptExit\"]\n__version__ = \"0.7.2\"\n\n\ndef levenshtein_norm(source: str, target: str) -> float:\n    \"\"\"Calculates the normalized Levenshtein distance between two string\n    arguments. The result will be a float in the range [0.0, 1.0], with 1.0\n    signifying the biggest possible distance between strings with these lengths\n    \"\"\"\n\n    # Compute Levenshtein distance using helper function. The max is always\n    # just the length of the longer string, so this is used to normalize result\n    # before returning it\n    distance = levenshtein(source, target)\n    return float(distance) / max(len(source), len(target))\n\n\ndef levenshtein(source: str, target: str) -> int:\n    \"\"\"Computes the Levenshtein\n    (https://en.wikipedia.org/wiki/Levenshtein_distance)\n    and restricted Damerau-Levenshtein\n    (https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)\n    distances between two Unicode strings with given lengths using the\n    Wagner-Fischer algorithm\n    (https://en.wikipedia.org/wiki/Wagner%E2%80%93Fischer_algorithm).\n    These distances are defined recursively, since the distance between two\n    strings is just the cost of adjusting the last one or two characters plus\n    the distance between the prefixes that exclude these characters (e.g. the\n    distance between \"tester\" and \"tested\" is 1 + the distance between \"teste\"\n    and \"teste\"). The Wagner-Fischer algorithm retains this idea but eliminates\n    redundant computations by storing the distances between various prefixes in\n    a matrix that is filled in iteratively.\n    \"\"\"\n\n    # Create matrix of correct size (this is s_len + 1 * t_len + 1 so that the\n    # empty prefixes \"\" can also be included). The leftmost column represents\n    # transforming various source prefixes into an empty string, which can\n    # always be done by deleting all characters in the respective prefix, and\n    # the top row represents transforming the empty string into various target\n    # prefixes, which can always be done by inserting every character in the\n    # respective prefix. The ternary used to build the list should ensure that\n    # this row and column are now filled correctly\n    s_range = range(len(source) + 1)\n    t_range = range(len(target) + 1)\n    matrix = [[(i if j == 0 else j) for j in t_range] for i in s_range]\n\n    # Iterate through rest of matrix, filling it in with Levenshtein\n    # distances for the remaining prefix combinations\n    for i in s_range[1:]:\n        for j in t_range[1:]:\n            # Applies the recursive logic outlined above using the values\n            # stored in the matrix so far. The options for the last pair of\n            # characters are deletion, insertion, and substitution, which\n            # amount to dropping the source character, the target character,\n            # or both and then calculating the distance for the resulting\n            # prefix combo. If the characters at this point are the same, the\n            # situation can be thought of as a free substitution\n            del_dist = matrix[i - 1][j] + 1\n            ins_dist = matrix[i][j - 1] + 1\n            sub_trans_cost = 0 if source[i - 1] == target[j - 1] else 1\n            sub_dist = matrix[i - 1][j - 1] + sub_trans_cost\n\n            # Choose option that produces smallest distance\n            matrix[i][j] = min(del_dist, ins_dist, sub_dist)\n\n    # At this point, the matrix is full, and the biggest prefixes are just the\n    # strings themselves, so this is the desired distance\n    return matrix[len(source)][len(target)]\n\n\nclass DocoptLanguageError(Exception):\n\n    \"\"\"Error in construction of usage-message by developer.\"\"\"\n\n\nclass DocoptExit(SystemExit):\n\n    \"\"\"Exit in case user invoked program with incorrect arguments.\"\"\"\n\n    usage = \"\"\n\n    def __init__(\n        self,\n        message: str = \"\",\n        collected: list[Pattern] = None,\n        left: list[Pattern] = None,\n    ) -> None:\n        self.collected = collected if collected is not None else []\n        self.left = left if left is not None else []\n        SystemExit.__init__(self, (message + \"\\n\" + self.usage).strip())\n\n\nclass Pattern:\n    def __init__(\n        self, name: str | None, value: list[str] | str | int | None = None\n    ) -> None:\n        self._name, self.value = name, value\n\n    @property\n    def name(self) -> str | None:\n        return self._name\n\n    def __eq__(self, other) -> bool:\n        return repr(self) == repr(other)\n\n    def __hash__(self) -> int:\n        return hash(repr(self))\n\n\ndef transform(pattern: BranchPattern) -> Either:\n    \"\"\"Expand pattern into an (almost) equivalent one, but with single Either.\n\n    Example: ((-a | -b) (-c | -d)) => (-a -c | -a -d | -b -c | -b -d)\n    Quirks: [-a] => (-a), (-a...) => (-a -a)\n\n    \"\"\"\n    result = []\n    groups = [[pattern]]\n    while groups:\n        children = groups.pop(0)\n        parents = [Required, NotRequired, OptionsShortcut, Either, OneOrMore]\n        if any(t in map(type, children) for t in parents):\n            child = [c for c in children if type(c) in parents][0]\n            children.remove(child)\n            if type(child) is Either:\n                for c in child.children:\n                    groups.append([c] + children)\n            elif type(child) is OneOrMore:\n                groups.append(child.children * 2 + children)\n            else:\n                groups.append(child.children + children)\n        else:\n            result.append(children)\n    return Either(*[Required(*e) for e in result])\n\n\nTSingleMatch = tuple[Union[int, None], Union[\"LeafPattern\", None]]\n\n\nclass LeafPattern(Pattern):\n\n    \"\"\"Leaf/terminal node of a pattern tree.\"\"\"\n\n    def __repr__(self) -> str:\n        return \"%s(%r, %r)\" % (self.__class__.__name__, self.name, self.value)\n\n    def single_match(self, left: list[LeafPattern]) -> TSingleMatch:\n        raise NotImplementedError  # pragma: no cover\n\n    def flat(self, *types) -> list[LeafPattern]:\n        return [self] if not types or type(self) in types else []\n\n    def match(\n        self, left: list[LeafPattern], collected: list[Pattern] = None\n    ) -> tuple[bool, list[LeafPattern], list[Pattern]]:\n        collected = [] if collected is None else collected\n        increment: Any | None = None\n        pos, match = self.single_match(left)\n        if match is None or pos is None:\n            return False, left, collected\n        left_ = left[:pos] + left[(pos + 1) :]\n        same_name = [a for a in collected if a.name == self.name]\n        if type(self.value) == int and len(same_name) > 0:\n            if isinstance(same_name[0].value, int):\n                same_name[0].value += 1\n            return True, left_, collected\n        if type(self.value) == int and not same_name:\n            match.value = 1\n            return True, left_, collected + [match]\n        if same_name and type(self.value) == list:\n            if type(match.value) == str:\n                increment = [match.value]\n            if same_name[0].value is not None and increment is not None:\n                if isinstance(same_name[0].value, type(increment)):\n                    same_name[0].value += increment\n            return True, left_, collected\n        elif not same_name and type(self.value) == list:\n            if isinstance(match.value, str):\n                match.value = [match.value]\n            return True, left_, collected + [match]\n        return True, left_, collected + [match]\n\n\nclass BranchPattern(Pattern):\n\n    \"\"\"Branch/inner node of a pattern tree.\"\"\"\n\n    def __init__(self, *children) -> None:\n        self.children = list(children)\n\n    def match(self, left: list[Pattern], collected: list[Pattern] = None) -> Any:\n        raise NotImplementedError  # pragma: no cover\n\n    def fix(self) -> \"BranchPattern\":\n        self.fix_identities()\n        self.fix_repeating_arguments()\n        return self\n\n    def fix_identities(self, uniq: Any | None = None) -> None:\n        \"\"\"Make pattern-tree tips point to same object if they are equal.\"\"\"\n        flattened = self.flat()\n        uniq = list(set(flattened)) if uniq is None else uniq\n        for i, child in enumerate(self.children):\n            if not hasattr(child, \"children\"):\n                assert child in uniq\n                self.children[i] = uniq[uniq.index(child)]\n            else:\n                child.fix_identities(uniq)\n        return None\n\n    def fix_repeating_arguments(self) -> BranchPattern:\n        \"\"\"Fix elements that should accumulate/increment values.\"\"\"\n        either = [list(child.children) for child in transform(self).children]\n        for case in either:\n            for e in [child for child in case if case.count(child) > 1]:\n                if type(e) is Argument or type(e) is Option and e.argcount:\n                    if e.value is None:\n                        e.value = []\n                    elif type(e.value) is not list:\n                        e.value = cast(str, e.value)\n                        e.value = e.value.split()\n                if type(e) is Command or type(e) is Option and e.argcount == 0:\n                    e.value = 0\n        return self\n\n    def __repr__(self) -> str:\n        return \"%s(%s)\" % (\n            self.__class__.__name__,\n            \", \".join(repr(a) for a in self.children),\n        )\n\n    def flat(self, *types) -> Any:\n        if type(self) in types:\n            return [self]\n        return sum([child.flat(*types) for child in self.children], [])\n\n\nclass Argument(LeafPattern):\n    def single_match(self, left: list[LeafPattern]) -> TSingleMatch:\n        for n, pattern in enumerate(left):\n            if type(pattern) is Argument:\n                return n, Argument(self.name, pattern.value)\n        return None, None\n\n\nclass Command(Argument):\n    def __init__(self, name: str | None, value: bool = False) -> None:\n        self._name, self.value = name, value\n\n    def single_match(self, left: list[LeafPattern]) -> TSingleMatch:\n        for n, pattern in enumerate(left):\n            if type(pattern) is Argument:\n                if pattern.value == self.name:\n                    return n, Command(self.name, True)\n                else:\n                    break\n        return None, None\n\n\nclass Option(LeafPattern):\n    def __init__(\n        self,\n        short: str | None = None,\n        longer: str | None = None,\n        argcount: int = 0,\n        value: list[str] | str | int | None = False,\n    ) -> None:\n        assert argcount in (0, 1)\n        self.short, self.longer, self.argcount = short, longer, argcount\n        self.value = None if value is False and argcount else value\n\n    @classmethod\n    def parse(class_, option_description: str) -> Option:\n        short, longer, argcount, value = None, None, 0, False\n        options, _, description = option_description.strip().partition(\"  \")\n        options = options.replace(\",\", \" \").replace(\"=\", \" \")\n        for s in options.split():\n            if s.startswith(\"--\"):\n                longer = s\n            elif s.startswith(\"-\"):\n                short = s\n            else:\n                argcount = 1\n        if argcount:\n            matched = re.findall(r\"\\[default: (.*)\\]\", description, flags=re.I)\n            value = matched[0] if matched else None\n        return class_(short, longer, argcount, value)\n\n    def single_match(self, left: list[LeafPattern]) -> TSingleMatch:\n        for n, pattern in enumerate(left):\n            if self.name == pattern.name:\n                return n, pattern\n        return None, None\n\n    @property\n    def name(self) -> str | None:\n        return self.longer or self.short\n\n    def __repr__(self) -> str:\n        return \"Option(%r, %r, %r, %r)\" % (\n            self.short,\n            self.longer,\n            self.argcount,\n            self.value,\n        )\n\n\nclass Required(BranchPattern):\n    def match(self, left: list[Pattern], collected: list[Pattern] | None = None) -> Any:\n        collected = [] if collected is None else collected\n        original_collected = collected\n        original_left = left\n        for pattern in self.children:\n            matched, left, collected = pattern.match(left, collected)\n            if not matched:\n                return False, original_left, original_collected\n        return True, left, collected\n\n\nclass NotRequired(BranchPattern):\n    def match(self, left: list[Pattern], collected: list[Pattern] = None) -> Any:\n        collected = [] if collected is None else collected\n        for pattern in self.children:\n            _, left, collected = pattern.match(left, collected)\n        return True, left, collected\n\n\nclass OptionsShortcut(NotRequired):\n\n    \"\"\"Marker/placeholder for [options] shortcut.\"\"\"\n\n\nclass OneOrMore(BranchPattern):\n    def match(self, left: list[Pattern], collected: list[Pattern] = None) -> Any:\n        assert len(self.children) == 1\n        collected = [] if collected is None else collected\n        original_collected = collected\n        original_left = left\n        last_left = None\n        matched = True\n        times = 0\n        while matched:\n            matched, left, collected = self.children[0].match(left, collected)\n            times += 1 if matched else 0\n            if last_left == left:\n                break\n            last_left = left\n        if times >= 1:\n            return True, left, collected\n        return False, original_left, original_collected\n\n\nclass Either(BranchPattern):\n    def match(self, left: list[Pattern], collected: list[Pattern] = None) -> Any:\n        collected = [] if collected is None else collected\n        outcomes = []\n        for pattern in self.children:\n            matched, _, _ = outcome = pattern.match(left, collected)\n            if matched:\n                outcomes.append(outcome)\n        if outcomes:\n            return min(outcomes, key=lambda outcome: len(outcome[1]))\n        return False, left, collected\n\n\nclass Tokens(list):\n    def __init__(\n        self,\n        source: list[str] | str,\n        error: Type[DocoptExit] | Type[DocoptLanguageError] = DocoptExit,\n    ) -> None:\n        if isinstance(source, list):\n            self += source\n        else:\n            self += source.split()\n        self.error = error\n\n    @staticmethod\n    def from_pattern(source: str) -> Tokens:\n        source = re.sub(r\"([\\[\\]\\(\\)\\|]|\\.\\.\\.)\", r\" \\1 \", source)\n        fragments = [s for s in re.split(r\"\\s+|(\\S*<.*?>)\", source) if s]\n        return Tokens(fragments, error=DocoptLanguageError)\n\n    def move(self) -> str | None:\n        return self.pop(0) if len(self) else None\n\n    def current(self) -> str | None:\n        return self[0] if len(self) else None\n\n\ndef parse_longer(\n    tokens: Tokens, options: list[Option], argv: bool = False, more_magic: bool = False\n) -> list[Pattern]:\n    \"\"\"longer ::= '--' chars [ ( ' ' | '=' ) chars ] ;\"\"\"\n    current_token = tokens.move()\n    if current_token is None or not current_token.startswith(\"--\"):\n        raise tokens.error(\n            f\"parse_longer got what appears to be an invalid token: {current_token}\"\n        )  # pragma: no cover\n    longer, maybe_eq, maybe_value = current_token.partition(\"=\")\n    if maybe_eq == maybe_value == \"\":\n        value = None\n    else:\n        value = maybe_value\n    similar = [o for o in options if o.longer and longer == o.longer]\n    start_collision = (\n        len(\n            [\n                o\n                for o in options\n                if o.longer and longer in o.longer and o.longer.startswith(longer)\n            ]\n        )\n        > 1\n    )\n    if argv and not len(similar) and not start_collision:\n        similar = [\n            o\n            for o in options\n            if o.longer and longer in o.longer and o.longer.startswith(longer)\n        ]\n    # try advanced matching\n    if more_magic and not similar:\n        corrected = [\n            (longer, o)\n            for o in options\n            if o.longer and levenshtein_norm(longer, o.longer) < 0.25\n        ]\n        if corrected:\n            print(f\"NB: Corrected {corrected[0][0]} to {corrected[0][1].longer}\")\n        similar = [correct for (original, correct) in corrected]\n    if len(similar) > 1:\n        raise tokens.error(\n            f\"{longer} is not a unique prefix: {similar}?\"\n        )  # pragma: no cover\n    elif len(similar) < 1:\n        argcount = 1 if maybe_eq == \"=\" else 0\n        o = Option(None, longer, argcount)\n        options.append(o)\n        if tokens.error is DocoptExit:\n            o = Option(None, longer, argcount, value if argcount else True)\n    else:\n        o = Option(\n            similar[0].short, similar[0].longer, similar[0].argcount, similar[0].value\n        )\n        if o.argcount == 0:\n            if value is not None:\n                raise tokens.error(\"%s must not have an argument\" % o.longer)\n        else:\n            if value is None:\n                if tokens.current() in [None, \"--\"]:\n                    raise tokens.error(\"%s requires argument\" % o.longer)\n                value = tokens.move()\n        if tokens.error is DocoptExit:\n            o.value = value if value is not None else True\n    return [o]\n\n\ndef parse_shorts(\n    tokens: Tokens, options: list[Option], more_magic: bool = False\n) -> list[Pattern]:\n    \"\"\"shorts ::= '-' ( chars )* [ [ ' ' ] chars ] ;\"\"\"\n    token = tokens.move()\n    if token is None or not token.startswith(\"-\") or token.startswith(\"--\"):\n        raise ValueError(\n            f\"parse_shorts got what appears to be an invalid token: {token}\"\n        )  # pragma: no cover\n    left = token.lstrip(\"-\")\n    parsed: list[Pattern] = []\n    while left != \"\":\n        short, left = \"-\" + left[0], left[1:]\n        transformations: dict[str | None, Callable[[str], str]] = {None: lambda x: x}\n        if more_magic:\n            transformations[\"lowercase\"] = lambda x: x.lower()\n            transformations[\"uppercase\"] = lambda x: x.upper()\n        # try identity, lowercase, uppercase, iff such resolves uniquely\n        # (ie if upper and lowercase are not both defined)\n        similar: list[Option] = []\n        de_abbreviated = False\n        for transform_name, transform in transformations.items():\n            transformed = list(set([transform(o.short) for o in options if o.short]))\n            no_collisions = len(\n                [\n                    o\n                    for o in options\n                    if o.short and transformed.count(transform(o.short)) == 1\n                ]\n            )  # == len(transformed)\n            if no_collisions:\n                similar = [\n                    o\n                    for o in options\n                    if o.short and transform(o.short) == transform(short)\n                ]\n                if similar:\n                    if transform_name:\n                        print(\n                            f\"NB: Corrected {short} to {similar[0].short} \"\n                            f\"via {transform_name}\"\n                        )\n                    break\n            # if transformations do not resolve, try abbreviations of 'longer' forms\n            # iff such resolves uniquely (ie if no two longer forms begin with the\n            # same letter)\n            if not similar and more_magic:\n                abbreviated = [\n                    transform(o.longer[1:3])\n                    for o in options\n                    if o.longer and not o.short\n                ] + [transform(o.short) for o in options if o.short and not o.longer]\n                nonredundantly_abbreviated_options = [\n                    o for o in options if o.longer and abbreviated.count(short) == 1\n                ]\n                no_collisions = len(nonredundantly_abbreviated_options) == len(\n                    abbreviated\n                )\n                if no_collisions:\n                    for o in options:\n                        if (\n                            not o.short\n                            and o.longer\n                            and transform(short) == transform(o.longer[1:3])\n                        ):\n                            similar = [o]\n                            print(\n                                f\"NB: Corrected {short} to {similar[0].longer} \"\n                                f\"via abbreviation (case change: {transform_name})\"\n                            )\n                            break\n                if len(similar):\n                    de_abbreviated = True\n                    break\n        if len(similar) > 1:\n            raise tokens.error(\n                \"%s is specified ambiguously %d times\" % (short, len(similar))\n            )\n        elif len(similar) < 1:\n            o = Option(short, None, 0)\n            options.append(o)\n            if tokens.error is DocoptExit:\n                o = Option(short, None, 0, True)\n        else:\n            if de_abbreviated:\n                option_short_value = None\n            else:\n                option_short_value = transform(short)\n            o = Option(\n                option_short_value,\n                similar[0].longer,\n                similar[0].argcount,\n                similar[0].value,\n            )\n            value = None\n            current_token = tokens.current()\n            if o.argcount != 0:\n                if left == \"\":\n                    if current_token is None or current_token == \"--\":\n                        raise tokens.error(\"%s requires argument\" % short)\n                    else:\n                        value = tokens.move()\n                else:\n                    value = left\n                    left = \"\"\n            if tokens.error is DocoptExit:\n                o.value = value if value is not None else True\n        parsed.append(o)\n    return parsed\n\n\ndef parse_pattern(source: str, options: list[Option]) -> Required:\n    tokens = Tokens.from_pattern(source)\n    result = parse_expr(tokens, options)\n    if tokens.current() is not None:\n        raise tokens.error(\"unexpected ending: %r\" % \" \".join(tokens))\n    return Required(*result)\n\n\ndef parse_expr(tokens: Tokens, options: list[Option]) -> list[Pattern]:\n    \"\"\"expr ::= seq ( '|' seq )* ;\"\"\"\n    result: list[Pattern] = []\n    seq_0: list[Pattern] = parse_seq(tokens, options)\n    if tokens.current() != \"|\":\n        return seq_0\n    if len(seq_0) > 1:\n        result.append(Required(*seq_0))\n    else:\n        result += seq_0\n    while tokens.current() == \"|\":\n        tokens.move()\n        seq_1 = parse_seq(tokens, options)\n        if len(seq_1) > 1:\n            result += [Required(*seq_1)]\n        else:\n            result += seq_1\n    return [Either(*result)]\n\n\ndef parse_seq(tokens: Tokens, options: list[Option]) -> list[Pattern]:\n    \"\"\"seq ::= ( atom [ '...' ] )* ;\"\"\"\n    result: list[Pattern] = []\n    while tokens.current() not in [None, \"]\", \")\", \"|\"]:\n        atom = parse_atom(tokens, options)\n        if tokens.current() == \"...\":\n            atom = [OneOrMore(*atom)]\n            tokens.move()\n        result += atom\n    return result\n\n\ndef parse_atom(tokens: Tokens, options: list[Option]) -> list[Pattern]:\n    \"\"\"atom ::= '(' expr ')' | '[' expr ']' | 'options'\n    | longer | shorts | argument | command ;\n    \"\"\"\n    token = tokens.current()\n    if not token:\n        return [Command(tokens.move())]  # pragma: no cover\n    elif token in \"([\":\n        tokens.move()\n        matching = {\"(\": \")\", \"[\": \"]\"}[token]\n        pattern = {\"(\": Required, \"[\": NotRequired}[token]\n        matched_pattern = pattern(*parse_expr(tokens, options))\n        if tokens.move() != matching:\n            raise tokens.error(\"unmatched '%s'\" % token)\n        return [matched_pattern]\n    elif token == \"options\":\n        tokens.move()\n        return [OptionsShortcut()]\n    elif token.startswith(\"--\") and token != \"--\":\n        return parse_longer(tokens, options)\n    elif token.startswith(\"-\") and token not in (\"-\", \"--\"):\n        return parse_shorts(tokens, options)\n    elif token.startswith(\"<\") and token.endswith(\">\") or token.isupper():\n        return [Argument(tokens.move())]\n    else:\n        return [Command(tokens.move())]\n\n\ndef parse_argv(\n    tokens: Tokens,\n    options: list[Option],\n    options_first: bool = False,\n    more_magic: bool = False,\n) -> list[Pattern]:\n    \"\"\"Parse command-line argument vector.\n\n    If options_first:\n        argv ::= [ longer | shorts ]* [ argument ]* [ '--' [ argument ]* ] ;\n    else:\n        argv ::= [ longer | shorts | argument ]* [ '--' [ argument ]* ] ;\n\n    \"\"\"\n\n    def isanumber(x):\n        try:\n            float(x)\n            return True\n        except ValueError:\n            return False\n\n    parsed: list[Pattern] = []\n    current_token = tokens.current()\n    while current_token is not None:\n        if current_token == \"--\":\n            return parsed + [Argument(None, v) for v in tokens]\n        elif current_token.startswith(\"--\"):\n            parsed += parse_longer(tokens, options, argv=True, more_magic=more_magic)\n        elif (\n            current_token.startswith(\"-\")\n            and current_token != \"-\"\n            and not isanumber(current_token)\n        ):\n            parsed += parse_shorts(tokens, options, more_magic=more_magic)\n        elif options_first:\n            return parsed + [Argument(None, v) for v in tokens]\n        else:\n            parsed.append(Argument(None, tokens.move()))\n        current_token = tokens.current()\n    return parsed\n\n\ndef parse_defaults(docstring: str) -> list[Option]:\n    defaults = []\n    for s in parse_section(\"options:\", docstring):\n        options_literal, _, s = s.partition(\":\")\n        if \" \" in options_literal:\n            _, _, options_literal = options_literal.partition(\" \")\n        assert options_literal.lower().strip() == \"options\"\n        split = re.split(r\"\\n[ \\t]*(-\\S+?)\", \"\\n\" + s)[1:]\n        split = [s1 + s2 for s1, s2 in zip(split[::2], split[1::2])]\n        for s in split:\n            if s.startswith(\"-\"):\n                arg, _, description = s.partition(\"  \")\n                flag, _, var = arg.replace(\"=\", \" \").partition(\" \")\n                option = Option.parse(s)\n                defaults.append(option)\n    return defaults\n\n\ndef parse_section(name: str, source: str) -> list[str]:\n    pattern = re.compile(\n        \"^([^\\n]*\" + name + \"[^\\n]*\\n?(?:[ \\t].*?(?:\\n|$))*)\",\n        re.IGNORECASE | re.MULTILINE,\n    )\n    r = [\n        s.strip() for s in pattern.findall(source) if s.strip().lower() != name.lower()\n    ]\n    return r\n\n\ndef formal_usage(section: str) -> str:\n    _, _, section = section.partition(\":\")  # drop \"usage:\"\n    pu = section.split()\n    return \"( \" + \" \".join(\") | (\" if s == pu[0] else s for s in pu[1:]) + \" )\"\n\n\ndef extras(\n    default_help: bool, version: None, options: list[Pattern], docstring: str\n) -> None:\n    if default_help and any(\n        (o.name in (\"-h\", \"--help\")) and o.value\n        for o in options\n        if isinstance(o, Option)\n    ):\n        print(docstring.strip(\"\\n\"))\n        sys.exit()\n    if version and any(\n        o.name == \"--version\" and o.value for o in options if isinstance(o, Option)\n    ):\n        print(version)\n        sys.exit()\n\n\nclass ParsedOptions(dict):\n    def __repr__(self):\n        return \"{%s}\" % \",\\n \".join(\"%r: %r\" % i for i in sorted(self.items()))\n\n    def __getattr__(self, name: str) -> str | bool | None:\n        return self.get(name) or {\n            name: self.get(k)\n            for k in self.keys()\n            if name in [k.lstrip(\"-\").replace(\"-\", \"_\"), k.lstrip(\"<\").rstrip(\">\")]\n        }.get(name)\n\n\ndef docopt(\n    docstring: str | None = None,\n    argv: list[str] | str | None = None,\n    default_help: bool = True,\n    version: Any = None,\n    options_first: bool = False,\n    more_magic: bool = False,\n) -> ParsedOptions:\n    \"\"\"Parse `argv` based on command-line interface described in `doc`.\n\n    `docopt` creates your command-line interface based on its\n    description that you pass as `docstring`. Such description can contain\n    --options, <positional-argument>, commands, which could be\n    [optional], (required), (mutually | exclusive) or repeated...\n\n    Parameters\n    ----------\n    docstring : str (default: first __doc__ in parent scope)\n        Description of your command-line interface.\n    argv : list of str, optional\n        Argument vector to be parsed. sys.argv[1:] is used if not\n        provided.\n    default_help : bool (default: True)\n        Set to False to disable automatic help on -h or --help\n        options.\n    version : any object\n        If passed, the object will be printed if --version is in\n        `argv`.\n    options_first : bool (default: False)\n        Set to True to require options precede positional arguments,\n        i.e. to forbid options and positional arguments intermix.\n    more_magic : bool (default: False)\n        Try to be extra-helpful; pull results into globals() of caller as 'arguments',\n        offer advanced pattern-matching and spellcheck.\n        Also activates if `docopt` aliased to a name containing 'magic'.\n\n    Returns\n    -------\n    arguments: dict-like\n        A dictionary, where keys are names of command-line elements\n        such as e.g. \"--verbose\" and \"<path>\", and values are the\n        parsed values of those elements. Also supports dot acccess.\n\n    Example\n    -------\n    >>> from docopt import docopt\n    >>> doc = '''\n    ... Usage:\n    ...     my_program tcp <host> <port> [--timeout=<seconds>]\n    ...     my_program serial <port> [--baud=<n>] [--timeout=<seconds>]\n    ...     my_program (-h | --help | --version)\n    ...\n    ... Options:\n    ...     -h, --help  Show this screen and exit.\n    ...     --baud=<n>  Baudrate [default: 9600]\n    ... '''\n    >>> argv = ['tcp', '127.0.0.1', '80', '--timeout', '30']\n    >>> docopt(doc, argv)\n    {'--baud': '9600',\n     '--help': False,\n     '--timeout': '30',\n     '--version': False,\n     '<host>': '127.0.0.1',\n     '<port>': '80',\n     'serial': False,\n     'tcp': True}\n\n    \"\"\"\n    argv = sys.argv[1:] if argv is None else argv\n    maybe_frame = inspect.currentframe()\n    if maybe_frame:\n        parent_frame = doc_parent_frame = magic_parent_frame = maybe_frame.f_back\n    if not more_magic:  # make sure 'magic' isn't in the calling name\n        while not more_magic and magic_parent_frame:\n            imported_as = {\n                v: k\n                for k, v in magic_parent_frame.f_globals.items()\n                if hasattr(v, \"__name__\") and v.__name__ == docopt.__name__\n            }.get(docopt)\n            if imported_as and \"magic\" in imported_as:\n                more_magic = True\n            else:\n                magic_parent_frame = magic_parent_frame.f_back\n    if not docstring:  # go look for one, if none exists, raise Exception\n        while not docstring and doc_parent_frame:\n            docstring = doc_parent_frame.f_locals.get(\"__doc__\")\n            if not docstring:\n                doc_parent_frame = doc_parent_frame.f_back\n        if not docstring:\n            raise DocoptLanguageError(\n                \"Either __doc__ must be defined in the scope of a parent \"\n                \"or passed as the first argument.\"\n            )\n    output_value_assigned = False\n    if more_magic and parent_frame:\n        import dis\n\n        instrs = dis.get_instructions(parent_frame.f_code)\n        for instr in instrs:\n            if instr.offset == parent_frame.f_lasti:\n                break\n        assert instr.opname.startswith(\"CALL_\")\n        MAYBE_STORE = next(instrs)\n        if MAYBE_STORE and (\n            MAYBE_STORE.opname.startswith(\"STORE\")\n            or MAYBE_STORE.opname.startswith(\"RETURN\")\n        ):\n            output_value_assigned = True\n    usage_sections = parse_section(\"usage:\", docstring)\n    if len(usage_sections) == 0:\n        raise DocoptLanguageError(\n            '\"usage:\" section (case-insensitive) not found. '\n            \"Perhaps missing indentation?\"\n        )\n    if len(usage_sections) > 1:\n        raise DocoptLanguageError('More than one \"usage:\" (case-insensitive).')\n    options_pattern = re.compile(r\"\\n\\s*?options:\", re.IGNORECASE)\n    if options_pattern.search(usage_sections[0]):\n        raise DocoptExit(\n            \"Warning: options (case-insensitive) was found in usage.\"\n            \"Use a blank line between each section..\"\n        )\n    DocoptExit.usage = usage_sections[0]\n    options = parse_defaults(docstring)\n    pattern = parse_pattern(formal_usage(DocoptExit.usage), options)\n    pattern_options = set(pattern.flat(Option))\n    for options_shortcut in pattern.flat(OptionsShortcut):\n        doc_options = parse_defaults(docstring)\n        options_shortcut.children = [\n            opt for opt in doc_options if opt not in pattern_options\n        ]\n    parsed_arg_vector = parse_argv(\n        Tokens(argv), list(options), options_first, more_magic\n    )\n    extras(default_help, version, parsed_arg_vector, docstring)\n    matched, left, collected = pattern.fix().match(parsed_arg_vector)\n    if matched and left == []:\n        output_obj = ParsedOptions(\n            (a.name, a.value) for a in (pattern.flat() + collected)\n        )\n        target_parent_frame = parent_frame or magic_parent_frame or doc_parent_frame\n        if more_magic and target_parent_frame and not output_value_assigned:\n            if not target_parent_frame.f_globals.get(\"arguments\"):\n                target_parent_frame.f_globals[\"arguments\"] = output_obj\n        return output_obj\n    if left:\n        raise DocoptExit(f\"Warning: found unmatched (duplicate?) arguments {left}\")\n    raise DocoptExit(collected=collected, left=left)\n\n\nmagic = magic_docopt = docopt\n",
            "file_path": "docopt/__init__.py",
            "human_label": "Check if the type of the given filename is 'doxyfile'\n\n    Args:\n        filename: filename to be check\n    Returns:\n        Return True if the type of the given filename in lower case is 'doxyfile'",
            "level": "slib_runnable",
            "lineno": "184",
            "name": "match",
            "oracle_context": "{ \"apis\" : \"['isinstance', 'type', 'len', 'single_match']\", \"classes\" : \"['Any', 'Pattern']\", \"vars\" : \"['name', 'value']\" }",
            "package": "",
            "project": "bazaar-projects/docopt-ng",
            "test_lineno": "42",
            "test_name": "test_match"
        },
        {
            "_id": "62ece4992e6aefcf4aabbd85",
            "all_context": "{ \"import\" : \"rdflib \", \"file\" : \"\", \"class\" : \"\" }",
            "code": "import rdflib\ndef find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    \"\"\"\n\n    non_roots: Set[Node] = set()\n    if roots is None:\n        roots = set()\n    for x, y in graph.subject_objects(prop):\n        non_roots.add(x)\n        if x in roots:\n            roots.remove(x)\n        if y not in non_roots:\n            roots.add(y)\n    return roots\n",
            "dependency": "",
            "docstring": "Find the roots in some sort of transitive hierarchy.\n\nfind_roots(graph, rdflib.RDFS.subClassOf)\nwill return a set of all roots of the sub-class hierarchy\n\nAssumes triple of the form (child, prop, parent), i.e. the direction of\nRDFS.subClassOf or SKOS.broader",
            "end_lineno": "405",
            "file_content": "\"\"\"\nSome utility functions.\n\nMiscellaneous utilities\n\n* list2set\n* first\n* uniq\n* more_than\n\nTerm characterisation and generation\n\n* to_term\n* from_n3\n\nDate/time utilities\n\n* date_time\n* parse_date_time\n\n\"\"\"\n\nfrom calendar import timegm\nfrom os.path import splitext\n\n# from time import daylight\nfrom time import altzone, gmtime, localtime, time, timezone\nfrom typing import (\n    TYPE_CHECKING,\n    Any,\n    Callable,\n    Iterable,\n    List,\n    Optional,\n    Set,\n    Tuple,\n    TypeVar,\n)\n\nimport rdflib.graph  # avoid circular dependency\nfrom rdflib.compat import sign\nfrom rdflib.namespace import XSD, Namespace, NamespaceManager\nfrom rdflib.term import BNode, IdentifiedNode, Literal, Node, URIRef\n\nif TYPE_CHECKING:\n    from rdflib.graph import Graph\n\n__all__ = [\n    \"list2set\",\n    \"first\",\n    \"uniq\",\n    \"more_than\",\n    \"to_term\",\n    \"from_n3\",\n    \"date_time\",\n    \"parse_date_time\",\n    \"guess_format\",\n    \"find_roots\",\n    \"get_tree\",\n    \"_coalesce\",\n]\n\n\ndef list2set(seq):\n    \"\"\"\n    Return a new list without duplicates.\n    Preserves the order, unlike set(seq)\n    \"\"\"\n    seen = set()\n    return [x for x in seq if x not in seen and not seen.add(x)]\n\n\ndef first(seq):\n    \"\"\"\n    return the first element in a python sequence\n    for graphs, use graph.value instead\n    \"\"\"\n    for result in seq:\n        return result\n    return None\n\n\ndef uniq(sequence, strip=0):\n    \"\"\"removes duplicate strings from the sequence.\"\"\"\n    if strip:\n        return set(s.strip() for s in sequence)\n    else:\n        return set(sequence)\n\n\ndef more_than(sequence, number):\n    \"Returns 1 if sequence has more items than number and 0 if not.\"\n    i = 0\n    for item in sequence:\n        i += 1\n        if i > number:\n            return 1\n    return 0\n\n\ndef to_term(s, default=None):\n    \"\"\"\n    Creates and returns an Identifier of type corresponding\n    to the pattern of the given positional argument string ``s``:\n\n    '' returns the ``default`` keyword argument value or ``None``\n\n    '<s>' returns ``URIRef(s)`` (i.e. without angle brackets)\n\n    '\"s\"' returns ``Literal(s)`` (i.e. without doublequotes)\n\n    '_s' returns ``BNode(s)`` (i.e. without leading underscore)\n\n    \"\"\"\n    if not s:\n        return default\n    elif s.startswith(\"<\") and s.endswith(\">\"):\n        return URIRef(s[1:-1])\n    elif s.startswith('\"') and s.endswith('\"'):\n        return Literal(s[1:-1])\n    elif s.startswith(\"_\"):\n        return BNode(s)\n    else:\n        msg = \"Unrecognised term syntax: '%s'\" % s\n        raise Exception(msg)\n\n\ndef from_n3(s: str, default=None, backend=None, nsm=None):\n    r'''\n    Creates the Identifier corresponding to the given n3 string.\n\n        >>> from_n3('<http://ex.com/foo>') == URIRef('http://ex.com/foo')\n        True\n        >>> from_n3('\"foo\"@de') == Literal('foo', lang='de')\n        True\n        >>> from_n3('\"\"\"multi\\nline\\nstring\"\"\"@en') == Literal(\n        ...     'multi\\nline\\nstring', lang='en')\n        True\n        >>> from_n3('42') == Literal(42)\n        True\n        >>> from_n3(Literal(42).n3()) == Literal(42)\n        True\n        >>> from_n3('\"42\"^^xsd:integer') == Literal(42)\n        True\n        >>> from rdflib import RDFS\n        >>> from_n3('rdfs:label') == RDFS['label']\n        True\n        >>> nsm = NamespaceManager(rdflib.graph.Graph())\n        >>> nsm.bind('dbpedia', 'http://dbpedia.org/resource/')\n        >>> berlin = URIRef('http://dbpedia.org/resource/Berlin')\n        >>> from_n3('dbpedia:Berlin', nsm=nsm) == berlin\n        True\n\n    '''\n    if not s:\n        return default\n    if s.startswith(\"<\"):\n        # Hack: this should correctly handle strings with either native unicode\n        # characters, or \\u1234 unicode escapes.\n        return URIRef(s[1:-1].encode(\"raw-unicode-escape\").decode(\"unicode-escape\"))\n    elif s.startswith('\"'):\n        if s.startswith('\"\"\"'):\n            quotes = '\"\"\"'\n        else:\n            quotes = '\"'\n        value, rest = s.rsplit(quotes, 1)\n        value = value[len(quotes) :]  # strip leading quotes\n        datatype = None\n        language = None\n\n        # as a given datatype overrules lang-tag check for it first\n        dtoffset = rest.rfind(\"^^\")\n        if dtoffset >= 0:\n            # found a datatype\n            # datatype has to come after lang-tag so ignore everything before\n            # see: http://www.w3.org/TR/2011/WD-turtle-20110809/\n            # #prod-turtle2-RDFLiteral\n            datatype = from_n3(rest[dtoffset + 2 :], default, backend, nsm)\n        else:\n            if rest.startswith(\"@\"):\n                language = rest[1:]  # strip leading at sign\n\n        value = value.replace(r\"\\\"\", '\"')\n        # unicode-escape interprets \\xhh as an escape sequence,\n        # but n3 does not define it as such.\n        value = value.replace(r\"\\x\", r\"\\\\x\")\n        # Hack: this should correctly handle strings with either native unicode\n        # characters, or \\u1234 unicode escapes.\n        value = value.encode(\"raw-unicode-escape\").decode(\"unicode-escape\")\n        return Literal(value, language, datatype)\n    elif s == \"true\" or s == \"false\":\n        return Literal(s == \"true\")\n    elif (\n        s.lower()\n        .replace('.', '', 1)\n        .replace('-', '', 1)\n        .replace('e', '', 1)\n        .isnumeric()\n    ):\n        if \"e\" in s.lower():\n            return Literal(s, datatype=XSD.double)\n        if \".\" in s:\n            return Literal(float(s), datatype=XSD.decimal)\n        return Literal(int(s), datatype=XSD.integer)\n\n    elif s.startswith(\"{\"):\n        identifier = from_n3(s[1:-1])\n        return rdflib.graph.QuotedGraph(backend, identifier)\n    elif s.startswith(\"[\"):\n        identifier = from_n3(s[1:-1])\n        return rdflib.graph.Graph(backend, identifier)\n    elif s.startswith(\"_:\"):\n        return BNode(s[2:])\n    elif \":\" in s:\n        if nsm is None:\n            # instantiate default NamespaceManager and rely on its defaults\n            nsm = NamespaceManager(rdflib.graph.Graph())\n        prefix, last_part = s.split(\":\", 1)\n        ns = dict(nsm.namespaces())[prefix]\n        return Namespace(ns)[last_part]\n    else:\n        return BNode(s)\n\n\ndef date_time(t=None, local_time_zone=False):\n    \"\"\"http://www.w3.org/TR/NOTE-datetime ex: 1997-07-16T19:20:30Z\n\n    >>> date_time(1126482850)\n    '2005-09-11T23:54:10Z'\n\n    @@ this will change depending on where it is run\n    #>>> date_time(1126482850, local_time_zone=True)\n    #'2005-09-11T19:54:10-04:00'\n\n    >>> date_time(1)\n    '1970-01-01T00:00:01Z'\n\n    >>> date_time(0)\n    '1970-01-01T00:00:00Z'\n    \"\"\"\n    if t is None:\n        t = time()\n\n    if local_time_zone:\n        time_tuple = localtime(t)\n        if time_tuple[8]:\n            tz_mins = altzone // 60\n        else:\n            tz_mins = timezone // 60\n        tzd = \"-%02d:%02d\" % (tz_mins // 60, tz_mins % 60)\n    else:\n        time_tuple = gmtime(t)\n        tzd = \"Z\"\n\n    year, month, day, hh, mm, ss, wd, y, z = time_tuple\n    s = \"%0004d-%02d-%02dT%02d:%02d:%02d%s\" % (year, month, day, hh, mm, ss, tzd)\n    return s\n\n\ndef parse_date_time(val):\n    \"\"\"always returns seconds in UTC\n\n    # tests are written like this to make any errors easier to understand\n    >>> parse_date_time('2005-09-11T23:54:10Z') - 1126482850.0\n    0.0\n\n    >>> parse_date_time('2005-09-11T16:54:10-07:00') - 1126482850.0\n    0.0\n\n    >>> parse_date_time('1970-01-01T00:00:01Z') - 1.0\n    0.0\n\n    >>> parse_date_time('1970-01-01T00:00:00Z') - 0.0\n    0.0\n    >>> parse_date_time(\"2005-09-05T10:42:00\") - 1125916920.0\n    0.0\n    \"\"\"\n\n    if \"T\" not in val:\n        val += \"T00:00:00Z\"\n\n    ymd, time = val.split(\"T\")\n    hms, tz_str = time[0:8], time[8:]\n\n    if not tz_str or tz_str == \"Z\":\n        time = time[:-1]\n        tz_offset = 0\n    else:\n        signed_hrs = int(tz_str[:3])\n        mins = int(tz_str[4:6])\n        secs = (sign(signed_hrs) * mins + signed_hrs * 60) * 60\n        tz_offset = -secs\n\n    year, month, day = ymd.split(\"-\")\n    hour, minute, second = hms.split(\":\")\n\n    t = timegm(\n        (int(year), int(month), int(day), int(hour), int(minute), int(second), 0, 0, 0)\n    )\n    t = t + tz_offset\n    return t\n\n\nSUFFIX_FORMAT_MAP = {\n    \"xml\": \"xml\",\n    \"rdf\": \"xml\",\n    \"owl\": \"xml\",\n    \"n3\": \"n3\",\n    \"ttl\": \"turtle\",\n    \"nt\": \"nt\",\n    \"trix\": \"trix\",\n    \"xhtml\": \"rdfa\",\n    \"html\": \"rdfa\",\n    \"svg\": \"rdfa\",\n    \"nq\": \"nquads\",\n    \"nquads\": \"nquads\",\n    \"trig\": \"trig\",\n    \"json\": \"json-ld\",\n    \"jsonld\": \"json-ld\",\n    \"json-ld\": \"json-ld\",\n}\n\n\ndef guess_format(fpath, fmap=None) -> Optional[str]:\n    \"\"\"\n    Guess RDF serialization based on file suffix. Uses\n    ``SUFFIX_FORMAT_MAP`` unless ``fmap`` is provided. Examples:\n\n        >>> guess_format('path/to/file.rdf')\n        'xml'\n        >>> guess_format('path/to/file.owl')\n        'xml'\n        >>> guess_format('path/to/file.ttl')\n        'turtle'\n        >>> guess_format('path/to/file.json')\n        'json-ld'\n        >>> guess_format('path/to/file.xhtml')\n        'rdfa'\n        >>> guess_format('path/to/file.svg')\n        'rdfa'\n        >>> guess_format('path/to/file.xhtml', {'xhtml': 'grddl'})\n        'grddl'\n\n    This also works with just the suffixes, with or without leading dot, and\n    regardless of letter case::\n\n        >>> guess_format('.rdf')\n        'xml'\n        >>> guess_format('rdf')\n        'xml'\n        >>> guess_format('RDF')\n        'xml'\n    \"\"\"\n    fmap = fmap or SUFFIX_FORMAT_MAP\n    return fmap.get(_get_ext(fpath)) or fmap.get(fpath.lower())\n\n\ndef _get_ext(fpath, lower=True):\n    \"\"\"\n    Gets the file extension from a file(path); stripped of leading '.' and in\n    lower case. Examples:\n\n        >>> _get_ext(\"path/to/file.txt\")\n        'txt'\n        >>> _get_ext(\"OTHER.PDF\")\n        'pdf'\n        >>> _get_ext(\"noext\")\n        ''\n        >>> _get_ext(\".rdf\")\n        'rdf'\n    \"\"\"\n    ext = splitext(fpath)[-1]\n    if ext == \"\" and fpath.startswith(\".\"):\n        ext = fpath\n    if lower:\n        ext = ext.lower()\n    if ext.startswith(\".\"):\n        ext = ext[1:]\n    return ext\n\n\ndef find_roots(\n    graph: \"Graph\", prop: \"URIRef\", roots: Optional[Set[\"Node\"]] = None\n) -> Set[\"Node\"]:\n    \"\"\"\n    Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    \"\"\"\n\n    non_roots: Set[Node] = set()\n    if roots is None:\n        roots = set()\n    for x, y in graph.subject_objects(prop):\n        non_roots.add(x)\n        if x in roots:\n            roots.remove(x)\n        if y not in non_roots:\n            roots.add(y)\n    return roots\n\n\ndef get_tree(\n    graph: \"Graph\",\n    root: \"IdentifiedNode\",\n    prop: \"URIRef\",\n    mapper: Callable[[\"IdentifiedNode\"], \"IdentifiedNode\"] = lambda x: x,\n    sortkey: Optional[Callable[[Any], Any]] = None,\n    done: Optional[Set[\"IdentifiedNode\"]] = None,\n    dir: str = \"down\",\n) -> Optional[Tuple[IdentifiedNode, List[Any]]]:\n    \"\"\"\n    Return a nested list/tuple structure representing the tree\n    built by the transitive property given, starting from the root given\n\n    i.e.\n\n    get_tree(graph,\n       rdflib.URIRef(\"http://xmlns.com/foaf/0.1/Person\"),\n       rdflib.RDFS.subClassOf)\n\n    will return the structure for the subClassTree below person.\n\n    dir='down' assumes triple of the form (child, prop, parent),\n    i.e. the direction of RDFS.subClassOf or SKOS.broader\n    Any other dir traverses in the other direction\n\n    \"\"\"\n\n    if done is None:\n        done = set()\n    if root in done:\n        # type error: Return value expected\n        return  # type: ignore[return-value]\n    done.add(root)\n    tree = []\n\n    branches: Iterable[IdentifiedNode]\n    if dir == \"down\":\n        branches = graph.subjects(prop, root)\n    else:\n        # type error: Incompatible types in assignment (expression has type \"Iterable[Node]\", variable has type \"Iterable[IdentifiedNode]\")\n        branches = graph.objects(root, prop)  # type: ignore[assignment]\n\n    for branch in branches:\n        t = get_tree(graph, branch, prop, mapper, sortkey, done, dir)\n        if t:\n            tree.append(t)\n\n    return (mapper(root), sorted(tree, key=sortkey))\n\n\n_AnyT = TypeVar(\"_AnyT\")\n\n\ndef _coalesce(*args: Optional[_AnyT]) -> Optional[_AnyT]:\n    \"\"\"\n    This is a null coalescing function, it will return the first non-`None`\n    argument passed to it, otherwise it will return `None`.\n\n    For more info regarding the rationale of this function see deferred `PEP\n    505 <https://peps.python.org/pep-0505/>`_.\n\n    :param args: Values to consider as candidates to return, the first arg that\n        is not `None` will be returned. If no argument is passed this function\n        will return None.\n    :return: The first ``arg`` that is not `None`, otherwise `None` if there\n        are no args or if all args are `None`.\n    \"\"\"\n    for arg in args:\n        if arg is not None:\n            return arg\n    return None\n",
            "file_path": "rdflib/util.py",
            "human_label": " Find the roots in some sort of transitive hierarchy.\n\n    find_roots(graph, rdflib.RDFS.subClassOf)\n    will return a set of all roots of the sub-class hierarchy\n\n    Assumes triple of the form (child, prop, parent), i.e. the direction of\n    RDFS.subClassOf or SKOS.broader\n\n    Args:\n        graph: Graph Class Object\n        prop: URIRef Class Object\n        roots: Optional list with set type\n    Return:\n        roots: a set with nodes",
            "level": "plib_runnable",
            "lineno": "382",
            "name": "find_roots",
            "oracle_context": "{ \"apis\" : \"['remove', 'set', 'subject_objects', 'add']\", \"classes\" : \"['Node', 'Optional', 'Set']\", \"vars\" : \"['non_roots']\" }",
            "package": "",
            "project": "mwatts15/rdflib",
            "test_lineno": "110",
            "test_name": "test_find_roots"
        },
        {
            "_id": "62ece4992e6aefcf4aabbd8a",
            "all_context": "{ \"import\" : \"logging \", \"file\" : \"\", \"class\" : \"\" }",
            "code": "import logging\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    '''\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    '''\n    # Dict mapping from config filename to corresponding parsed config dict.\n    configs = collections.OrderedDict()\n    logs = []\n\n    # Parse and load each configuration file.\n    for config_filename in config_filenames:\n        try:\n            configs[config_filename] = validate.parse_configuration(\n                config_filename, validate.schema_filename(), overrides, resolve_env\n            )\n        except PermissionError:\n            logs.extend(\n                [\n                    logging.makeLogRecord(\n                        dict(\n                            levelno=logging.WARNING,\n                            levelname='WARNING',\n                            msg='{}: Insufficient permissions to read configuration file'.format(\n                                config_filename\n                            ),\n                        )\n                    ),\n                ]\n            )\n        except (ValueError, OSError, validate.Validation_error) as error:\n            logs.extend(\n                [\n                    logging.makeLogRecord(\n                        dict(\n                            levelno=logging.CRITICAL,\n                            levelname='CRITICAL',\n                            msg='{}: Error parsing configuration file'.format(config_filename),\n                        )\n                    ),\n                    logging.makeLogRecord(\n                        dict(levelno=logging.CRITICAL, levelname='CRITICAL', msg=error)\n                    ),\n                ]\n            )\n\n    return (configs, logs)\n",
            "dependency": "",
            "docstring": "Given a sequence of configuration filenames, load and validate each configuration file. Return\nthe results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.",
            "end_lineno": "699",
            "file_content": "import collections\nimport copy\nimport json\nimport logging\nimport os\nimport sys\nimport time\nfrom queue import Queue\nfrom subprocess import CalledProcessError\n\nimport colorama\nimport pkg_resources\n\nimport borgmatic.commands.completion\nfrom borgmatic.borg import borg as borg_borg\nfrom borgmatic.borg import check as borg_check\nfrom borgmatic.borg import compact as borg_compact\nfrom borgmatic.borg import create as borg_create\nfrom borgmatic.borg import environment as borg_environment\nfrom borgmatic.borg import export_tar as borg_export_tar\nfrom borgmatic.borg import extract as borg_extract\nfrom borgmatic.borg import feature as borg_feature\nfrom borgmatic.borg import info as borg_info\nfrom borgmatic.borg import init as borg_init\nfrom borgmatic.borg import list as borg_list\nfrom borgmatic.borg import mount as borg_mount\nfrom borgmatic.borg import prune as borg_prune\nfrom borgmatic.borg import umount as borg_umount\nfrom borgmatic.borg import version as borg_version\nfrom borgmatic.commands.arguments import parse_arguments\nfrom borgmatic.config import checks, collect, convert, validate\nfrom borgmatic.hooks import command, dispatch, dump, monitor\nfrom borgmatic.logger import configure_logging, should_do_markup\nfrom borgmatic.signals import configure_signals\nfrom borgmatic.verbosity import verbosity_to_log_level\n\nlogger = logging.getLogger(__name__)\n\nLEGACY_CONFIG_PATH = '/etc/borgmatic/config'\n\n\ndef run_configuration(config_filename, config, arguments):\n    '''\n    Given a config filename, the corresponding parsed config dict, and command-line arguments as a\n    dict from subparser name to a namespace of parsed arguments, execute the defined prune, compact,\n    create, check, and/or other actions.\n\n    Yield a combination of:\n\n      * JSON output strings from successfully executing any actions that produce JSON\n      * logging.LogRecord instances containing errors from any actions or backup hooks that fail\n    '''\n    (location, storage, retention, consistency, hooks) = (\n        config.get(section_name, {})\n        for section_name in ('location', 'storage', 'retention', 'consistency', 'hooks')\n    )\n    global_arguments = arguments['global']\n\n    local_path = location.get('local_path', 'borg')\n    remote_path = location.get('remote_path')\n    retries = storage.get('retries', 0)\n    retry_wait = storage.get('retry_wait', 0)\n    borg_environment.initialize(storage)\n    encountered_error = None\n    error_repository = ''\n    using_primary_action = {'prune', 'compact', 'create', 'check'}.intersection(arguments)\n    monitoring_log_level = verbosity_to_log_level(global_arguments.monitoring_verbosity)\n\n    try:\n        local_borg_version = borg_version.local_borg_version(local_path)\n    except (OSError, CalledProcessError, ValueError) as error:\n        yield from log_error_records(\n            '{}: Error getting local Borg version'.format(config_filename), error\n        )\n        return\n\n    try:\n        if using_primary_action:\n            dispatch.call_hooks(\n                'initialize_monitor',\n                hooks,\n                config_filename,\n                monitor.MONITOR_HOOK_NAMES,\n                monitoring_log_level,\n                global_arguments.dry_run,\n            )\n        if using_primary_action:\n            dispatch.call_hooks(\n                'ping_monitor',\n                hooks,\n                config_filename,\n                monitor.MONITOR_HOOK_NAMES,\n                monitor.State.START,\n                monitoring_log_level,\n                global_arguments.dry_run,\n            )\n    except (OSError, CalledProcessError) as error:\n        if command.considered_soft_failure(config_filename, error):\n            return\n\n        encountered_error = error\n        yield from log_error_records('{}: Error pinging monitor'.format(config_filename), error)\n\n    if not encountered_error:\n        repo_queue = Queue()\n        for repo in location['repositories']:\n            repo_queue.put((repo, 0),)\n\n        while not repo_queue.empty():\n            repository_path, retry_num = repo_queue.get()\n            timeout = retry_num * retry_wait\n            if timeout:\n                logger.warning(f'{config_filename}: Sleeping {timeout}s before next retry')\n                time.sleep(timeout)\n            try:\n                yield from run_actions(\n                    arguments=arguments,\n                    config_filename=config_filename,\n                    location=location,\n                    storage=storage,\n                    retention=retention,\n                    consistency=consistency,\n                    hooks=hooks,\n                    local_path=local_path,\n                    remote_path=remote_path,\n                    local_borg_version=local_borg_version,\n                    repository_path=repository_path,\n                )\n            except (OSError, CalledProcessError, ValueError) as error:\n                if retry_num < retries:\n                    repo_queue.put((repository_path, retry_num + 1),)\n                    tuple(  # Consume the generator so as to trigger logging.\n                        log_error_records(\n                            '{}: Error running actions for repository'.format(repository_path),\n                            error,\n                            levelno=logging.WARNING,\n                            log_command_error_output=True,\n                        )\n                    )\n                    logger.warning(\n                        f'{config_filename}: Retrying... attempt {retry_num + 1}/{retries}'\n                    )\n                    continue\n\n                if command.considered_soft_failure(config_filename, error):\n                    return\n\n                yield from log_error_records(\n                    '{}: Error running actions for repository'.format(repository_path), error\n                )\n                encountered_error = error\n                error_repository = repository_path\n\n    if not encountered_error:\n        try:\n            if using_primary_action:\n                dispatch.call_hooks(\n                    'ping_monitor',\n                    hooks,\n                    config_filename,\n                    monitor.MONITOR_HOOK_NAMES,\n                    monitor.State.FINISH,\n                    monitoring_log_level,\n                    global_arguments.dry_run,\n                )\n                dispatch.call_hooks(\n                    'destroy_monitor',\n                    hooks,\n                    config_filename,\n                    monitor.MONITOR_HOOK_NAMES,\n                    monitoring_log_level,\n                    global_arguments.dry_run,\n                )\n        except (OSError, CalledProcessError) as error:\n            if command.considered_soft_failure(config_filename, error):\n                return\n\n            encountered_error = error\n            yield from log_error_records('{}: Error pinging monitor'.format(config_filename), error)\n\n    if encountered_error and using_primary_action:\n        try:\n            command.execute_hook(\n                hooks.get('on_error'),\n                hooks.get('umask'),\n                config_filename,\n                'on-error',\n                global_arguments.dry_run,\n                repository=error_repository,\n                error=encountered_error,\n                output=getattr(encountered_error, 'output', ''),\n            )\n            dispatch.call_hooks(\n                'ping_monitor',\n                hooks,\n                config_filename,\n                monitor.MONITOR_HOOK_NAMES,\n                monitor.State.FAIL,\n                monitoring_log_level,\n                global_arguments.dry_run,\n            )\n            dispatch.call_hooks(\n                'destroy_monitor',\n                hooks,\n                config_filename,\n                monitor.MONITOR_HOOK_NAMES,\n                monitoring_log_level,\n                global_arguments.dry_run,\n            )\n        except (OSError, CalledProcessError) as error:\n            if command.considered_soft_failure(config_filename, error):\n                return\n\n            yield from log_error_records(\n                '{}: Error running on-error hook'.format(config_filename), error\n            )\n\n\ndef run_actions(\n    *,\n    arguments,\n    config_filename,\n    location,\n    storage,\n    retention,\n    consistency,\n    hooks,\n    local_path,\n    remote_path,\n    local_borg_version,\n    repository_path,\n):\n    '''\n    Given parsed command-line arguments as an argparse.ArgumentParser instance, the configuration\n    filename, several different configuration dicts, local and remote paths to Borg, a local Borg\n    version string, and a repository name, run all actions from the command-line arguments on the\n    given repository.\n\n    Yield JSON output strings from executing any actions that produce JSON.\n\n    Raise OSError or subprocess.CalledProcessError if an error occurs running a command for an\n    action or a hook. Raise ValueError if the arguments or configuration passed to action are\n    invalid.\n    '''\n    repository = os.path.expanduser(repository_path)\n    global_arguments = arguments['global']\n    dry_run_label = ' (dry run; not making any changes)' if global_arguments.dry_run else ''\n    hook_context = {\n        'repository': repository_path,\n        # Deprecated: For backwards compatibility with borgmatic < 1.6.0.\n        'repositories': ','.join(location['repositories']),\n    }\n\n    if 'init' in arguments:\n        logger.info('{}: Initializing repository'.format(repository))\n        borg_init.initialize_repository(\n            repository,\n            storage,\n            arguments['init'].encryption_mode,\n            arguments['init'].append_only,\n            arguments['init'].storage_quota,\n            local_path=local_path,\n            remote_path=remote_path,\n        )\n    if 'prune' in arguments:\n        command.execute_hook(\n            hooks.get('before_prune'),\n            hooks.get('umask'),\n            config_filename,\n            'pre-prune',\n            global_arguments.dry_run,\n            **hook_context,\n        )\n        logger.info('{}: Pruning archives{}'.format(repository, dry_run_label))\n        borg_prune.prune_archives(\n            global_arguments.dry_run,\n            repository,\n            storage,\n            retention,\n            local_path=local_path,\n            remote_path=remote_path,\n            stats=arguments['prune'].stats,\n            files=arguments['prune'].files,\n        )\n        command.execute_hook(\n            hooks.get('after_prune'),\n            hooks.get('umask'),\n            config_filename,\n            'post-prune',\n            global_arguments.dry_run,\n            **hook_context,\n        )\n    if 'compact' in arguments:\n        command.execute_hook(\n            hooks.get('before_compact'),\n            hooks.get('umask'),\n            config_filename,\n            'pre-compact',\n            global_arguments.dry_run,\n        )\n        if borg_feature.available(borg_feature.Feature.COMPACT, local_borg_version):\n            logger.info('{}: Compacting segments{}'.format(repository, dry_run_label))\n            borg_compact.compact_segments(\n                global_arguments.dry_run,\n                repository,\n                storage,\n                local_path=local_path,\n                remote_path=remote_path,\n                progress=arguments['compact'].progress,\n                cleanup_commits=arguments['compact'].cleanup_commits,\n                threshold=arguments['compact'].threshold,\n            )\n        else:  # pragma: nocover\n            logger.info(\n                '{}: Skipping compact (only available/needed in Borg 1.2+)'.format(repository)\n            )\n        command.execute_hook(\n            hooks.get('after_compact'),\n            hooks.get('umask'),\n            config_filename,\n            'post-compact',\n            global_arguments.dry_run,\n        )\n    if 'create' in arguments:\n        command.execute_hook(\n            hooks.get('before_backup'),\n            hooks.get('umask'),\n            config_filename,\n            'pre-backup',\n            global_arguments.dry_run,\n            **hook_context,\n        )\n        logger.info('{}: Creating archive{}'.format(repository, dry_run_label))\n        dispatch.call_hooks(\n            'remove_database_dumps',\n            hooks,\n            repository,\n            dump.DATABASE_HOOK_NAMES,\n            location,\n            global_arguments.dry_run,\n        )\n        active_dumps = dispatch.call_hooks(\n            'dump_databases',\n            hooks,\n            repository,\n            dump.DATABASE_HOOK_NAMES,\n            location,\n            global_arguments.dry_run,\n        )\n        stream_processes = [process for processes in active_dumps.values() for process in processes]\n\n        json_output = borg_create.create_archive(\n            global_arguments.dry_run,\n            repository,\n            location,\n            storage,\n            local_borg_version,\n            local_path=local_path,\n            remote_path=remote_path,\n            progress=arguments['create'].progress,\n            stats=arguments['create'].stats,\n            json=arguments['create'].json,\n            files=arguments['create'].files,\n            stream_processes=stream_processes,\n        )\n        if json_output:  # pragma: nocover\n            yield json.loads(json_output)\n\n        dispatch.call_hooks(\n            'remove_database_dumps',\n            hooks,\n            config_filename,\n            dump.DATABASE_HOOK_NAMES,\n            location,\n            global_arguments.dry_run,\n        )\n        command.execute_hook(\n            hooks.get('after_backup'),\n            hooks.get('umask'),\n            config_filename,\n            'post-backup',\n            global_arguments.dry_run,\n            **hook_context,\n        )\n\n    if 'check' in arguments and checks.repository_enabled_for_checks(repository, consistency):\n        command.execute_hook(\n            hooks.get('before_check'),\n            hooks.get('umask'),\n            config_filename,\n            'pre-check',\n            global_arguments.dry_run,\n            **hook_context,\n        )\n        logger.info('{}: Running consistency checks'.format(repository))\n        borg_check.check_archives(\n            repository,\n            location,\n            storage,\n            consistency,\n            local_path=local_path,\n            remote_path=remote_path,\n            progress=arguments['check'].progress,\n            repair=arguments['check'].repair,\n            only_checks=arguments['check'].only,\n            force=arguments['check'].force,\n        )\n        command.execute_hook(\n            hooks.get('after_check'),\n            hooks.get('umask'),\n            config_filename,\n            'post-check',\n            global_arguments.dry_run,\n            **hook_context,\n        )\n    if 'extract' in arguments:\n        command.execute_hook(\n            hooks.get('before_extract'),\n            hooks.get('umask'),\n            config_filename,\n            'pre-extract',\n            global_arguments.dry_run,\n            **hook_context,\n        )\n        if arguments['extract'].repository is None or validate.repositories_match(\n            repository, arguments['extract'].repository\n        ):\n            logger.info(\n                '{}: Extracting archive {}'.format(repository, arguments['extract'].archive)\n            )\n            borg_extract.extract_archive(\n                global_arguments.dry_run,\n                repository,\n                borg_list.resolve_archive_name(\n                    repository, arguments['extract'].archive, storage, local_path, remote_path\n                ),\n                arguments['extract'].paths,\n                location,\n                storage,\n                local_borg_version,\n                local_path=local_path,\n                remote_path=remote_path,\n                destination_path=arguments['extract'].destination,\n                strip_components=arguments['extract'].strip_components,\n                progress=arguments['extract'].progress,\n            )\n        command.execute_hook(\n            hooks.get('after_extract'),\n            hooks.get('umask'),\n            config_filename,\n            'post-extract',\n            global_arguments.dry_run,\n            **hook_context,\n        )\n    if 'export-tar' in arguments:\n        if arguments['export-tar'].repository is None or validate.repositories_match(\n            repository, arguments['export-tar'].repository\n        ):\n            logger.info(\n                '{}: Exporting archive {} as tar file'.format(\n                    repository, arguments['export-tar'].archive\n                )\n            )\n            borg_export_tar.export_tar_archive(\n                global_arguments.dry_run,\n                repository,\n                borg_list.resolve_archive_name(\n                    repository, arguments['export-tar'].archive, storage, local_path, remote_path\n                ),\n                arguments['export-tar'].paths,\n                arguments['export-tar'].destination,\n                storage,\n                local_path=local_path,\n                remote_path=remote_path,\n                tar_filter=arguments['export-tar'].tar_filter,\n                files=arguments['export-tar'].files,\n                strip_components=arguments['export-tar'].strip_components,\n            )\n    if 'mount' in arguments:\n        if arguments['mount'].repository is None or validate.repositories_match(\n            repository, arguments['mount'].repository\n        ):\n            if arguments['mount'].archive:\n                logger.info(\n                    '{}: Mounting archive {}'.format(repository, arguments['mount'].archive)\n                )\n            else:  # pragma: nocover\n                logger.info('{}: Mounting repository'.format(repository))\n\n            borg_mount.mount_archive(\n                repository,\n                borg_list.resolve_archive_name(\n                    repository, arguments['mount'].archive, storage, local_path, remote_path\n                ),\n                arguments['mount'].mount_point,\n                arguments['mount'].paths,\n                arguments['mount'].foreground,\n                arguments['mount'].options,\n                storage,\n                local_path=local_path,\n                remote_path=remote_path,\n            )\n    if 'restore' in arguments:  # pragma: nocover\n        if arguments['restore'].repository is None or validate.repositories_match(\n            repository, arguments['restore'].repository\n        ):\n            logger.info(\n                '{}: Restoring databases from archive {}'.format(\n                    repository, arguments['restore'].archive\n                )\n            )\n            dispatch.call_hooks(\n                'remove_database_dumps',\n                hooks,\n                repository,\n                dump.DATABASE_HOOK_NAMES,\n                location,\n                global_arguments.dry_run,\n            )\n\n            restore_names = arguments['restore'].databases or []\n            if 'all' in restore_names:\n                restore_names = []\n\n            archive_name = borg_list.resolve_archive_name(\n                repository, arguments['restore'].archive, storage, local_path, remote_path\n            )\n            found_names = set()\n\n            for hook_name, per_hook_restore_databases in hooks.items():\n                if hook_name not in dump.DATABASE_HOOK_NAMES:\n                    continue\n\n                for restore_database in per_hook_restore_databases:\n                    database_name = restore_database['name']\n                    if restore_names and database_name not in restore_names:\n                        continue\n\n                    found_names.add(database_name)\n                    dump_pattern = dispatch.call_hooks(\n                        'make_database_dump_pattern',\n                        hooks,\n                        repository,\n                        dump.DATABASE_HOOK_NAMES,\n                        location,\n                        database_name,\n                    )[hook_name]\n\n                    # Kick off a single database extract to stdout.\n                    extract_process = borg_extract.extract_archive(\n                        dry_run=global_arguments.dry_run,\n                        repository=repository,\n                        archive=archive_name,\n                        paths=dump.convert_glob_patterns_to_borg_patterns([dump_pattern]),\n                        location_config=location,\n                        storage_config=storage,\n                        local_borg_version=local_borg_version,\n                        local_path=local_path,\n                        remote_path=remote_path,\n                        destination_path='/',\n                        # A directory format dump isn't a single file, and therefore can't extract\n                        # to stdout. In this case, the extract_process return value is None.\n                        extract_to_stdout=bool(restore_database.get('format') != 'directory'),\n                    )\n\n                    # Run a single database restore, consuming the extract stdout (if any).\n                    dispatch.call_hooks(\n                        'restore_database_dump',\n                        {hook_name: [restore_database]},\n                        repository,\n                        dump.DATABASE_HOOK_NAMES,\n                        location,\n                        global_arguments.dry_run,\n                        extract_process,\n                    )\n\n            dispatch.call_hooks(\n                'remove_database_dumps',\n                hooks,\n                repository,\n                dump.DATABASE_HOOK_NAMES,\n                location,\n                global_arguments.dry_run,\n            )\n\n            if not restore_names and not found_names:\n                raise ValueError('No databases were found to restore')\n\n            missing_names = sorted(set(restore_names) - found_names)\n            if missing_names:\n                raise ValueError(\n                    'Cannot restore database(s) {} missing from borgmatic\\'s configuration'.format(\n                        ', '.join(missing_names)\n                    )\n                )\n\n    if 'list' in arguments:\n        if arguments['list'].repository is None or validate.repositories_match(\n            repository, arguments['list'].repository\n        ):\n            list_arguments = copy.copy(arguments['list'])\n            if not list_arguments.json:  # pragma: nocover\n                logger.warning('{}: Listing archives'.format(repository))\n            list_arguments.archive = borg_list.resolve_archive_name(\n                repository, list_arguments.archive, storage, local_path, remote_path\n            )\n            json_output = borg_list.list_archives(\n                repository,\n                storage,\n                list_arguments=list_arguments,\n                local_path=local_path,\n                remote_path=remote_path,\n            )\n            if json_output:  # pragma: nocover\n                yield json.loads(json_output)\n    if 'info' in arguments:\n        if arguments['info'].repository is None or validate.repositories_match(\n            repository, arguments['info'].repository\n        ):\n            info_arguments = copy.copy(arguments['info'])\n            if not info_arguments.json:  # pragma: nocover\n                logger.warning('{}: Displaying summary info for archives'.format(repository))\n            info_arguments.archive = borg_list.resolve_archive_name(\n                repository, info_arguments.archive, storage, local_path, remote_path\n            )\n            json_output = borg_info.display_archives_info(\n                repository,\n                storage,\n                info_arguments=info_arguments,\n                local_path=local_path,\n                remote_path=remote_path,\n            )\n            if json_output:  # pragma: nocover\n                yield json.loads(json_output)\n    if 'borg' in arguments:\n        if arguments['borg'].repository is None or validate.repositories_match(\n            repository, arguments['borg'].repository\n        ):\n            logger.warning('{}: Running arbitrary Borg command'.format(repository))\n            archive_name = borg_list.resolve_archive_name(\n                repository, arguments['borg'].archive, storage, local_path, remote_path\n            )\n            borg_borg.run_arbitrary_borg(\n                repository,\n                storage,\n                options=arguments['borg'].options,\n                archive=archive_name,\n                local_path=local_path,\n                remote_path=remote_path,\n            )\n\n\ndef load_configurations(config_filenames, overrides=None, resolve_env=True):\n    '''\n    Given a sequence of configuration filenames, load and validate each configuration file. Return\n    the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\n    and sequence of logging.LogRecord instances containing any parse errors.\n    '''\n    # Dict mapping from config filename to corresponding parsed config dict.\n    configs = collections.OrderedDict()\n    logs = []\n\n    # Parse and load each configuration file.\n    for config_filename in config_filenames:\n        try:\n            configs[config_filename] = validate.parse_configuration(\n                config_filename, validate.schema_filename(), overrides, resolve_env\n            )\n        except PermissionError:\n            logs.extend(\n                [\n                    logging.makeLogRecord(\n                        dict(\n                            levelno=logging.WARNING,\n                            levelname='WARNING',\n                            msg='{}: Insufficient permissions to read configuration file'.format(\n                                config_filename\n                            ),\n                        )\n                    ),\n                ]\n            )\n        except (ValueError, OSError, validate.Validation_error) as error:\n            logs.extend(\n                [\n                    logging.makeLogRecord(\n                        dict(\n                            levelno=logging.CRITICAL,\n                            levelname='CRITICAL',\n                            msg='{}: Error parsing configuration file'.format(config_filename),\n                        )\n                    ),\n                    logging.makeLogRecord(\n                        dict(levelno=logging.CRITICAL, levelname='CRITICAL', msg=error)\n                    ),\n                ]\n            )\n\n    return (configs, logs)\n\n\ndef log_record(suppress_log=False, **kwargs):\n    '''\n    Create a log record based on the given makeLogRecord() arguments, one of which must be\n    named \"levelno\". Log the record (unless suppress log is set) and return it.\n    '''\n    record = logging.makeLogRecord(kwargs)\n    if suppress_log:\n        return record\n\n    logger.handle(record)\n    return record\n\n\ndef log_error_records(\n    message, error=None, levelno=logging.CRITICAL, log_command_error_output=False\n):\n    '''\n    Given error message text, an optional exception object, an optional log level, and whether to\n    log the error output of a CalledProcessError (if any), log error summary information and also\n    yield it as a series of logging.LogRecord instances.\n\n    Note that because the logs are yielded as a generator, logs won't get logged unless you consume\n    the generator output.\n    '''\n    level_name = logging._levelToName[levelno]\n\n    if not error:\n        yield log_record(levelno=levelno, levelname=level_name, msg=message)\n        return\n\n    try:\n        raise error\n    except CalledProcessError as error:\n        yield log_record(levelno=levelno, levelname=level_name, msg=message)\n        if error.output:\n            # Suppress these logs for now and save full error output for the log summary at the end.\n            yield log_record(\n                levelno=levelno,\n                levelname=level_name,\n                msg=error.output,\n                suppress_log=not log_command_error_output,\n            )\n        yield log_record(levelno=levelno, levelname=level_name, msg=error)\n    except (ValueError, OSError) as error:\n        yield log_record(levelno=levelno, levelname=level_name, msg=message)\n        yield log_record(levelno=levelno, levelname=level_name, msg=error)\n    except:  # noqa: E722\n        # Raising above only as a means of determining the error type. Swallow the exception here\n        # because we don't want the exception to propagate out of this function.\n        pass\n\n\ndef get_local_path(configs):\n    '''\n    Arbitrarily return the local path from the first configuration dict. Default to \"borg\" if not\n    set.\n    '''\n    return next(iter(configs.values())).get('location', {}).get('local_path', 'borg')\n\n\ndef collect_configuration_run_summary_logs(configs, arguments):\n    '''\n    Given a dict of configuration filename to corresponding parsed configuration, and parsed\n    command-line arguments as a dict from subparser name to a parsed namespace of arguments, run\n    each configuration file and yield a series of logging.LogRecord instances containing summary\n    information about each run.\n\n    As a side effect of running through these configuration files, output their JSON results, if\n    any, to stdout.\n    '''\n    # Run cross-file validation checks.\n    if 'extract' in arguments:\n        repository = arguments['extract'].repository\n    elif 'list' in arguments and arguments['list'].archive:\n        repository = arguments['list'].repository\n    elif 'mount' in arguments:\n        repository = arguments['mount'].repository\n    else:\n        repository = None\n\n    if repository:\n        try:\n            validate.guard_configuration_contains_repository(repository, configs)\n        except ValueError as error:\n            yield from log_error_records(str(error))\n            return\n\n    if not configs:\n        yield from log_error_records(\n            '{}: No valid configuration files found'.format(\n                ' '.join(arguments['global'].config_paths)\n            )\n        )\n        return\n\n    if 'create' in arguments:\n        try:\n            for config_filename, config in configs.items():\n                hooks = config.get('hooks', {})\n                command.execute_hook(\n                    hooks.get('before_everything'),\n                    hooks.get('umask'),\n                    config_filename,\n                    'pre-everything',\n                    arguments['global'].dry_run,\n                )\n        except (CalledProcessError, ValueError, OSError) as error:\n            yield from log_error_records('Error running pre-everything hook', error)\n            return\n\n    # Execute the actions corresponding to each configuration file.\n    json_results = []\n    for config_filename, config in configs.items():\n        results = list(run_configuration(config_filename, config, arguments))\n        error_logs = tuple(result for result in results if isinstance(result, logging.LogRecord))\n\n        if error_logs:\n            yield from log_error_records(\n                '{}: Error running configuration file'.format(config_filename)\n            )\n            yield from error_logs\n        else:\n            yield logging.makeLogRecord(\n                dict(\n                    levelno=logging.INFO,\n                    levelname='INFO',\n                    msg='{}: Successfully ran configuration file'.format(config_filename),\n                )\n            )\n            if results:\n                json_results.extend(results)\n\n    if 'umount' in arguments:\n        logger.info('Unmounting mount point {}'.format(arguments['umount'].mount_point))\n        try:\n            borg_umount.unmount_archive(\n                mount_point=arguments['umount'].mount_point, local_path=get_local_path(configs)\n            )\n        except (CalledProcessError, OSError) as error:\n            yield from log_error_records('Error unmounting mount point', error)\n\n    if json_results:\n        sys.stdout.write(json.dumps(json_results))\n\n    if 'create' in arguments:\n        try:\n            for config_filename, config in configs.items():\n                hooks = config.get('hooks', {})\n                command.execute_hook(\n                    hooks.get('after_everything'),\n                    hooks.get('umask'),\n                    config_filename,\n                    'post-everything',\n                    arguments['global'].dry_run,\n                )\n        except (CalledProcessError, ValueError, OSError) as error:\n            yield from log_error_records('Error running post-everything hook', error)\n\n\ndef exit_with_help_link():  # pragma: no cover\n    '''\n    Display a link to get help and exit with an error code.\n    '''\n    logger.critical('')\n    logger.critical('Need some help? https://torsion.org/borgmatic/#issues')\n    sys.exit(1)\n\n\ndef main():  # pragma: no cover\n    configure_signals()\n\n    try:\n        arguments = parse_arguments(*sys.argv[1:])\n    except ValueError as error:\n        configure_logging(logging.CRITICAL)\n        logger.critical(error)\n        exit_with_help_link()\n    except SystemExit as error:\n        if error.code == 0:\n            raise error\n        configure_logging(logging.CRITICAL)\n        logger.critical('Error parsing arguments: {}'.format(' '.join(sys.argv)))\n        exit_with_help_link()\n\n    global_arguments = arguments['global']\n    if global_arguments.version:\n        print(pkg_resources.require('borgmatic')[0].version)\n        sys.exit(0)\n    if global_arguments.bash_completion:\n        print(borgmatic.commands.completion.bash_completion())\n        sys.exit(0)\n\n    config_filenames = tuple(collect.collect_config_filenames(global_arguments.config_paths))\n    configs, parse_logs = load_configurations(\n        config_filenames, global_arguments.overrides, global_arguments.resolve_env\n    )\n\n    any_json_flags = any(\n        getattr(sub_arguments, 'json', False) for sub_arguments in arguments.values()\n    )\n    colorama.init(\n        autoreset=True,\n        strip=not should_do_markup(global_arguments.no_color or any_json_flags, configs),\n    )\n    try:\n        configure_logging(\n            verbosity_to_log_level(global_arguments.verbosity),\n            verbosity_to_log_level(global_arguments.syslog_verbosity),\n            verbosity_to_log_level(global_arguments.log_file_verbosity),\n            verbosity_to_log_level(global_arguments.monitoring_verbosity),\n            global_arguments.log_file,\n        )\n    except (FileNotFoundError, PermissionError) as error:\n        configure_logging(logging.CRITICAL)\n        logger.critical('Error configuring logging: {}'.format(error))\n        exit_with_help_link()\n\n    logger.debug('Ensuring legacy configuration is upgraded')\n    convert.guard_configuration_upgraded(LEGACY_CONFIG_PATH, config_filenames)\n\n    summary_logs = parse_logs + list(collect_configuration_run_summary_logs(configs, arguments))\n    summary_logs_max_level = max(log.levelno for log in summary_logs)\n\n    for message in ('', 'summary:'):\n        log_record(\n            levelno=summary_logs_max_level,\n            levelname=logging.getLevelName(summary_logs_max_level),\n            msg=message,\n        )\n\n    for log in summary_logs:\n        logger.handle(log)\n\n    if summary_logs_max_level >= logging.CRITICAL:\n        exit_with_help_link()\n",
            "file_path": "borgmatic/commands/borgmatic.py",
            "human_label": "Given a sequence of configuration filenames, load and validate each configuration file. If the configuration file\ncannot be read due to insufficient permissions or error parsing configuration file, the error log will\nbe recorded. Otherwise, return the results as a tuple of: dict of configuration filename to corresponding parsed configuration,\nand sequence of logging.LogRecord instances containing any parse errors.",
            "level": "plib_runnable",
            "lineno": "653",
            "name": "load_configurations",
            "oracle_context": "{ \"apis\" : \"['dict', 'OrderedDict', 'format', 'schema_filename', 'extend', 'makeLogRecord', 'parse_configuration']\", \"classes\" : \"['logging', 'collections', 'validate']\", \"vars\" : \"['Str', 'CRITICAL', 'WARNING', 'Validation_error']\" }",
            "package": "",
            "project": "witten/borgmatic",
            "test_lineno": "72",
            "test_name": "test_load_configurations"
        },
        {
            "_id": "62e60723d76274f8a4026b76",
            "all_context": "{ \"import\" : \"time functools re datetime neo4j functools pytz datetime \", \"file\" : \"__all__ ; MIN_INT64 ; MAX_INT64 ; MIN_YEAR ; MAX_YEAR ; DATE_ISO_PATTERN ; TIME_ISO_PATTERN ; DURATION_ISO_PATTERN ; NANO_SECONDS ; AVERAGE_SECONDS_IN_MONTH ; AVERAGE_SECONDS_IN_DAY ; _is_leap_year(year) ; IS_LEAP_YEAR ; _days_in_year(year) ; DAYS_IN_YEAR ; _days_in_month(year,month) ; DAYS_IN_MONTH ; _normalize_day(year,month,day) ; ZeroDate ; Midnight ; Midday ; Never ; UnixEpoch ; \", \"class\" : \"self.from_ticks(cls,ticks,tz) ; self.from_iso_format(cls,s) ; self.to_clock_time(self) ; self.__lt__(self,other) ; self._utc_offset ; self.__copy__(self) ; self.__str__(self) ; self.__getattr__(self,name) ; self._get_both_normalized_ticks(self,other,strict) ; self.utc_now(cls) ; self.__le__(self,other) ; self.from_clock_time(cls,clock_time,epoch) ; self.hour(self) ; self.tzinfo(self) ; self._get_both_normalized_ticks ; self.__normalize_hour(cls,hour) ; self.__deepcopy__(self) ; self.__new__(cls,hour,minute,second,nanosecond,tzinfo) ; self.__eq__ ; self.__normalize_minute(cls,hour,minute) ; self.ticks ; self.__gt__(self,other) ; self.iso_format(self) ; self.__new(cls,ticks,hour,minute,second,nanosecond,tzinfo) ; self.__nanosecond ; self.__hour ; self.tzinfo ; self.__copy__ ; self.to_native ; self.__second ; self.__tzinfo ; self.now(cls,tz) ; self.__minute ; self.hour_minute_second_nanosecond ; self.__repr__(self) ; self.hour_minute_second_nanosecond(self) ; self.__eq__(self,other) ; self.nanosecond(self) ; self.__format__(self,format_spec) ; self.ticks(self) ; self.__ticks ; self.minute(self) ; self.dst(self) ; self.iso_format ; self.__ge__(self,other) ; self.__hash__(self) ; self.__ne__(self,other) ; self.tzname(self) ; self.to_native(self) ; self.__normalize_second(cls,hour,minute,second) ; self.utc_offset(self) ; self.__new ; self.utc_offset ; self.__normalize_nanosecond(cls,hour,minute,second,nanosecond) ; self.replace(self) ; self.from_native(cls,t) ; self.second(self) ; self._utc_offset(self,dt) ; \" }",
            "code": "    @classmethod\n    def from_ticks(cls, ticks, tz=None):\n        \"\"\"Create a time from ticks (nanoseconds since midnight).\n\n        :param ticks: nanoseconds since midnight\n        :type ticks: int\n        :param tz: optional timezone\n        :type tz: datetime.tzinfo\n\n        :rtype: Time\n\n        :raises ValueError: if ticks is out of bounds\n            (0 <= ticks < 86400000000000)\n        \"\"\"\n        if not isinstance(ticks, int):\n            raise TypeError(\"Ticks must be int\")\n        if 0 <= ticks < 86400000000000:\n            second, nanosecond = divmod(ticks, NANO_SECONDS)\n            minute, second = divmod(second, 60)\n            hour, minute = divmod(minute, 60)\n            return cls.__new(ticks, hour, minute, second, nanosecond, tz)\n        raise ValueError(\"Ticks out of range (0..86400000000000)\")\n",
            "dependency": "",
            "docstring": "Create a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)",
            "end_lineno": "1529",
            "file_content": "# Copyright (c) \"Neo4j\"\n# Neo4j Sweden AB [https://neo4j.com]\n#\n# This file is part of Neo4j.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     https://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\"\"\"\nThis module contains the fundamental types used for temporal accounting as well\nas a number of utility functions.\n\"\"\"\n\n\nfrom datetime import (\n    date,\n    datetime,\n    time,\n    timedelta,\n    timezone,\n)\nfrom functools import total_ordering\nfrom re import compile as re_compile\nfrom time import (\n    gmtime,\n    mktime,\n    struct_time,\n)\n\nfrom ._arithmetic import (\n    nano_add,\n    nano_div,\n    round_half_to_even,\n    symmetric_divmod,\n)\nfrom ._metaclasses import (\n    DateTimeType,\n    DateType,\n    TimeType,\n)\n\n\n__all__ = [\n    \"MIN_INT64\",\n    \"MAX_INT64\",\n    \"MIN_YEAR\",\n    \"MAX_YEAR\",\n    \"Duration\",\n    \"Date\",\n    \"ZeroDate\",\n    \"Time\",\n    \"Midnight\",\n    \"Midday\",\n    \"DateTime\",\n    \"Never\",\n    \"UnixEpoch\",\n]\n\n\nMIN_INT64 = -(2 ** 63)\nMAX_INT64 = (2 ** 63) - 1\n\n#: The smallest year number allowed in a :class:`.Date` or :class:`.DateTime`\n#: object to be compatible with :class:`datetime.date` and\n#: :class:`datetime.datetime`.\nMIN_YEAR = 1\n\n#: The largest year number allowed in a :class:`.Date` or :class:`.DateTime`\n#: object to be compatible with :class:`datetime.date` and\n#: :class:`datetime.datetime`.\nMAX_YEAR = 9999\n\nDATE_ISO_PATTERN = re_compile(r\"^(\\d{4})-(\\d{2})-(\\d{2})$\")\nTIME_ISO_PATTERN = re_compile(\n    r\"^(\\d{2})(:(\\d{2})(:((\\d{2})\"\n    r\"(\\.\\d*)?))?)?(([+-])(\\d{2}):(\\d{2})(:((\\d{2})(\\.\\d*)?))?)?$\"\n)\nDURATION_ISO_PATTERN = re_compile(\n    r\"^P((\\d+)Y)?((\\d+)M)?((\\d+)D)?\"\n    r\"(T((\\d+)H)?((\\d+)M)?(((\\d+)(\\.\\d+)?)?S)?)?$\"\n)\n\nNANO_SECONDS = 1000000000\nAVERAGE_SECONDS_IN_MONTH = 2629746\nAVERAGE_SECONDS_IN_DAY = 86400\n\n\ndef _is_leap_year(year):\n    if year % 4 != 0:\n        return False\n    if year % 100 != 0:\n        return True\n    return year % 400 == 0\n\n\nIS_LEAP_YEAR = {year: _is_leap_year(year) for year in range(MIN_YEAR, MAX_YEAR + 1)}\n\n\ndef _days_in_year(year):\n    return 366 if IS_LEAP_YEAR[year] else 365\n\n\nDAYS_IN_YEAR = {year: _days_in_year(year) for year in range(MIN_YEAR, MAX_YEAR + 1)}\n\n\ndef _days_in_month(year, month):\n    if month in (9, 4, 6, 11):\n        return 30\n    elif month != 2:\n        return 31\n    else:\n        return 29 if IS_LEAP_YEAR[year] else 28\n\n\nDAYS_IN_MONTH = {(year, month): _days_in_month(year, month)\n                 for year in range(MIN_YEAR, MAX_YEAR + 1) for month in range(1, 13)}\n\n\ndef _normalize_day(year, month, day):\n    \"\"\" Coerce the day of the month to an internal value that may or\n    may not match the \"public\" value.\n\n    With the exception of the last three days of every month, all\n    days are stored as-is. The last three days are instead stored\n    as -1 (the last), -2 (the second to last) and -3 (the third to\n    last).\n\n    Therefore, for a 28-day month, the last week is as follows:\n\n        Day   | 22 23 24 25 26 27 28\n        Value | 22 23 24 25 -3 -2 -1\n\n    For a 29-day month, the last week is as follows:\n\n        Day   | 23 24 25 26 27 28 29\n        Value | 23 24 25 26 -3 -2 -1\n\n    For a 30-day month, the last week is as follows:\n\n        Day   | 24 25 26 27 28 29 30\n        Value | 24 25 26 27 -3 -2 -1\n\n    For a 31-day month, the last week is as follows:\n\n        Day   | 25 26 27 28 29 30 31\n        Value | 25 26 27 28 -3 -2 -1\n\n    This slightly unintuitive system makes some temporal arithmetic\n    produce a more desirable outcome.\n\n    :param year:\n    :param month:\n    :param day:\n    :return:\n    \"\"\"\n    if year < MIN_YEAR or year > MAX_YEAR:\n        raise ValueError(\"Year out of range (%d..%d)\" % (MIN_YEAR, MAX_YEAR))\n    if month < 1 or month > 12:\n        raise ValueError(\"Month out of range (1..12)\")\n    days_in_month = DAYS_IN_MONTH[(year, month)]\n    if day in (days_in_month, -1):\n        return year, month, -1\n    if day in (days_in_month - 1, -2):\n        return year, month, -2\n    if day in (days_in_month - 2, -3):\n        return year, month, -3\n    if 1 <= day <= days_in_month - 3:\n        return year, month, int(day)\n    # TODO improve this error message\n    raise ValueError(\"Day %d out of range (1..%d, -1, -2 ,-3)\" % (day, days_in_month))\n\n\nclass ClockTime(tuple):\n    \"\"\" A count of `seconds` and `nanoseconds`. This class can be used to\n    mark a particular point in time, relative to an externally-specified\n    epoch.\n\n    The `seconds` and `nanoseconds` values provided to the constructor can\n    can have any sign but will be normalized internally into a positive or\n    negative `seconds` value along with a positive `nanoseconds` value\n    between `0` and `999,999,999`. Therefore ``ClockTime(-1, -1)`` is\n    normalized to ``ClockTime(-2, 999999999)``.\n\n    Note that the structure of a :class:`.ClockTime` object is similar to\n    the ``timespec`` struct in C.\n    \"\"\"\n\n    def __new__(cls, seconds=0, nanoseconds=0):\n        seconds, nanoseconds = divmod(\n            int(NANO_SECONDS * seconds) + int(nanoseconds), NANO_SECONDS\n        )\n        return tuple.__new__(cls, (seconds, nanoseconds))\n\n    def __add__(self, other):\n        if isinstance(other, (int, float)):\n            other = ClockTime(other)\n        if isinstance(other, ClockTime):\n            return ClockTime(self.seconds + other.seconds, self.nanoseconds + other.nanoseconds)\n        if isinstance(other, Duration):\n            if other.months or other.days:\n                raise ValueError(\"Cannot add Duration with months or days\")\n            return ClockTime(self.seconds + other.seconds, self.nanoseconds +\n                             int(other.nanoseconds))\n        return NotImplemented\n\n    def __sub__(self, other):\n        if isinstance(other, (int, float)):\n            other = ClockTime(other)\n        if isinstance(other, ClockTime):\n            return ClockTime(self.seconds - other.seconds, self.nanoseconds - other.nanoseconds)\n        if isinstance(other, Duration):\n            if other.months or other.days:\n                raise ValueError(\"Cannot subtract Duration with months or days\")\n            return ClockTime(self.seconds - other.seconds, self.nanoseconds - int(other.nanoseconds))\n        return NotImplemented\n\n    def __repr__(self):\n        return \"ClockTime(seconds=%r, nanoseconds=%r)\" % self\n\n    @property\n    def seconds(self):\n        return self[0]\n\n    @property\n    def nanoseconds(self):\n        return self[1]\n\n\nclass Clock:\n    \"\"\" Accessor for time values. This class is fulfilled by implementations\n    that subclass :class:`.Clock`. These implementations are contained within\n    the ``neo4j.time.clock_implementations`` module, and are not intended to be\n    accessed directly.\n\n    Creating a new :class:`.Clock` instance will produce the highest\n    precision clock implementation available.\n\n        >>> clock = Clock()\n        >>> type(clock)                                         # doctest: +SKIP\n        neo4j.time.clock_implementations.LibCClock\n        >>> clock.local_time()                                  # doctest: +SKIP\n        ClockTime(seconds=1525265942, nanoseconds=506844026)\n\n    \"\"\"\n\n    __implementations = None\n\n    def __new__(cls):\n        if cls.__implementations is None:\n            # Find an available clock with the best precision\n            import neo4j.time._clock_implementations\n            cls.__implementations = sorted((clock for clock in Clock.__subclasses__() if clock.available()),\n                                           key=lambda clock: clock.precision(), reverse=True)\n        if not cls.__implementations:\n            raise RuntimeError(\"No clock implementations available\")\n        instance = object.__new__(cls.__implementations[0])\n        return instance\n\n    @classmethod\n    def precision(cls):\n        \"\"\" The precision of this clock implementation, represented as a\n        number of decimal places. Therefore, for a nanosecond precision\n        clock, this function returns `9`.\n        \"\"\"\n        raise NotImplementedError(\"No clock implementation selected\")\n\n    @classmethod\n    def available(cls):\n        \"\"\" A boolean flag to indicate whether or not this clock\n        implementation is available on this platform.\n        \"\"\"\n        raise NotImplementedError(\"No clock implementation selected\")\n\n    @classmethod\n    def local_offset(cls):\n        \"\"\"The offset from UTC for local time read from this clock.\n        This may raise OverflowError if not supported, because of platform depending C libraries.\n\n        :returns:\n        :rtype:\n\n        :raises OverflowError:\n        \"\"\"\n        # Adding and subtracting two days to avoid passing a pre-epoch time to\n        # `mktime`, which can cause a `OverflowError` on some platforms (e.g.,\n        # Windows).\n        return ClockTime(-int(mktime(gmtime(172800))) + 172800)\n\n    def local_time(self):\n        \"\"\" Read and return the current local time from this clock, measured relative to the Unix Epoch.\n        This may raise OverflowError if not supported, because of platform depending C libraries.\n\n        :returns:\n        :rtype:\n\n        :raises OverflowError:\n        \"\"\"\n        return self.utc_time() + self.local_offset()\n\n    def utc_time(self):\n        \"\"\" Read and return the current UTC time from this clock, measured\n        relative to the Unix Epoch.\n        \"\"\"\n        raise NotImplementedError(\"No clock implementation selected\")\n\n\nclass Duration(tuple):\n    \"\"\"A difference between two points in time.\n\n    A :class:`.Duration` represents the difference between two points in time.\n    Duration objects store a composite value of `months`, `days`, `seconds`,\n    and `nanoseconds`. Unlike :class:`datetime.timedelta` however, days, and\n    seconds/nanoseconds are never interchanged. All values except seconds and\n    nanoseconds are applied separately in calculations (element-wise).\n\n    A :class:`.Duration` stores four primary instance attributes internally:\n    `months`, `days`, `seconds` and `nanoseconds`. These are maintained as\n    individual values and are immutable. Each of these four attributes can carry\n    its own sign, with the exception of `nanoseconds`, which always has the same\n    sign as `seconds`. The constructor will establish this state, should the\n    duration be initialized with conflicting `seconds` and `nanoseconds` signs.\n    This structure allows the modelling of durations such as\n    `3 months minus 2 days`.\n\n    To determine if a :class:`Duration` `d` is overflowing the accepted values\n    of the database, first, all `nanoseconds` outside the range -999_999_999 and\n    999_999_999 are transferred into the seconds field. Then, `months`, `days`,\n    and `seconds` are summed up like so:\n    `months * 2629746 + days * 86400 + d.seconds + d.nanoseconds // 1000000000`.\n    (Like the integer division in Python, this one is to be understood as\n    rounding down rather than towards 0.)\n    This value must be between -(2\\\\ :sup:`63`) and (2\\\\ :sup:`63` - 1)\n    inclusive.\n\n    :param years: will be added times 12 to `months`\n    :type years: float\n    :param months: will be truncated to :class:`int` (`int(months)`)\n    :type months: float\n    :param weeks: will be added times 7 to `days`\n    :type weeks: float\n    :param days: will be truncated to :class:`int` (`int(days)`)\n    :type days: float\n    :param hours: will be added times 3,600,000,000,000 to `nanoseconds`\n    :type hours: float\n    :param minutes: will be added times 60,000,000,000 to `nanoseconds`\n    :type minutes: float\n    :param seconds: will be added times 1,000,000,000 to `nanoseconds``\n    :type seconds: float\n    :param milliseconds: will be added times 1,000,000 to `nanoseconds`\n    :type microseconds: float\n    :param microseconds: will be added times 1,000 to `nanoseconds`\n    :type milliseconds: float\n    :param nanoseconds: will be truncated to :class:`int` (`int(nanoseconds)`)\n    :type nanoseconds: float\n\n    :raises ValueError: the components exceed the limits as described above.\n    \"\"\"\n\n    # i64: i64:i64: i32\n\n    min = None\n    \"\"\"The lowest duration value possible.\"\"\"\n\n    max = None\n    \"\"\"The highest duration value possible.\"\"\"\n\n    def __new__(cls, years=0, months=0, weeks=0, days=0, hours=0, minutes=0,\n                seconds=0, milliseconds=0, microseconds=0, nanoseconds=0):\n        mo = int(12 * years + months)\n        if mo < MIN_INT64 or mo > MAX_INT64:\n            raise ValueError(\"Months value out of range\")\n        d = int(7 * weeks + days)\n        ns = (int(3600000000000 * hours) +\n              int(60000000000 * minutes) +\n              int(1000000000 * seconds) +\n              int(1000000 * milliseconds) +\n              int(1000 * microseconds) +\n              int(nanoseconds))\n        s, ns = symmetric_divmod(ns, NANO_SECONDS)\n        avg_total_seconds = (mo * AVERAGE_SECONDS_IN_MONTH\n                             + d * AVERAGE_SECONDS_IN_DAY\n                             + s\n                             - (1 if ns < 0 else 0))\n        if avg_total_seconds < MIN_INT64 or avg_total_seconds > MAX_INT64:\n            raise ValueError(\"Duration value out of range: %r\",\n                             cls.__repr__((mo, d, s, ns)))\n        return tuple.__new__(cls, (mo, d, s, ns))\n\n    def __bool__(self):\n        \"\"\"Falsy if all primary instance attributes are.\"\"\"\n        return any(map(bool, self))\n\n    __nonzero__ = __bool__\n\n    def __add__(self, other):\n        \"\"\"Add a :class:`.Duration` or :class:`datetime.timedelta`.\n\n        :rtype: Duration\n        \"\"\"\n        if isinstance(other, Duration):\n            return Duration(\n                months=self[0] + int(other.months),\n                days=self[1] + int(other.days),\n                seconds=self[2] + int(other.seconds),\n                nanoseconds=self[3] + int(other.nanoseconds)\n            )\n        if isinstance(other, timedelta):\n            return Duration(\n                months=self[0], days=self[1] + other.days,\n                seconds=self[2] + other.seconds,\n                nanoseconds=self[3] + other.microseconds * 1000\n            )\n        return NotImplemented\n\n    def __sub__(self, other):\n        \"\"\"Subtract a :class:`.Duration` or :class:`datetime.timedelta`.\n\n        :rtype: Duration\n        \"\"\"\n        if isinstance(other, Duration):\n            return Duration(\n                months=self[0] - int(other.months),\n                days=self[1] - int(other.days),\n                seconds=self[2] - int(other.seconds),\n                nanoseconds=self[3] - int(other.nanoseconds)\n            )\n        if isinstance(other, timedelta):\n            return Duration(\n                months=self[0],\n                days=self[1] - other.days,\n                seconds=self[2] - other.seconds,\n                nanoseconds=self[3] - other.microseconds * 1000\n            )\n        return NotImplemented\n\n    def __mul__(self, other):\n        \"\"\"Multiply by an :class:`int` or :class:`float`.\n\n        The operation is performed element-wise on\n        ``(months, days, nanaoseconds)`` where\n\n        * years go into months,\n        * weeks go into days,\n        * seconds and all sub-second units go into nanoseconds.\n\n        Each element will be rounded to the nearest integer (.5 towards even).\n\n        :rtype: Duration\n        \"\"\"\n        if isinstance(other, (int, float)):\n            return Duration(\n                months=round_half_to_even(self[0] * other),\n                days=round_half_to_even(self[1] * other),\n                nanoseconds=round_half_to_even(\n                    self[2] * NANO_SECONDS * other\n                    + self[3] * other\n                )\n            )\n        return NotImplemented\n\n    def __floordiv__(self, other):\n        \"\"\"Integer division by an :class:`int`.\n\n        The operation is performed element-wise on\n        ``(months, days, nanaoseconds)`` where\n\n        * years go into months,\n        * weeks go into days,\n        * seconds and all sub-second units go into nanoseconds.\n\n        Each element will be rounded towards -inf.\n\n        :rtype: Duration\n        \"\"\"\n        if isinstance(other, int):\n            return Duration(\n                months=self[0] // other, days=self[1] // other,\n                nanoseconds=(self[2] * NANO_SECONDS + self[3]) // other\n            )\n        return NotImplemented\n\n    def __mod__(self, other):\n        \"\"\"Modulo operation by an :class:`int`.\n\n        The operation is performed element-wise on\n        ``(months, days, nanaoseconds)`` where\n\n        * years go into months,\n        * weeks go into days,\n        * seconds and all sub-second units go into nanoseconds.\n\n        :rtype: Duration\n        \"\"\"\n        if isinstance(other, int):\n            return Duration(\n                months=self[0] % other, days=self[1] % other,\n                nanoseconds=(self[2] * NANO_SECONDS + self[3]) % other\n            )\n        return NotImplemented\n\n    def __divmod__(self, other):\n        \"\"\"Division and modulo operation by an :class:`int`.\n\n        See :meth:`__floordiv__` and :meth:`__mod__`.\n\n        :rtype: (Duration, Duration)\n        \"\"\"\n        if isinstance(other, int):\n            return self.__floordiv__(other), self.__mod__(other)\n        return NotImplemented\n\n    def __truediv__(self, other):\n        \"\"\"Division by an :class:`int` or :class:`float`.\n\n        The operation is performed element-wise on\n        ``(months, days, nanaoseconds)`` where\n\n        * years go into months,\n        * weeks go into days,\n        * seconds and all sub-second units go into nanoseconds.\n\n        Each element will be rounded to the nearest integer (.5 towards even).\n\n        :rtype: Duration\n        \"\"\"\n        if isinstance(other, (int, float)):\n            return Duration(\n                months=round_half_to_even(self[0] / other),\n                days=round_half_to_even(self[1] / other),\n                nanoseconds=round_half_to_even(\n                    self[2] * NANO_SECONDS / other\n                    + self[3] / other\n                )\n            )\n        return NotImplemented\n\n    def __pos__(self):\n        \"\"\"\"\"\"\n        return self\n\n    def __neg__(self):\n        \"\"\"\"\"\"\n        return Duration(months=-self[0], days=-self[1], seconds=-self[2],\n                        nanoseconds=-self[3])\n\n    def __abs__(self):\n        \"\"\"\"\"\"\n        return Duration(months=abs(self[0]), days=abs(self[1]),\n                        seconds=abs(self[2]), nanoseconds=abs(self[3]))\n\n    def __repr__(self):\n        \"\"\"\"\"\"\n        return \"Duration(months=%r, days=%r, seconds=%r, nanoseconds=%r)\" % self\n\n    def __str__(self):\n        \"\"\"\"\"\"\n        return self.iso_format()\n\n    def __copy__(self):\n        return self.__new__(self.__class__, months=self[0], days=self[1],\n                            seconds=self[2], nanoseconds=self[3])\n\n    def __deepcopy__(self, memodict={}):\n        return self.__copy__()\n\n    @classmethod\n    def from_iso_format(cls, s):\n        \"\"\"Parse a ISO formatted duration string.\n\n        Accepted formats (all lowercase letters are placeholders):\n            'P', a zero length duration\n            'PyY', y being a number of years\n            'PmM', m being a number of months\n            'PdD', d being a number of days\n\n            Any combination of the above, e.g., 'P25Y1D' for 25 years and 1 day.\n\n            'PThH', h being a number of hours\n            'PTmM', h being a number of minutes\n            'PTsS', h being a number of seconds\n            'PTs.sss...S', h being a fractional number of seconds\n\n            Any combination of the above, e.g. 'PT5H1.2S' for 5 hours and 1.2\n            seconds.\n            Any combination of all options, e.g. 'P13MT100M' for 13 months and\n            100 minutes.\n\n        :param s: String to parse\n        :type s: str\n\n        :rtype: Duration\n\n        :raises ValueError: if the string does not match the required format.\n        \"\"\"\n        match = DURATION_ISO_PATTERN.match(s)\n        if match:\n            ns = 0\n            if match.group(15):\n                ns = int(match.group(15)[1:10].ljust(9, \"0\"))\n            return cls(\n                years=int(match.group(2) or 0),\n                months=int(match.group(4) or 0),\n                days=int(match.group(6) or 0),\n                hours=int(match.group(9) or 0),\n                minutes=int(match.group(11) or 0),\n                seconds=int(match.group(14) or 0),\n                nanoseconds=ns\n            )\n        raise ValueError(\"Duration string must be in ISO format\")\n\n    fromisoformat = from_iso_format\n\n    def iso_format(self, sep=\"T\"):\n        \"\"\"Return the :class:`Duration` as ISO formatted string.\n\n        :param sep: the separator before the time components.\n        :type sep: str\n\n        :rtype: str\n        \"\"\"\n        parts = []\n        hours, minutes, seconds, nanoseconds = \\\n            self.hours_minutes_seconds_nanoseconds\n        if hours:\n            parts.append(\"%dH\" % hours)\n        if minutes:\n            parts.append(\"%dM\" % minutes)\n        if nanoseconds:\n            if seconds >= 0 and nanoseconds >= 0:\n                parts.append(\"%d.%sS\" %\n                             (seconds,\n                              str(nanoseconds).rjust(9, \"0\").rstrip(\"0\")))\n            elif seconds <= 0 and nanoseconds <= 0:\n                parts.append(\"-%d.%sS\" %\n                             (abs(seconds),\n                              str(abs(nanoseconds)).rjust(9, \"0\").rstrip(\"0\")))\n\n            else:\n                assert False and \"Please report this issue\"\n        elif seconds:\n            parts.append(\"%dS\" % seconds)\n        if parts:\n            parts.insert(0, sep)\n        years, months, days = self.years_months_days\n        if days:\n            parts.insert(0, \"%dD\" % days)\n        if months:\n            parts.insert(0, \"%dM\" % months)\n        if years:\n            parts.insert(0, \"%dY\" % years)\n        if parts:\n            parts.insert(0, \"P\")\n            return \"\".join(parts)\n        else:\n            return \"PT0S\"\n\n    @property\n    def months(self):\n        \"\"\"The months of the :class:`Duration`.\n\n        :type: int\n        \"\"\"\n        return self[0]\n\n    @property\n    def days(self):\n        \"\"\"The days of the :class:`Duration`.\n\n        :type: int\n        \"\"\"\n        return self[1]\n\n    @property\n    def seconds(self):\n        \"\"\"The seconds of the :class:`Duration`.\n\n        :type: int\n        \"\"\"\n        return self[2]\n\n    @property\n    def nanoseconds(self):\n        \"\"\"The nanoseconds of the :class:`Duration`.\n\n        :type: int\n        \"\"\"\n        return self[3]\n\n    @property\n    def years_months_days(self):\n        \"\"\"\n\n        :return:\n        \"\"\"\n        years, months = symmetric_divmod(self[0], 12)\n        return years, months, self[1]\n\n    @property\n    def hours_minutes_seconds_nanoseconds(self):\n        \"\"\" A 4-tuple of (hours, minutes, seconds, nanoseconds).\n\n        :type: (int, int, int, int)\n        \"\"\"\n        minutes, seconds = symmetric_divmod(self[2], 60)\n        hours, minutes = symmetric_divmod(minutes, 60)\n        return hours, minutes, seconds, self[3]\n\n\nDuration.min = Duration(seconds=MIN_INT64, nanoseconds=0)\nDuration.max = Duration(seconds=MAX_INT64, nanoseconds=999999999)\n\n\nclass Date(metaclass=DateType):\n    \"\"\"Idealized date representation.\n\n    A :class:`.Date` object represents a date (year, month, and day) in the\n    `proleptic Gregorian Calendar\n    <https://en.wikipedia.org/wiki/Proleptic_Gregorian_calendar>`_.\n\n    Years between `0001` and `9999` are supported, with additional support for\n    the \"zero date\" used in some contexts.\n\n    Each date is based on a proleptic Gregorian ordinal, which models\n    1 Jan 0001 as `day 1` and counts each subsequent day up to, and including,\n    31 Dec 9999. The standard `year`, `month` and `day` value of each date is\n    also available.\n\n    Internally, the day of the month is always stored as-is, with the exception\n    of the last three days of that month. These are always stored as\n    -1, -2 and -3 (counting from the last day). This system allows some temporal\n    arithmetic (particularly adding or subtracting months) to produce a more\n    desirable outcome than would otherwise be produced. Externally, the day\n    number is always the same as would be written on a calendar.\n\n    :param year: the year. Minimum :attr:`.MIN_YEAR` (0001), maximum\n        :attr:`.MAX_YEAR` (9999).\n    :type year: int\n    :param month: the month. Minimum 1, maximum 12.\n    :type month: int\n    :param day: the day. Minimum 1, maximum\n        :attr:`Date.days_in_month(year, month) <Date.days_in_month>`.\n    :type day: int\n\n    A zero date can also be acquired by passing all zeroes to the\n    :class:`neo4j.time.Date` constructor or by using the :attr:`ZeroDate`\n    constant.\n    \"\"\"\n\n    # CONSTRUCTOR #\n\n    def __new__(cls, year, month, day):\n        if year == month == day == 0:\n            return ZeroDate\n        year, month, day = _normalize_day(year, month, day)\n        ordinal = cls.__calc_ordinal(year, month, day)\n        return cls.__new(ordinal, year, month, day)\n\n    @classmethod\n    def __new(cls, ordinal, year, month, day):\n        instance = object.__new__(cls)\n        instance.__ordinal = int(ordinal)\n        instance.__year = int(year)\n        instance.__month = int(month)\n        instance.__day = int(day)\n        return instance\n\n    def __getattr__(self, name):\n        \"\"\" Map standard library attribute names to local attribute names,\n        for compatibility.\n        \"\"\"\n        try:\n            return {\n                \"isocalendar\": self.iso_calendar,\n                \"isoformat\": self.iso_format,\n                \"isoweekday\": self.iso_weekday,\n                \"strftime\": self.__format__,\n                \"toordinal\": self.to_ordinal,\n                \"timetuple\": self.time_tuple,\n            }[name]\n        except KeyError:\n            raise AttributeError(\"Date has no attribute %r\" % name)\n\n    # CLASS METHODS #\n\n    @classmethod\n    def today(cls, tz=None):\n        \"\"\"Get the current date.\n\n        :param tz: timezone or None to get the local :class:`.Date`.\n        :type tz: datetime.tzinfo or None\n\n        :rtype: Date\n\n        :raises OverflowError: if the timestamp is out of the range of values\n            supported by the platform C localtime() function. It’s common for\n            this to be restricted to years from 1970 through 2038.\n        \"\"\"\n        if tz is None:\n            return cls.from_clock_time(Clock().local_time(), UnixEpoch)\n        else:\n            return (\n                DateTime.utc_now()\n                .replace(tzinfo=timezone.utc).astimezone(tz)\n                .date()\n            )\n\n    @classmethod\n    def utc_today(cls):\n        \"\"\"Get the current date as UTC local date.\n\n        :rtype: Date\n        \"\"\"\n        return cls.from_clock_time(Clock().utc_time(), UnixEpoch)\n\n    @classmethod\n    def from_timestamp(cls, timestamp, tz=None):\n        \"\"\":class:`.Date` from a time stamp (seconds since unix epoch).\n\n        :param timestamp: the unix timestamp (seconds since unix epoch).\n        :type timestamp: float\n        :param tz: timezone. Set to None to create a local :class:`.Date`.\n        :type tz: datetime.tzinfo or None\n\n        :rtype: Date\n\n        :raises OverflowError: if the timestamp is out of the range of values\n            supported by the platform C localtime() function. It’s common for\n            this to be restricted to years from 1970 through 2038.\n        \"\"\"\n        return cls.from_native(datetime.fromtimestamp(timestamp, tz))\n\n    @classmethod\n    def utc_from_timestamp(cls, timestamp):\n        \"\"\":class:`.Date` from a time stamp (seconds since unix epoch).\n\n        Returns the `Date` as local date `Date` in UTC.\n\n        :rtype: Date\n        \"\"\"\n        return cls.from_clock_time((timestamp, 0), UnixEpoch)\n\n    @classmethod\n    def from_ordinal(cls, ordinal):\n        \"\"\"\n        The :class:`.Date` that corresponds to the proleptic Gregorian ordinal.\n\n        `0001-01-01` has ordinal 1 and `9999-12-31` has ordinal 3,652,059.\n        Values outside of this range trigger a :exc:`ValueError`.\n        The corresponding instance method for the reverse date-to-ordinal\n        transformation is :meth:`.to_ordinal`.\n        The ordinal 0 has a special semantic and will return :attr:`ZeroDate`.\n\n        :rtype: Date\n\n        :raises ValueError: if the ordinal is outside the range [0, 3652059]\n            (both values included).\n        \"\"\"\n        if ordinal == 0:\n            return ZeroDate\n        if ordinal >= 736695:\n            year = 2018     # Project release year\n            month = 1\n            day = int(ordinal - 736694)\n        elif ordinal >= 719163:\n            year = 1970     # Unix epoch\n            month = 1\n            day = int(ordinal - 719162)\n        else:\n            year = 1\n            month = 1\n            day = int(ordinal)\n        if day < 1 or day > 3652059:\n            # Note: this requires a maximum of 22 bits for storage\n            # Could be transferred in 3 bytes.\n            raise ValueError(\"Ordinal out of range (1..3652059)\")\n        if year < MIN_YEAR or year > MAX_YEAR:\n            raise ValueError(\"Year out of range (%d..%d)\" % (MIN_YEAR, MAX_YEAR))\n        days_in_year = DAYS_IN_YEAR[year]\n        while day > days_in_year:\n            day -= days_in_year\n            year += 1\n            days_in_year = DAYS_IN_YEAR[year]\n        days_in_month = DAYS_IN_MONTH[(year, month)]\n        while day > days_in_month:\n            day -= days_in_month\n            month += 1\n            days_in_month = DAYS_IN_MONTH[(year, month)]\n        year, month, day = _normalize_day(year, month, day)\n        return cls.__new(ordinal, year, month, day)\n\n    @classmethod\n    def parse(cls, s):\n        \"\"\"Parse a string to produce a :class:`.Date`.\n\n        Accepted formats:\n            'Y-M-D'\n\n        :param s: the string to be parsed.\n        :type s: str\n\n        :rtype: Date\n\n        :raises ValueError: if the string could not be parsed.\n        \"\"\"\n        try:\n            numbers = map(int, s.split(\"-\"))\n        except (ValueError, AttributeError):\n            raise ValueError(\"Date string must be in format YYYY-MM-DD\")\n        else:\n            numbers = list(numbers)\n            if len(numbers) == 3:\n                return cls(*numbers)\n            raise ValueError(\"Date string must be in format YYYY-MM-DD\")\n\n    @classmethod\n    def from_iso_format(cls, s):\n        \"\"\"Parse a ISO formatted Date string.\n\n        Accepted formats:\n            'YYYY-MM-DD'\n\n        :param s: the string to be parsed.\n        :type s: str\n\n        :rtype: Date\n\n        :raises ValueError: if the string could not be parsed.\n        \"\"\"\n        m = DATE_ISO_PATTERN.match(s)\n        if m:\n            year = int(m.group(1))\n            month = int(m.group(2))\n            day = int(m.group(3))\n            return cls(year, month, day)\n        raise ValueError(\"Date string must be in format YYYY-MM-DD\")\n\n    @classmethod\n    def from_native(cls, d):\n        \"\"\"Convert from a native Python `datetime.date` value.\n\n        :param d: the date to convert.\n        :type d: datetime.date\n\n        :rtype: Date\n        \"\"\"\n        return Date.from_ordinal(d.toordinal())\n\n    @classmethod\n    def from_clock_time(cls, clock_time, epoch):\n        \"\"\"Convert from a ClockTime relative to a given epoch.\n\n        :param clock_time: the clock time as :class:`.ClockTime` or as tuple of\n            (seconds, nanoseconds)\n        :type clock_time: ClockTime or (float, int)\n        :param epoch: the epoch to which `clock_time` is relative\n        :type epoch: DateTime\n\n        :rtype: Date\n        \"\"\"\n        try:\n            clock_time = ClockTime(*clock_time)\n        except (TypeError, ValueError):\n            raise ValueError(\"Clock time must be a 2-tuple of (s, ns)\")\n        else:\n            ordinal = clock_time.seconds // 86400\n            return Date.from_ordinal(ordinal + epoch.date().to_ordinal())\n\n    @classmethod\n    def is_leap_year(cls, year):\n        \"\"\"Indicates whether or not `year` is a leap year.\n\n        :param year: the year to look up\n        :type year: int\n\n        :rtype: bool\n\n        :raises ValueError: if `year` is out of range:\n            :attr:`MIN_YEAR` <= year <= :attr:`MAX_YEAR`\n        \"\"\"\n        if year < MIN_YEAR or year > MAX_YEAR:\n            raise ValueError(\"Year out of range (%d..%d)\" % (MIN_YEAR, MAX_YEAR))\n        return IS_LEAP_YEAR[year]\n\n    @classmethod\n    def days_in_year(cls, year):\n        \"\"\"Return the number of days in `year`.\n\n        :param year: the year to look up\n        :type year: int\n\n        :rtype: int\n\n        :raises ValueError: if `year` is out of range:\n            :attr:`MIN_YEAR` <= year <= :attr:`MAX_YEAR`\n        \"\"\"\n        if year < MIN_YEAR or year > MAX_YEAR:\n            raise ValueError(\"Year out of range (%d..%d)\" % (MIN_YEAR, MAX_YEAR))\n        return DAYS_IN_YEAR[year]\n\n    @classmethod\n    def days_in_month(cls, year, month):\n        \"\"\"Return the number of days in `month` of `year`.\n\n        :param year: the year to look up\n        :type year: int\n        :param year: the month to look up\n        :type year: int\n\n        :rtype: int\n\n        :raises ValueError: if `year` or `month` is out of range:\n            :attr:`MIN_YEAR` <= year <= :attr:`MAX_YEAR`;\n            1 <= year <= 12\n        \"\"\"\n        if year < MIN_YEAR or year > MAX_YEAR:\n            raise ValueError(\"Year out of range (%d..%d)\" % (MIN_YEAR, MAX_YEAR))\n        if month < 1 or month > 12:\n            raise ValueError(\"Month out of range (1..12)\")\n        return DAYS_IN_MONTH[(year, month)]\n\n    @classmethod\n    def __calc_ordinal(cls, year, month, day):\n        if day < 0:\n            day = cls.days_in_month(year, month) + int(day) + 1\n        # The built-in date class does this faster than a\n        # long-hand pure Python algorithm could\n        return date(year, month, day).toordinal()\n\n    # CLASS ATTRIBUTES #\n\n    min = None\n    \"\"\"The earliest date value possible.\"\"\"\n\n    max = None\n    \"\"\"The latest date value possible.\"\"\"\n\n    resolution = None\n    \"\"\"The minimum resolution supported.\"\"\"\n\n    # INSTANCE ATTRIBUTES #\n\n    __ordinal = 0\n\n    __year = 0\n\n    __month = 0\n\n    __day = 0\n\n    @property\n    def year(self):\n        \"\"\"The year of the date.\n\n        :type: int\n        \"\"\"\n        return self.__year\n\n    @property\n    def month(self):\n        \"\"\"The month of the date.\n\n        :type: int\n        \"\"\"\n        return self.__month\n\n    @property\n    def day(self):\n        \"\"\"The day of the date.\n\n        :type: int\n        \"\"\"\n        if self.__day == 0:\n            return 0\n        if self.__day >= 1:\n            return self.__day\n        return self.days_in_month(self.__year, self.__month) + self.__day + 1\n\n    @property\n    def year_month_day(self):\n        \"\"\"3-tuple of (year, month, day) describing the date.\n\n        :rtype: (int, int, int)\n        \"\"\"\n        return self.year, self.month, self.day\n\n    @property\n    def year_week_day(self):\n        \"\"\"3-tuple of (year, week_of_year, day_of_week) describing the date.\n\n        `day_of_week` will be 1 for Monday and 7 for Sunday.\n\n        :rtype: (int, int, int)\n        \"\"\"\n        ordinal = self.__ordinal\n        year = self.__year\n\n        def day_of_week(o):\n            return ((o - 1) % 7) + 1\n\n        def iso_week_1(y):\n            j4 = Date(y, 1, 4)\n            return j4 + Duration(days=(1 - day_of_week(j4.to_ordinal())))\n\n        if ordinal >= Date(year, 12, 29).to_ordinal():\n            week1 = iso_week_1(year + 1)\n            if ordinal < week1.to_ordinal():\n                week1 = iso_week_1(year)\n            else:\n                year += 1\n        else:\n            week1 = iso_week_1(year)\n            if ordinal < week1.to_ordinal():\n                year -= 1\n                week1 = iso_week_1(year)\n        return (year, int((ordinal - week1.to_ordinal()) / 7 + 1),\n                day_of_week(ordinal))\n\n    @property\n    def year_day(self):\n        \"\"\"2-tuple of (year, day_of_the_year) describing the date.\n\n        This is the number of the day relative to the start of the year,\n        with `1 Jan` corresponding to `1`.\n\n        :rtype: (int, int)\n        \"\"\"\n        return (self.__year,\n                self.toordinal() - Date(self.__year, 1, 1).toordinal() + 1)\n\n    # OPERATIONS #\n\n    def __hash__(self):\n        \"\"\"\"\"\"\n        return hash(self.toordinal())\n\n    def __eq__(self, other):\n        \"\"\"`==` comparison with :class:`.Date` or :class:`datetime.date`.\"\"\"\n        if isinstance(other, (Date, date)):\n            return self.toordinal() == other.toordinal()\n        return False\n\n    def __ne__(self, other):\n        \"\"\"`!=` comparison with :class:`.Date` or :class:`datetime.date`.\"\"\"\n        return not self.__eq__(other)\n\n    def __lt__(self, other):\n        \"\"\"`<` comparison with :class:`.Date` or :class:`datetime.date`.\"\"\"\n        if isinstance(other, (Date, date)):\n            return self.toordinal() < other.toordinal()\n        raise TypeError(\"'<' not supported between instances of 'Date' and %r\" % type(other).__name__)\n\n    def __le__(self, other):\n        \"\"\"`<=` comparison with :class:`.Date` or :class:`datetime.date`.\"\"\"\n        if isinstance(other, (Date, date)):\n            return self.toordinal() <= other.toordinal()\n        raise TypeError(\"'<=' not supported between instances of 'Date' and %r\" % type(other).__name__)\n\n    def __ge__(self, other):\n        \"\"\"`>=` comparison with :class:`.Date` or :class:`datetime.date`.\"\"\"\n        if isinstance(other, (Date, date)):\n            return self.toordinal() >= other.toordinal()\n        raise TypeError(\"'>=' not supported between instances of 'Date' and %r\" % type(other).__name__)\n\n    def __gt__(self, other):\n        \"\"\"`>` comparison with :class:`.Date` or :class:`datetime.date`.\"\"\"\n        if isinstance(other, (Date, date)):\n            return self.toordinal() > other.toordinal()\n        raise TypeError(\"'>' not supported between instances of 'Date' and %r\" % type(other).__name__)\n\n    def __add__(self, other):\n        \"\"\"Add a :class:`.Duration`.\n\n        :rtype: Date\n\n        :raises ValueError: if the added duration has a time component.\n        \"\"\"\n        def add_months(d, months):\n            years, months = symmetric_divmod(months, 12)\n            year = d.__year + years\n            month = d.__month + months\n            while month > 12:\n                year += 1\n                month -= 12\n            while month < 1:\n                year -= 1\n                month += 12\n            d.__year = year\n            d.__month = month\n\n        def add_days(d, days):\n            assert 1 <= d.__day <= 28 or -28 <= d.__day <= -1\n            if d.__day >= 1:\n                new_days = d.__day + days\n                if 1 <= new_days <= 27:\n                    d.__day = new_days\n                    return\n            d0 = Date.from_ordinal(d.__ordinal + days)\n            d.__year, d.__month, d.__day = d0.__year, d0.__month, d0.__day\n\n        if isinstance(other, Duration):\n            if other.seconds or other.nanoseconds:\n                raise ValueError(\"Cannot add a Duration with seconds or \"\n                                 \"nanoseconds to a Date\")\n            if other.months == other.days == 0:\n                return self\n            new_date = self.replace()\n            # Add days before months as the former sometimes\n            # requires the current ordinal to be correct.\n            if other.days:\n                add_days(new_date, other.days)\n            if other.months:\n                add_months(new_date, other.months)\n            new_date.__ordinal = self.__calc_ordinal(new_date.year, new_date.month, new_date.day)\n            return new_date\n        return NotImplemented\n\n    def __sub__(self, other):\n        \"\"\"Subtract a :class:`.Date` or :class:`.Duration`.\n\n        :returns: If a :class:`.Date` is subtracted, the time between the two\n            dates is returned as :class:`.Duration`. If a :class:`.Duration` is\n            subtracted, a new :class:`.Date` is returned.\n        :rtype: Date or Duration\n\n        :raises ValueError: if the added duration has a time component.\n        \"\"\"\n        if isinstance(other, (Date, date)):\n            return Duration(days=(self.toordinal() - other.toordinal()))\n        try:\n            return self.__add__(-other)\n        except TypeError:\n            return NotImplemented\n\n    def __copy__(self):\n        return self.__new(self.__ordinal, self.__year, self.__month, self.__day)\n\n    def __deepcopy__(self, *args, **kwargs):\n        return self.__copy__()\n\n    # INSTANCE METHODS #\n\n    def replace(self, **kwargs):\n        \"\"\"Return a :class:`.Date` with one or more components replaced.\n\n        :Keyword Arguments:\n           * **year** (`int`): overwrite the year -\n             default: `self.year`\n           * **month** (`int`): overwrite the month -\n             default: `self.month`\n           * **day** (`int`): overwrite the day -\n             default: `self.day`\n        \"\"\"\n        return Date(kwargs.get(\"year\", self.__year),\n                    kwargs.get(\"month\", self.__month),\n                    kwargs.get(\"day\", self.__day))\n\n    def time_tuple(self):\n        \"\"\"Convert the date to :class:`time.struct_time`.\n\n        :rtype: time.struct_time\n        \"\"\"\n        _, _, day_of_week = self.year_week_day\n        _, day_of_year = self.year_day\n        return struct_time((self.year, self.month, self.day, 0, 0, 0, day_of_week - 1, day_of_year, -1))\n\n    def to_ordinal(self):\n        \"\"\"The date's proleptic Gregorian ordinal.\n\n        The corresponding class method for the reverse ordinal-to-date\n        transformation is :meth:`.Date.from_ordinal`.\n\n        :rtype: int\n        \"\"\"\n        return self.__ordinal\n\n    def to_clock_time(self, epoch):\n        \"\"\"Convert the date to :class:`ClockTime` relative to `epoch`.\n\n        :param epoch: the epoch to which the date is relative\n        :type epoch: Date\n\n        :rtype: ClockTime\n        \"\"\"\n        try:\n            return ClockTime(86400 * (self.to_ordinal() - epoch.to_ordinal()))\n        except AttributeError:\n            raise TypeError(\"Epoch has no ordinal value\")\n\n    def to_native(self):\n        \"\"\"Convert to a native Python :class:`datetime.date` value.\n\n        :rtype: datetime.date\n        \"\"\"\n        return date.fromordinal(self.to_ordinal())\n\n    def weekday(self):\n        \"\"\"The day of the week where Monday is 0 and Sunday is 6.\n\n        :rtype: int\n        \"\"\"\n        return self.year_week_day[2] - 1\n\n    def iso_weekday(self):\n        \"\"\"The day of the week where Monday is 1 and Sunday is 7.\n\n        :rtype: int\n        \"\"\"\n        return self.year_week_day[2]\n\n    def iso_calendar(self):\n        \"\"\"Alias for :attr:`.year_week_day`\"\"\"\n        return self.year_week_day\n\n    def iso_format(self):\n        \"\"\"Return the :class:`.Date` as ISO formatted string.\n\n        :rtype: str\n        \"\"\"\n        if self.__ordinal == 0:\n            return \"0000-00-00\"\n        return \"%04d-%02d-%02d\" % self.year_month_day\n\n    def __repr__(self):\n        \"\"\"\"\"\"\n        if self.__ordinal == 0:\n            return \"neo4j.time.ZeroDate\"\n        return \"neo4j.time.Date(%r, %r, %r)\" % self.year_month_day\n\n    def __str__(self):\n        \"\"\"\"\"\"\n        return self.iso_format()\n\n    def __format__(self, format_spec):\n        \"\"\"\"\"\"\n        raise NotImplementedError()\n\n\nDate.min = Date.from_ordinal(1)\nDate.max = Date.from_ordinal(3652059)\nDate.resolution = Duration(days=1)\n\n#: A :class:`neo4j.time.Date` instance set to `0000-00-00`.\n#: This has an ordinal value of `0`.\nZeroDate = object.__new__(Date)\n\n\nclass Time(metaclass=TimeType):\n    \"\"\"Time of day.\n\n    The :class:`.Time` class is a nanosecond-precision drop-in replacement for\n    the standard library :class:`datetime.time` class.\n\n    A high degree of API compatibility with the standard library classes is\n    provided.\n\n    :class:`neo4j.time.Time` objects introduce the concept of ``ticks``.\n    This is simply a count of the number of nanoseconds since midnight,\n    in many ways analogous to the :class:`neo4j.time.Date` ordinal.\n    `ticks` values are integers, with a minimum value of `0` and a maximum\n    of `86_399_999_999_999`.\n\n    Local times are represented by :class:`.Time` with no ``tzinfo``.\n\n    :param hour: the hour of the time. Must be in range 0 <= hour < 24.\n    :type hour: int\n    :param minute: the minute of the time. Must be in range 0 <= minute < 60.\n    :type minute: int\n    :param second: the second of the time. Must be in range 0 <= second < 60.\n    :type second: int\n    :param nanosecond: the nanosecond of the time.\n        Must be in range 0 <= nanosecond < 999999999.\n    :type nanosecond: int\n    :param tzinfo: timezone or None to get a local :class:`.Time`.\n    :type tzinfo: datetime.tzinfo or None\n\n    :raises ValueError: if one of the parameters is out of range.\n    \"\"\"\n\n    # CONSTRUCTOR #\n\n    def __new__(cls, hour=0, minute=0, second=0, nanosecond=0, tzinfo=None):\n        hour, minute, second, nanosecond = cls.__normalize_nanosecond(\n            hour, minute, second, nanosecond\n        )\n        ticks = (3600000000000 * hour\n                 + 60000000000 * minute\n                 + 1000000000 * second\n                 + nanosecond)\n        return cls.__new(ticks, hour, minute, second, nanosecond, tzinfo)\n\n    @classmethod\n    def __new(cls, ticks, hour, minute, second, nanosecond, tzinfo):\n        instance = object.__new__(cls)\n        instance.__ticks = int(ticks)\n        instance.__hour = int(hour)\n        instance.__minute = int(minute)\n        instance.__second = int(second)\n        instance.__nanosecond = int(nanosecond)\n        instance.__tzinfo = tzinfo\n        return instance\n\n    def __getattr__(self, name):\n        \"\"\"Map standard library attribute names to local attribute names,\n        for compatibility.\n        \"\"\"\n        try:\n            return {\n                \"isoformat\": self.iso_format,\n                \"utcoffset\": self.utc_offset,\n            }[name]\n        except KeyError:\n            raise AttributeError(\"Date has no attribute %r\" % name)\n\n    # CLASS METHODS #\n\n    @classmethod\n    def now(cls, tz=None):\n        \"\"\"Get the current time.\n\n        :param tz: optional timezone\n        :type tz: datetime.tzinfo\n        :rtype: Time\n\n        :raises OverflowError: if the timestamp is out of the range of values\n            supported by the platform C localtime() function. It’s common for\n            this to be restricted to years from 1970 through 2038.\n        \"\"\"\n        if tz is None:\n            return cls.from_clock_time(Clock().local_time(), UnixEpoch)\n        else:\n            return (\n                DateTime.utc_now()\n                .replace(tzinfo=timezone.utc).astimezone(tz)\n                .timetz()\n            )\n\n    @classmethod\n    def utc_now(cls):\n        \"\"\"Get the current time as UTC local time.\n\n        :rtype: Time\n        \"\"\"\n        return cls.from_clock_time(Clock().utc_time(), UnixEpoch)\n\n    @classmethod\n    def from_iso_format(cls, s):\n        \"\"\"Parse a ISO formatted time string.\n\n        Accepted formats:\n            Local times:\n                'hh'\n                'hh:mm'\n                'hh:mm:ss'\n                'hh:mm:ss.ssss...'\n            Times with timezones (UTC offset):\n                '<local time>+hh:mm'\n                '<local time>+hh:mm:ss'\n                '<local time>+hh:mm:ss.ssss....'\n                '<local time>-hh:mm'\n                '<local time>-hh:mm:ss'\n                '<local time>-hh:mm:ss.ssss....'\n\n                Where the UTC offset will only respect hours and minutes.\n                Seconds and sub-seconds are ignored.\n\n        :param s: String to parse\n        :type s: str\n\n        :rtype: Time\n\n        :raises ValueError: if the string does not match the required format.\n        \"\"\"\n        from pytz import FixedOffset\n        m = TIME_ISO_PATTERN.match(s)\n        if m:\n            hour = int(m.group(1))\n            minute = int(m.group(3) or 0)\n            second = int(m.group(6) or 0)\n            nanosecond = m.group(7)\n            if nanosecond:\n                nanosecond = int(nanosecond[1:10].ljust(9, \"0\"))\n            else:\n                nanosecond = 0\n            if m.group(8) is None:\n                return cls(hour, minute, second, nanosecond)\n            else:\n                offset_multiplier = 1 if m.group(9) == \"+\" else -1\n                offset_hour = int(m.group(10))\n                offset_minute = int(m.group(11))\n                # pytz only supports offsets of minute resolution\n                # so we can ignore this part\n                # offset_second = float(m.group(13) or 0.0)\n                offset = 60 * offset_hour + offset_minute\n                return cls(hour, minute, second, nanosecond,\n                           tzinfo=FixedOffset(offset_multiplier * offset))\n        raise ValueError(\"Time string is not in ISO format\")\n\n    @classmethod\n    def from_ticks(cls, ticks, tz=None):\n        \"\"\"Create a time from ticks (nanoseconds since midnight).\n\n        :param ticks: nanoseconds since midnight\n        :type ticks: int\n        :param tz: optional timezone\n        :type tz: datetime.tzinfo\n\n        :rtype: Time\n\n        :raises ValueError: if ticks is out of bounds\n            (0 <= ticks < 86400000000000)\n        \"\"\"\n        if not isinstance(ticks, int):\n            raise TypeError(\"Ticks must be int\")\n        if 0 <= ticks < 86400000000000:\n            second, nanosecond = divmod(ticks, NANO_SECONDS)\n            minute, second = divmod(second, 60)\n            hour, minute = divmod(minute, 60)\n            return cls.__new(ticks, hour, minute, second, nanosecond, tz)\n        raise ValueError(\"Ticks out of range (0..86400000000000)\")\n\n    @classmethod\n    def from_native(cls, t):\n        \"\"\"Convert from a native Python :class:`datetime.time` value.\n\n        :param t: time to convert from\n        :type t: datetime.time\n\n        :rtype: Time\n        \"\"\"\n        nanosecond = t.microsecond * 1000\n        return Time(t.hour, t.minute, t.second, nanosecond, t.tzinfo)\n\n    @classmethod\n    def from_clock_time(cls, clock_time, epoch):\n        \"\"\"Convert from a :class:`.ClockTime` relative to a given epoch.\n\n        This method, in contrast to most others of this package, assumes days of\n        exactly 24 hours.\n\n        :param clock_time: the clock time as :class:`.ClockTime` or as tuple of\n            (seconds, nanoseconds)\n        :type clock_time: ClockTime or (float, int)\n        :param epoch: the epoch to which `clock_time` is relative\n        :type epoch: DateTime\n\n        :rtype: Time\n        \"\"\"\n        clock_time = ClockTime(*clock_time)\n        ts = clock_time.seconds % 86400\n        nanoseconds = int(NANO_SECONDS * ts + clock_time.nanoseconds)\n        ticks = (epoch.time().ticks + nanoseconds) % (86400 * NANO_SECONDS)\n        return Time.from_ticks(ticks)\n\n    @classmethod\n    def __normalize_hour(cls, hour):\n        hour = int(hour)\n        if 0 <= hour < 24:\n            return hour\n        raise ValueError(\"Hour out of range (0..23)\")\n\n    @classmethod\n    def __normalize_minute(cls, hour, minute):\n        hour = cls.__normalize_hour(hour)\n        minute = int(minute)\n        if 0 <= minute < 60:\n            return hour, minute\n        raise ValueError(\"Minute out of range (0..59)\")\n\n    @classmethod\n    def __normalize_second(cls, hour, minute, second):\n        hour, minute = cls.__normalize_minute(hour, minute)\n        second = int(second)\n        if 0 <= second < 60:\n            return hour, minute, second\n        raise ValueError(\"Second out of range (0..59)\")\n\n    @classmethod\n    def __normalize_nanosecond(cls, hour, minute, second, nanosecond):\n        hour, minute, second = cls.__normalize_second(hour, minute, second)\n        if 0 <= nanosecond < NANO_SECONDS:\n            return hour, minute, second, nanosecond\n        raise ValueError(\"Nanosecond out of range (0..%s)\" % (NANO_SECONDS - 1))\n\n    # CLASS ATTRIBUTES #\n\n    min = None\n    \"\"\"The earliest time value possible.\"\"\"\n\n    max = None\n    \"\"\"The latest time value possible.\"\"\"\n\n    resolution = None\n    \"\"\"The minimum resolution supported.\"\"\"\n\n    # INSTANCE ATTRIBUTES #\n\n    __ticks = 0\n\n    __hour = 0\n\n    __minute = 0\n\n    __second = 0\n\n    __nanosecond = 0\n\n    __tzinfo = None\n\n    @property\n    def ticks(self):\n        \"\"\"The total number of nanoseconds since midnight.\n\n        :type: int\n        \"\"\"\n        return self.__ticks\n\n    @property\n    def hour(self):\n        \"\"\"The hours of the time.\n\n        :type: int\n        \"\"\"\n        return self.__hour\n\n    @property\n    def minute(self):\n        \"\"\"The minutes of the time.\n\n        :type: int\n        \"\"\"\n        return self.__minute\n\n    @property\n    def second(self):\n        \"\"\"The seconds of the time.\n\n        :type: int\n        \"\"\"\n        return self.__second\n\n    @property\n    def nanosecond(self):\n        \"\"\"The nanoseconds of the time.\n\n        :type: int\n        \"\"\"\n        return self.__nanosecond\n\n    @property\n    def hour_minute_second_nanosecond(self):\n        \"\"\"The time as a tuple of (hour, minute, second, nanosecond).\n\n        :type: (int, int, int, int)\"\"\"\n        return self.__hour, self.__minute, self.__second, self.__nanosecond\n\n    @property\n    def tzinfo(self):\n        \"\"\"The timezone of this time.\n\n        :type: datetime.tzinfo or None\"\"\"\n        return self.__tzinfo\n\n    # OPERATIONS #\n\n    def _get_both_normalized_ticks(self, other, strict=True):\n        if (isinstance(other, (time, Time))\n                and ((self.utc_offset() is None)\n                     ^ (other.utcoffset() is None))):\n            if strict:\n                raise TypeError(\"can't compare offset-naive and offset-aware \"\n                                \"times\")\n            else:\n                return None, None\n        if isinstance(other, Time):\n            other_ticks = other.__ticks\n        elif isinstance(other, time):\n            other_ticks = int(3600000000000 * other.hour\n                              + 60000000000 * other.minute\n                              + NANO_SECONDS * other.second\n                              + 1000 * other.microsecond)\n        else:\n            return None, None\n        utc_offset = other.utcoffset()\n        if utc_offset is not None:\n            other_ticks -= utc_offset.total_seconds() * NANO_SECONDS\n        self_ticks = self.__ticks\n        utc_offset = self.utc_offset()\n        if utc_offset is not None:\n            self_ticks -= utc_offset.total_seconds() * NANO_SECONDS\n        return self_ticks, other_ticks\n\n    def __hash__(self):\n        \"\"\"\"\"\"\n        if self.__nanosecond % 1000 == 0:\n            return hash(self.to_native())\n        self_ticks = self.__ticks\n        if self.utc_offset() is not None:\n            self_ticks -= self.utc_offset().total_seconds() * NANO_SECONDS\n        return hash(self_ticks)\n\n    def __eq__(self, other):\n        \"\"\"`==` comparison with :class:`.Time` or :class:`datetime.time`.\"\"\"\n        self_ticks, other_ticks = self._get_both_normalized_ticks(other,\n                                                                  strict=False)\n        if self_ticks is None:\n            return False\n        return self_ticks == other_ticks\n\n    def __ne__(self, other):\n        \"\"\"`!=` comparison with :class:`.Time` or :class:`datetime.time`.\"\"\"\n        return not self.__eq__(other)\n\n    def __lt__(self, other):\n        \"\"\"`<` comparison with :class:`.Time` or :class:`datetime.time`.\"\"\"\n        self_ticks, other_ticks = self._get_both_normalized_ticks(other)\n        if self_ticks is None:\n            return NotImplemented\n        return self_ticks < other_ticks\n\n    def __le__(self, other):\n        \"\"\"`<=` comparison with :class:`.Time` or :class:`datetime.time`.\"\"\"\n        self_ticks, other_ticks = self._get_both_normalized_ticks(other)\n        if self_ticks is None:\n            return NotImplemented\n        return self_ticks <= other_ticks\n\n    def __ge__(self, other):\n        \"\"\"`>=` comparison with :class:`.Time` or :class:`datetime.time`.\"\"\"\n        self_ticks, other_ticks = self._get_both_normalized_ticks(other)\n        if self_ticks is None:\n            return NotImplemented\n        return self_ticks >= other_ticks\n\n    def __gt__(self, other):\n        \"\"\"`>` comparison with :class:`.Time` or :class:`datetime.time`.\"\"\"\n        self_ticks, other_ticks = self._get_both_normalized_ticks(other)\n        if self_ticks is None:\n            return NotImplemented\n        return self_ticks > other_ticks\n\n    def __copy__(self):\n        return self.__new(self.__ticks, self.__hour, self.__minute,\n                          self.__second, self.__nanosecond, self.__tzinfo)\n\n    def __deepcopy__(self, *args, **kwargs):\n        return self.__copy__()\n\n    # INSTANCE METHODS #\n\n    def replace(self, **kwargs):\n        \"\"\"Return a :class:`.Time` with one or more components replaced.\n\n        :Keyword Arguments:\n           * **hour** (`int`): overwrite the hour -\n             default: `self.hour`\n           * **minute** (`int`): overwrite the minute -\n             default: `self.minute`\n           * **second** (`int`): overwrite the second -\n             default: `int(self.second)`\n           * **nanosecond** (`int`): overwrite the nanosecond -\n             default: `self.nanosecond`\n           * **tzinfo** (`datetime.tzinfo` or `None`): overwrite the timezone -\n             default: `self.tzinfo`\n\n        :rtype: Time\n        \"\"\"\n        return Time(hour=kwargs.get(\"hour\", self.__hour),\n                    minute=kwargs.get(\"minute\", self.__minute),\n                    second=kwargs.get(\"second\", self.__second),\n                    nanosecond=kwargs.get(\"nanosecond\", self.__nanosecond),\n                    tzinfo=kwargs.get(\"tzinfo\", self.__tzinfo))\n\n    def _utc_offset(self, dt=None):\n        if self.tzinfo is None:\n            return None\n        try:\n            value = self.tzinfo.utcoffset(dt)\n        except TypeError:\n            # For timezone implementations not compatible with the custom\n            # datetime implementations, we can't do better than this.\n            value = self.tzinfo.utcoffset(dt.to_native())\n        if value is None:\n            return None\n        if isinstance(value, timedelta):\n            s = value.total_seconds()\n            if not (-86400 < s < 86400):\n                raise ValueError(\"utcoffset must be less than a day\")\n            if s % 60 != 0 or value.microseconds != 0:\n                raise ValueError(\"utcoffset must be a whole number of minutes\")\n            return value\n        raise TypeError(\"utcoffset must be a timedelta\")\n\n    def utc_offset(self):\n        \"\"\"Return the UTC offset of this time.\n\n        :return: None if this is a local time (:attr:`.tzinfo` is None), else\n            returns `self.tzinfo.utcoffset(self)`.\n        :rtype: datetime.timedelta\n\n        :raises ValueError: if `self.tzinfo.utcoffset(self)` is not None and a\n            :class:`timedelta` with a magnitude greater equal 1 day or that is\n            not a whole number of minutes.\n        :raises TypeError: if `self.tzinfo.utcoffset(self)` does return anything but\n            None or a :class:`datetime.timedelta`.\n        \"\"\"\n        return self._utc_offset()\n\n    def dst(self):\n        \"\"\"Get the daylight saving time adjustment (DST).\n\n        :return: None if this is a local time (:attr:`.tzinfo` is None), else\n            returns `self.tzinfo.dst(self)`.\n        :rtype: datetime.timedelta\n\n        :raises ValueError: if `self.tzinfo.dst(self)` is not None and a\n            :class:`timedelta` with a magnitude greater equal 1 day or that is\n            not a whole number of minutes.\n        :raises TypeError: if `self.tzinfo.dst(self)` does return anything but\n            None or a :class:`datetime.timedelta`.\n        \"\"\"\n        if self.tzinfo is None:\n            return None\n        try:\n            value = self.tzinfo.dst(self)\n        except TypeError:\n            # For timezone implementations not compatible with the custom\n            # datetime implementations, we can't do better than this.\n            value = self.tzinfo.dst(self.to_native())\n        if value is None:\n            return None\n        if isinstance(value, timedelta):\n            if value.days != 0:\n                raise ValueError(\"dst must be less than a day\")\n            if value.seconds % 60 != 0 or value.microseconds != 0:\n                raise ValueError(\"dst must be a whole number of minutes\")\n            return value\n        raise TypeError(\"dst must be a timedelta\")\n\n    def tzname(self):\n        \"\"\"Get the name of the :class:`.Time`'s timezone.\n\n        :returns: None if the time is local (i.e., has no timezone), else return\n            `self.tzinfo.tzname(self)`\n\n        :rtype: str or None\n        \"\"\"\n        if self.tzinfo is None:\n            return None\n        try:\n            return self.tzinfo.tzname(self)\n        except TypeError:\n            # For timezone implementations not compatible with the custom\n            # datetime implementations, we can't do better than this.\n            return self.tzinfo.tzname(self.to_native())\n\n    def to_clock_time(self):\n        \"\"\"Convert to :class:`.ClockTime`.\n\n        :rtype: ClockTime\n        \"\"\"\n        seconds, nanoseconds = divmod(self.ticks, NANO_SECONDS)\n        return ClockTime(seconds, nanoseconds)\n\n    def to_native(self):\n        \"\"\"Convert to a native Python `datetime.time` value.\n\n        This conversion is lossy as the native time implementation only supports\n        a resolution of microseconds instead of nanoseconds.\n\n        :rtype: datetime.time\n        \"\"\"\n        h, m, s, ns = self.hour_minute_second_nanosecond\n        µs = round_half_to_even(ns / 1000)\n        tz = self.tzinfo\n        return time(h, m, s, µs, tz)\n\n    def iso_format(self):\n        \"\"\"Return the :class:`.Time` as ISO formatted string.\n\n        :rtype: str\n        \"\"\"\n        s = \"%02d:%02d:%02d.%09d\" % self.hour_minute_second_nanosecond\n        offset = self.utc_offset()\n        if offset is not None:\n            s += \"%+03d:%02d\" % divmod(offset.total_seconds() // 60, 60)\n        return s\n\n    def __repr__(self):\n        \"\"\"\"\"\"\n        if self.tzinfo is None:\n            return \"neo4j.time.Time(%r, %r, %r, %r)\" % \\\n                   self.hour_minute_second_nanosecond\n        else:\n            return \"neo4j.time.Time(%r, %r, %r, %r, tzinfo=%r)\" % \\\n                   (self.hour_minute_second_nanosecond + (self.tzinfo,))\n\n    def __str__(self):\n        \"\"\"\"\"\"\n        return self.iso_format()\n\n    def __format__(self, format_spec):\n        \"\"\"\"\"\"\n        raise NotImplementedError()\n\n\nTime.min = Time(hour=0, minute=0, second=0, nanosecond=0)\nTime.max = Time(hour=23, minute=59, second=59, nanosecond=999999999)\nTime.resolution = Duration(nanoseconds=1)\n\n#: A :class:`.Time` instance set to `00:00:00`.\n#: This has a :attr:`.ticks` value of `0`.\nMidnight = Time.min\n\n#: A :class:`.Time` instance set to `12:00:00`.\n#: This has a :attr:`.ticks` value of `43200000000000`.\nMidday = Time(hour=12)\n\n\n@total_ordering\nclass DateTime(metaclass=DateTimeType):\n    \"\"\"A point in time represented as a date and a time.\n\n    The :class:`.DateTime` class is a nanosecond-precision drop-in replacement\n    for the standard library :class:`datetime.datetime` class.\n\n    As such, it contains both :class:`.Date` and :class:`.Time` information and\n    draws functionality from those individual classes.\n\n    A :class:`.DateTime` object is fully compatible with the Python time zone\n    library `pytz <https://pypi.org/project/pytz/>`_. Functions such as\n    `normalize` and `localize` can be used in the same way as they are with the\n    standard library classes.\n\n    Regular construction of a :class:`.DateTime` object requires at\n    least the `year`, `month` and `day` arguments to be supplied. The\n    optional `hour`, `minute` and `second` arguments default to zero and\n    `tzinfo` defaults to :const:`None`.\n\n    `year`, `month`, and `day` are passed to the constructor of :class:`.Date`.\n    `hour`, `minute`, `second`, `nanosecond`, and `tzinfo` are passed to the\n    constructor of :class:`.Time`. See their documentation for more details.\n\n        >>> dt = DateTime(2018, 4, 30, 12, 34, 56, 789123456); dt\n        neo4j.time.DateTime(2018, 4, 30, 12, 34, 56, 789123456)\n        >>> dt.second\n        56.789123456\n    \"\"\"\n\n    # CONSTRUCTOR #\n\n    def __new__(cls, year, month, day, hour=0, minute=0, second=0, nanosecond=0,\n                tzinfo=None):\n        return cls.combine(Date(year, month, day),\n                           Time(hour, minute, second, nanosecond, tzinfo))\n\n    def __getattr__(self, name):\n        \"\"\" Map standard library attribute names to local attribute names,\n        for compatibility.\n        \"\"\"\n        try:\n            return {\n                \"astimezone\": self.as_timezone,\n                \"isocalendar\": self.iso_calendar,\n                \"isoformat\": self.iso_format,\n                \"isoweekday\": self.iso_weekday,\n                \"strftime\": self.__format__,\n                \"toordinal\": self.to_ordinal,\n                \"timetuple\": self.time_tuple,\n                \"utcoffset\": self.utc_offset,\n                \"utctimetuple\": self.utc_time_tuple,\n            }[name]\n        except KeyError:\n            raise AttributeError(\"DateTime has no attribute %r\" % name)\n\n    # CLASS METHODS #\n\n    @classmethod\n    def now(cls, tz=None):\n        \"\"\"Get the current date and time.\n\n        :param tz: timezone. Set to None to create a local :class:`.DateTime`.\n        :type tz: datetime.tzinfo` or None\n\n        :rtype: DateTime\n\n        :raises OverflowError: if the timestamp is out of the range of values\n            supported by the platform C localtime() function. It’s common for\n            this to be restricted to years from 1970 through 2038.\n        \"\"\"\n        if tz is None:\n            return cls.from_clock_time(Clock().local_time(), UnixEpoch)\n        else:\n            try:\n                return tz.fromutc(cls.from_clock_time(\n                    Clock().utc_time(), UnixEpoch\n                ).replace(tzinfo=tz))\n            except TypeError:\n                # For timezone implementations not compatible with the custom\n                # datetime implementations, we can't do better than this.\n                utc_now = cls.from_clock_time(\n                    Clock().utc_time(), UnixEpoch\n                )\n                utc_now_native = utc_now.to_native()\n                now_native = tz.fromutc(utc_now_native)\n                now = cls.from_native(now_native)\n                return now.replace(\n                    nanosecond=(now.nanosecond\n                                + utc_now.nanosecond\n                                - utc_now_native.microsecond * 1000)\n                )\n\n    @classmethod\n    def utc_now(cls):\n        \"\"\"Get the current date and time in UTC\n\n        :rtype: DateTime\n        \"\"\"\n        return cls.from_clock_time(Clock().utc_time(), UnixEpoch)\n\n    @classmethod\n    def from_iso_format(cls, s):\n        \"\"\"Parse a ISO formatted date with time string.\n\n        :param s: String to parse\n        :type s: str\n\n        :rtype: Time\n\n        :raises ValueError: if the string does not match the ISO format.\n        \"\"\"\n        try:\n            return cls.combine(Date.from_iso_format(s[0:10]),\n                               Time.from_iso_format(s[11:]))\n        except ValueError:\n            raise ValueError(\"DateTime string is not in ISO format\")\n\n    @classmethod\n    def from_timestamp(cls, timestamp, tz=None):\n        \"\"\":class:`.DateTime` from a time stamp (seconds since unix epoch).\n\n        :param timestamp: the unix timestamp (seconds since unix epoch).\n        :type timestamp: float\n        :param tz: timezone. Set to None to create a local :class:`.DateTime`.\n        :type tz: datetime.tzinfo or None\n\n        :rtype: DateTime\n\n        :raises OverflowError: if the timestamp is out of the range of values\n            supported by the platform C localtime() function. It’s common for\n            this to be restricted to years from 1970 through 2038.\n        \"\"\"\n        if tz is None:\n            return cls.from_clock_time(\n                ClockTime(timestamp) + Clock().local_offset(), UnixEpoch\n            )\n        else:\n            return (\n                cls.utc_from_timestamp(timestamp)\n                .replace(tzinfo=timezone.utc).astimezone(tz)\n            )\n\n    @classmethod\n    def utc_from_timestamp(cls, timestamp):\n        \"\"\":class:`.DateTime` from a time stamp (seconds since unix epoch).\n\n        Returns the `DateTime` as local date `DateTime` in UTC.\n\n        :rtype: DateTime\n        \"\"\"\n        return cls.from_clock_time((timestamp, 0), UnixEpoch)\n\n    @classmethod\n    def from_ordinal(cls, ordinal):\n        \"\"\":class:`.DateTime` from an ordinal.\n\n        For more info about ordinals see :meth:`.Date.from_ordinal`.\n\n        :rtype: DateTime\n        \"\"\"\n        return cls.combine(Date.from_ordinal(ordinal), Midnight)\n\n    @classmethod\n    def combine(cls, date, time):\n        \"\"\"Combine a :class:`.Date` and a :class:`.Time` to a :class:`DateTime`.\n\n        :param date: the date\n        :type date: Date\n        :param time: the time\n        :type time: Time\n\n        :rtype: DateTime\n\n        :raises AssertionError: if the parameter types don't match.\n        \"\"\"\n        assert isinstance(date, Date)\n        assert isinstance(time, Time)\n        instance = object.__new__(cls)\n        instance.__date = date\n        instance.__time = time\n        return instance\n\n    @classmethod\n    def parse(cls, date_string, format):\n        raise NotImplementedError()\n\n    @classmethod\n    def from_native(cls, dt):\n        \"\"\"Convert from a native Python :class:`datetime.datetime` value.\n\n        :param dt: the datetime to convert\n        :type dt: datetime.datetime\n\n        :rtype: DateTime\n        \"\"\"\n        return cls.combine(Date.from_native(dt.date()), Time.from_native(dt.timetz()))\n\n    @classmethod\n    def from_clock_time(cls, clock_time, epoch):\n        \"\"\"Convert from a :class:`ClockTime` relative to a given epoch.\n\n        :param clock_time: the clock time as :class:`.ClockTime` or as tuple of\n            (seconds, nanoseconds)\n        :type clock_time: ClockTime or (float, int)\n        :param epoch: the epoch to which `clock_time` is relative\n        :type epoch: DateTime\n\n        :rtype: DateTime\n\n        :raises ValueError: if `clock_time` is invalid.\n        \"\"\"\n        try:\n            seconds, nanoseconds = ClockTime(*clock_time)\n        except (TypeError, ValueError):\n            raise ValueError(\"Clock time must be a 2-tuple of (s, ns)\")\n        else:\n            ordinal, seconds = divmod(seconds, 86400)\n            ticks = epoch.time().ticks + seconds * NANO_SECONDS + nanoseconds\n            days, ticks = divmod(ticks, 86400 * NANO_SECONDS)\n            ordinal += days\n            date_ = Date.from_ordinal(ordinal + epoch.date().to_ordinal())\n            time_ = Time.from_ticks(ticks)\n            return cls.combine(date_, time_)\n\n    # CLASS ATTRIBUTES #\n\n    min = None\n    \"\"\"The earliest date time value possible.\"\"\"\n\n    max = None\n    \"\"\"The latest date time value possible.\"\"\"\n\n    resolution = None\n    \"\"\"The minimum resolution supported.\"\"\"\n\n    # INSTANCE ATTRIBUTES #\n\n    @property\n    def year(self):\n        \"\"\"The year of the :class:`.DateTime`.\n\n        See :attr:`.Date.year`.\n        \"\"\"\n        return self.__date.year\n\n    @property\n    def month(self):\n        \"\"\"The year of the :class:`.DateTime`.\n\n        See :attr:`.Date.year`.\"\"\"\n        return self.__date.month\n\n    @property\n    def day(self):\n        \"\"\"The day of the :class:`.DateTime`'s date.\n\n        See :attr:`.Date.day`.\"\"\"\n        return self.__date.day\n\n    @property\n    def year_month_day(self):\n        \"\"\"The year_month_day of the :class:`.DateTime`'s date.\n\n        See :attr:`.Date.year_month_day`.\"\"\"\n        return self.__date.year_month_day\n\n    @property\n    def year_week_day(self):\n        \"\"\"The year_week_day of the :class:`.DateTime`'s date.\n\n        See :attr:`.Date.year_week_day`.\"\"\"\n        return self.__date.year_week_day\n\n    @property\n    def year_day(self):\n        \"\"\"The year_day of the :class:`.DateTime`'s date.\n\n        See :attr:`.Date.year_day`.\"\"\"\n        return self.__date.year_day\n\n    @property\n    def hour(self):\n        \"\"\"The hour of the :class:`.DateTime`'s time.\n\n        See :attr:`.Time.hour`.\"\"\"\n        return self.__time.hour\n\n    @property\n    def minute(self):\n        \"\"\"The minute of the :class:`.DateTime`'s time.\n\n        See :attr:`.Time.minute`.\"\"\"\n        return self.__time.minute\n\n    @property\n    def second(self):\n        \"\"\"The second of the :class:`.DateTime`'s time.\n\n        See :attr:`.Time.second`.\"\"\"\n        return self.__time.second\n\n    @property\n    def nanosecond(self):\n        \"\"\"The nanosecond of the :class:`.DateTime`'s time.\n\n        See :attr:`.Time.nanosecond`.\"\"\"\n        return self.__time.nanosecond\n\n    @property\n    def tzinfo(self):\n        \"\"\"The tzinfo of the :class:`.DateTime`'s time.\n\n        See :attr:`.Time.tzinfo`.\"\"\"\n        return self.__time.tzinfo\n\n    @property\n    def hour_minute_second_nanosecond(self):\n        \"\"\"The hour_minute_second_nanosecond of the :class:`.DateTime`'s time.\n\n        See :attr:`.Time.hour_minute_second_nanosecond`.\"\"\"\n        return self.__time.hour_minute_second_nanosecond\n\n    # OPERATIONS #\n\n    def _get_both_normalized(self, other, strict=True):\n        if (isinstance(other, (datetime, DateTime))\n                and ((self.utc_offset() is None)\n                     ^ (other.utcoffset() is None))):\n            if strict:\n                raise TypeError(\"can't compare offset-naive and offset-aware \"\n                                \"datetimes\")\n            else:\n                return None, None\n        self_norm = self\n        utc_offset = self.utc_offset()\n        if utc_offset is not None:\n            self_norm -= utc_offset\n        self_norm = self_norm.replace(tzinfo=None)\n        other_norm = other\n        if isinstance(other, (datetime, DateTime)):\n            utc_offset = other.utcoffset()\n            if utc_offset is not None:\n                other_norm -= utc_offset\n            other_norm = other_norm.replace(tzinfo=None)\n        else:\n            return None, None\n        return self_norm, other_norm\n\n    def __hash__(self):\n        \"\"\"\"\"\"\n        if self.nanosecond % 1000 == 0:\n            return hash(self.to_native())\n        self_norm = self\n        utc_offset = self.utc_offset()\n        if utc_offset is not None:\n            self_norm -= utc_offset\n        return hash(self_norm.date()) ^ hash(self_norm.time())\n\n    def __eq__(self, other):\n        \"\"\"\n        `==` comparison with :class:`.DateTime` or :class:`datetime.datetime`.\n        \"\"\"\n        if not isinstance(other, (datetime, DateTime)):\n            return NotImplemented\n        if self.utc_offset() == other.utcoffset():\n            return self.date() == other.date() and self.time() == other.time()\n        self_norm, other_norm = self._get_both_normalized(other, strict=False)\n        if self_norm is None:\n            return False\n        return self_norm == other_norm\n\n    def __ne__(self, other):\n        \"\"\"\n        `!=` comparison with :class:`.DateTime` or :class:`datetime.datetime`.\n        \"\"\"\n        return not self.__eq__(other)\n\n    def __lt__(self, other):\n        \"\"\"\n        `<` comparison with :class:`.DateTime` or :class:`datetime.datetime`.\n        \"\"\"\n        if not isinstance(other, (datetime, DateTime)):\n            return NotImplemented\n        if self.utc_offset() == other.utcoffset():\n            if self.date() == other.date():\n                return self.time() < other.time()\n            return self.date() < other.date()\n        self_norm, other_norm = self._get_both_normalized(other)\n        return (self_norm.date() < other_norm.date()\n                or self_norm.time() < other_norm.time())\n\n    def __le__(self, other):\n        \"\"\"\n        `<=` comparison with :class:`.DateTime` or :class:`datetime.datetime`.\n        \"\"\"\n        if not isinstance(other, (datetime, DateTime)):\n            return NotImplemented\n        if self.utc_offset() == other.utcoffset():\n            if self.date() == other.date():\n                return self.time() <= other.time()\n            return self.date() <= other.date()\n        self_norm, other_norm = self._get_both_normalized(other)\n        return self_norm <= other_norm\n\n    def __ge__(self, other):\n        \"\"\"\n        `>=` comparison with :class:`.DateTime` or :class:`datetime.datetime`.\n        \"\"\"\n        if not isinstance(other, (datetime, DateTime)):\n            return NotImplemented\n        if self.utc_offset() == other.utcoffset():\n            if self.date() == other.date():\n                return self.time() >= other.time()\n            return self.date() >= other.date()\n        self_norm, other_norm = self._get_both_normalized(other)\n        return self_norm >= other_norm\n\n    def __gt__(self, other):\n        \"\"\"\n        `>` comparison with :class:`.DateTime` or :class:`datetime.datetime`.\n        \"\"\"\n        if not isinstance(other, (datetime, DateTime)):\n            return NotImplemented\n        if self.utc_offset() == other.utcoffset():\n            if self.date() == other.date():\n                return self.time() > other.time()\n            return self.date() > other.date()\n        self_norm, other_norm = self._get_both_normalized(other)\n        return (self_norm.date() > other_norm.date()\n                or self_norm.time() > other_norm.time())\n\n    def __add__(self, other):\n        \"\"\"Add a :class:`datetime.timedelta`.\n\n        :rtype: DateTime\n        \"\"\"\n        if isinstance(other, timedelta):\n            t = (self.to_clock_time()\n                 + ClockTime(86400 * other.days + other.seconds,\n                             other.microseconds * 1000))\n            days, seconds = symmetric_divmod(t.seconds, 86400)\n            date_ = Date.from_ordinal(days + 1)\n            time_ = Time.from_ticks(round_half_to_even(\n                seconds * NANO_SECONDS + t.nanoseconds\n            ))\n            return self.combine(date_, time_).replace(tzinfo=self.tzinfo)\n        if isinstance(other, Duration):\n            t = (self.to_clock_time()\n                 + ClockTime(other.seconds, other.nanoseconds))\n            days, seconds = symmetric_divmod(t.seconds, 86400)\n            date_ = self.date() + Duration(months=other.months,\n                                           days=days + other.days)\n            time_ = Time.from_ticks(seconds * NANO_SECONDS + t.nanoseconds)\n            return self.combine(date_, time_).replace(tzinfo=self.tzinfo)\n        return NotImplemented\n\n    def __sub__(self, other):\n        \"\"\"Subtract a datetime or a timedelta.\n\n         Supported :class:`.DateTime` (returns :class:`.Duration`),\n         :class:`datetime.datetime` (returns :class:`datetime.timedelta`), and\n         :class:`datetime.timedelta` (returns :class:`.DateTime`).\n\n        :rtype: Duration or datetime.timedelta or DateTime\n        \"\"\"\n        if isinstance(other, DateTime):\n            self_month_ordinal = 12 * (self.year - 1) + self.month\n            other_month_ordinal = 12 * (other.year - 1) + other.month\n            months = self_month_ordinal - other_month_ordinal\n            days = self.day - other.day\n            t = self.time().to_clock_time() - other.time().to_clock_time()\n            return Duration(months=months, days=days, seconds=t.seconds,\n                            nanoseconds=t.nanoseconds)\n        if isinstance(other, datetime):\n            days = self.to_ordinal() - other.toordinal()\n            t = (self.time().to_clock_time()\n                 - ClockTime(\n                       3600 * other.hour + 60 * other.minute + other.second,\n                       other.microsecond * 1000\n                    ))\n            return timedelta(days=days, seconds=t.seconds,\n                             microseconds=(t.nanoseconds // 1000))\n        if isinstance(other, Duration):\n            return self.__add__(-other)\n        if isinstance(other, timedelta):\n            return self.__add__(-other)\n        return NotImplemented\n\n    def __copy__(self):\n        return self.combine(self.__date, self.__time)\n\n    def __deepcopy__(self, *args, **kwargs):\n        return self.__copy__()\n\n    # INSTANCE METHODS #\n\n    def date(self):\n        \"\"\"The date\n\n        :rtype: Date\n        \"\"\"\n        return self.__date\n\n    def time(self):\n        \"\"\"The time without timezone info\n\n        :rtype: Time\n        \"\"\"\n        return self.__time.replace(tzinfo=None)\n\n    def timetz(self):\n        \"\"\"The time with timezone info\n\n        :rtype: Time\n        \"\"\"\n        return self.__time\n\n    def replace(self, **kwargs):\n        \"\"\"Return a :class:`.DateTime` with one or more components replaced.\n\n        See :meth:`.Date.replace` and :meth:`.Time.replace` for available\n        arguments.\n\n        :rtype: DateTime\n        \"\"\"\n        date_ = self.__date.replace(**kwargs)\n        time_ = self.__time.replace(**kwargs)\n        return self.combine(date_, time_)\n\n    def as_timezone(self, tz):\n        \"\"\"Convert this :class:`.DateTime` to another timezone.\n\n        :param tz: the new timezone\n        :type tz: datetime.tzinfo or None\n\n        :return: the same object if `tz` is None. Else, a new :class:`.DateTime`\n            that's the same point in time but in a different timezone.\n        :rtype: DateTime\n        \"\"\"\n        if self.tzinfo is None:\n            return self\n        utc = (self - self.utc_offset()).replace(tzinfo=tz)\n        try:\n            return tz.fromutc(utc)\n        except TypeError:\n            # For timezone implementations not compatible with the custom\n            # datetime implementations, we can't do better than this.\n            native_utc = utc.to_native()\n            native_res = tz.fromutc(native_utc)\n            res = self.from_native(native_res)\n            return res.replace(\n                nanosecond=(native_res.microsecond * 1000\n                            + self.nanosecond % 1000)\n            )\n\n    def utc_offset(self):\n        \"\"\"Get the date times utc offset.\n\n        See :meth:`.Time.utc_offset`.\n        \"\"\"\n\n        return self.__time._utc_offset(self)\n\n    def dst(self):\n        \"\"\"Get the daylight saving time adjustment (DST).\n\n        See :meth:`.Time.dst`.\n        \"\"\"\n        return self.__time.dst()\n\n    def tzname(self):\n        \"\"\"Get the timezone name.\n\n        See :meth:`.Time.tzname`.\n        \"\"\"\n        return self.__time.tzname()\n\n    def time_tuple(self):\n        raise NotImplementedError()\n\n    def utc_time_tuple(self):\n        raise NotImplementedError()\n\n    def to_ordinal(self):\n        \"\"\"Get the ordinal of the :class:`.DateTime`'s date.\n\n        See :meth:`.Date.to_ordinal`\n        \"\"\"\n        return self.__date.to_ordinal()\n\n    def to_clock_time(self):\n        \"\"\"Convert to :class:`.ClockTime`.\n\n        :rtype: ClockTime\n        \"\"\"\n        total_seconds = 0\n        for year in range(1, self.year):\n            total_seconds += 86400 * DAYS_IN_YEAR[year]\n        for month in range(1, self.month):\n            total_seconds += 86400 * Date.days_in_month(self.year, month)\n        total_seconds += 86400 * (self.day - 1)\n        seconds, nanoseconds = divmod(self.__time.ticks, NANO_SECONDS)\n        return ClockTime(total_seconds + seconds, nanoseconds)\n\n    def to_native(self):\n        \"\"\"Convert to a native Python :class:`datetime.datetime` value.\n\n        This conversion is lossy as the native time implementation only supports\n        a resolution of microseconds instead of nanoseconds.\n\n        :rtype: datetime.datetime\n        \"\"\"\n        y, mo, d = self.year_month_day\n        h, m, s, ns = self.hour_minute_second_nanosecond\n        ms = int(ns / 1000)\n        tz = self.tzinfo\n        return datetime(y, mo, d, h, m, s, ms, tz)\n\n    def weekday(self):\n        \"\"\"Get the weekday.\n\n        See :meth:`.Date.weekday`\n        \"\"\"\n        return self.__date.weekday()\n\n    def iso_weekday(self):\n        \"\"\"Get the ISO weekday.\n\n        See :meth:`.Date.iso_weekday`\n        \"\"\"\n        return self.__date.iso_weekday()\n\n    def iso_calendar(self):\n        \"\"\"Get date as ISO tuple.\n\n        See :meth:`.Date.iso_calendar`\n        \"\"\"\n        return self.__date.iso_calendar()\n\n    def iso_format(self, sep=\"T\"):\n        \"\"\"Return the :class:`.DateTime` as ISO formatted string.\n\n        This method joins `self.date().iso_format()` (see\n        :meth:`.Date.iso_format`) and `self.timetz().iso_format()` (see\n        :meth:`.Time.iso_format`) with `sep` in between.\n\n        :param sep: the separator between the formatted date and time.\n        :type sep: str\n\n        :rtype: str\n        \"\"\"\n        s = \"%s%s%s\" % (self.date().iso_format(), sep,\n                        self.timetz().iso_format())\n        time_tz = self.timetz()\n        offset = time_tz.utc_offset()\n        if offset is not None:\n            # the time component will have taken care of formatting the offset\n            return s\n        offset = self.utc_offset()\n        if offset is not None:\n            s += \"%+03d:%02d\" % divmod(offset.total_seconds() // 60, 60)\n        return s\n\n    def __repr__(self):\n        \"\"\"\"\"\"\n        if self.tzinfo is None:\n            fields = (*self.year_month_day,\n                      *self.hour_minute_second_nanosecond)\n            return \"neo4j.time.DateTime(%r, %r, %r, %r, %r, %r, %r)\" % fields\n        else:\n            fields = (*self.year_month_day,\n                      *self.hour_minute_second_nanosecond, self.tzinfo)\n            return (\"neo4j.time.DateTime(%r, %r, %r, %r, %r, %r, %r, tzinfo=%r)\"\n                    % fields)\n\n    def __str__(self):\n        \"\"\"\"\"\"\n        return self.iso_format()\n\n    def __format__(self, format_spec):\n        \"\"\"\"\"\"\n        raise NotImplementedError()\n\n\nDateTime.min = DateTime.combine(Date.min, Time.min)\nDateTime.max = DateTime.combine(Date.max, Time.max)\nDateTime.resolution = Time.resolution\n\n#: A :class:`.DateTime` instance set to `0000-00-00T00:00:00`.\n#: This has a :class:`.Date` component equal to :attr:`ZeroDate` and a\nNever = DateTime.combine(ZeroDate, Midnight)\n\n#: A :class:`.DateTime` instance set to `1970-01-01T00:00:00`.\nUnixEpoch = DateTime(1970, 1, 1, 0, 0, 0)\n",
            "file_path": "neo4j/time/__init__.py",
            "human_label": "Create a time from ticks (nanoseconds since midnight).\n\n:param ticks: nanoseconds since midnight\n:type ticks: int\n:param tz: optional timezone\n:type tz: datetime.tzinfo\n\n:rtype: Time\n\n:raises ValueError: if ticks is out of bounds\n    (0 <= ticks < 86400000000000)",
            "level": "file_runnable",
            "lineno": "1508",
            "name": "from_ticks",
            "oracle_context": "{ \"apis\" : \"['second', 'isinstance', '__new', 'minute', 'divmod', 'ticks', 'nanosecond', 'hour']\", \"classes\" : \"['TypeError', 'ValueError']\", \"vars\" : \"['NANO_SECONDS']\" }",
            "package": "__init__",
            "project": "neo4j/neo4j-python-driver",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b87b4f9a0c4fa8b80b3581",
            "all_context": "{ \"import\" : \"copy lena  \", \"file\" : \"\", \"class\" : \"self.__repr__(self) ; self._scale ; self.nbins ; self.dim ; self.__eq__(self,other) ; self._update_context(self,context) ; self.scale(self,other,recompute) ; self.__init__(self,edges,bins,initial_value) ; self.edges ; self.fill(self,coord,weight) ; self.ranges ; self.scale ; self.bins ; \" }",
            "code": "    def scale(self, other=None, recompute=False):\n        \"\"\"Compute or set scale (integral of the histogram).\n\n        If *other* is ``None``, return scale of this histogram.\n        If its scale was not computed before,\n        it is computed and stored for subsequent use\n        (unless explicitly asked to *recompute*).\n        Note that after changing (filling) the histogram\n        one must explicitly recompute the scale\n        if it was computed before.\n\n        If a float *other* is provided, rescale self to *other*.\n\n        Histograms with scale equal to zero can't be rescaled.\n        :exc:`.LenaValueError` is raised if one tries to do that.\n        \"\"\"\n        # see graph.scale comments why this is called simply \"scale\"\n        # (not set_scale, get_scale, etc.)\n        if other is None:\n            # return scale\n            if self._scale is None or recompute:\n                self._scale = hf.integral(\n                    *hf.unify_1_md(self.bins, self.edges)\n                )\n            return self._scale\n        else:\n            # rescale from other\n            scale = self.scale()\n            if scale == 0:\n                raise lena.core.LenaValueError(\n                    \"can not rescale histogram with zero scale\"\n                )\n            self.bins = lena.math.md_map(lambda binc: binc*float(other) / scale,\n                                         self.bins)\n            self._scale = other\n            return None\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : false, \"public_lib\" : true, \"current_class\" : true, \"current_file\" : false, \"current_project\" : false, \"external\" : false }",
            "docstring": "Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.",
            "end_lineno": "222",
            "file_content": "\"\"\"Histogram structure *histogram* and element *Histogram*.\"\"\"\nimport copy\n\nimport lena.context\nimport lena.core\nimport lena.flow\nimport lena.math\nfrom . import hist_functions as hf\n\n\nclass histogram():\n    \"\"\"A multidimensional histogram.\n\n    Arbitrary dimension, variable bin size and weights are supported.\n    Lower bin edge is included, upper edge is excluded.\n    Underflow and overflow values are skipped.\n    Bin content can be of arbitrary type,\n    which is defined during initialization.\n\n    Examples:\n\n    >>> # a two-dimensional histogram\n    >>> hist = histogram([[0, 1, 2], [0, 1, 2]])\n    >>> hist.fill([0, 1])\n    >>> hist.bins\n    [[0, 1], [0, 0]]\n    >>> values = [[0, 0], [1, 0], [1, 1]]\n    >>> # fill the histogram with values\n    >>> for v in values:\n    ...     hist.fill(v)\n    >>> hist.bins\n    [[1, 1], [1, 1]]\n    \"\"\"\n    # Note the differences from existing packages.\n    # Numpy 1.16 (numpy.histogram): all but the last\n    # (righthand-most) bin is half-open.\n    # This histogram class has bin limits as in ROOT\n    # (but without overflow and underflow).\n\n    # Numpy: the first element of the range must be less than or equal to the second.\n    # This histogram requires strictly increasing edges.\n    # https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html\n    # https://root.cern.ch/root/htmldoc/guides/users-guide/Histograms.html#bin-numbering\n\n    def __init__(self, edges, bins=None, initial_value=0):\n        \"\"\"*edges* is a sequence of one-dimensional arrays,\n        each containing strictly increasing bin edges.\n\n        Histogram's bins by default\n        are initialized with *initial_value*.\n        It can be any object that supports addition with *weight*\n        during *fill* (but that is not necessary\n        if you don't plan to fill the histogram).\n        If the *initial_value* is compound and requires special copying,\n        create initial bins yourself (see :func:`.init_bins`).\n\n        A histogram can be created from existing *bins* and *edges*.\n        In this case a simple check of the shape of *bins* is done\n        (raising :exc:`.LenaValueError` if failed).\n\n        **Attributes**\n\n        :attr:`edges` is a list of edges on each dimension.\n        Edges mark the borders of the bin.\n        Edges along each dimension are one-dimensional lists,\n        and the multidimensional bin is the result of all intersections\n        of one-dimensional edges.\n        For example, a 3-dimensional histogram has edges of the form\n        *[x_edges, y_edges, z_edges]*,\n        and the 0th bin has borders\n        *((x[0], x[1]), (y[0], y[1]), (z[0], z[1]))*.\n\n        Index in the edges is a tuple, where a given position corresponds\n        to a dimension, and the content at that position\n        to the bin along that dimension.\n        For example, index *(0, 1, 3)* corresponds to the bin\n        with lower edges *(x[0], y[1], z[3])*.\n\n        :attr:`bins` is a list of nested lists.\n        Same index as for edges can be used to get bin content:\n        bin at *(0, 1, 3)* can be obtained as *bins[0][1][3]*.\n        Most nested arrays correspond to highest\n        (further from x) coordinates.\n        For example, for a 3-dimensional histogram bins equal to\n        *[[[1, 1], [0, 0]], [[0, 0], [0, 0]]]*\n        mean that the only filled bins are those\n        where x and y indices are 0, and z index is 0 and 1.\n\n        :attr:`dim` is the dimension of a histogram\n        (length of its *edges* for a multidimensional histogram).\n\n        If subarrays of *edges* are not increasing\n        or if any of them has length less than 2,\n        :exc:`.LenaValueError` is raised.\n\n        .. admonition:: Programmer's note\n\n            one- and multidimensional histograms\n            have different *bins* and *edges* format.\n            To be unified, 1-dimensional edges should be\n            nested in a list (like *[[1, 2, 3]]*).\n            Instead, they are simply the x-edges list,\n            because it is more intuitive and one-dimensional histograms\n            are used more often.\n            To unify the interface for bins and edges in your code,\n            use :func:`.unify_1_md` function.\n        \"\"\"\n        # todo: allow creation of *edges* from tuples\n        # (without lena.math.mesh). Allow bin_size in this case.\n        hf.check_edges_increasing(edges)\n        self.edges = edges\n        self._scale = None\n\n        if hasattr(edges[0], \"__iter__\"):\n            self.dim = len(edges)\n        else:\n            self.dim = 1\n\n        # todo: add a kwarg no_check=False to disable bins testing\n        if bins is None:\n            self.bins = hf.init_bins(self.edges, initial_value)\n        else:\n            self.bins = bins\n            # We can't make scale for an arbitrary histogram,\n            # because it may contain compound values.\n            # self._scale = self.make_scale()\n            wrong_bins_error = lena.core.LenaValueError(\n                \"bins of incorrect shape given, {}\".format(bins)\n            )\n            if self.dim == 1:\n                if len(bins) != len(edges) - 1:\n                    raise wrong_bins_error\n            else:\n                if len(bins) != len(edges[0]) - 1:\n                    raise wrong_bins_error\n        if self.dim > 1:\n            self.ranges = [(axis[0], axis[-1]) for axis in edges]\n            self.nbins =  [len(axis) - 1 for axis in edges]\n        else:\n            self.ranges = [(edges[0], edges[-1])]\n            self.nbins = [len(edges)-1]\n\n    def __eq__(self, other):\n        \"\"\"Two histograms are equal, if and only if they have\n        equal bins and equal edges.\n\n        If *other* is not a :class:`.histogram`, return ``False``.\n\n        Note that floating numbers should be compared\n        approximately (using :func:`math.isclose`).\n        \"\"\"\n        if not isinstance(other, histogram):\n            # in Python comparison between different types is allowed\n            return False\n        return self.bins == other.bins and self.edges == other.edges\n\n    def fill(self, coord, weight=1):\n        \"\"\"Fill histogram at *coord* with the given *weight*.\n\n        Coordinates outside the histogram edges are ignored.\n        \"\"\"\n        indices = hf.get_bin_on_value(coord, self.edges)\n        subarr = self.bins\n        for ind in indices[:-1]:\n            # underflow\n            if ind < 0:\n                return\n            try:\n                subarr = subarr[ind]\n            # overflow\n            except IndexError:\n                return\n        ind = indices[-1]\n        # underflow\n        if ind < 0:\n            return\n\n        # fill\n        try:\n            subarr[ind] += weight\n        except IndexError:\n            return\n\n    def __repr__(self):\n        return \"histogram({}, bins={})\".format(self.edges, self.bins)\n\n    def scale(self, other=None, recompute=False):\n        \"\"\"Compute or set scale (integral of the histogram).\n\n        If *other* is ``None``, return scale of this histogram.\n        If its scale was not computed before,\n        it is computed and stored for subsequent use\n        (unless explicitly asked to *recompute*).\n        Note that after changing (filling) the histogram\n        one must explicitly recompute the scale\n        if it was computed before.\n\n        If a float *other* is provided, rescale self to *other*.\n\n        Histograms with scale equal to zero can't be rescaled.\n        :exc:`.LenaValueError` is raised if one tries to do that.\n        \"\"\"\n        # see graph.scale comments why this is called simply \"scale\"\n        # (not set_scale, get_scale, etc.)\n        if other is None:\n            # return scale\n            if self._scale is None or recompute:\n                self._scale = hf.integral(\n                    *hf.unify_1_md(self.bins, self.edges)\n                )\n            return self._scale\n        else:\n            # rescale from other\n            scale = self.scale()\n            if scale == 0:\n                raise lena.core.LenaValueError(\n                    \"can not rescale histogram with zero scale\"\n                )\n            self.bins = lena.math.md_map(lambda binc: binc*float(other) / scale,\n                                         self.bins)\n            self._scale = other\n            return None\n\n    def _update_context(self, context):\n        \"\"\"Update *context* with the properties of this histogram.\n\n        *context.histogram* is updated with \"dim\", \"nbins\"\n        and \"ranges\" with values for this histogram.\n        If this histogram has a computed scale, it is also added\n        to the context.\n\n        Called on \"destruction\" of the histogram structure (for example,\n        in :class:`.ToCSV`). See graph._update_context for more details.\n        \"\"\"\n\n        hist_context = {\n            \"dim\": self.dim,\n            \"nbins\": self.nbins,\n            \"ranges\": self.ranges\n        }\n\n        if self._scale is not None:\n            hist_context[\"scale\"] = self._scale\n\n        lena.context.update_recursively(context, {\"histogram\": hist_context})\n\n\nclass Histogram():\n    \"\"\"An element to produce histograms.\"\"\"\n\n    def __init__(self, edges, bins=None, make_bins=None, initial_value=0):\n        \"\"\"*edges*, *bins* and *initial_value* have the same meaning\n        as during creation of a :class:`histogram`.\n\n        *make_bins* is a function without arguments\n        that creates new bins\n        (it will be called during :meth:`__init__` and :meth:`reset`).\n        *initial_value* in this case is ignored, but bin check is made.\n        If both *bins* and *make_bins* are provided,\n        :exc:`.LenaTypeError` is raised.\n        \"\"\"\n        self._hist = histogram(edges, bins)\n\n        if make_bins is not None and bins is not None:\n            raise lena.core.LenaTypeError(\n                \"either initial bins or make_bins must be provided, \"\n                \"not both: {} and {}\".format(bins, make_bins)\n            )\n\n        # may be None\n        self._initial_bins = copy.deepcopy(bins)\n\n        # todo: bins, make_bins, initial_value look redundant\n        # and may be reconsidered when really using reset().\n        if make_bins:\n            bins = make_bins()\n        self._make_bins = make_bins\n\n        self._cur_context = {}\n\n    def fill(self, value):\n        \"\"\"Fill the histogram with *value*.\n\n        *value* can be a *(data, context)* pair. \n        Values outside the histogram edges are ignored.\n        \"\"\"\n        data, self._cur_context = lena.flow.get_data_context(value)\n        self._hist.fill(data)\n        # filling with weight is only allowed in histogram structure\n        # self._hist.fill(data, weight)\n\n    def compute(self):\n        \"\"\"Yield histogram with context.\"\"\"\n        yield (self._hist, self._cur_context)\n\n    def reset(self):\n        \"\"\"Reset the histogram.\n\n        Current context is reset to an empty dict.\n        Bins are reinitialized with the *initial_value*\n        or with *make_bins()* (depending on the initialization).\n        \"\"\"\n        if self._make_bins is not None:\n            self.bins = self._make_bins()\n        elif self._initial_bins is not None:\n            self.bins = copy.deepcopy(self._initial_bins)\n        else:\n            self.bins = hf.init_bins(self.edges, self._initial_value)\n\n        self._cur_context = {}\n",
            "file_path": "lena/structures/histogram.py",
            "human_label": "Compute or set scale (integral of the histogram).\n\nIf *other* is ``None``, return scale of this histogram.\nIf its scale was not computed before,\nit is computed and stored for subsequent use\n(unless explicitly asked to *recompute*).\nNote that after changing (filling) the histogram\none must explicitly recompute the scale\nif it was computed before.\n\nIf a float *other* is provided, rescale self to *other*.\n\nHistograms with scale equal to zero can't be rescaled.\n:exc:`.LenaValueError` is raised if one tries to do that.",
            "level": "class_runnable",
            "lineno": "187",
            "name": "scale",
            "oracle_context": "{ \"apis\" : \"['md_map', 'LenaValueError', 'unify_1_md', 'integral', 'float']\", \"classes\" : \"['hf']\", \"vars\" : \"['bins', 'math', 'scale', 'edges', 'core', '_scale', 'lena']\" }",
            "package": "histogram",
            "project": "ynikitenko/lena",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b87b519a0c4fa8b80b3583",
            "all_context": "{ \"import\" : \"warnings operator functools re copy lena functools \", \"file\" : \"\", \"class\" : \"self.__repr__(self) ; self._scale ; self.coords ; self.dim ; self._get_err_indices ; self._parse_error_names(self,field_names) ; self.__eq__(self,other) ; self._parse_error_names ; self.__iter__(self) ; self._update_context(self,context) ; self._parsed_error_names ; self._get_err_indices(self,coord_name) ; self.__init__(self,coords,field_names,scale) ; self._coord_names ; self.scale(self,other) ; self.field_names ; \" }",
            "code": "    def scale(self, other=None):\n        \"\"\"Get or set the scale of the graph.\n\n        If *other* is ``None``, return the scale of this graph.\n\n        If a numeric *other* is provided, rescale to that value.\n        If the graph has unknown or zero scale,\n        rescaling that will raise :exc:`~.LenaValueError`.\n\n        To get meaningful results, graph's fields are used.\n        Only the last coordinate is rescaled.\n        For example, if the graph has *x* and *y* coordinates,\n        then *y* will be rescaled, and for a 3-dimensional graph\n        *z* will be rescaled.\n        All errors are rescaled together with their coordinate.\n        \"\"\"\n        # this method is called scale() for uniformity with histograms\n        # And this looks really good: explicit for computations\n        # (not a subtle graph.scale, like a constant field (which is,\n        #  however, the case in graph - but not in other structures))\n        # and easy to remember (set_scale? rescale? change_scale_to?..)\n\n        # We modify the graph in place,\n        # because that would be redundant (not optimal)\n        # to create a new graph\n        # if we only want to change the scale of the existing one.\n\n        if other is None:\n            return self._scale\n\n        if not self._scale:\n            raise lena.core.LenaValueError(\n                \"can't rescale a graph with zero or unknown scale\"\n            )\n\n        last_coord_ind = self.dim - 1\n        last_coord_name = self.field_names[last_coord_ind]\n\n        last_coord_indices = ([last_coord_ind] +\n                self._get_err_indices(last_coord_name)\n        )\n\n        # In Python 2 3/2 is 1, so we want to be safe;\n        # the downside is that integer-valued graphs\n        # will become floating, but that is doubtfully an issue.\n        # Remove when/if dropping support for Python 2.\n        rescale = float(other) / self._scale\n\n        mul = operator.mul\n        partial = functools.partial\n\n        # a version with lambda is about 50% slower:\n        # timeit.timeit('[*map(lambda val: val*2, vals)]', \\\n        #     setup='vals = list(range(45)); from operator import mul; \\\n        #     from functools import partial')\n        # 3.159\n        # same setup for\n        # timeit.timeit('[*map(partial(mul, 2), vals)]',...):\n        # 2.075\n        # \n        # [*map(...)] is very slightly faster than list(map(...)),\n        # but it's unavailable in Python 2 (and anyway less readable).\n\n        # rescale arrays of values and errors\n        for ind, arr in enumerate(self.coords):\n            if ind in last_coord_indices:\n                # Python lists are faster than arrays,\n                # https://stackoverflow.com/a/62399645/952234\n                # (because each time taking a value from an array\n                #  creates a Python object)\n                self.coords[ind] = list(map(partial(mul, rescale),\n                                            arr))\n\n        self._scale = other\n\n        # as suggested in PEP 8\n        return None\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : true, \"public_lib\" : true, \"current_class\" : true, \"current_file\" : false, \"current_project\" : false, \"external\" : false }",
            "docstring": "Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.",
            "end_lineno": "268",
            "file_content": "\"\"\"A graph is a function at given coordinates.\"\"\"\nimport copy\nimport functools\nimport operator\nimport re\nimport warnings\n\nimport lena.core\nimport lena.context\nimport lena.flow\n\n\nclass graph():\n    \"\"\"Numeric arrays of equal size.\"\"\"\n\n    def __init__(self, coords, field_names=(\"x\", \"y\"), scale=None):\n        \"\"\"This structure generally corresponds\n        to the graph of a function\n        and represents arrays of coordinates and the function values\n        of arbitrary dimensions.\n\n        *coords* is a list of one-dimensional\n        coordinate and value sequences (usually lists).\n        There is little to no distinction between them,\n        and \"values\" can also be called \"coordinates\".\n\n        *field_names* provide the meaning of these arrays.\n        For example, a 3-dimensional graph could be distinguished\n        from a 2-dimensional graph with errors by its fields\n        (\"x\", \"y\", \"z\") versus (\"x\", \"y\", \"error_y\").\n        Field names don't affect drawing graphs:\n        for that :class:`~Variable`-s should be used.\n        Default field names,\n        provided for the most used 2-dimensional graphs,\n        are \"x\" and \"y\".\n\n        *field_names* can be a string separated by whitespace\n        and/or commas or a tuple of strings, such as (\"x\", \"y\").\n        *field_names* must have as many elements\n        as *coords* and each field name must be unique.\n        Otherwise field names are arbitrary.\n        Error fields must go after all other coordinates.\n        Name of a coordinate error is \"error\\\\_\"\n        appended by coordinate name. Further error details\n        are appended after '_'. They could be arbitrary depending\n        on the problem: \"low\", \"high\", \"low_90%_cl\", etc. Example:\n        (\"E\", \"time\", \"error_E_low\", \"error_time\").\n\n        *scale* of the graph is a kind of its norm. It could be\n        the integral of the function or its other property.\n        A scale of a normalised probability density\n        function would be one.\n        An initialized *scale* is required if one needs\n        to renormalise the graph in :meth:`scale`\n        (for example, to plot it with other graphs).\n\n        Coordinates of a function graph would usually be arrays\n        of increasing values, which is not required here.\n        Neither is it checked that coordinates indeed\n        contain one-dimensional numeric values.\n        However, non-standard graphs\n        will likely lead to errors during plotting\n        and will require more programmer's work and caution,\n        so use them only if you understand what you are doing.\n\n        A graph can be iterated yielding tuples of numbers\n        for each point.\n\n        **Attributes**\n\n        :attr:`coords` is a list \\\n            of one-dimensional lists of coordinates.\n\n        :attr:`field_names`\n\n        :attr:`dim` is the dimension of the graph,\n        that is of all its coordinates without errors.\n\n        In case of incorrect initialization arguments,\n        :exc:`~.LenaTypeError` or :exc:`~.LenaValueError` is raised.\n\n        .. versionadded:: 0.5\n        \"\"\"\n        if not coords:\n            raise lena.core.LenaValueError(\n                \"coords must be a non-empty sequence \"\n                \"of coordinate sequences\"\n            )\n\n        # require coords to be of the same size\n        pt_len = len(coords[0])\n        for arr in coords[1:]:\n            if len(arr) != pt_len:\n                raise lena.core.LenaValueError(\n                    \"coords must have subsequences of equal lengths\"\n                )\n\n        # Unicode (Python 2) field names would be just bad,\n        # so we don't check for it here.\n        if isinstance(field_names, str):\n            # split(', ') won't work.\n            # From https://stackoverflow.com/a/44785447/952234:\n            # \\s stands for whitespace.\n            field_names = tuple(re.findall(r'[^,\\s]+', field_names))\n        elif not isinstance(field_names, tuple):\n            # todo: why field_names are a tuple,\n            # while coords are a list?\n            # It might be non-Pythonic to require a tuple\n            # (to prohibit a list), but it's important\n            # for comparisons and uniformity\n            raise lena.core.LenaTypeError(\n                \"field_names must be a string or a tuple\"\n            )\n\n        if len(field_names) != len(coords):\n            raise lena.core.LenaValueError(\n                \"field_names must have must have the same size as coords\"\n            )\n\n        if len(set(field_names)) != len(field_names):\n            raise lena.core.LenaValueError(\n                \"field_names contains duplicates\"\n            )\n\n        self.coords = coords\n        self._scale = scale\n\n        # field_names are better than fields,\n        # because they are unambigous (as in namedtuple).\n        self.field_names = field_names\n\n        # decided to use \"error_x_low\" (like in ROOT).\n        # Other versions were x_error (looked better than x_err),\n        # but x_err_low looked much better than x_error_low).\n        try:\n            parsed_error_names = self._parse_error_names(field_names)\n        except lena.core.LenaValueError as err:\n            raise err\n            # in Python 3\n            # raise err from None\n        self._parsed_error_names = parsed_error_names\n\n        dim = len(field_names) - len(parsed_error_names)\n        self._coord_names = field_names[:dim]\n        self.dim = dim\n\n        # todo: add subsequences of coords as attributes\n        # with field names.\n        # In case if someone wants to create a graph of another function\n        # at the same coordinates.\n        # Should a) work when we rescale the graph\n        #        b) not interfere with other fields and methods\n\n        # Probably we won't add methods __del__(n), __add__(*coords),\n        # since it might change the scale.\n\n    def __eq__(self, other):\n        \"\"\"Two graphs are equal, if and only if they have\n        equal coordinates, field names and scales.\n\n        If *other* is not a :class:`.graph`, return ``False``.\n\n        Note that floating numbers should be compared\n        approximately (using :func:`math.isclose`).\n        Therefore this comparison may give false negatives.\n        \"\"\"\n        if not isinstance(other, graph):\n            # in Python comparison between different types is allowed\n            return False\n        return (self.coords == other.coords and self._scale == other._scale\n                and self.field_names == other.field_names)\n\n    def _get_err_indices(self, coord_name):\n        \"\"\"Get error indices corresponding to a coordinate.\"\"\"\n        err_indices = []\n        dim = self.dim\n        for ind, err in enumerate(self._parsed_error_names):\n            if err[1] == coord_name:\n                err_indices.append(ind+dim)\n        return err_indices\n\n    def __iter__(self):\n        \"\"\"Iterate graph coords one by one.\"\"\"\n        for val in zip(*self.coords):\n            yield val\n\n    def __repr__(self):\n        return \"\"\"graph({}, field_names={}, scale={})\"\"\".format(\n            self.coords, self.field_names, self._scale\n        )\n\n    def scale(self, other=None):\n        \"\"\"Get or set the scale of the graph.\n\n        If *other* is ``None``, return the scale of this graph.\n\n        If a numeric *other* is provided, rescale to that value.\n        If the graph has unknown or zero scale,\n        rescaling that will raise :exc:`~.LenaValueError`.\n\n        To get meaningful results, graph's fields are used.\n        Only the last coordinate is rescaled.\n        For example, if the graph has *x* and *y* coordinates,\n        then *y* will be rescaled, and for a 3-dimensional graph\n        *z* will be rescaled.\n        All errors are rescaled together with their coordinate.\n        \"\"\"\n        # this method is called scale() for uniformity with histograms\n        # And this looks really good: explicit for computations\n        # (not a subtle graph.scale, like a constant field (which is,\n        #  however, the case in graph - but not in other structures))\n        # and easy to remember (set_scale? rescale? change_scale_to?..)\n\n        # We modify the graph in place,\n        # because that would be redundant (not optimal)\n        # to create a new graph\n        # if we only want to change the scale of the existing one.\n\n        if other is None:\n            return self._scale\n\n        if not self._scale:\n            raise lena.core.LenaValueError(\n                \"can't rescale a graph with zero or unknown scale\"\n            )\n\n        last_coord_ind = self.dim - 1\n        last_coord_name = self.field_names[last_coord_ind]\n\n        last_coord_indices = ([last_coord_ind] +\n                self._get_err_indices(last_coord_name)\n        )\n\n        # In Python 2 3/2 is 1, so we want to be safe;\n        # the downside is that integer-valued graphs\n        # will become floating, but that is doubtfully an issue.\n        # Remove when/if dropping support for Python 2.\n        rescale = float(other) / self._scale\n\n        mul = operator.mul\n        partial = functools.partial\n\n        # a version with lambda is about 50% slower:\n        # timeit.timeit('[*map(lambda val: val*2, vals)]', \\\n        #     setup='vals = list(range(45)); from operator import mul; \\\n        #     from functools import partial')\n        # 3.159\n        # same setup for\n        # timeit.timeit('[*map(partial(mul, 2), vals)]',...):\n        # 2.075\n        # \n        # [*map(...)] is very slightly faster than list(map(...)),\n        # but it's unavailable in Python 2 (and anyway less readable).\n\n        # rescale arrays of values and errors\n        for ind, arr in enumerate(self.coords):\n            if ind in last_coord_indices:\n                # Python lists are faster than arrays,\n                # https://stackoverflow.com/a/62399645/952234\n                # (because each time taking a value from an array\n                #  creates a Python object)\n                self.coords[ind] = list(map(partial(mul, rescale),\n                                            arr))\n\n        self._scale = other\n\n        # as suggested in PEP 8\n        return None\n\n    def _parse_error_names(self, field_names):\n        # field_names is a parameter for easier testing,\n        # usually object's field_names are used.\n        errors = []\n\n        # collect all error fields and check that they are\n        # strictly after other fields\n        in_error_fields = False\n        # there is at least one field\n        last_coord_ind = 0\n        for ind, field in enumerate(field_names):\n            if field.startswith(\"error_\"):\n                in_error_fields = True\n                errors.append((field, ind))\n            else:\n                last_coord_ind = ind\n                if in_error_fields:\n                    raise lena.core.LenaValueError(\n                        \"errors must go after coordinate fields\"\n                    )\n\n        coords = set(field_names[:last_coord_ind+1])\n        parsed_errors = []\n\n        for err, ind in errors:\n            err_coords = []\n            for coord in coords:\n                err_main = err[6:]  # all after \"error_\"\n                if err_main == coord or err_main.startswith(coord + \"_\"):\n                    err_coords.append(coord)\n                    err_tail = err_main[len(coord)+1:]\n            if not err_coords:\n                raise lena.core.LenaValueError(\n                    \"no coordinate corresponding to {} given\".format(err)\n                )\n            elif len(err_coords) > 1:\n                raise lena.core.LenaValueError(\n                    \"ambiguous error \" + err +\\\n                    \" corresponding to several coordinates given\"\n                )\n            # \"error\" may be redundant, but it is explicit.\n            parsed_errors.append((\"error\", err_coords[0], err_tail, ind))\n\n        return parsed_errors\n\n    def _update_context(self, context):\n        \"\"\"Update *context* with the properties of this graph.\n\n        *context.error* is appended with indices of errors.\n        Example subcontext for a graph with fields \"E,t,error_E_low\":\n        {\"error\": {\"x_low\": {\"index\": 2}}}.\n        Note that error names are called \"x\", \"y\" and \"z\"\n        (this corresponds to first three coordinates,\n        if they are present), which allows to simplify plotting.\n        Existing values are not removed\n        from *context.value* and its subcontexts.\n\n        Called on \"destruction\" of the graph (for example,\n        in :class:`.ToCSV`). By destruction we mean conversion\n        to another structure (like text) in the flow.\n        The graph object is not really destroyed in this process.\n        \"\"\"\n        # this method is private, because we encourage users to yield\n        # graphs into the flow and process them with ToCSV element\n        # (not manually).\n\n        if not self._parsed_error_names:\n            # no error fields present\n            return\n\n        dim = self.dim\n\n        xyz_coord_names = self._coord_names[:3]\n        for name, coord_name in zip([\"x\", \"y\", \"z\"], xyz_coord_names):\n            for err in self._parsed_error_names:\n                if err[1] == coord_name:\n                    error_ind = err[3]\n                    if err[2]:\n                        # add error suffix\n                        error_name = name + \"_\" + err[2]\n                    else:\n                        error_name = name\n                    lena.context.update_recursively(\n                        context,\n                        \"error.{}.index\".format(error_name),\n                        # error can correspond both to variable and\n                        # value, so we put it outside value.\n                        # \"value.error.{}.index\".format(error_name),\n                        error_ind\n                    )\n\n\n# used in deprecated Graph\ndef _rescale_value(rescale, value):\n    return rescale * lena.flow.get_data(value)\n\n\nclass Graph(object):\n    \"\"\"\n    .. deprecated:: 0.5\n       use :class:`graph`.\n       This class may be used in the future,\n       but with a changed interface.\n\n    Function at given coordinates (arbitraty dimensions).\n\n    Graph points can be set during the initialization and\n    during :meth:`fill`. It can be rescaled (producing a new :class:`Graph`).\n    A point is a tuple of *(coordinate, value)*, where both *coordinate*\n    and *value* can be tuples of numbers.\n    *Coordinate* corresponds to a point in N-dimensional space,\n    while *value* is some function's value at this point\n    (the function can take a value in M-dimensional space).\n    Coordinate and value dimensions must be the same for all points.\n\n    One can get graph points as :attr:`Graph.points` attribute.\n    They will be sorted each time before return\n    if *sort* was set to ``True``.\n    An attempt to change points\n    (use :attr:`Graph.points` on the left of '=')\n    will raise Python's :exc:`AttributeError`.\n    \"\"\"\n\n    def __init__(self, points=None, context=None, scale=None, sort=True):\n        \"\"\"*points* is an array of *(coordinate, value)* tuples.\n\n        *context* is the same as the most recent context\n        during *fill*. Use it to provide a context\n        when initializing a :class:`Graph` from existing points.\n\n        *scale* sets the scale of the graph.\n        It is used during plotting if rescaling is needed.\n\n        Graph coordinates are sorted by default.\n        This is usually needed to plot graphs of functions.\n        If you need to keep the order of insertion, set *sort* to ``False``.\n\n        By default, sorting is done using standard Python\n        lists and functions. You can disable *sort* and provide your own\n        sorting container for *points*.\n        Some implementations are compared\n        `here <http://www.grantjenks.com/docs/sortedcontainers/performance.html>`_.\n        Note that a rescaled graph uses a default list.\n\n        Note that :class:`Graph` does not reduce data.\n        All filled values will be stored in it.\n        To reduce data, use histograms.\n        \"\"\"\n        warnings.warn(\"Graph is deprecated since Lena 0.5. Use graph.\",\n                      DeprecationWarning, stacklevel=2)\n\n        self._points = points if points is not None else []\n        # todo: add some sanity checks for points\n        self._scale = scale\n        self._init_context = {\"scale\": scale}\n        if context is None:\n            self._cur_context = {}\n        elif not isinstance(context, dict):\n            raise lena.core.LenaTypeError(\n                \"context must be a dict, {} provided\".format(context)\n            )\n        else:\n            self._cur_context = context\n        self._sort = sort\n\n        # todo: probably, scale from context is not needed.\n\n        ## probably this function is not needed.\n        ## it can't be copied, graphs won't be possible to compare.\n        # *rescale_value* is a function, which can be used to scale\n        # complex graph values.\n        # It must accept a rescale parameter and the value at a data point.\n        # By default, it is multiplication of rescale and the value\n        # (which must be a number).\n        # if rescale_value is None:\n        #     self._rescale_value = _rescale_value\n        self._rescale_value = _rescale_value\n        self._update()\n\n    def fill(self, value):\n        \"\"\"Fill the graph with *value*.\n\n        *Value* can be a *(data, context)* tuple.\n        *Data* part must be a *(coordinates, value)* pair,\n        where both coordinates and value are also tuples.\n        For example, *value* can contain the principal number\n        and its precision.\n        \"\"\"\n        point, self._cur_context = lena.flow.get_data_context(value)\n        # coords, val = point\n        self._points.append(point)\n\n    def request(self):\n        \"\"\"Yield graph with context.\n\n        If *sort* was initialized ``True``, graph points will be sorted.\n        \"\"\"\n        # If flow contained *scale* it the context, it is set now.\n        self._update()\n        yield (self, self._context)\n\n    # compute method shouldn't be in this class,\n    # because it is a pure FillRequest.\n    # def compute(self):\n    #     \"\"\"Yield graph with context (as in :meth:`request`),\n    #     and :meth:`reset`.\"\"\"\n    #     self._update()\n    #     yield (self, self._context)\n    #     self.reset()\n\n    @property\n    def points(self):\n        \"\"\"Get graph points (read only).\"\"\"\n        # sort points before giving them\n        self._update()\n        return self._points\n\n    def reset(self):\n        \"\"\"Reset points to an empty list\n        and current context to an empty dict.\n        \"\"\"\n        self._points = []\n        self._cur_context = {}\n\n    def __repr__(self):\n        self._update()\n        return (\"Graph(points={}, scale={}, sort={})\"\n                .format(self._points, self._scale, self._sort))\n\n    def scale(self, other=None):\n        \"\"\"Get or set the scale.\n\n        Graph's scale comes from an external source.\n        For example, if the graph was computed from a function,\n        this may be its integral passed via context during :meth:`fill`.\n        Once the scale is set, it is stored in the graph.\n        If one attempts to use scale which was not set,\n        :exc:`.LenaAttributeError` is raised.\n\n        If *other* is None, return the scale.\n\n        If a ``float`` *other* is provided, rescale to *other*.\n        A new graph with the scale equal to *other*\n        is returned, the original one remains unchanged.\n        Note that in this case its *points* will be a simple list\n        and new graph *sort* parameter will be ``True``.\n\n        Graphs with scale equal to zero can't be rescaled. \n        Attempts to do that raise :exc:`.LenaValueError`.\n        \"\"\"\n        if other is None:\n            # return scale\n            self._update()\n            if self._scale is None:\n                raise lena.core.LenaAttributeError(\n                    \"scale must be explicitly set before using that\"\n                )\n            return self._scale\n        else:\n            # rescale from other\n            scale = self.scale()\n            if scale == 0:\n                raise lena.core.LenaValueError(\n                    \"can't rescale graph with 0 scale\"\n                )\n\n            # new_init_context = copy.deepcopy(self._init_context)\n            # new_init_context.update({\"scale\": other})\n\n            rescale = float(other) / scale\n            new_points = []\n            for coord, val in self._points:\n                # probably not needed, because tuples are immutable:\n                # make a deep copy so that new values\n                # are completely independent from old ones.\n                new_points.append((coord, self._rescale_value(rescale, val)))\n            # todo: should it inherit context?\n            # Probably yes, but watch out scale.\n            new_graph = Graph(points=new_points, scale=other,\n                              sort=self._sort)\n            return new_graph\n\n    def to_csv(self, separator=\",\", header=None):\n        \"\"\".. deprecated:: 0.5 in Lena 0.5 to_csv is not used.\n              Iterables are converted to tables.\n\n        Convert graph's points to CSV.\n\n        *separator* delimits values, the default is comma.\n\n        *header*, if not ``None``, is the first string of the output\n        (new line is added automatically).\n\n        Since a graph can be multidimensional,\n        for each point first its coordinate is converted to string\n        (separated by *separator*), then each part of its value.\n\n        To convert :class:`Graph` to CSV inside a Lena sequence,\n        use :class:`lena.output.ToCSV`.\n        \"\"\"\n        if self._sort:\n            self._update()\n\n        def unpack_pt(pt):\n            coord = pt[0]\n            value = pt[1]\n            if isinstance(coord, tuple):\n                unpacked = list(coord)\n            else:\n                unpacked = [coord]\n            if isinstance(value, tuple):\n                unpacked += list(value)\n            else:\n                unpacked.append(value)\n            return unpacked\n\n        def pt_to_str(pt, separ):\n            return separ.join([str(val) for val in unpack_pt(pt)])\n\n        if header is not None:\n            # if one needs an empty header line, they may provide \"\"\n            lines = header + \"\\n\"\n        else:\n            lines = \"\"\n        lines += \"\\n\".join([pt_to_str(pt, separator) for pt in self.points])\n\n        return lines\n\n    #     *context* will be added to graph context.\n    #     If it contains \"scale\", :meth:`scale` method will be available.\n    #     Otherwise, if \"scale\" is contained in the context\n    #     during :meth:`fill`, it will be used.\n    #     In this case it is assumed that this scale\n    #     is same for all values (only the last filled context is checked).\n    #     Context from flow takes precedence over the initialized one.\n\n    def _update(self):\n        \"\"\"Sort points if needed, update context.\"\"\"\n        # todo: probably remove this context_scale?\n        context_scale = self._cur_context.get(\"scale\")\n        if context_scale is not None:\n            # this complex check is fine with rescale,\n            # because that returns a new graph (this scale unchanged).\n            if self._scale is not None and self._scale != context_scale:\n                raise lena.core.LenaRuntimeError(\n                    \"Initialization and context scale differ, \"\n                    \"{} and {} from context {}\"\n                    .format(self._scale, context_scale, self._cur_context)\n                )\n            self._scale = context_scale\n        if self._sort:\n            self._points = sorted(self._points)\n\n        self._context = copy.deepcopy(self._cur_context)\n        self._context.update(self._init_context)\n        # why this? Not *graph.scale*?\n        self._context.update({\"scale\": self._scale})\n        # self._context.update(lena.context.make_context(self, \"_scale\"))\n\n        # todo: make this check during fill. Probably initialize self._dim\n        # with kwarg dim. (dim of coordinates or values?)\n        if self._points:\n            # check points correctness\n            points = self._points\n            def coord_dim(coord):\n                if not hasattr(coord, \"__len__\"):\n                    return 1\n                return len(coord)\n            first_coord = points[0][0]\n            dim = coord_dim(first_coord)\n            same_dim = all(coord_dim(point[0]) == dim for point in points)\n            if not same_dim:\n                raise lena.core.LenaValueError(\n                    \"coordinates tuples must have same dimension, \"\n                    \"{} given\".format(points)\n                )\n            self.dim = dim\n            self._context[\"dim\"] = self.dim\n\n    def __eq__(self, other):\n        if not isinstance(other, Graph):\n            return False\n        if self.points != other.points:\n            return False\n        if self._scale is None and other._scale is None:\n            return True\n        try:\n            result = self.scale() == other.scale()\n        except lena.core.LenaAttributeError:\n            # one scale couldn't be computed\n            return False\n        else:\n            return result\n",
            "file_path": "lena/structures/graph.py",
            "human_label": "Get or set the scale of the graph.\n\nIf *other* is ``None``, return the scale of this graph.\n\nIf a numeric *other* is provided, rescale to that value.\nIf the graph has unknown or zero scale,\nrescaling that will raise :exc:`~.LenaValueError`.\n\nTo get meaningful results, graph's fields are used.\nOnly the last coordinate is rescaled.\nFor example, if the graph has *x* and *y* coordinates,\nthen *y* will be rescaled, and for a 3-dimensional graph\n*z* will be rescaled.\nAll errors are rescaled together with their coordinate.",
            "level": "class_runnable",
            "lineno": "192",
            "name": "scale",
            "oracle_context": "{ \"apis\" : \"['LenaValueError', 'float', 'partial', 'list', 'map', 'enumerate', '_get_err_indices']\", \"classes\" : \"['functools', 'operator']\", \"vars\" : \"['field_names', 'mul', 'coords', 'dim', 'core', '_scale', 'lena']\" }",
            "package": "graph",
            "project": "ynikitenko/lena",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b87b869a0c4fa8b80b35e1",
            "all_context": "{ \"import\" : \"\", \"file\" : \"\", \"class\" : \"\" }",
            "code": "def hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n    By default it is bin content.\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars.\n    For example, to create a graph with errors\n    from a histogram where bins contain\n    a named tuple with fields *mean*, *mean_error* and a context\n    one could use\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n    *get_coordinate* defines what the coordinate\n    of a graph point created from a histogram bin will be.\n    It can be \"left\" (default), \"right\" and \"middle\".\n\n    *field_names* set field names of the graph. Their number\n    must be the same as the dimension of the result.\n    For a *make_value* above they would be\n    *(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n    *scale* becomes the graph's scale (unknown by default).\n    If it is ``True``, it uses the histogram scale.\n\n    *hist* must contain only numeric bins (without context)\n    or *make_value* must remove context when creating a numeric graph.\n\n    Return the resulting graph.\n    \"\"\"\n    ## Could have allowed get_coordinate to be callable\n    # (for generality), but 1) first find a use case,\n    # 2) histogram bins could be adjusted in the first place.\n    # -- don't understand 2.\n    if get_coordinate == \"left\":\n        get_coord = lambda edges: tuple(coord[0] for coord in edges)\n    elif get_coordinate == \"right\":\n        get_coord = lambda edges: tuple(coord[1] for coord in edges)\n    # *middle* between the two edges, not the *center* of the bin\n    # as a whole (because the graph corresponds to a point)\n    elif get_coordinate == \"middle\":\n        get_coord = lambda edges: tuple(0.5*(coord[0] + coord[1])\n                                        for coord in edges)\n    else:\n        raise lena.core.LenaValueError(\n            'get_coordinate must be one of \"left\", \"right\" or \"middle\"; '\n            '\"{}\" provided'.format(get_coordinate)\n        )\n\n    # todo: make_value may be bad design.\n    # Maybe allow to change the graph in the sequence.\n    # However, make_value allows not to recreate a graph\n    # or its coordinates (if that is not needed).\n\n    if isinstance(field_names, str):\n        # copied from graph.__init__\n        field_names = tuple(re.findall(r'[^,\\s]+', field_names))\n    elif not isinstance(field_names, tuple):\n        raise lena.core.LenaTypeError(\n            \"field_names must be a string or a tuple\"\n        )\n    coords = [[] for _ in field_names]\n\n    chain = itertools.chain\n\n    if scale is True:\n        scale = hist.scale()\n\n    for value, edges in iter_bins_with_edges(hist.bins, hist.edges):\n        coord = get_coord(edges)\n\n        # Since we never use contexts here, it will be optimal\n        # to ignore them completely (remove them elsewhere).\n        # bin_value = lena.flow.get_data(value)\n        bin_value = value\n\n        if make_value is None:\n            graph_value = bin_value\n        else:\n            graph_value = make_value(bin_value)\n\n        # for iteration below\n        if not hasattr(graph_value, \"__iter__\"):\n            graph_value = (graph_value,)\n\n        # add each coordinate to respective array\n        for arr, coord_ in zip(coords, chain(coord, graph_value)):\n            arr.append(coord_)\n\n    return _graph(coords, field_names=field_names, scale=scale)\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : true, \"public_lib\" : true, \"current_class\" : false, \"current_file\" : true, \"current_project\" : true, \"external\" : false }",
            "docstring": "Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.",
            "end_lineno": "391",
            "file_content": "\"\"\"Functions for histograms.\n\nThese functions are used for low-level work\nwith histograms and their contents.\nThey are not needed for normal usage.\n\"\"\"\nimport collections\nimport copy\nimport itertools\nimport operator\nimport re\nimport sys\nif sys.version_info.major == 3:\n    from functools import reduce as _reduce\nelse:\n    _reduce = reduce\n\nimport lena.core\nfrom .graph import graph as _graph\n\n\nclass HistCell(collections.namedtuple(\"HistCell\", (\"edges, bin, index\"))):\n    \"\"\"A namedtuple with fields *edges, bin, index*.\"\"\"\n    # from Aaron Hall's answer https://stackoverflow.com/a/28568351/952234\n    __slots__ = ()\n\n\ndef cell_to_string(\n        cell_edges, var_context=None, coord_names=None,\n        coord_fmt=\"{}_lte_{}_lt_{}\", coord_join=\"_\", reverse=False):\n    \"\"\"Transform cell edges into a string.\n\n    *cell_edges* is a tuple of pairs *(lower bound, upper bound)*\n    for each coordinate.\n\n    *coord_names* is a list of coordinates names.\n\n    *coord_fmt* is a string,\n    which defines how to format individual coordinates.\n\n    *coord_join* is a string, which joins coordinate pairs.\n\n    If *reverse* is True, coordinates are joined in reverse order.\n    \"\"\"\n    # todo: do we really need var_context?\n    # todo: even if so, why isn't that a {}? Is that dangerous?\n    if coord_names is None:\n        if var_context is None:\n            coord_names = [\n                \"coord{}\".format(ind) for ind in range(len(cell_edges))\n            ]\n        else:\n            if \"combine\" in var_context:\n                coord_names = [var[\"name\"]\n                               for var in var_context[\"combine\"]]\n            else:\n                coord_names = [var_context[\"name\"]]\n    if len(cell_edges) != len(coord_names):\n        raise lena.core.LenaValueError(\n            \"coord_names must have same length as cell_edges, \"\n            \"{} and {} given\".format(coord_names, cell_edges)\n        )\n    coord_strings = [coord_fmt.format(edge[0], coord_names[ind], edge[1])\n                     for (ind, edge) in enumerate(cell_edges)]\n    if reverse:\n        coord_strings = reversed(coord_strings)\n    coord_str = coord_join.join(coord_strings)\n    return coord_str\n\n\ndef _check_edges_increasing_1d(arr):\n    if len(arr) <= 1:\n        raise lena.core.LenaValueError(\"size of edges should be more than one,\"\n                                       \" {} provided\".format(arr))\n    increasing = (tup[0] < tup[1] for tup in zip(arr, arr[1:]))\n    if not all(increasing):\n        raise lena.core.LenaValueError(\n            \"expected strictly increasing values, \"\n            \"{} provided\".format(arr)\n        )\n\n\ndef check_edges_increasing(edges):\n    \"\"\"Assure that multidimensional *edges* are increasing.\n\n    If length of *edges* or its subarray is less than 2\n    or if some subarray of *edges*\n    contains not strictly increasing values,\n    :exc:`.LenaValueError` is raised.\n    \"\"\"\n    if not len(edges):\n        raise lena.core.LenaValueError(\"edges must be non-empty\")\n    elif not hasattr(edges[0], '__iter__'):\n        _check_edges_increasing_1d(edges)\n        return\n    for arr in edges:\n        if len(arr) <= 1:\n            raise lena.core.LenaValueError(\n                \"size of edges should be more than one. \"\n                \"{} provided\".format(arr)\n            )\n        _check_edges_increasing_1d(arr)\n\n\ndef get_bin_edges(index, edges):\n    \"\"\"Return edges of the bin for the given *edges* of a histogram.\n\n    In one-dimensional case *index* must be an integer and a tuple\n    of *(x_low_edge, x_high_edge)* for that bin is returned.\n\n    In a multidimensional case *index* is a container of numeric indices\n    in each dimension.\n    A list of bin edges in each dimension is returned.\"\"\"\n    # todo: maybe give up this 1- and multidimensional unification\n    # and write separate functions for each case.\n    if not hasattr(edges[0], '__iter__'):\n        # 1-dimensional edges\n        if hasattr(index, '__iter__'):\n            index = index[0]\n        return (edges[index], edges[index+1])\n    # multidimensional edges\n    return [(edges[coord][i], edges[coord][i+1])\n            for coord, i in enumerate(index)]\n\n\ndef get_bin_on_index(index, bins):\n    \"\"\"Return bin corresponding to multidimensional *index*.\n\n    *index* can be a number or a list/tuple.\n    If *index* length is less than dimension of *bins*,\n    a subarray of *bins* is returned.\n\n    In case of an index error, :exc:`.LenaIndexError` is raised.\n\n    Example:\n\n    >>> from lena.structures import histogram, get_bin_on_index\n    >>> hist = histogram([0, 1], [0])\n    >>> get_bin_on_index(0, hist.bins)\n    0\n    >>> get_bin_on_index((0, 1), [[0, 1], [0, 0]])\n    1\n    >>> get_bin_on_index(0, [[0, 1], [0, 0]])\n    [0, 1]\n    \"\"\"\n    if not isinstance(index, (list, tuple)):\n        index = [index]\n    subarr = bins\n    for ind in index:\n        try:\n            subarr = subarr[ind]\n        except IndexError:\n            raise lena.core.LenaIndexError(\n                \"bad index: {}, bins = {}\".format(index, bins)\n            )\n    return subarr\n\n\ndef get_bin_on_value_1d(val, arr):\n    \"\"\"Return index for value in one-dimensional array.\n\n    *arr* must contain strictly increasing values\n    (not necessarily equidistant),\n    it is not checked.\n\n    \"Linear binary search\" is used,\n    that is our array search by default assumes\n    the array to be split on equidistant steps.\n\n    Example:\n\n    >>> from lena.structures import get_bin_on_value_1d\n    >>> arr = [0, 1, 4, 5, 7, 10]\n    >>> get_bin_on_value_1d(0, arr)\n    0\n    >>> get_bin_on_value_1d(4.5, arr)\n    2\n    >>> # upper range is excluded\n    >>> get_bin_on_value_1d(10, arr)\n    5\n    >>> # underflow\n    >>> get_bin_on_value_1d(-10, arr)\n    -1\n    \"\"\"\n    # may also use numpy.searchsorted\n    # https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.searchsorted.html\n    ind_min = 0\n    ind_max = len(arr) - 1\n    while True:\n        if ind_max - ind_min <= 1:\n            # lower bound is close\n            if val < arr[ind_min]:\n                return ind_min - 1\n            # upper bound is open\n            elif val >= arr[ind_max]:\n                return ind_max\n            else:\n                return ind_min\n        if val == arr[ind_min]:\n            return ind_min\n        if val < arr[ind_min]:\n            return ind_min - 1\n        elif val >= arr[ind_max]:\n            return ind_max\n        else:\n            shift = int(\n                (ind_max - ind_min) * (\n                    float(val - arr[ind_min]) / (arr[ind_max] - arr[ind_min])\n                ))\n            ind_guess = ind_min + shift\n\n            if ind_min == ind_guess:\n                ind_min += 1\n                continue\n            # ind_max is always more that ind_guess,\n            # because val < arr[ind_max] (see the formula for shift).\n            # This branch is not needed and can't be tested.\n            # But for the sake of numerical inaccuracies, let us keep this\n            # so that we never get into an infinite loop.\n            elif ind_max == ind_guess:\n                ind_max -= 1\n                continue\n\n            if val < arr[ind_guess]:\n                ind_max = ind_guess\n            else:\n                ind_min = ind_guess\n\n\ndef get_bin_on_value(arg, edges):\n    \"\"\"Get the bin index for *arg* in a multidimensional array *edges*.\n\n    *arg* is a 1-dimensional array of numbers\n    (or a number for 1-dimensional *edges*),\n    and corresponds to a point in N-dimensional space.\n\n    *edges* is an array of N-1 dimensional arrays (lists or tuples) of numbers.\n    Each 1-dimensional subarray consists of increasing numbers.\n\n    *arg* and *edges* must have the same length\n    (otherwise :exc:`.LenaValueError` is raised).\n    *arg* and *edges* must be iterable and support *len()*.\n\n    Return list of indices in *edges* corresponding to *arg*.\n\n    If any coordinate is out of its corresponding edge range,\n    its index will be ``-1`` for underflow\n    or ``len(edge)-1`` for overflow.\n\n    Examples:\n\n    >>> from lena.structures import get_bin_on_value\n    >>> edges = [[1, 2, 3], [1, 3.5]]\n    >>> get_bin_on_value((1.5, 2), edges)\n    [0, 0]\n    >>> get_bin_on_value((1.5, 0), edges)\n    [0, -1]\n    >>> # the upper edge is excluded\n    >>> get_bin_on_value((3, 2), edges)\n    [2, 0]\n    >>> # one-dimensional edges\n    >>> edges = [1, 2, 3]\n    >>> get_bin_on_value(2, edges)\n    [1]\n    \"\"\"\n    # arg is a one-dimensional index\n    if not isinstance(arg, (tuple, list)):\n        return [get_bin_on_value_1d(arg, edges)]\n    # arg is a multidimensional index\n    if len(arg) != len(edges):\n        raise lena.core.LenaValueError(\n            \"argument should have same dimension as edges. \"\n            \"arg = {}, edges = {}\".format(arg, edges)\n        )\n    indices = []\n    for ind, array in enumerate(edges):\n        cur_bin = get_bin_on_value_1d(arg[ind], array)\n        indices.append(cur_bin)\n    return indices\n\n\ndef get_example_bin(struct):\n    \"\"\"Return bin with zero index on each axis of the histogram bins.\n\n    For example, if the histogram is two-dimensional, return hist[0][0].\n\n    *struct* can be a :class:`.histogram`\n    or an array of bins.\n    \"\"\"\n    if isinstance(struct, lena.structures.histogram):\n        return lena.structures.get_bin_on_index([0] * struct.dim, struct.bins)\n    else:\n        bins = struct\n        while isinstance(bins, list):\n            bins = bins[0]\n        return bins\n\n\ndef hist_to_graph(hist, make_value=None, get_coordinate=\"left\",\n                  field_names=(\"x\", \"y\"), scale=None):\n    \"\"\"Convert a :class:`.histogram` to a :class:`.graph`.\n\n    *make_value* is a function to set the value of a graph's point.\n    By default it is bin content.\n    *make_value* accepts a single value (bin content) without context.\n\n    This option could be used to create graph's error bars.\n    For example, to create a graph with errors\n    from a histogram where bins contain\n    a named tuple with fields *mean*, *mean_error* and a context\n    one could use\n\n    >>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n    *get_coordinate* defines what the coordinate\n    of a graph point created from a histogram bin will be.\n    It can be \"left\" (default), \"right\" and \"middle\".\n\n    *field_names* set field names of the graph. Their number\n    must be the same as the dimension of the result.\n    For a *make_value* above they would be\n    *(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n    *scale* becomes the graph's scale (unknown by default).\n    If it is ``True``, it uses the histogram scale.\n\n    *hist* must contain only numeric bins (without context)\n    or *make_value* must remove context when creating a numeric graph.\n\n    Return the resulting graph.\n    \"\"\"\n    ## Could have allowed get_coordinate to be callable\n    # (for generality), but 1) first find a use case,\n    # 2) histogram bins could be adjusted in the first place.\n    # -- don't understand 2.\n    if get_coordinate == \"left\":\n        get_coord = lambda edges: tuple(coord[0] for coord in edges)\n    elif get_coordinate == \"right\":\n        get_coord = lambda edges: tuple(coord[1] for coord in edges)\n    # *middle* between the two edges, not the *center* of the bin\n    # as a whole (because the graph corresponds to a point)\n    elif get_coordinate == \"middle\":\n        get_coord = lambda edges: tuple(0.5*(coord[0] + coord[1])\n                                        for coord in edges)\n    else:\n        raise lena.core.LenaValueError(\n            'get_coordinate must be one of \"left\", \"right\" or \"middle\"; '\n            '\"{}\" provided'.format(get_coordinate)\n        )\n\n    # todo: make_value may be bad design.\n    # Maybe allow to change the graph in the sequence.\n    # However, make_value allows not to recreate a graph\n    # or its coordinates (if that is not needed).\n\n    if isinstance(field_names, str):\n        # copied from graph.__init__\n        field_names = tuple(re.findall(r'[^,\\s]+', field_names))\n    elif not isinstance(field_names, tuple):\n        raise lena.core.LenaTypeError(\n            \"field_names must be a string or a tuple\"\n        )\n    coords = [[] for _ in field_names]\n\n    chain = itertools.chain\n\n    if scale is True:\n        scale = hist.scale()\n\n    for value, edges in iter_bins_with_edges(hist.bins, hist.edges):\n        coord = get_coord(edges)\n\n        # Since we never use contexts here, it will be optimal\n        # to ignore them completely (remove them elsewhere).\n        # bin_value = lena.flow.get_data(value)\n        bin_value = value\n\n        if make_value is None:\n            graph_value = bin_value\n        else:\n            graph_value = make_value(bin_value)\n\n        # for iteration below\n        if not hasattr(graph_value, \"__iter__\"):\n            graph_value = (graph_value,)\n\n        # add each coordinate to respective array\n        for arr, coord_ in zip(coords, chain(coord, graph_value)):\n            arr.append(coord_)\n\n    return _graph(coords, field_names=field_names, scale=scale)\n\n\ndef init_bins(edges, value=0, deepcopy=False):\n    \"\"\"Initialize cells of the form *edges* with the given *value*.\n\n    Return bins filled with copies of *value*.\n\n    *Value* must be copyable, usual numbers will suit.\n    If the value is mutable, use *deepcopy =* ``True``\n    (or the content of cells will be identical).\n\n    Examples:\n\n    >>> edges = [[0, 1], [0, 1]]\n    >>> # one cell\n    >>> init_bins(edges)\n    [[0]]\n    >>> # no need to use floats,\n    >>> # because integers will automatically be cast to floats\n    >>> # when used together\n    >>> init_bins(edges, 0.0)\n    [[0.0]]\n    >>> init_bins([[0, 1, 2], [0, 1, 2]])\n    [[0, 0], [0, 0]]\n    >>> init_bins([0, 1, 2])\n    [0, 0]\n    \"\"\"\n    nbins = len(edges) - 1\n    if not isinstance(edges[0], (list, tuple)):\n        # edges is one-dimensional\n        if deepcopy:\n            return [copy.deepcopy(value) for _ in range(nbins)]\n        else:\n            return [value] * nbins\n    for ind, arr in enumerate(edges):\n        if ind == nbins:\n            if deepcopy:\n                return [copy.deepcopy(value) for _ in range(len(arr)-1)]\n            else:\n                return list([value] * (len(arr)-1))\n        bins = []\n        for _ in range(len(arr)-1):\n            bins.append(init_bins(edges[ind+1:], value, deepcopy))\n        return bins\n\n\ndef integral(bins, edges):\n    \"\"\"Compute integral (scale for a histogram).\n\n    *bins* contain values, and *edges* form the mesh\n    for the integration.\n    Their format is defined in :class:`.histogram` description.\n    \"\"\"\n    total = 0\n    for ind, bin_content in iter_bins(bins):\n        bin_lengths = [\n            edges[coord][i+1] - edges[coord][i]\n            for coord, i in enumerate(ind)\n        ]\n        # product\n        vol = _reduce(operator.mul, bin_lengths, 1)\n        cell_integral = vol * bin_content\n        total += cell_integral\n    return total\n\n\ndef iter_bins(bins):\n    \"\"\"Iterate on *bins*. Yield *(index, bin content)*.\n\n    Edges with higher index are iterated first\n    (that is z, then y, then x for a 3-dimensional histogram).\n    \"\"\"\n    # if not isinstance(bins, (list, tuple)):\n    if not hasattr(bins, '__iter__'):\n        # cell\n        yield ((), bins)\n    else:\n        for ind, _ in enumerate(bins):\n            for sub_ind, val in iter_bins(bins[ind]):\n                yield (((ind,) + sub_ind), val)\n\n\ndef iter_bins_with_edges(bins, edges):\n    \"\"\"Generate *(bin content, bin edges)* pairs.\n\n    Bin edges is a tuple, such that\n    its item at index i is *(lower bound, upper bound)*\n    of the bin at i-th coordinate.\n\n    Examples:\n\n    >>> from lena.math import mesh\n    >>> list(iter_bins_with_edges([0, 1, 2], edges=mesh((0, 3), 3)))\n    [(0, ((0, 1.0),)), (1, ((1.0, 2.0),)), (2, ((2.0, 3),))]\n    >>>\n    >>> # 2-dimensional histogram\n    >>> list(iter_bins_with_edges(\n    ...     bins=[[2]], edges=mesh(((0, 1), (0, 1)), (1, 1))\n    ... ))\n    [(2, ((0, 1), (0, 1)))]\n\n    .. versionadded:: 0.5\n       made public.\n    \"\"\"\n    # todo: only a list or also a tuple, an array?\n    if not isinstance(edges[0], list):\n        edges = [edges]\n    bins_sizes = [len(edge)-1 for edge in edges]\n    indices = [list(range(nbins)) for nbins in bins_sizes]\n    for index in itertools.product(*indices):\n        bin_ = lena.structures.get_bin_on_index(index, bins)\n        edges_low = []\n        edges_high = []\n        for var, var_ind in enumerate(index):\n            edges_low.append(edges[var][var_ind])\n            edges_high.append(edges[var][var_ind+1])\n        yield (bin_, tuple(zip(edges_low, edges_high)))\n\n\ndef iter_cells(hist, ranges=None, coord_ranges=None):\n    \"\"\"Iterate cells of a histogram *hist*, possibly in a subrange.\n\n    For each bin, yield a :class:`HistCell`\n    containing *bin edges, bin content* and *bin index*.\n    The order of iteration is the same as for :func:`iter_bins`.\n\n    *ranges* are the ranges of bin indices to be used\n    for each coordinate\n    (the lower value is included, the upper value is excluded).\n\n    *coord_ranges* set real coordinate ranges based on histogram edges.\n    Obviously, they can be not exactly bin edges.\n    If one of the ranges for the given coordinate\n    is outside the histogram edges,\n    then only existing histogram edges within the range are selected.\n    If the coordinate range is completely outside histogram edges,\n    nothing is yielded.\n    If a lower or upper *coord_range*\n    falls within a bin, this bin is yielded.\n    Note that if a coordinate range falls on a bin edge,\n    the number of generated bins can be unstable\n    because of limited float precision.\n\n    *ranges* and *coord_ranges* are tuples of tuples of limits\n    in corresponding dimensions. \n    For one-dimensional histogram it must be a tuple \n    containing a tuple, for example\n    *((None, None),)*.\n\n    ``None`` as an upper or lower *range* means no limit\n    (*((None, None),)* is equivalent to *((0, len(bins)),)*\n    for a 1-dimensional histogram).\n\n    If a *range* index is lower than 0 or higher than possible index,\n    :exc:`.LenaValueError` is raised.\n    If both *coord_ranges* and *ranges* are provided,\n    :exc:`.LenaTypeError` is raised.\n    \"\"\"\n    # for bin_ind, bin_ in iter_bins(hist.bins):\n    #     yield HistCell(get_bin_edges(bin_ind, hist.edges), bin_, bin_ind)\n    # if bins and edges are calculated each time, save the result now\n    bins, edges = hist.bins, hist.edges\n    # todo: hist.edges must be same\n    # for 1- and multidimensional histograms.\n    if hist.dim == 1:\n        edges = (edges,)\n\n    if coord_ranges is not None:\n        if ranges is not None:\n            raise lena.core.LenaTypeError(\n                \"only ranges or coord_ranges can be provided, not both\"\n            )\n        ranges = []\n        if not isinstance(coord_ranges[0], (tuple, list)):\n            coord_ranges = (coord_ranges, )\n        for coord, coord_range in enumerate(coord_ranges):\n            # todo: (dis?)allow None as an infinite range.\n            # todo: raise or transpose unordered coordinates?\n            # todo: change the order of function arguments.\n            lower_bin_ind = get_bin_on_value_1d(coord_range[0], edges[coord])\n            if lower_bin_ind == -1:\n                 lower_bin_ind = 0\n            upper_bin_ind = get_bin_on_value_1d(coord_range[1], edges[coord])\n            max_ind = len(edges[coord])\n            if upper_bin_ind == max_ind:\n                 upper_bin_ind -= 1\n            if lower_bin_ind >= max_ind or upper_bin_ind <= 0:\n                 # histogram edges are outside the range.\n                 return\n            ranges.append((lower_bin_ind, upper_bin_ind))\n\n    if not ranges:\n        ranges = ((None, None),) * hist.dim\n\n    real_ind_ranges = []\n    for coord, coord_range in enumerate(ranges):\n        low, up = coord_range\n        if low is None:\n            low = 0\n        else:\n            # negative indices should not be supported\n            if low < 0:\n                raise lena.core.LenaValueError(\n                    \"low must be not less than 0 if provided\"\n                )\n        max_ind = len(edges[coord]) - 1\n        if up is None:\n            up = max_ind\n        else:\n            # huge indices should not be supported as well.\n            if up > max_ind:\n                raise lena.core.LenaValueError(\n                    \"up must not be greater than len(edges)-1, if provided\"\n                )\n        real_ind_ranges.append(list(range(low, up)))\n\n    indices = list(itertools.product(*real_ind_ranges))\n    for ind in indices:\n        yield HistCell(get_bin_edges(ind, edges),\n                       get_bin_on_index(ind, bins),\n                       ind)\n\n\ndef make_hist_context(hist, context):\n    \"\"\"Update a deep copy of *context* with the context\n    of a :class:`.histogram` *hist*.\n\n    .. deprecated:: 0.5\n       histogram context is updated automatically\n       during conversion in :class:`~.output.ToCSV`.\n       Use histogram._update_context explicitly if needed.\n    \"\"\"\n    # absolutely unnecessary.\n    context = copy.deepcopy(context)\n\n    hist_context = {\n        \"histogram\": {\n            \"dim\": hist.dim,\n            \"nbins\": hist.nbins,\n            \"ranges\": hist.ranges\n        }\n    }\n    context.update(hist_context)\n    # just bad.\n    return context\n\n\ndef unify_1_md(bins, edges):\n    \"\"\"Unify 1- and multidimensional bins and edges.\n\n    Return a tuple of *(bins, edges)*.  \n    Bins and multidimensional *edges* return unchanged,\n    while one-dimensional *edges* are inserted into a list.\n    \"\"\"\n    if hasattr(edges[0], '__iter__'):\n    # if isinstance(edges[0], (list, tuple)):\n        return (bins, edges)\n    else:\n        return (bins, [edges])\n",
            "file_path": "lena/structures/hist_functions.py",
            "human_label": "Convert a :class:`.histogram` to a :class:`.graph`.\n\n*make_value* is a function to set the value of a graph's point.\nBy default it is bin content.\n*make_value* accepts a single value (bin content) without context.\n\nThis option could be used to create graph's error bars.\nFor example, to create a graph with errors\nfrom a histogram where bins contain\na named tuple with fields *mean*, *mean_error* and a context\none could use\n\n>>> make_value = lambda bin_: (bin_.mean, bin_.mean_error)\n\n*get_coordinate* defines what the coordinate\nof a graph point created from a histogram bin will be.\nIt can be \"left\" (default), \"right\" and \"middle\".\n\n*field_names* set field names of the graph. Their number\nmust be the same as the dimension of the result.\nFor a *make_value* above they would be\n*(\"x\", \"y_mean\", \"y_mean_error\")*.\n\n*scale* becomes the graph's scale (unknown by default).\nIf it is ``True``, it uses the histogram scale.\n\n*hist* must contain only numeric bins (without context)\nor *make_value* must remove context when creating a numeric graph.\n\nReturn the resulting graph.",
            "level": "project_runnable",
            "lineno": "299",
            "name": "hist_to_graph",
            "oracle_context": "{ \"apis\" : \"['chain', 'get_coord', 'isinstance', 'LenaValueError', 'iter_bins_with_edges', '_graph', 'append', 'make_value', 'format', 'tuple', 'LenaTypeError', 'findall', 'scale', 'zip', 'hasattr']\", \"classes\" : \"['itertools', 're', '_graph']\", \"vars\" : \"['Str', 'arr', 'bins', 'edges', 'core', 'lena']\" }",
            "package": "hist_functions",
            "project": "ynikitenko/lena",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b8b4baeb7e40a82d2d1136",
            "all_context": "{ \"import\" : \"inspect sys types __future__ zope \", \"file\" : \"__all__ ; MethodTypes ; _verify(iface,candidate,tentative,vtype) ; _verify_element(iface,name,desc,candidate,vtype) ; verifyClass(iface,candidate,tentative) ; verifyObject(iface,candidate,tentative) ; _MSG_TOO_MANY ; _KNOWN_PYPY2_FALSE_POSITIVES ; _pypy2_false_positive(msg,candidate,vtype) ; _incompat(required,implemented) ; \", \"class\" : \"\" }",
            "code": "def _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n\n    if vtype == 'c':\n        tester = iface.implementedBy\n    else:\n        tester = iface.providedBy\n\n    excs = []\n    if not tentative and not tester(candidate):\n        excs.append(DoesNotImplement(iface, candidate))\n\n    for name, desc in iface.namesAndDescriptions(all=True):\n        try:\n            _verify_element(iface, name, desc, candidate, vtype)\n        except Invalid as e:\n            excs.append(e)\n\n    if excs:\n        if len(excs) == 1:\n            raise excs[0]\n        raise MultipleInvalid(iface, candidate, excs)\n\n    return True\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : false, \"public_lib\" : true, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.",
            "end_lineno": "94",
            "file_content": "##############################################################################\n#\n# Copyright (c) 2001, 2002 Zope Foundation and Contributors.\n# All Rights Reserved.\n#\n# This software is subject to the provisions of the Zope Public License,\n# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.\n# THIS SOFTWARE IS PROVIDED \"AS IS\" AND ANY AND ALL EXPRESS OR IMPLIED\n# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS\n# FOR A PARTICULAR PURPOSE.\n#\n##############################################################################\n\"\"\"Verify interface implementations\n\"\"\"\nfrom __future__ import print_function\nimport inspect\nimport sys\nfrom types import FunctionType\nfrom types import MethodType\n\nfrom zope.interface._compat import PYPY2\n\nfrom zope.interface.exceptions import BrokenImplementation\nfrom zope.interface.exceptions import BrokenMethodImplementation\nfrom zope.interface.exceptions import DoesNotImplement\nfrom zope.interface.exceptions import Invalid\nfrom zope.interface.exceptions import MultipleInvalid\n\nfrom zope.interface.interface import fromMethod, fromFunction, Method\n\n__all__ = [\n    'verifyObject',\n    'verifyClass',\n]\n\n# This will be monkey-patched when running under Zope 2, so leave this\n# here:\nMethodTypes = (MethodType, )\n\n\ndef _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n\n    if vtype == 'c':\n        tester = iface.implementedBy\n    else:\n        tester = iface.providedBy\n\n    excs = []\n    if not tentative and not tester(candidate):\n        excs.append(DoesNotImplement(iface, candidate))\n\n    for name, desc in iface.namesAndDescriptions(all=True):\n        try:\n            _verify_element(iface, name, desc, candidate, vtype)\n        except Invalid as e:\n            excs.append(e)\n\n    if excs:\n        if len(excs) == 1:\n            raise excs[0]\n        raise MultipleInvalid(iface, candidate, excs)\n\n    return True\n\ndef _verify_element(iface, name, desc, candidate, vtype):\n    # Here the `desc` is either an `Attribute` or `Method` instance\n    try:\n        attr = getattr(candidate, name)\n    except AttributeError:\n        if (not isinstance(desc, Method)) and vtype == 'c':\n            # We can't verify non-methods on classes, since the\n            # class may provide attrs in it's __init__.\n            return\n        # TODO: On Python 3, this should use ``raise...from``\n        raise BrokenImplementation(iface, desc, candidate)\n\n    if not isinstance(desc, Method):\n        # If it's not a method, there's nothing else we can test\n        return\n\n    if inspect.ismethoddescriptor(attr) or inspect.isbuiltin(attr):\n        # The first case is what you get for things like ``dict.pop``\n        # on CPython (e.g., ``verifyClass(IFullMapping, dict))``). The\n        # second case is what you get for things like ``dict().pop`` on\n        # CPython (e.g., ``verifyObject(IFullMapping, dict()))``.\n        # In neither case can we get a signature, so there's nothing\n        # to verify. Even the inspect module gives up and raises\n        # ValueError: no signature found. The ``__text_signature__`` attribute\n        # isn't typically populated either.\n        #\n        # Note that on PyPy 2 or 3 (up through 7.3 at least), these are\n        # not true for things like ``dict.pop`` (but might be true for C extensions?)\n        return\n\n    if isinstance(attr, FunctionType):\n        if sys.version_info[0] >= 3 and isinstance(candidate, type) and vtype == 'c':\n            # This is an \"unbound method\" in Python 3.\n            # Only unwrap this if we're verifying implementedBy;\n            # otherwise we can unwrap @staticmethod on classes that directly\n            # provide an interface.\n            meth = fromFunction(attr, iface, name=name,\n                                imlevel=1)\n        else:\n            # Nope, just a normal function\n            meth = fromFunction(attr, iface, name=name)\n    elif (isinstance(attr, MethodTypes)\n          and type(attr.__func__) is FunctionType):\n        meth = fromMethod(attr, iface, name)\n    elif isinstance(attr, property) and vtype == 'c':\n        # Without an instance we cannot be sure it's not a\n        # callable.\n        # TODO: This should probably check inspect.isdatadescriptor(),\n        # a more general form than ``property``\n        return\n\n    else:\n        if not callable(attr):\n            raise BrokenMethodImplementation(desc, \"implementation is not a method\",\n                                             attr, iface, candidate)\n        # sigh, it's callable, but we don't know how to introspect it, so\n        # we have to give it a pass.\n        return\n\n    # Make sure that the required and implemented method signatures are\n    # the same.\n    mess = _incompat(desc.getSignatureInfo(), meth.getSignatureInfo())\n    if mess:\n        if PYPY2 and _pypy2_false_positive(mess, candidate, vtype):\n            return\n        raise BrokenMethodImplementation(desc, mess, attr, iface, candidate)\n\n\n\ndef verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    return _verify(iface, candidate, tentative, vtype='c')\n\ndef verifyObject(iface, candidate, tentative=False):\n    return _verify(iface, candidate, tentative, vtype='o')\n\nverifyObject.__doc__ = _verify.__doc__\n\n_MSG_TOO_MANY = 'implementation requires too many arguments'\n_KNOWN_PYPY2_FALSE_POSITIVES = frozenset((\n    _MSG_TOO_MANY,\n))\n\n\ndef _pypy2_false_positive(msg, candidate, vtype):\n    # On PyPy2, builtin methods and functions like\n    # ``dict.pop`` that take pseudo-optional arguments\n    # (those with no default, something you can't express in Python 2\n    # syntax; CPython uses special internal APIs to implement these methods)\n    # return false failures because PyPy2 doesn't expose any way\n    # to detect this pseudo-optional status. PyPy3 doesn't have this problem\n    # because of __defaults_count__, and CPython never gets here because it\n    # returns true for ``ismethoddescriptor`` or ``isbuiltin``.\n    #\n    # We can't catch all such cases, but we can handle the common ones.\n    #\n    if msg not in _KNOWN_PYPY2_FALSE_POSITIVES:\n        return False\n\n    known_builtin_types = vars(__builtins__).values()\n    candidate_type = candidate if vtype == 'c' else type(candidate)\n    if candidate_type in known_builtin_types:\n        return True\n\n    return False\n\n\ndef _incompat(required, implemented):\n    #if (required['positional'] !=\n    #    implemented['positional'][:len(required['positional'])]\n    #    and implemented['kwargs'] is None):\n    #    return 'imlementation has different argument names'\n    if len(implemented['required']) > len(required['required']):\n        return _MSG_TOO_MANY\n    if ((len(implemented['positional']) < len(required['positional']))\n        and not implemented['varargs']):\n        return \"implementation doesn't allow enough arguments\"\n    if required['kwargs'] and not implemented['kwargs']:\n        return \"implementation doesn't support keyword arguments\"\n    if required['varargs'] and not implemented['varargs']:\n        return \"implementation doesn't support variable arguments\"\n",
            "file_path": "src/zope/interface/verify.py",
            "human_label": "Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.",
            "level": "file_runnable",
            "lineno": "42",
            "name": "_verify",
            "oracle_context": "{ \"apis\" : \"['tester', 'append', '_verify_element', 'len', 'namesAndDescriptions']\", \"classes\" : \"['Invalid', 'DoesNotImplement', 'MultipleInvalid']\", \"vars\" : \"['implementedBy', 'providedBy']\" }",
            "package": "verify",
            "project": "pexip/os-zope",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b8b4baeb7e40a82d2d1137",
            "all_context": "{ \"import\" : \"inspect sys types __future__ zope \", \"file\" : \"__all__ ; MethodTypes ; _verify(iface,candidate,tentative,vtype) ; _verify_element(iface,name,desc,candidate,vtype) ; verifyClass(iface,candidate,tentative) ; verifyObject(iface,candidate,tentative) ; _MSG_TOO_MANY ; _KNOWN_PYPY2_FALSE_POSITIVES ; _pypy2_false_positive(msg,candidate,vtype) ; _incompat(required,implemented) ; \", \"class\" : \"\" }",
            "code": "def verifyObject(iface, candidate, tentative=False):\n    return _verify(iface, candidate, tentative, vtype='o')\n",
            "dependency": "{ \"builtin\" : false, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.",
            "end_lineno": "172",
            "file_content": "##############################################################################\n#\n# Copyright (c) 2001, 2002 Zope Foundation and Contributors.\n# All Rights Reserved.\n#\n# This software is subject to the provisions of the Zope Public License,\n# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.\n# THIS SOFTWARE IS PROVIDED \"AS IS\" AND ANY AND ALL EXPRESS OR IMPLIED\n# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS\n# FOR A PARTICULAR PURPOSE.\n#\n##############################################################################\n\"\"\"Verify interface implementations\n\"\"\"\nfrom __future__ import print_function\nimport inspect\nimport sys\nfrom types import FunctionType\nfrom types import MethodType\n\nfrom zope.interface._compat import PYPY2\n\nfrom zope.interface.exceptions import BrokenImplementation\nfrom zope.interface.exceptions import BrokenMethodImplementation\nfrom zope.interface.exceptions import DoesNotImplement\nfrom zope.interface.exceptions import Invalid\nfrom zope.interface.exceptions import MultipleInvalid\n\nfrom zope.interface.interface import fromMethod, fromFunction, Method\n\n__all__ = [\n    'verifyObject',\n    'verifyClass',\n]\n\n# This will be monkey-patched when running under Zope 2, so leave this\n# here:\nMethodTypes = (MethodType, )\n\n\ndef _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n\n    if vtype == 'c':\n        tester = iface.implementedBy\n    else:\n        tester = iface.providedBy\n\n    excs = []\n    if not tentative and not tester(candidate):\n        excs.append(DoesNotImplement(iface, candidate))\n\n    for name, desc in iface.namesAndDescriptions(all=True):\n        try:\n            _verify_element(iface, name, desc, candidate, vtype)\n        except Invalid as e:\n            excs.append(e)\n\n    if excs:\n        if len(excs) == 1:\n            raise excs[0]\n        raise MultipleInvalid(iface, candidate, excs)\n\n    return True\n\ndef _verify_element(iface, name, desc, candidate, vtype):\n    # Here the `desc` is either an `Attribute` or `Method` instance\n    try:\n        attr = getattr(candidate, name)\n    except AttributeError:\n        if (not isinstance(desc, Method)) and vtype == 'c':\n            # We can't verify non-methods on classes, since the\n            # class may provide attrs in it's __init__.\n            return\n        # TODO: On Python 3, this should use ``raise...from``\n        raise BrokenImplementation(iface, desc, candidate)\n\n    if not isinstance(desc, Method):\n        # If it's not a method, there's nothing else we can test\n        return\n\n    if inspect.ismethoddescriptor(attr) or inspect.isbuiltin(attr):\n        # The first case is what you get for things like ``dict.pop``\n        # on CPython (e.g., ``verifyClass(IFullMapping, dict))``). The\n        # second case is what you get for things like ``dict().pop`` on\n        # CPython (e.g., ``verifyObject(IFullMapping, dict()))``.\n        # In neither case can we get a signature, so there's nothing\n        # to verify. Even the inspect module gives up and raises\n        # ValueError: no signature found. The ``__text_signature__`` attribute\n        # isn't typically populated either.\n        #\n        # Note that on PyPy 2 or 3 (up through 7.3 at least), these are\n        # not true for things like ``dict.pop`` (but might be true for C extensions?)\n        return\n\n    if isinstance(attr, FunctionType):\n        if sys.version_info[0] >= 3 and isinstance(candidate, type) and vtype == 'c':\n            # This is an \"unbound method\" in Python 3.\n            # Only unwrap this if we're verifying implementedBy;\n            # otherwise we can unwrap @staticmethod on classes that directly\n            # provide an interface.\n            meth = fromFunction(attr, iface, name=name,\n                                imlevel=1)\n        else:\n            # Nope, just a normal function\n            meth = fromFunction(attr, iface, name=name)\n    elif (isinstance(attr, MethodTypes)\n          and type(attr.__func__) is FunctionType):\n        meth = fromMethod(attr, iface, name)\n    elif isinstance(attr, property) and vtype == 'c':\n        # Without an instance we cannot be sure it's not a\n        # callable.\n        # TODO: This should probably check inspect.isdatadescriptor(),\n        # a more general form than ``property``\n        return\n\n    else:\n        if not callable(attr):\n            raise BrokenMethodImplementation(desc, \"implementation is not a method\",\n                                             attr, iface, candidate)\n        # sigh, it's callable, but we don't know how to introspect it, so\n        # we have to give it a pass.\n        return\n\n    # Make sure that the required and implemented method signatures are\n    # the same.\n    mess = _incompat(desc.getSignatureInfo(), meth.getSignatureInfo())\n    if mess:\n        if PYPY2 and _pypy2_false_positive(mess, candidate, vtype):\n            return\n        raise BrokenMethodImplementation(desc, mess, attr, iface, candidate)\n\n\n\ndef verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    return _verify(iface, candidate, tentative, vtype='c')\n\ndef verifyObject(iface, candidate, tentative=False):\n    return _verify(iface, candidate, tentative, vtype='o')\n\nverifyObject.__doc__ = _verify.__doc__\n\n_MSG_TOO_MANY = 'implementation requires too many arguments'\n_KNOWN_PYPY2_FALSE_POSITIVES = frozenset((\n    _MSG_TOO_MANY,\n))\n\n\ndef _pypy2_false_positive(msg, candidate, vtype):\n    # On PyPy2, builtin methods and functions like\n    # ``dict.pop`` that take pseudo-optional arguments\n    # (those with no default, something you can't express in Python 2\n    # syntax; CPython uses special internal APIs to implement these methods)\n    # return false failures because PyPy2 doesn't expose any way\n    # to detect this pseudo-optional status. PyPy3 doesn't have this problem\n    # because of __defaults_count__, and CPython never gets here because it\n    # returns true for ``ismethoddescriptor`` or ``isbuiltin``.\n    #\n    # We can't catch all such cases, but we can handle the common ones.\n    #\n    if msg not in _KNOWN_PYPY2_FALSE_POSITIVES:\n        return False\n\n    known_builtin_types = vars(__builtins__).values()\n    candidate_type = candidate if vtype == 'c' else type(candidate)\n    if candidate_type in known_builtin_types:\n        return True\n\n    return False\n\n\ndef _incompat(required, implemented):\n    #if (required['positional'] !=\n    #    implemented['positional'][:len(required['positional'])]\n    #    and implemented['kwargs'] is None):\n    #    return 'imlementation has different argument names'\n    if len(implemented['required']) > len(required['required']):\n        return _MSG_TOO_MANY\n    if ((len(implemented['positional']) < len(required['positional']))\n        and not implemented['varargs']):\n        return \"implementation doesn't allow enough arguments\"\n    if required['kwargs'] and not implemented['kwargs']:\n        return \"implementation doesn't support keyword arguments\"\n    if required['varargs'] and not implemented['varargs']:\n        return \"implementation doesn't support variable arguments\"\n",
            "file_path": "src/zope/interface/verify.py",
            "human_label": "Verify that *candidate* might correctly provide *iface*.\n\nThis involves:\n\n- Making sure the candidate claims that it provides the\n  interface using ``iface.providedBy`` (unless *tentative* is `True`,\n  in which case this step is skipped). This means that the candidate's class\n  declares that it `implements <zope.interface.implementer>` the interface,\n  or the candidate itself declares that it `provides <zope.interface.provider>`\n  the interface\n\n- Making sure the candidate defines all the necessary methods\n\n- Making sure the methods have the correct signature (to the\n  extent possible)\n\n- Making sure the candidate defines all the necessary attributes\n\n:return bool: Returns a true value if everything that could be\n   checked passed.\n:raises zope.interface.Invalid: If any of the previous\n   conditions does not hold.\n\n.. versionchanged:: 5.0\n    If multiple methods or attributes are invalid, all such errors\n    are collected and reported. Previously, only the first error was reported.\n    As a special case, if only one such error is present, it is raised\n    alone, like before.",
            "level": "file_runnable",
            "lineno": "171",
            "name": "verifyObject",
            "oracle_context": "{ \"apis\" : \"['_verify']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }",
            "package": "verify",
            "project": "pexip/os-zope",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b8b4c1eb7e40a82d2d1139",
            "all_context": "{ \"import\" : \"inspect sys types __future__ zope \", \"file\" : \"__all__ ; MethodTypes ; _verify(iface,candidate,tentative,vtype) ; _verify_element(iface,name,desc,candidate,vtype) ; verifyClass(iface,candidate,tentative) ; verifyObject(iface,candidate,tentative) ; _MSG_TOO_MANY ; _KNOWN_PYPY2_FALSE_POSITIVES ; _pypy2_false_positive(msg,candidate,vtype) ; _incompat(required,implemented) ; \", \"class\" : \"\" }",
            "code": "def verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    return _verify(iface, candidate, tentative, vtype='c')\n",
            "dependency": "{ \"builtin\" : false, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Verify that the *candidate* might correctly provide *iface*.",
            "end_lineno": "169",
            "file_content": "##############################################################################\n#\n# Copyright (c) 2001, 2002 Zope Foundation and Contributors.\n# All Rights Reserved.\n#\n# This software is subject to the provisions of the Zope Public License,\n# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.\n# THIS SOFTWARE IS PROVIDED \"AS IS\" AND ANY AND ALL EXPRESS OR IMPLIED\n# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS\n# FOR A PARTICULAR PURPOSE.\n#\n##############################################################################\n\"\"\"Verify interface implementations\n\"\"\"\nfrom __future__ import print_function\nimport inspect\nimport sys\nfrom types import FunctionType\nfrom types import MethodType\n\nfrom zope.interface._compat import PYPY2\n\nfrom zope.interface.exceptions import BrokenImplementation\nfrom zope.interface.exceptions import BrokenMethodImplementation\nfrom zope.interface.exceptions import DoesNotImplement\nfrom zope.interface.exceptions import Invalid\nfrom zope.interface.exceptions import MultipleInvalid\n\nfrom zope.interface.interface import fromMethod, fromFunction, Method\n\n__all__ = [\n    'verifyObject',\n    'verifyClass',\n]\n\n# This will be monkey-patched when running under Zope 2, so leave this\n# here:\nMethodTypes = (MethodType, )\n\n\ndef _verify(iface, candidate, tentative=False, vtype=None):\n    \"\"\"\n    Verify that *candidate* might correctly provide *iface*.\n\n    This involves:\n\n    - Making sure the candidate claims that it provides the\n      interface using ``iface.providedBy`` (unless *tentative* is `True`,\n      in which case this step is skipped). This means that the candidate's class\n      declares that it `implements <zope.interface.implementer>` the interface,\n      or the candidate itself declares that it `provides <zope.interface.provider>`\n      the interface\n\n    - Making sure the candidate defines all the necessary methods\n\n    - Making sure the methods have the correct signature (to the\n      extent possible)\n\n    - Making sure the candidate defines all the necessary attributes\n\n    :return bool: Returns a true value if everything that could be\n       checked passed.\n    :raises zope.interface.Invalid: If any of the previous\n       conditions does not hold.\n\n    .. versionchanged:: 5.0\n        If multiple methods or attributes are invalid, all such errors\n        are collected and reported. Previously, only the first error was reported.\n        As a special case, if only one such error is present, it is raised\n        alone, like before.\n    \"\"\"\n\n    if vtype == 'c':\n        tester = iface.implementedBy\n    else:\n        tester = iface.providedBy\n\n    excs = []\n    if not tentative and not tester(candidate):\n        excs.append(DoesNotImplement(iface, candidate))\n\n    for name, desc in iface.namesAndDescriptions(all=True):\n        try:\n            _verify_element(iface, name, desc, candidate, vtype)\n        except Invalid as e:\n            excs.append(e)\n\n    if excs:\n        if len(excs) == 1:\n            raise excs[0]\n        raise MultipleInvalid(iface, candidate, excs)\n\n    return True\n\ndef _verify_element(iface, name, desc, candidate, vtype):\n    # Here the `desc` is either an `Attribute` or `Method` instance\n    try:\n        attr = getattr(candidate, name)\n    except AttributeError:\n        if (not isinstance(desc, Method)) and vtype == 'c':\n            # We can't verify non-methods on classes, since the\n            # class may provide attrs in it's __init__.\n            return\n        # TODO: On Python 3, this should use ``raise...from``\n        raise BrokenImplementation(iface, desc, candidate)\n\n    if not isinstance(desc, Method):\n        # If it's not a method, there's nothing else we can test\n        return\n\n    if inspect.ismethoddescriptor(attr) or inspect.isbuiltin(attr):\n        # The first case is what you get for things like ``dict.pop``\n        # on CPython (e.g., ``verifyClass(IFullMapping, dict))``). The\n        # second case is what you get for things like ``dict().pop`` on\n        # CPython (e.g., ``verifyObject(IFullMapping, dict()))``.\n        # In neither case can we get a signature, so there's nothing\n        # to verify. Even the inspect module gives up and raises\n        # ValueError: no signature found. The ``__text_signature__`` attribute\n        # isn't typically populated either.\n        #\n        # Note that on PyPy 2 or 3 (up through 7.3 at least), these are\n        # not true for things like ``dict.pop`` (but might be true for C extensions?)\n        return\n\n    if isinstance(attr, FunctionType):\n        if sys.version_info[0] >= 3 and isinstance(candidate, type) and vtype == 'c':\n            # This is an \"unbound method\" in Python 3.\n            # Only unwrap this if we're verifying implementedBy;\n            # otherwise we can unwrap @staticmethod on classes that directly\n            # provide an interface.\n            meth = fromFunction(attr, iface, name=name,\n                                imlevel=1)\n        else:\n            # Nope, just a normal function\n            meth = fromFunction(attr, iface, name=name)\n    elif (isinstance(attr, MethodTypes)\n          and type(attr.__func__) is FunctionType):\n        meth = fromMethod(attr, iface, name)\n    elif isinstance(attr, property) and vtype == 'c':\n        # Without an instance we cannot be sure it's not a\n        # callable.\n        # TODO: This should probably check inspect.isdatadescriptor(),\n        # a more general form than ``property``\n        return\n\n    else:\n        if not callable(attr):\n            raise BrokenMethodImplementation(desc, \"implementation is not a method\",\n                                             attr, iface, candidate)\n        # sigh, it's callable, but we don't know how to introspect it, so\n        # we have to give it a pass.\n        return\n\n    # Make sure that the required and implemented method signatures are\n    # the same.\n    mess = _incompat(desc.getSignatureInfo(), meth.getSignatureInfo())\n    if mess:\n        if PYPY2 and _pypy2_false_positive(mess, candidate, vtype):\n            return\n        raise BrokenMethodImplementation(desc, mess, attr, iface, candidate)\n\n\n\ndef verifyClass(iface, candidate, tentative=False):\n    \"\"\"\n    Verify that the *candidate* might correctly provide *iface*.\n    \"\"\"\n    return _verify(iface, candidate, tentative, vtype='c')\n\ndef verifyObject(iface, candidate, tentative=False):\n    return _verify(iface, candidate, tentative, vtype='o')\n\nverifyObject.__doc__ = _verify.__doc__\n\n_MSG_TOO_MANY = 'implementation requires too many arguments'\n_KNOWN_PYPY2_FALSE_POSITIVES = frozenset((\n    _MSG_TOO_MANY,\n))\n\n\ndef _pypy2_false_positive(msg, candidate, vtype):\n    # On PyPy2, builtin methods and functions like\n    # ``dict.pop`` that take pseudo-optional arguments\n    # (those with no default, something you can't express in Python 2\n    # syntax; CPython uses special internal APIs to implement these methods)\n    # return false failures because PyPy2 doesn't expose any way\n    # to detect this pseudo-optional status. PyPy3 doesn't have this problem\n    # because of __defaults_count__, and CPython never gets here because it\n    # returns true for ``ismethoddescriptor`` or ``isbuiltin``.\n    #\n    # We can't catch all such cases, but we can handle the common ones.\n    #\n    if msg not in _KNOWN_PYPY2_FALSE_POSITIVES:\n        return False\n\n    known_builtin_types = vars(__builtins__).values()\n    candidate_type = candidate if vtype == 'c' else type(candidate)\n    if candidate_type in known_builtin_types:\n        return True\n\n    return False\n\n\ndef _incompat(required, implemented):\n    #if (required['positional'] !=\n    #    implemented['positional'][:len(required['positional'])]\n    #    and implemented['kwargs'] is None):\n    #    return 'imlementation has different argument names'\n    if len(implemented['required']) > len(required['required']):\n        return _MSG_TOO_MANY\n    if ((len(implemented['positional']) < len(required['positional']))\n        and not implemented['varargs']):\n        return \"implementation doesn't allow enough arguments\"\n    if required['kwargs'] and not implemented['kwargs']:\n        return \"implementation doesn't support keyword arguments\"\n    if required['varargs'] and not implemented['varargs']:\n        return \"implementation doesn't support variable arguments\"\n",
            "file_path": "src/zope/interface/verify.py",
            "human_label": "Verify that the *candidate* might correctly provide *iface*.",
            "level": "file_runnable",
            "lineno": "165",
            "name": "verifyClass",
            "oracle_context": "{ \"apis\" : \"['_verify']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }",
            "package": "verify",
            "project": "pexip/os-zope",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b8b559eb7e40a82d2d11f6",
            "all_context": "{ \"import\" : \"types sys \", \"file\" : \"__all__ ; getFrameInfo(frame) ; addClassAdvisor(callback,depth) ; isClassAdvisor(ob) ; determineMetaclass(bases,explicit_mc) ; minimalBases(classes) ; \", \"class\" : \"\" }",
            "code": "def determineMetaclass(bases, explicit_mc=None):\n    \"\"\"Determine metaclass from 1+ bases and optional explicit __metaclass__\"\"\"\n\n    meta = [getattr(b,'__class__',type(b)) for b in bases]\n\n    if explicit_mc is not None:\n        # The explicit metaclass needs to be verified for compatibility\n        # as well, and allowed to resolve the incompatible bases, if any\n        meta.append(explicit_mc)\n\n    if len(meta)==1:\n        # easy case\n        return meta[0]\n\n    candidates = minimalBases(meta) # minimal set of metaclasses\n\n    if not candidates: # pragma: no cover\n        # they're all \"classic\" classes\n        assert(not __python3) # This should not happen under Python 3\n        return ClassType\n\n    elif len(candidates)>1:\n        # We could auto-combine, but for now we won't...\n        raise TypeError(\"Incompatible metatypes\",bases)\n\n    # Just one, return it\n    return candidates[0]\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : false, \"public_lib\" : false, \"current_class\" : false, \"current_file\" : true, \"current_project\" : false, \"external\" : false }",
            "docstring": "Determine metaclass from 1+ bases and optional explicit __metaclass__",
            "end_lineno": "193",
            "file_content": "##############################################################################\n#\n# Copyright (c) 2003 Zope Foundation and Contributors.\n# All Rights Reserved.\n#\n# This software is subject to the provisions of the Zope Public License,\n# Version 2.1 (ZPL).  A copy of the ZPL should accompany this distribution.\n# THIS SOFTWARE IS PROVIDED \"AS IS\" AND ANY AND ALL EXPRESS OR IMPLIED\n# WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n# WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND FITNESS\n# FOR A PARTICULAR PURPOSE.\n#\n##############################################################################\n\"\"\"Class advice.\n\nThis module was adapted from 'protocols.advice', part of the Python\nEnterprise Application Kit (PEAK).  Please notify the PEAK authors\n(pje@telecommunity.com and tsarna@sarna.org) if bugs are found or\nZope-specific changes are required, so that the PEAK version of this module\ncan be kept in sync.\n\nPEAK is a Python application framework that interoperates with (but does\nnot require) Zope 3 and Twisted.  It provides tools for manipulating UML\nmodels, object-relational persistence, aspect-oriented programming, and more.\nVisit the PEAK home page at http://peak.telecommunity.com for more information.\n\"\"\"\n\nfrom types import FunctionType\ntry:\n    from types import ClassType\nexcept ImportError:\n    __python3 = True\nelse:\n    __python3 = False\n\n__all__ = [\n    'addClassAdvisor',\n    'determineMetaclass',\n    'getFrameInfo',\n    'isClassAdvisor',\n    'minimalBases',\n]\n\nimport sys\n\ndef getFrameInfo(frame):\n    \"\"\"Return (kind,module,locals,globals) for a frame\n\n    'kind' is one of \"exec\", \"module\", \"class\", \"function call\", or \"unknown\".\n    \"\"\"\n\n    f_locals = frame.f_locals\n    f_globals = frame.f_globals\n\n    sameNamespace = f_locals is f_globals\n    hasModule = '__module__' in f_locals\n    hasName = '__name__' in f_globals\n\n    sameName = hasModule and hasName\n    sameName = sameName and f_globals['__name__']==f_locals['__module__']\n\n    module = hasName and sys.modules.get(f_globals['__name__']) or None\n\n    namespaceIsModule = module and module.__dict__ is f_globals\n\n    if not namespaceIsModule:\n        # some kind of funky exec\n        kind = \"exec\"\n    elif sameNamespace and not hasModule:\n        kind = \"module\"\n    elif sameName and not sameNamespace:\n        kind = \"class\"\n    elif not sameNamespace:\n        kind = \"function call\"\n    else: # pragma: no cover\n        # How can you have f_locals is f_globals, and have '__module__' set?\n        # This is probably module-level code, but with a '__module__' variable.\n        kind = \"unknown\"\n    return kind, module, f_locals, f_globals\n\n\ndef addClassAdvisor(callback, depth=2):\n    \"\"\"Set up 'callback' to be passed the containing class upon creation\n\n    This function is designed to be called by an \"advising\" function executed\n    in a class suite.  The \"advising\" function supplies a callback that it\n    wishes to have executed when the containing class is created.  The\n    callback will be given one argument: the newly created containing class.\n    The return value of the callback will be used in place of the class, so\n    the callback should return the input if it does not wish to replace the\n    class.\n\n    The optional 'depth' argument to this function determines the number of\n    frames between this function and the targeted class suite.  'depth'\n    defaults to 2, since this skips this function's frame and one calling\n    function frame.  If you use this function from a function called directly\n    in the class suite, the default will be correct, otherwise you will need\n    to determine the correct depth yourself.\n\n    This function works by installing a special class factory function in\n    place of the '__metaclass__' of the containing class.  Therefore, only\n    callbacks *after* the last '__metaclass__' assignment in the containing\n    class will be executed.  Be sure that classes using \"advising\" functions\n    declare any '__metaclass__' *first*, to ensure all callbacks are run.\"\"\"\n    # This entire approach is invalid under Py3K.  Don't even try to fix\n    # the coverage for this block there. :(\n    if __python3: # pragma: no cover\n        raise TypeError('Class advice impossible in Python3')\n\n    frame = sys._getframe(depth)\n    kind, module, caller_locals, caller_globals = getFrameInfo(frame)\n\n    # This causes a problem when zope interfaces are used from doctest.\n    # In these cases, kind == \"exec\".\n    #\n    #if kind != \"class\":\n    #    raise SyntaxError(\n    #        \"Advice must be in the body of a class statement\"\n    #    )\n\n    previousMetaclass = caller_locals.get('__metaclass__')\n    if __python3:   # pragma: no cover\n        defaultMetaclass  = caller_globals.get('__metaclass__', type)\n    else:\n        defaultMetaclass  = caller_globals.get('__metaclass__', ClassType)\n\n\n    def advise(name, bases, cdict):\n\n        if '__metaclass__' in cdict:\n            del cdict['__metaclass__']\n\n        if previousMetaclass is None:\n            if bases:\n                # find best metaclass or use global __metaclass__ if no bases\n                meta = determineMetaclass(bases)\n            else:\n                meta = defaultMetaclass\n\n        elif isClassAdvisor(previousMetaclass):\n            # special case: we can't compute the \"true\" metaclass here,\n            # so we need to invoke the previous metaclass and let it\n            # figure it out for us (and apply its own advice in the process)\n            meta = previousMetaclass\n\n        else:\n            meta = determineMetaclass(bases, previousMetaclass)\n\n        newClass = meta(name,bases,cdict)\n\n        # this lets the callback replace the class completely, if it wants to\n        return callback(newClass)\n\n    # introspection data only, not used by inner function\n    advise.previousMetaclass = previousMetaclass\n    advise.callback = callback\n\n    # install the advisor\n    caller_locals['__metaclass__'] = advise\n\n\ndef isClassAdvisor(ob):\n    \"\"\"True if 'ob' is a class advisor function\"\"\"\n    return isinstance(ob,FunctionType) and hasattr(ob,'previousMetaclass')\n\n\ndef determineMetaclass(bases, explicit_mc=None):\n    \"\"\"Determine metaclass from 1+ bases and optional explicit __metaclass__\"\"\"\n\n    meta = [getattr(b,'__class__',type(b)) for b in bases]\n\n    if explicit_mc is not None:\n        # The explicit metaclass needs to be verified for compatibility\n        # as well, and allowed to resolve the incompatible bases, if any\n        meta.append(explicit_mc)\n\n    if len(meta)==1:\n        # easy case\n        return meta[0]\n\n    candidates = minimalBases(meta) # minimal set of metaclasses\n\n    if not candidates: # pragma: no cover\n        # they're all \"classic\" classes\n        assert(not __python3) # This should not happen under Python 3\n        return ClassType\n\n    elif len(candidates)>1:\n        # We could auto-combine, but for now we won't...\n        raise TypeError(\"Incompatible metatypes\",bases)\n\n    # Just one, return it\n    return candidates[0]\n\n\ndef minimalBases(classes):\n    \"\"\"Reduce a list of base classes to its ordered minimum equivalent\"\"\"\n\n    if not __python3: # pragma: no cover\n        classes = [c for c in classes if c is not ClassType]\n    candidates = []\n\n    for m in classes:\n        for n in classes:\n            if issubclass(n,m) and m is not n:\n                break\n        else:\n            # m has no subclasses in 'classes'\n            if m in candidates:\n                candidates.remove(m)    # ensure that we're later in the list\n            candidates.append(m)\n\n    return candidates\n",
            "file_path": "src/zope/interface/advice.py",
            "human_label": "Determine metaclass from 1+ bases and optional explicit __metaclass__",
            "level": "file_runnable",
            "lineno": "167",
            "name": "determineMetaclass",
            "oracle_context": "{ \"apis\" : \"['type', 'append', 'getattr', 'len', 'minimalBases']\", \"classes\" : \"['ClassType', 'TypeError']\", \"vars\" : \"[]\" }",
            "package": "advice",
            "project": "pexip/os-zope",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b8966c755ee91dce50a154",
            "all_context": "{ \"import\" : \"functools re calendar datetime functools six datetime \", \"file\" : \"\", \"class\" : \"self._calculate_weekdate(self,year,week,day) ; self._parse_isotime(self,timestr) ; self._parse_isodate_uncommon ; self.__init__(self,sep) ; self.isoparse(self,dt_str) ; self.parse_tzstr(self,tzstr,zero_as_utc) ; self._parse_isodate(self,dt_str) ; self._parse_isotime ; self._DATE_SEP ; self.parse_isodate(self,datestr) ; self._parse_isodate_common ; self._parse_tzstr(self,tzstr,zero_as_utc) ; self._calculate_weekdate ; self._parse_tzstr ; self._TIME_SEP ; self._parse_isodate_uncommon(self,dt_str) ; self._parse_isodate_common(self,dt_str) ; self.parse_isotime(self,timestr) ; self._sep ; self._FRACTION_REGEX ; self._parse_isodate ; \" }",
            "code": "    @_takes_ascii\n    def isoparse(self, dt_str):\n        \"\"\"\n        Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n        An ISO-8601 datetime string consists of a date portion, followed\n        optionally by a time portion - the date and time portions are separated\n        by a single character separator, which is ``T`` in the official\n        standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n        combined with a time portion.\n\n        Supported date formats are:\n\n        Common:\n\n        - ``YYYY``\n        - ``YYYY-MM`` or ``YYYYMM``\n        - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n        Uncommon:\n\n        - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n        - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n        The ISO week and day numbering follows the same logic as\n        :func:`datetime.date.isocalendar`.\n\n        Supported time formats are:\n\n        - ``hh``\n        - ``hh:mm`` or ``hhmm``\n        - ``hh:mm:ss`` or ``hhmmss``\n        - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n        Midnight is a special case for `hh`, as the standard supports both\n        00:00 and 24:00 as a representation. The decimal separator can be\n        either a dot or a comma.\n\n\n        .. caution::\n\n            Support for fractional components other than seconds is part of the\n            ISO-8601 standard, but is not currently implemented in this parser.\n\n        Supported time zone offset formats are:\n\n        - `Z` (UTC)\n        - `±HH:MM`\n        - `±HHMM`\n        - `±HH`\n\n        Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n        with the exception of UTC, which will be represented as\n        :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n        as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n        :param dt_str:\n            A string or stream containing only an ISO-8601 datetime string\n\n        :return:\n            Returns a :class:`datetime.datetime` representing the string.\n            Unspecified components default to their lowest value.\n\n        .. warning::\n\n            As of version 2.7.0, the strictness of the parser should not be\n            considered a stable part of the contract. Any valid ISO-8601 string\n            that parses correctly with the default settings will continue to\n            parse correctly in future versions, but invalid strings that\n            currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n            guaranteed to continue failing in future versions if they encode\n            a valid date.\n\n        .. versionadded:: 2.7.0\n        \"\"\"\n        components, pos = self._parse_isodate(dt_str)\n\n        if len(dt_str) > pos:\n            if self._sep is None or dt_str[pos:pos + 1] == self._sep:\n                components += self._parse_isotime(dt_str[pos + 1:])\n            else:\n                raise ValueError('String contains unknown ISO components')\n\n        if len(components) > 3 and components[3] == 24:\n            components[3] = 0\n            return datetime(*components) + timedelta(days=1)\n\n        return datetime(*components)\n",
            "dependency": "",
            "docstring": "Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `±HH:MM`\n- `±HHMM`\n- `±HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0",
            "end_lineno": "146",
            "file_content": "# -*- coding: utf-8 -*-\n\"\"\"\nThis module offers a parser for ISO-8601 strings\n\nIt is intended to support all valid date, time and datetime formats per the\nISO-8601 specification.\n\n..versionadded:: 2.7.0\n\"\"\"\nfrom datetime import datetime, timedelta, time, date\nimport calendar\nfrom dateutil import tz\n\nfrom functools import wraps\n\nimport re\nimport six\n\n__all__ = [\"isoparse\", \"isoparser\"]\n\n\ndef _takes_ascii(f):\n    @wraps(f)\n    def func(self, str_in, *args, **kwargs):\n        # If it's a stream, read the whole thing\n        str_in = getattr(str_in, 'read', lambda: str_in)()\n\n        # If it's unicode, turn it into bytes, since ISO-8601 only covers ASCII\n        if isinstance(str_in, six.text_type):\n            # ASCII is the same in UTF-8\n            try:\n                str_in = str_in.encode('ascii')\n            except UnicodeEncodeError as e:\n                msg = 'ISO-8601 strings should contain only ASCII characters'\n                six.raise_from(ValueError(msg), e)\n\n        return f(self, str_in, *args, **kwargs)\n\n    return func\n\n\nclass isoparser(object):\n    def __init__(self, sep=None):\n        \"\"\"\n        :param sep:\n            A single character that separates date and time portions. If\n            ``None``, the parser will accept any single character.\n            For strict ISO-8601 adherence, pass ``'T'``.\n        \"\"\"\n        if sep is not None:\n            if (len(sep) != 1 or ord(sep) >= 128 or sep in '0123456789'):\n                raise ValueError('Separator must be a single, non-numeric ' +\n                                 'ASCII character')\n\n            sep = sep.encode('ascii')\n\n        self._sep = sep\n\n    @_takes_ascii\n    def isoparse(self, dt_str):\n        \"\"\"\n        Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\n        An ISO-8601 datetime string consists of a date portion, followed\n        optionally by a time portion - the date and time portions are separated\n        by a single character separator, which is ``T`` in the official\n        standard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\n        combined with a time portion.\n\n        Supported date formats are:\n\n        Common:\n\n        - ``YYYY``\n        - ``YYYY-MM`` or ``YYYYMM``\n        - ``YYYY-MM-DD`` or ``YYYYMMDD``\n\n        Uncommon:\n\n        - ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n        - ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\n        The ISO week and day numbering follows the same logic as\n        :func:`datetime.date.isocalendar`.\n\n        Supported time formats are:\n\n        - ``hh``\n        - ``hh:mm`` or ``hhmm``\n        - ``hh:mm:ss`` or ``hhmmss``\n        - ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\n        Midnight is a special case for `hh`, as the standard supports both\n        00:00 and 24:00 as a representation. The decimal separator can be\n        either a dot or a comma.\n\n\n        .. caution::\n\n            Support for fractional components other than seconds is part of the\n            ISO-8601 standard, but is not currently implemented in this parser.\n\n        Supported time zone offset formats are:\n\n        - `Z` (UTC)\n        - `±HH:MM`\n        - `±HHMM`\n        - `±HH`\n\n        Offsets will be represented as :class:`dateutil.tz.tzoffset` objects,\n        with the exception of UTC, which will be represented as\n        :class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\n        as `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n        :param dt_str:\n            A string or stream containing only an ISO-8601 datetime string\n\n        :return:\n            Returns a :class:`datetime.datetime` representing the string.\n            Unspecified components default to their lowest value.\n\n        .. warning::\n\n            As of version 2.7.0, the strictness of the parser should not be\n            considered a stable part of the contract. Any valid ISO-8601 string\n            that parses correctly with the default settings will continue to\n            parse correctly in future versions, but invalid strings that\n            currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n            guaranteed to continue failing in future versions if they encode\n            a valid date.\n\n        .. versionadded:: 2.7.0\n        \"\"\"\n        components, pos = self._parse_isodate(dt_str)\n\n        if len(dt_str) > pos:\n            if self._sep is None or dt_str[pos:pos + 1] == self._sep:\n                components += self._parse_isotime(dt_str[pos + 1:])\n            else:\n                raise ValueError('String contains unknown ISO components')\n\n        if len(components) > 3 and components[3] == 24:\n            components[3] = 0\n            return datetime(*components) + timedelta(days=1)\n\n        return datetime(*components)\n\n    @_takes_ascii\n    def parse_isodate(self, datestr):\n        \"\"\"\n        Parse the date portion of an ISO string.\n\n        :param datestr:\n            The string portion of an ISO string, without a separator\n\n        :return:\n            Returns a :class:`datetime.date` object\n        \"\"\"\n        components, pos = self._parse_isodate(datestr)\n        if pos < len(datestr):\n            raise ValueError('String contains unknown ISO ' +\n                             'components: {}'.format(datestr))\n        return date(*components)\n\n    @_takes_ascii\n    def parse_isotime(self, timestr):\n        \"\"\"\n        Parse the time portion of an ISO string.\n\n        :param timestr:\n            The time portion of an ISO string, without a separator\n\n        :return:\n            Returns a :class:`datetime.time` object\n        \"\"\"\n        components = self._parse_isotime(timestr)\n        if components[0] == 24:\n            components[0] = 0\n        return time(*components)\n\n    @_takes_ascii\n    def parse_tzstr(self, tzstr, zero_as_utc=True):\n        \"\"\"\n        Parse a valid ISO time zone string.\n\n        See :func:`isoparser.isoparse` for details on supported formats.\n\n        :param tzstr:\n            A string representing an ISO time zone offset\n\n        :param zero_as_utc:\n            Whether to return :class:`dateutil.tz.tzutc` for zero-offset zones\n\n        :return:\n            Returns :class:`dateutil.tz.tzoffset` for offsets and\n            :class:`dateutil.tz.tzutc` for ``Z`` and (if ``zero_as_utc`` is\n            specified) offsets equivalent to UTC.\n        \"\"\"\n        return self._parse_tzstr(tzstr, zero_as_utc=zero_as_utc)\n\n    # Constants\n    _DATE_SEP = b'-'\n    _TIME_SEP = b':'\n    _FRACTION_REGEX = re.compile(b'[\\\\.,]([0-9]+)')\n\n    def _parse_isodate(self, dt_str):\n        try:\n            return self._parse_isodate_common(dt_str)\n        except ValueError:\n            return self._parse_isodate_uncommon(dt_str)\n\n    def _parse_isodate_common(self, dt_str):\n        len_str = len(dt_str)\n        components = [1, 1, 1]\n\n        if len_str < 4:\n            raise ValueError('ISO string too short')\n\n        # Year\n        components[0] = int(dt_str[0:4])\n        pos = 4\n        if pos >= len_str:\n            return components, pos\n\n        has_sep = dt_str[pos:pos + 1] == self._DATE_SEP\n        if has_sep:\n            pos += 1\n\n        # Month\n        if len_str - pos < 2:\n            raise ValueError('Invalid common month')\n\n        components[1] = int(dt_str[pos:pos + 2])\n        pos += 2\n\n        if pos >= len_str:\n            if has_sep:\n                return components, pos\n            else:\n                raise ValueError('Invalid ISO format')\n\n        if has_sep:\n            if dt_str[pos:pos + 1] != self._DATE_SEP:\n                raise ValueError('Invalid separator in ISO string')\n            pos += 1\n\n        # Day\n        if len_str - pos < 2:\n            raise ValueError('Invalid common day')\n        components[2] = int(dt_str[pos:pos + 2])\n        return components, pos + 2\n\n    def _parse_isodate_uncommon(self, dt_str):\n        if len(dt_str) < 4:\n            raise ValueError('ISO string too short')\n\n        # All ISO formats start with the year\n        year = int(dt_str[0:4])\n\n        has_sep = dt_str[4:5] == self._DATE_SEP\n\n        pos = 4 + has_sep       # Skip '-' if it's there\n        if dt_str[pos:pos + 1] == b'W':\n            # YYYY-?Www-?D?\n            pos += 1\n            weekno = int(dt_str[pos:pos + 2])\n            pos += 2\n\n            dayno = 1\n            if len(dt_str) > pos:\n                if (dt_str[pos:pos + 1] == self._DATE_SEP) != has_sep:\n                    raise ValueError('Inconsistent use of dash separator')\n\n                pos += has_sep\n\n                dayno = int(dt_str[pos:pos + 1])\n                pos += 1\n\n            base_date = self._calculate_weekdate(year, weekno, dayno)\n        else:\n            # YYYYDDD or YYYY-DDD\n            if len(dt_str) - pos < 3:\n                raise ValueError('Invalid ordinal day')\n\n            ordinal_day = int(dt_str[pos:pos + 3])\n            pos += 3\n\n            if ordinal_day < 1 or ordinal_day > (365 + calendar.isleap(year)):\n                raise ValueError('Invalid ordinal day' +\n                                 ' {} for year {}'.format(ordinal_day, year))\n\n            base_date = date(year, 1, 1) + timedelta(days=ordinal_day - 1)\n\n        components = [base_date.year, base_date.month, base_date.day]\n        return components, pos\n\n    def _calculate_weekdate(self, year, week, day):\n        \"\"\"\n        Calculate the day of corresponding to the ISO year-week-day calendar.\n\n        This function is effectively the inverse of\n        :func:`datetime.date.isocalendar`.\n\n        :param year:\n            The year in the ISO calendar\n\n        :param week:\n            The week in the ISO calendar - range is [1, 53]\n\n        :param day:\n            The day in the ISO calendar - range is [1 (MON), 7 (SUN)]\n\n        :return:\n            Returns a :class:`datetime.date`\n        \"\"\"\n        if not 0 < week < 54:\n            raise ValueError('Invalid week: {}'.format(week))\n\n        if not 0 < day < 8:     # Range is 1-7\n            raise ValueError('Invalid weekday: {}'.format(day))\n\n        # Get week 1 for the specific year:\n        jan_4 = date(year, 1, 4)   # Week 1 always has January 4th in it\n        week_1 = jan_4 - timedelta(days=jan_4.isocalendar()[2] - 1)\n\n        # Now add the specific number of weeks and days to get what we want\n        week_offset = (week - 1) * 7 + (day - 1)\n        return week_1 + timedelta(days=week_offset)\n\n    def _parse_isotime(self, timestr):\n        len_str = len(timestr)\n        components = [0, 0, 0, 0, None]\n        pos = 0\n        comp = -1\n\n        if len(timestr) < 2:\n            raise ValueError('ISO time too short')\n\n        has_sep = len_str >= 3 and timestr[2:3] == self._TIME_SEP\n\n        while pos < len_str and comp < 5:\n            comp += 1\n\n            if timestr[pos:pos + 1] in b'-+Zz':\n                # Detect time zone boundary\n                components[-1] = self._parse_tzstr(timestr[pos:])\n                pos = len_str\n                break\n\n            if comp < 3:\n                # Hour, minute, second\n                components[comp] = int(timestr[pos:pos + 2])\n                pos += 2\n                if (has_sep and pos < len_str and\n                        timestr[pos:pos + 1] == self._TIME_SEP):\n                    pos += 1\n\n            if comp == 3:\n                # Fraction of a second\n                frac = self._FRACTION_REGEX.match(timestr[pos:])\n                if not frac:\n                    continue\n\n                us_str = frac.group(1)[:6]  # Truncate to microseconds\n                components[comp] = int(us_str) * 10**(6 - len(us_str))\n                pos += len(frac.group())\n\n        if pos < len_str:\n            raise ValueError('Unused components in ISO string')\n\n        if components[0] == 24:\n            # Standard supports 00:00 and 24:00 as representations of midnight\n            if any(component != 0 for component in components[1:4]):\n                raise ValueError('Hour may only be 24 at 24:00:00.000')\n\n        return components\n\n    def _parse_tzstr(self, tzstr, zero_as_utc=True):\n        if tzstr == b'Z' or tzstr == b'z':\n            return tz.UTC\n\n        if len(tzstr) not in {3, 5, 6}:\n            raise ValueError('Time zone offset must be 1, 3, 5 or 6 characters')\n\n        if tzstr[0:1] == b'-':\n            mult = -1\n        elif tzstr[0:1] == b'+':\n            mult = 1\n        else:\n            raise ValueError('Time zone offset requires sign')\n\n        hours = int(tzstr[1:3])\n        if len(tzstr) == 3:\n            minutes = 0\n        else:\n            minutes = int(tzstr[(4 if tzstr[3:4] == self._TIME_SEP else 3):])\n\n        if zero_as_utc and hours == 0 and minutes == 0:\n            return tz.UTC\n        else:\n            if minutes > 59:\n                raise ValueError('Invalid minutes in time zone offset')\n\n            if hours > 23:\n                raise ValueError('Invalid hours in time zone offset')\n\n            return tz.tzoffset(None, mult * (hours * 60 + minutes) * 60)\n\n\nDEFAULT_ISOPARSER = isoparser()\nisoparse = DEFAULT_ISOPARSER.isoparse\n",
            "file_path": "dateutil/parser/isoparser.py",
            "human_label": "Parse an ISO-8601 datetime string into a :class:`datetime.datetime`.\n\nAn ISO-8601 datetime string consists of a date portion, followed\noptionally by a time portion - the date and time portions are separated\nby a single character separator, which is ``T`` in the official\nstandard. Incomplete date formats (such as ``YYYY-MM``) may *not* be\ncombined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\n- ``YYYY``\n- ``YYYY-MM`` or ``YYYYMM``\n- ``YYYY-MM-DD`` or ``YYYYMMDD``\n\nUncommon:\n\n- ``YYYY-Www`` or ``YYYYWww`` - ISO week (day defaults to 0)\n- ``YYYY-Www-D`` or ``YYYYWwwD`` - ISO week and day\n\nThe ISO week and day numbering follows the same logic as\n:func:`datetime.date.isocalendar`.\n\nSupported time formats are:\n\n- ``hh``\n- ``hh:mm`` or ``hhmm``\n- ``hh:mm:ss`` or ``hhmmss``\n- ``hh:mm:ss.ssssss`` (Up to 6 sub-second digits)\n\nMidnight is a special case for `hh`, as the standard supports both\n00:00 and 24:00 as a representation. The decimal separator can be\neither a dot or a comma.\n\n\n.. caution::\n\n    Support for fractional components other than seconds is part of the\n    ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\n- `Z` (UTC)\n- `±HH:MM`\n- `±HHMM`\n- `±HH`\n\nOffsets will be represented as :class:`dateutil.tz.tzoffset` objects,\nwith the exception of UTC, which will be represented as\n:class:`dateutil.tz.tzutc`. Time zone offsets equivalent to UTC (such\nas `+00:00`) will also be represented as :class:`dateutil.tz.tzutc`.\n\n:param dt_str:\n    A string or stream containing only an ISO-8601 datetime string\n\n:return:\n    Returns a :class:`datetime.datetime` representing the string.\n    Unspecified components default to their lowest value.\n\n.. warning::\n\n    As of version 2.7.0, the strictness of the parser should not be\n    considered a stable part of the contract. Any valid ISO-8601 string\n    that parses correctly with the default settings will continue to\n    parse correctly in future versions, but invalid strings that\n    currently fail (e.g. ``2017-01-01T00:00+00:00:00``) are not\n    guaranteed to continue failing in future versions if they encode\n    a valid date.\n\n.. versionadded:: 2.7.0",
            "level": "class_runnable",
            "lineno": "59",
            "name": "isoparse",
            "oracle_context": "{ \"apis\" : \"['_parse_isodate', 'len', '_parse_isotime', 'datetime', 'timedelta']\", \"classes\" : \"['timedelta', 'ValueError', 'datetime']\", \"vars\" : \"['_sep']\" }",
            "package": "isoparser",
            "project": "pexip/os-python-dateutil",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b896de755ee91dce50a183",
            "all_context": "{ \"import\" : \"warnings time string io __future__ decimal re calendar datetime  six datetime \", \"file\" : \"\", \"class\" : \"self.parse(self,timestr,default,ignoretz,tzinfos) ; self._parse(self,timestr,dayfirst,yearfirst,fuzzy,fuzzy_with_tokens) ; self.__init__(self,info) ; self._parsems(self,value) ; self._could_be_tzname ; self._ampm_valid ; self._assign_tzname ; self._build_tzaware(self,naive,res,tzinfos) ; self._build_naive(self,res,default) ; self._parse_numeric_token ; self._to_decimal(self,val) ; self._assign_hms(self,res,value_repr,hms) ; self._build_tzinfo(self,tzinfos,tzname,tzoffset) ; self._parsems ; self._adjust_ampm ; self._find_hms_idx ; self._parse_hms ; self._parse_min_sec(self,value) ; self._ampm_valid(self,hour,ampm,fuzzy) ; self._build_tzaware ; self._recombine_skipped ; self._parse_hms(self,idx,tokens,info,hms_idx) ; self._build_naive ; self._assign_tzname(self,dt,tzname) ; self.info ; self._build_tzinfo ; self._parse_min_sec ; self._result ; self._parse_numeric_token(self,tokens,idx,info,ymd,res,fuzzy) ; self._adjust_ampm(self,hour,ampm) ; self._recombine_skipped(self,tokens,skipped_idxs) ; self._parse ; self._find_hms_idx(self,idx,tokens,info,allow_jump) ; self._assign_hms ; self._to_decimal ; self._could_be_tzname(self,hour,tzname,tzoffset,token) ; \" }",
            "code": "    def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n        \"\"\"\n        Parse the date/time string into a :class:`datetime.datetime` object.\n\n        :param timestr:\n            Any date/time string using the supported formats.\n\n        :param default:\n            The default datetime object, if this is a datetime object and not\n            ``None``, elements specified in ``timestr`` replace elements in the\n            default object.\n\n        :param ignoretz:\n            If set ``True``, time zones in parsed strings are ignored and a\n            naive :class:`datetime.datetime` object is returned.\n\n        :param tzinfos:\n            Additional time zone names / aliases which may be present in the\n            string. This argument maps time zone names (and optionally offsets\n            from those time zones) to time zones. This parameter can be a\n            dictionary with timezone aliases mapping time zone names to time\n            zones or a function taking two parameters (``tzname`` and\n            ``tzoffset``) and returning a time zone.\n\n            The timezones to which the names are mapped can be an integer\n            offset from UTC in seconds or a :class:`tzinfo` object.\n\n            .. doctest::\n               :options: +NORMALIZE_WHITESPACE\n\n                >>> from dateutil.parser import parse\n                >>> from dateutil.tz import gettz\n                >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n                >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n                datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n                >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n                datetime.datetime(2012, 1, 19, 17, 21,\n                                  tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n            This parameter is ignored if ``ignoretz`` is set.\n\n        :param \\\\*\\\\*kwargs:\n            Keyword arguments as passed to ``_parse()``.\n\n        :return:\n            Returns a :class:`datetime.datetime` object or, if the\n            ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n            first element being a :class:`datetime.datetime` object, the second\n            a tuple containing the fuzzy tokens.\n\n        :raises ParserError:\n            Raised for invalid or unknown string format, if the provided\n            :class:`tzinfo` is not in a valid format, or if an invalid date\n            would be created.\n\n        :raises TypeError:\n            Raised for non-string or character stream input.\n\n        :raises OverflowError:\n            Raised if the parsed date exceeds the largest valid C integer on\n            your system.\n        \"\"\"\n\n        if default is None:\n            default = datetime.datetime.now().replace(hour=0, minute=0,\n                                                      second=0, microsecond=0)\n\n        res, skipped_tokens = self._parse(timestr, **kwargs)\n\n        if res is None:\n            raise ParserError(\"Unknown string format: %s\", timestr)\n\n        if len(res) == 0:\n            raise ParserError(\"String does not contain a date: %s\", timestr)\n\n        try:\n            ret = self._build_naive(res, default)\n        except ValueError as e:\n            six.raise_from(ParserError(e.args[0] + \": %s\", timestr), e)\n\n        if not ignoretz:\n            ret = self._build_tzaware(ret, res, tzinfos)\n\n        if kwargs.get('fuzzy_with_tokens', False):\n            return ret, skipped_tokens\n        else:\n            return ret\n",
            "dependency": "{ \"builtin\" : true, \"standard_lib\" : true, \"public_lib\" : true, \"current_class\" : true, \"current_file\" : true, \"current_project\" : true, \"external\" : false }",
            "docstring": "Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.",
            "end_lineno": "665",
            "file_content": "# -*- coding: utf-8 -*-\n\"\"\"\nThis module offers a generic date/time string parser which is able to parse\nmost known formats to represent a date and/or time.\n\nThis module attempts to be forgiving with regards to unlikely input formats,\nreturning a datetime object even for dates which are ambiguous. If an element\nof a date/time stamp is omitted, the following rules are applied:\n\n- If AM or PM is left unspecified, a 24-hour clock is assumed, however, an hour\n  on a 12-hour clock (``0 <= hour <= 12``) *must* be specified if AM or PM is\n  specified.\n- If a time zone is omitted, a timezone-naive datetime is returned.\n\nIf any other elements are missing, they are taken from the\n:class:`datetime.datetime` object passed to the parameter ``default``. If this\nresults in a day number exceeding the valid number of days per month, the\nvalue falls back to the end of the month.\n\nAdditional resources about date/time string formats can be found below:\n\n- `A summary of the international standard date and time notation\n  <http://www.cl.cam.ac.uk/~mgk25/iso-time.html>`_\n- `W3C Date and Time Formats <http://www.w3.org/TR/NOTE-datetime>`_\n- `Time Formats (Planetary Rings Node) <https://pds-rings.seti.org:443/tools/time_formats.html>`_\n- `CPAN ParseDate module\n  <http://search.cpan.org/~muir/Time-modules-2013.0912/lib/Time/ParseDate.pm>`_\n- `Java SimpleDateFormat Class\n  <https://docs.oracle.com/javase/6/docs/api/java/text/SimpleDateFormat.html>`_\n\"\"\"\nfrom __future__ import unicode_literals\n\nimport datetime\nimport re\nimport string\nimport time\nimport warnings\n\nfrom calendar import monthrange\nfrom io import StringIO\n\nimport six\nfrom six import integer_types, text_type\n\nfrom decimal import Decimal\n\nfrom warnings import warn\n\nfrom .. import relativedelta\nfrom .. import tz\n\n__all__ = [\"parse\", \"parserinfo\", \"ParserError\"]\n\n\n# TODO: pandas.core.tools.datetimes imports this explicitly.  Might be worth\n# making public and/or figuring out if there is something we can\n# take off their plate.\nclass _timelex(object):\n    # Fractional seconds are sometimes split by a comma\n    _split_decimal = re.compile(\"([.,])\")\n\n    def __init__(self, instream):\n        if six.PY2:\n            # In Python 2, we can't duck type properly because unicode has\n            # a 'decode' function, and we'd be double-decoding\n            if isinstance(instream, (bytes, bytearray)):\n                instream = instream.decode()\n        else:\n            if getattr(instream, 'decode', None) is not None:\n                instream = instream.decode()\n\n        if isinstance(instream, text_type):\n            instream = StringIO(instream)\n        elif getattr(instream, 'read', None) is None:\n            raise TypeError('Parser must be a string or character stream, not '\n                            '{itype}'.format(itype=instream.__class__.__name__))\n\n        self.instream = instream\n        self.charstack = []\n        self.tokenstack = []\n        self.eof = False\n\n    def get_token(self):\n        \"\"\"\n        This function breaks the time string into lexical units (tokens), which\n        can be parsed by the parser. Lexical units are demarcated by changes in\n        the character set, so any continuous string of letters is considered\n        one unit, any continuous string of numbers is considered one unit.\n\n        The main complication arises from the fact that dots ('.') can be used\n        both as separators (e.g. \"Sep.20.2009\") or decimal points (e.g.\n        \"4:30:21.447\"). As such, it is necessary to read the full context of\n        any dot-separated strings before breaking it into tokens; as such, this\n        function maintains a \"token stack\", for when the ambiguous context\n        demands that multiple tokens be parsed at once.\n        \"\"\"\n        if self.tokenstack:\n            return self.tokenstack.pop(0)\n\n        seenletters = False\n        token = None\n        state = None\n\n        while not self.eof:\n            # We only realize that we've reached the end of a token when we\n            # find a character that's not part of the current token - since\n            # that character may be part of the next token, it's stored in the\n            # charstack.\n            if self.charstack:\n                nextchar = self.charstack.pop(0)\n            else:\n                nextchar = self.instream.read(1)\n                while nextchar == '\\x00':\n                    nextchar = self.instream.read(1)\n\n            if not nextchar:\n                self.eof = True\n                break\n            elif not state:\n                # First character of the token - determines if we're starting\n                # to parse a word, a number or something else.\n                token = nextchar\n                if self.isword(nextchar):\n                    state = 'a'\n                elif self.isnum(nextchar):\n                    state = '0'\n                elif self.isspace(nextchar):\n                    token = ' '\n                    break  # emit token\n                else:\n                    break  # emit token\n            elif state == 'a':\n                # If we've already started reading a word, we keep reading\n                # letters until we find something that's not part of a word.\n                seenletters = True\n                if self.isword(nextchar):\n                    token += nextchar\n                elif nextchar == '.':\n                    token += nextchar\n                    state = 'a.'\n                else:\n                    self.charstack.append(nextchar)\n                    break  # emit token\n            elif state == '0':\n                # If we've already started reading a number, we keep reading\n                # numbers until we find something that doesn't fit.\n                if self.isnum(nextchar):\n                    token += nextchar\n                elif nextchar == '.' or (nextchar == ',' and len(token) >= 2):\n                    token += nextchar\n                    state = '0.'\n                else:\n                    self.charstack.append(nextchar)\n                    break  # emit token\n            elif state == 'a.':\n                # If we've seen some letters and a dot separator, continue\n                # parsing, and the tokens will be broken up later.\n                seenletters = True\n                if nextchar == '.' or self.isword(nextchar):\n                    token += nextchar\n                elif self.isnum(nextchar) and token[-1] == '.':\n                    token += nextchar\n                    state = '0.'\n                else:\n                    self.charstack.append(nextchar)\n                    break  # emit token\n            elif state == '0.':\n                # If we've seen at least one dot separator, keep going, we'll\n                # break up the tokens later.\n                if nextchar == '.' or self.isnum(nextchar):\n                    token += nextchar\n                elif self.isword(nextchar) and token[-1] == '.':\n                    token += nextchar\n                    state = 'a.'\n                else:\n                    self.charstack.append(nextchar)\n                    break  # emit token\n\n        if (state in ('a.', '0.') and (seenletters or token.count('.') > 1 or\n                                       token[-1] in '.,')):\n            l = self._split_decimal.split(token)\n            token = l[0]\n            for tok in l[1:]:\n                if tok:\n                    self.tokenstack.append(tok)\n\n        if state == '0.' and token.count('.') == 0:\n            token = token.replace(',', '.')\n\n        return token\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        token = self.get_token()\n        if token is None:\n            raise StopIteration\n\n        return token\n\n    def next(self):\n        return self.__next__()  # Python 2.x support\n\n    @classmethod\n    def split(cls, s):\n        return list(cls(s))\n\n    @classmethod\n    def isword(cls, nextchar):\n        \"\"\" Whether or not the next character is part of a word \"\"\"\n        return nextchar.isalpha()\n\n    @classmethod\n    def isnum(cls, nextchar):\n        \"\"\" Whether the next character is part of a number \"\"\"\n        return nextchar.isdigit()\n\n    @classmethod\n    def isspace(cls, nextchar):\n        \"\"\" Whether the next character is whitespace \"\"\"\n        return nextchar.isspace()\n\n\nclass _resultbase(object):\n\n    def __init__(self):\n        for attr in self.__slots__:\n            setattr(self, attr, None)\n\n    def _repr(self, classname):\n        l = []\n        for attr in self.__slots__:\n            value = getattr(self, attr)\n            if value is not None:\n                l.append(\"%s=%s\" % (attr, repr(value)))\n        return \"%s(%s)\" % (classname, \", \".join(l))\n\n    def __len__(self):\n        return (sum(getattr(self, attr) is not None\n                    for attr in self.__slots__))\n\n    def __repr__(self):\n        return self._repr(self.__class__.__name__)\n\n\nclass parserinfo(object):\n    \"\"\"\n    Class which handles what inputs are accepted. Subclass this to customize\n    the language and acceptable values for each parameter.\n\n    :param dayfirst:\n        Whether to interpret the first value in an ambiguous 3-integer date\n        (e.g. 01/05/09) as the day (``True``) or month (``False``). If\n        ``yearfirst`` is set to ``True``, this distinguishes between YDM\n        and YMD. Default is ``False``.\n\n    :param yearfirst:\n        Whether to interpret the first value in an ambiguous 3-integer date\n        (e.g. 01/05/09) as the year. If ``True``, the first number is taken\n        to be the year, otherwise the last number is taken to be the year.\n        Default is ``False``.\n    \"\"\"\n\n    # m from a.m/p.m, t from ISO T separator\n    JUMP = [\" \", \".\", \",\", \";\", \"-\", \"/\", \"'\",\n            \"at\", \"on\", \"and\", \"ad\", \"m\", \"t\", \"of\",\n            \"st\", \"nd\", \"rd\", \"th\"]\n\n    WEEKDAYS = [(\"Mon\", \"Monday\"),\n                (\"Tue\", \"Tuesday\"),     # TODO: \"Tues\"\n                (\"Wed\", \"Wednesday\"),\n                (\"Thu\", \"Thursday\"),    # TODO: \"Thurs\"\n                (\"Fri\", \"Friday\"),\n                (\"Sat\", \"Saturday\"),\n                (\"Sun\", \"Sunday\")]\n    MONTHS = [(\"Jan\", \"January\"),\n              (\"Feb\", \"February\"),      # TODO: \"Febr\"\n              (\"Mar\", \"March\"),\n              (\"Apr\", \"April\"),\n              (\"May\", \"May\"),\n              (\"Jun\", \"June\"),\n              (\"Jul\", \"July\"),\n              (\"Aug\", \"August\"),\n              (\"Sep\", \"Sept\", \"September\"),\n              (\"Oct\", \"October\"),\n              (\"Nov\", \"November\"),\n              (\"Dec\", \"December\")]\n    HMS = [(\"h\", \"hour\", \"hours\"),\n           (\"m\", \"minute\", \"minutes\"),\n           (\"s\", \"second\", \"seconds\")]\n    AMPM = [(\"am\", \"a\"),\n            (\"pm\", \"p\")]\n    UTCZONE = [\"UTC\", \"GMT\", \"Z\", \"z\"]\n    PERTAIN = [\"of\"]\n    TZOFFSET = {}\n    # TODO: ERA = [\"AD\", \"BC\", \"CE\", \"BCE\", \"Stardate\",\n    #              \"Anno Domini\", \"Year of Our Lord\"]\n\n    def __init__(self, dayfirst=False, yearfirst=False):\n        self._jump = self._convert(self.JUMP)\n        self._weekdays = self._convert(self.WEEKDAYS)\n        self._months = self._convert(self.MONTHS)\n        self._hms = self._convert(self.HMS)\n        self._ampm = self._convert(self.AMPM)\n        self._utczone = self._convert(self.UTCZONE)\n        self._pertain = self._convert(self.PERTAIN)\n\n        self.dayfirst = dayfirst\n        self.yearfirst = yearfirst\n\n        self._year = time.localtime().tm_year\n        self._century = self._year // 100 * 100\n\n    def _convert(self, lst):\n        dct = {}\n        for i, v in enumerate(lst):\n            if isinstance(v, tuple):\n                for v in v:\n                    dct[v.lower()] = i\n            else:\n                dct[v.lower()] = i\n        return dct\n\n    def jump(self, name):\n        return name.lower() in self._jump\n\n    def weekday(self, name):\n        try:\n            return self._weekdays[name.lower()]\n        except KeyError:\n            pass\n        return None\n\n    def month(self, name):\n        try:\n            return self._months[name.lower()] + 1\n        except KeyError:\n            pass\n        return None\n\n    def hms(self, name):\n        try:\n            return self._hms[name.lower()]\n        except KeyError:\n            return None\n\n    def ampm(self, name):\n        try:\n            return self._ampm[name.lower()]\n        except KeyError:\n            return None\n\n    def pertain(self, name):\n        return name.lower() in self._pertain\n\n    def utczone(self, name):\n        return name.lower() in self._utczone\n\n    def tzoffset(self, name):\n        if name in self._utczone:\n            return 0\n\n        return self.TZOFFSET.get(name)\n\n    def convertyear(self, year, century_specified=False):\n        \"\"\"\n        Converts two-digit years to year within [-50, 49]\n        range of self._year (current local time)\n        \"\"\"\n\n        # Function contract is that the year is always positive\n        assert year >= 0\n\n        if year < 100 and not century_specified:\n            # assume current century to start\n            year += self._century\n\n            if year >= self._year + 50:  # if too far in future\n                year -= 100\n            elif year < self._year - 50:  # if too far in past\n                year += 100\n\n        return year\n\n    def validate(self, res):\n        # move to info\n        if res.year is not None:\n            res.year = self.convertyear(res.year, res.century_specified)\n\n        if ((res.tzoffset == 0 and not res.tzname) or\n             (res.tzname == 'Z' or res.tzname == 'z')):\n            res.tzname = \"UTC\"\n            res.tzoffset = 0\n        elif res.tzoffset != 0 and res.tzname and self.utczone(res.tzname):\n            res.tzoffset = 0\n        return True\n\n\nclass _ymd(list):\n    def __init__(self, *args, **kwargs):\n        super(self.__class__, self).__init__(*args, **kwargs)\n        self.century_specified = False\n        self.dstridx = None\n        self.mstridx = None\n        self.ystridx = None\n\n    @property\n    def has_year(self):\n        return self.ystridx is not None\n\n    @property\n    def has_month(self):\n        return self.mstridx is not None\n\n    @property\n    def has_day(self):\n        return self.dstridx is not None\n\n    def could_be_day(self, value):\n        if self.has_day:\n            return False\n        elif not self.has_month:\n            return 1 <= value <= 31\n        elif not self.has_year:\n            # Be permissive, assume leap year\n            month = self[self.mstridx]\n            return 1 <= value <= monthrange(2000, month)[1]\n        else:\n            month = self[self.mstridx]\n            year = self[self.ystridx]\n            return 1 <= value <= monthrange(year, month)[1]\n\n    def append(self, val, label=None):\n        if hasattr(val, '__len__'):\n            if val.isdigit() and len(val) > 2:\n                self.century_specified = True\n                if label not in [None, 'Y']:  # pragma: no cover\n                    raise ValueError(label)\n                label = 'Y'\n        elif val > 100:\n            self.century_specified = True\n            if label not in [None, 'Y']:  # pragma: no cover\n                raise ValueError(label)\n            label = 'Y'\n\n        super(self.__class__, self).append(int(val))\n\n        if label == 'M':\n            if self.has_month:\n                raise ValueError('Month is already set')\n            self.mstridx = len(self) - 1\n        elif label == 'D':\n            if self.has_day:\n                raise ValueError('Day is already set')\n            self.dstridx = len(self) - 1\n        elif label == 'Y':\n            if self.has_year:\n                raise ValueError('Year is already set')\n            self.ystridx = len(self) - 1\n\n    def _resolve_from_stridxs(self, strids):\n        \"\"\"\n        Try to resolve the identities of year/month/day elements using\n        ystridx, mstridx, and dstridx, if enough of these are specified.\n        \"\"\"\n        if len(self) == 3 and len(strids) == 2:\n            # we can back out the remaining stridx value\n            missing = [x for x in range(3) if x not in strids.values()]\n            key = [x for x in ['y', 'm', 'd'] if x not in strids]\n            assert len(missing) == len(key) == 1\n            key = key[0]\n            val = missing[0]\n            strids[key] = val\n\n        assert len(self) == len(strids)  # otherwise this should not be called\n        out = {key: self[strids[key]] for key in strids}\n        return (out.get('y'), out.get('m'), out.get('d'))\n\n    def resolve_ymd(self, yearfirst, dayfirst):\n        len_ymd = len(self)\n        year, month, day = (None, None, None)\n\n        strids = (('y', self.ystridx),\n                  ('m', self.mstridx),\n                  ('d', self.dstridx))\n\n        strids = {key: val for key, val in strids if val is not None}\n        if (len(self) == len(strids) > 0 or\n                (len(self) == 3 and len(strids) == 2)):\n            return self._resolve_from_stridxs(strids)\n\n        mstridx = self.mstridx\n\n        if len_ymd > 3:\n            raise ValueError(\"More than three YMD values\")\n        elif len_ymd == 1 or (mstridx is not None and len_ymd == 2):\n            # One member, or two members with a month string\n            if mstridx is not None:\n                month = self[mstridx]\n                # since mstridx is 0 or 1, self[mstridx-1] always\n                # looks up the other element\n                other = self[mstridx - 1]\n            else:\n                other = self[0]\n\n            if len_ymd > 1 or mstridx is None:\n                if other > 31:\n                    year = other\n                else:\n                    day = other\n\n        elif len_ymd == 2:\n            # Two members with numbers\n            if self[0] > 31:\n                # 99-01\n                year, month = self\n            elif self[1] > 31:\n                # 01-99\n                month, year = self\n            elif dayfirst and self[1] <= 12:\n                # 13-01\n                day, month = self\n            else:\n                # 01-13\n                month, day = self\n\n        elif len_ymd == 3:\n            # Three members\n            if mstridx == 0:\n                if self[1] > 31:\n                    # Apr-2003-25\n                    month, year, day = self\n                else:\n                    month, day, year = self\n            elif mstridx == 1:\n                if self[0] > 31 or (yearfirst and self[2] <= 31):\n                    # 99-Jan-01\n                    year, month, day = self\n                else:\n                    # 01-Jan-01\n                    # Give precedence to day-first, since\n                    # two-digit years is usually hand-written.\n                    day, month, year = self\n\n            elif mstridx == 2:\n                # WTF!?\n                if self[1] > 31:\n                    # 01-99-Jan\n                    day, year, month = self\n                else:\n                    # 99-01-Jan\n                    year, day, month = self\n\n            else:\n                if (self[0] > 31 or\n                    self.ystridx == 0 or\n                        (yearfirst and self[1] <= 12 and self[2] <= 31)):\n                    # 99-01-01\n                    if dayfirst and self[2] <= 12:\n                        year, day, month = self\n                    else:\n                        year, month, day = self\n                elif self[0] > 12 or (dayfirst and self[1] <= 12):\n                    # 13-01-01\n                    day, month, year = self\n                else:\n                    # 01-13-01\n                    month, day, year = self\n\n        return year, month, day\n\n\nclass parser(object):\n    def __init__(self, info=None):\n        self.info = info or parserinfo()\n\n    def parse(self, timestr, default=None,\n              ignoretz=False, tzinfos=None, **kwargs):\n        \"\"\"\n        Parse the date/time string into a :class:`datetime.datetime` object.\n\n        :param timestr:\n            Any date/time string using the supported formats.\n\n        :param default:\n            The default datetime object, if this is a datetime object and not\n            ``None``, elements specified in ``timestr`` replace elements in the\n            default object.\n\n        :param ignoretz:\n            If set ``True``, time zones in parsed strings are ignored and a\n            naive :class:`datetime.datetime` object is returned.\n\n        :param tzinfos:\n            Additional time zone names / aliases which may be present in the\n            string. This argument maps time zone names (and optionally offsets\n            from those time zones) to time zones. This parameter can be a\n            dictionary with timezone aliases mapping time zone names to time\n            zones or a function taking two parameters (``tzname`` and\n            ``tzoffset``) and returning a time zone.\n\n            The timezones to which the names are mapped can be an integer\n            offset from UTC in seconds or a :class:`tzinfo` object.\n\n            .. doctest::\n               :options: +NORMALIZE_WHITESPACE\n\n                >>> from dateutil.parser import parse\n                >>> from dateutil.tz import gettz\n                >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n                >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n                datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n                >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n                datetime.datetime(2012, 1, 19, 17, 21,\n                                  tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n            This parameter is ignored if ``ignoretz`` is set.\n\n        :param \\\\*\\\\*kwargs:\n            Keyword arguments as passed to ``_parse()``.\n\n        :return:\n            Returns a :class:`datetime.datetime` object or, if the\n            ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n            first element being a :class:`datetime.datetime` object, the second\n            a tuple containing the fuzzy tokens.\n\n        :raises ParserError:\n            Raised for invalid or unknown string format, if the provided\n            :class:`tzinfo` is not in a valid format, or if an invalid date\n            would be created.\n\n        :raises TypeError:\n            Raised for non-string or character stream input.\n\n        :raises OverflowError:\n            Raised if the parsed date exceeds the largest valid C integer on\n            your system.\n        \"\"\"\n\n        if default is None:\n            default = datetime.datetime.now().replace(hour=0, minute=0,\n                                                      second=0, microsecond=0)\n\n        res, skipped_tokens = self._parse(timestr, **kwargs)\n\n        if res is None:\n            raise ParserError(\"Unknown string format: %s\", timestr)\n\n        if len(res) == 0:\n            raise ParserError(\"String does not contain a date: %s\", timestr)\n\n        try:\n            ret = self._build_naive(res, default)\n        except ValueError as e:\n            six.raise_from(ParserError(e.args[0] + \": %s\", timestr), e)\n\n        if not ignoretz:\n            ret = self._build_tzaware(ret, res, tzinfos)\n\n        if kwargs.get('fuzzy_with_tokens', False):\n            return ret, skipped_tokens\n        else:\n            return ret\n\n    class _result(_resultbase):\n        __slots__ = [\"year\", \"month\", \"day\", \"weekday\",\n                     \"hour\", \"minute\", \"second\", \"microsecond\",\n                     \"tzname\", \"tzoffset\", \"ampm\",\"any_unused_tokens\"]\n\n    def _parse(self, timestr, dayfirst=None, yearfirst=None, fuzzy=False,\n               fuzzy_with_tokens=False):\n        \"\"\"\n        Private method which performs the heavy lifting of parsing, called from\n        ``parse()``, which passes on its ``kwargs`` to this function.\n\n        :param timestr:\n            The string to parse.\n\n        :param dayfirst:\n            Whether to interpret the first value in an ambiguous 3-integer date\n            (e.g. 01/05/09) as the day (``True``) or month (``False``). If\n            ``yearfirst`` is set to ``True``, this distinguishes between YDM\n            and YMD. If set to ``None``, this value is retrieved from the\n            current :class:`parserinfo` object (which itself defaults to\n            ``False``).\n\n        :param yearfirst:\n            Whether to interpret the first value in an ambiguous 3-integer date\n            (e.g. 01/05/09) as the year. If ``True``, the first number is taken\n            to be the year, otherwise the last number is taken to be the year.\n            If this is set to ``None``, the value is retrieved from the current\n            :class:`parserinfo` object (which itself defaults to ``False``).\n\n        :param fuzzy:\n            Whether to allow fuzzy parsing, allowing for string like \"Today is\n            January 1, 2047 at 8:21:00AM\".\n\n        :param fuzzy_with_tokens:\n            If ``True``, ``fuzzy`` is automatically set to True, and the parser\n            will return a tuple where the first element is the parsed\n            :class:`datetime.datetime` datetimestamp and the second element is\n            a tuple containing the portions of the string which were ignored:\n\n            .. doctest::\n\n                >>> from dateutil.parser import parse\n                >>> parse(\"Today is January 1, 2047 at 8:21:00AM\", fuzzy_with_tokens=True)\n                (datetime.datetime(2047, 1, 1, 8, 21), (u'Today is ', u' ', u'at '))\n\n        \"\"\"\n        if fuzzy_with_tokens:\n            fuzzy = True\n\n        info = self.info\n\n        if dayfirst is None:\n            dayfirst = info.dayfirst\n\n        if yearfirst is None:\n            yearfirst = info.yearfirst\n\n        res = self._result()\n        l = _timelex.split(timestr)         # Splits the timestr into tokens\n\n        skipped_idxs = []\n\n        # year/month/day list\n        ymd = _ymd()\n\n        len_l = len(l)\n        i = 0\n        try:\n            while i < len_l:\n\n                # Check if it's a number\n                value_repr = l[i]\n                try:\n                    value = float(value_repr)\n                except ValueError:\n                    value = None\n\n                if value is not None:\n                    # Numeric token\n                    i = self._parse_numeric_token(l, i, info, ymd, res, fuzzy)\n\n                # Check weekday\n                elif info.weekday(l[i]) is not None:\n                    value = info.weekday(l[i])\n                    res.weekday = value\n\n                # Check month name\n                elif info.month(l[i]) is not None:\n                    value = info.month(l[i])\n                    ymd.append(value, 'M')\n\n                    if i + 1 < len_l:\n                        if l[i + 1] in ('-', '/'):\n                            # Jan-01[-99]\n                            sep = l[i + 1]\n                            ymd.append(l[i + 2])\n\n                            if i + 3 < len_l and l[i + 3] == sep:\n                                # Jan-01-99\n                                ymd.append(l[i + 4])\n                                i += 2\n\n                            i += 2\n\n                        elif (i + 4 < len_l and l[i + 1] == l[i + 3] == ' ' and\n                              info.pertain(l[i + 2])):\n                            # Jan of 01\n                            # In this case, 01 is clearly year\n                            if l[i + 4].isdigit():\n                                # Convert it here to become unambiguous\n                                value = int(l[i + 4])\n                                year = str(info.convertyear(value))\n                                ymd.append(year, 'Y')\n                            else:\n                                # Wrong guess\n                                pass\n                                # TODO: not hit in tests\n                            i += 4\n\n                # Check am/pm\n                elif info.ampm(l[i]) is not None:\n                    value = info.ampm(l[i])\n                    val_is_ampm = self._ampm_valid(res.hour, res.ampm, fuzzy)\n\n                    if val_is_ampm:\n                        res.hour = self._adjust_ampm(res.hour, value)\n                        res.ampm = value\n\n                    elif fuzzy:\n                        skipped_idxs.append(i)\n\n                # Check for a timezone name\n                elif self._could_be_tzname(res.hour, res.tzname, res.tzoffset, l[i]):\n                    res.tzname = l[i]\n                    res.tzoffset = info.tzoffset(res.tzname)\n\n                    # Check for something like GMT+3, or BRST+3. Notice\n                    # that it doesn't mean \"I am 3 hours after GMT\", but\n                    # \"my time +3 is GMT\". If found, we reverse the\n                    # logic so that timezone parsing code will get it\n                    # right.\n                    if i + 1 < len_l and l[i + 1] in ('+', '-'):\n                        l[i + 1] = ('+', '-')[l[i + 1] == '+']\n                        res.tzoffset = None\n                        if info.utczone(res.tzname):\n                            # With something like GMT+3, the timezone\n                            # is *not* GMT.\n                            res.tzname = None\n\n                # Check for a numbered timezone\n                elif res.hour is not None and l[i] in ('+', '-'):\n                    signal = (-1, 1)[l[i] == '+']\n                    len_li = len(l[i + 1])\n\n                    # TODO: check that l[i + 1] is integer?\n                    if len_li == 4:\n                        # -0300\n                        hour_offset = int(l[i + 1][:2])\n                        min_offset = int(l[i + 1][2:])\n                    elif i + 2 < len_l and l[i + 2] == ':':\n                        # -03:00\n                        hour_offset = int(l[i + 1])\n                        min_offset = int(l[i + 3])  # TODO: Check that l[i+3] is minute-like?\n                        i += 2\n                    elif len_li <= 2:\n                        # -[0]3\n                        hour_offset = int(l[i + 1][:2])\n                        min_offset = 0\n                    else:\n                        raise ValueError(timestr)\n\n                    res.tzoffset = signal * (hour_offset * 3600 + min_offset * 60)\n\n                    # Look for a timezone name between parenthesis\n                    if (i + 5 < len_l and\n                            info.jump(l[i + 2]) and l[i + 3] == '(' and\n                            l[i + 5] == ')' and\n                            3 <= len(l[i + 4]) and\n                            self._could_be_tzname(res.hour, res.tzname,\n                                                  None, l[i + 4])):\n                        # -0300 (BRST)\n                        res.tzname = l[i + 4]\n                        i += 4\n\n                    i += 1\n\n                # Check jumps\n                elif not (info.jump(l[i]) or fuzzy):\n                    raise ValueError(timestr)\n\n                else:\n                    skipped_idxs.append(i)\n                i += 1\n\n            # Process year/month/day\n            year, month, day = ymd.resolve_ymd(yearfirst, dayfirst)\n\n            res.century_specified = ymd.century_specified\n            res.year = year\n            res.month = month\n            res.day = day\n\n        except (IndexError, ValueError):\n            return None, None\n\n        if not info.validate(res):\n            return None, None\n\n        if fuzzy_with_tokens:\n            skipped_tokens = self._recombine_skipped(l, skipped_idxs)\n            return res, tuple(skipped_tokens)\n        else:\n            return res, None\n\n    def _parse_numeric_token(self, tokens, idx, info, ymd, res, fuzzy):\n        # Token is a number\n        value_repr = tokens[idx]\n        try:\n            value = self._to_decimal(value_repr)\n        except Exception as e:\n            six.raise_from(ValueError('Unknown numeric token'), e)\n\n        len_li = len(value_repr)\n\n        len_l = len(tokens)\n\n        if (len(ymd) == 3 and len_li in (2, 4) and\n            res.hour is None and\n            (idx + 1 >= len_l or\n             (tokens[idx + 1] != ':' and\n              info.hms(tokens[idx + 1]) is None))):\n            # 19990101T23[59]\n            s = tokens[idx]\n            res.hour = int(s[:2])\n\n            if len_li == 4:\n                res.minute = int(s[2:])\n\n        elif len_li == 6 or (len_li > 6 and tokens[idx].find('.') == 6):\n            # YYMMDD or HHMMSS[.ss]\n            s = tokens[idx]\n\n            if not ymd and '.' not in tokens[idx]:\n                ymd.append(s[:2])\n                ymd.append(s[2:4])\n                ymd.append(s[4:])\n            else:\n                # 19990101T235959[.59]\n\n                # TODO: Check if res attributes already set.\n                res.hour = int(s[:2])\n                res.minute = int(s[2:4])\n                res.second, res.microsecond = self._parsems(s[4:])\n\n        elif len_li in (8, 12, 14):\n            # YYYYMMDD\n            s = tokens[idx]\n            ymd.append(s[:4], 'Y')\n            ymd.append(s[4:6])\n            ymd.append(s[6:8])\n\n            if len_li > 8:\n                res.hour = int(s[8:10])\n                res.minute = int(s[10:12])\n\n                if len_li > 12:\n                    res.second = int(s[12:])\n\n        elif self._find_hms_idx(idx, tokens, info, allow_jump=True) is not None:\n            # HH[ ]h or MM[ ]m or SS[.ss][ ]s\n            hms_idx = self._find_hms_idx(idx, tokens, info, allow_jump=True)\n            (idx, hms) = self._parse_hms(idx, tokens, info, hms_idx)\n            if hms is not None:\n                # TODO: checking that hour/minute/second are not\n                # already set?\n                self._assign_hms(res, value_repr, hms)\n\n        elif idx + 2 < len_l and tokens[idx + 1] == ':':\n            # HH:MM[:SS[.ss]]\n            res.hour = int(value)\n            value = self._to_decimal(tokens[idx + 2])  # TODO: try/except for this?\n            (res.minute, res.second) = self._parse_min_sec(value)\n\n            if idx + 4 < len_l and tokens[idx + 3] == ':':\n                res.second, res.microsecond = self._parsems(tokens[idx + 4])\n\n                idx += 2\n\n            idx += 2\n\n        elif idx + 1 < len_l and tokens[idx + 1] in ('-', '/', '.'):\n            sep = tokens[idx + 1]\n            ymd.append(value_repr)\n\n            if idx + 2 < len_l and not info.jump(tokens[idx + 2]):\n                if tokens[idx + 2].isdigit():\n                    # 01-01[-01]\n                    ymd.append(tokens[idx + 2])\n                else:\n                    # 01-Jan[-01]\n                    value = info.month(tokens[idx + 2])\n\n                    if value is not None:\n                        ymd.append(value, 'M')\n                    else:\n                        raise ValueError()\n\n                if idx + 3 < len_l and tokens[idx + 3] == sep:\n                    # We have three members\n                    value = info.month(tokens[idx + 4])\n\n                    if value is not None:\n                        ymd.append(value, 'M')\n                    else:\n                        ymd.append(tokens[idx + 4])\n                    idx += 2\n\n                idx += 1\n            idx += 1\n\n        elif idx + 1 >= len_l or info.jump(tokens[idx + 1]):\n            if idx + 2 < len_l and info.ampm(tokens[idx + 2]) is not None:\n                # 12 am\n                hour = int(value)\n                res.hour = self._adjust_ampm(hour, info.ampm(tokens[idx + 2]))\n                idx += 1\n            else:\n                # Year, month or day\n                ymd.append(value)\n            idx += 1\n\n        elif info.ampm(tokens[idx + 1]) is not None and (0 <= value < 24):\n            # 12am\n            hour = int(value)\n            res.hour = self._adjust_ampm(hour, info.ampm(tokens[idx + 1]))\n            idx += 1\n\n        elif ymd.could_be_day(value):\n            ymd.append(value)\n\n        elif not fuzzy:\n            raise ValueError()\n\n        return idx\n\n    def _find_hms_idx(self, idx, tokens, info, allow_jump):\n        len_l = len(tokens)\n\n        if idx+1 < len_l and info.hms(tokens[idx+1]) is not None:\n            # There is an \"h\", \"m\", or \"s\" label following this token.  We take\n            # assign the upcoming label to the current token.\n            # e.g. the \"12\" in 12h\"\n            hms_idx = idx + 1\n\n        elif (allow_jump and idx+2 < len_l and tokens[idx+1] == ' ' and\n              info.hms(tokens[idx+2]) is not None):\n            # There is a space and then an \"h\", \"m\", or \"s\" label.\n            # e.g. the \"12\" in \"12 h\"\n            hms_idx = idx + 2\n\n        elif idx > 0 and info.hms(tokens[idx-1]) is not None:\n            # There is a \"h\", \"m\", or \"s\" preceding this token.  Since neither\n            # of the previous cases was hit, there is no label following this\n            # token, so we use the previous label.\n            # e.g. the \"04\" in \"12h04\"\n            hms_idx = idx-1\n\n        elif (1 < idx == len_l-1 and tokens[idx-1] == ' ' and\n              info.hms(tokens[idx-2]) is not None):\n            # If we are looking at the final token, we allow for a\n            # backward-looking check to skip over a space.\n            # TODO: Are we sure this is the right condition here?\n            hms_idx = idx - 2\n\n        else:\n            hms_idx = None\n\n        return hms_idx\n\n    def _assign_hms(self, res, value_repr, hms):\n        # See GH issue #427, fixing float rounding\n        value = self._to_decimal(value_repr)\n\n        if hms == 0:\n            # Hour\n            res.hour = int(value)\n            if value % 1:\n                res.minute = int(60*(value % 1))\n\n        elif hms == 1:\n            (res.minute, res.second) = self._parse_min_sec(value)\n\n        elif hms == 2:\n            (res.second, res.microsecond) = self._parsems(value_repr)\n\n    def _could_be_tzname(self, hour, tzname, tzoffset, token):\n        return (hour is not None and\n                tzname is None and\n                tzoffset is None and\n                len(token) <= 5 and\n                (all(x in string.ascii_uppercase for x in token)\n                 or token in self.info.UTCZONE))\n\n    def _ampm_valid(self, hour, ampm, fuzzy):\n        \"\"\"\n        For fuzzy parsing, 'a' or 'am' (both valid English words)\n        may erroneously trigger the AM/PM flag. Deal with that\n        here.\n        \"\"\"\n        val_is_ampm = True\n\n        # If there's already an AM/PM flag, this one isn't one.\n        if fuzzy and ampm is not None:\n            val_is_ampm = False\n\n        # If AM/PM is found and hour is not, raise a ValueError\n        if hour is None:\n            if fuzzy:\n                val_is_ampm = False\n            else:\n                raise ValueError('No hour specified with AM or PM flag.')\n        elif not 0 <= hour <= 12:\n            # If AM/PM is found, it's a 12 hour clock, so raise\n            # an error for invalid range\n            if fuzzy:\n                val_is_ampm = False\n            else:\n                raise ValueError('Invalid hour specified for 12-hour clock.')\n\n        return val_is_ampm\n\n    def _adjust_ampm(self, hour, ampm):\n        if hour < 12 and ampm == 1:\n            hour += 12\n        elif hour == 12 and ampm == 0:\n            hour = 0\n        return hour\n\n    def _parse_min_sec(self, value):\n        # TODO: Every usage of this function sets res.second to the return\n        # value. Are there any cases where second will be returned as None and\n        # we *don't* want to set res.second = None?\n        minute = int(value)\n        second = None\n\n        sec_remainder = value % 1\n        if sec_remainder:\n            second = int(60 * sec_remainder)\n        return (minute, second)\n\n    def _parse_hms(self, idx, tokens, info, hms_idx):\n        # TODO: Is this going to admit a lot of false-positives for when we\n        # just happen to have digits and \"h\", \"m\" or \"s\" characters in non-date\n        # text?  I guess hex hashes won't have that problem, but there's plenty\n        # of random junk out there.\n        if hms_idx is None:\n            hms = None\n            new_idx = idx\n        elif hms_idx > idx:\n            hms = info.hms(tokens[hms_idx])\n            new_idx = hms_idx\n        else:\n            # Looking backwards, increment one.\n            hms = info.hms(tokens[hms_idx]) + 1\n            new_idx = idx\n\n        return (new_idx, hms)\n\n    # ------------------------------------------------------------------\n    # Handling for individual tokens.  These are kept as methods instead\n    #  of functions for the sake of customizability via subclassing.\n\n    def _parsems(self, value):\n        \"\"\"Parse a I[.F] seconds value into (seconds, microseconds).\"\"\"\n        if \".\" not in value:\n            return int(value), 0\n        else:\n            i, f = value.split(\".\")\n            return int(i), int(f.ljust(6, \"0\")[:6])\n\n    def _to_decimal(self, val):\n        try:\n            decimal_value = Decimal(val)\n            # See GH 662, edge case, infinite value should not be converted\n            #  via `_to_decimal`\n            if not decimal_value.is_finite():\n                raise ValueError(\"Converted decimal value is infinite or NaN\")\n        except Exception as e:\n            msg = \"Could not convert %s to decimal\" % val\n            six.raise_from(ValueError(msg), e)\n        else:\n            return decimal_value\n\n    # ------------------------------------------------------------------\n    # Post-Parsing construction of datetime output.  These are kept as\n    #  methods instead of functions for the sake of customizability via\n    #  subclassing.\n\n    def _build_tzinfo(self, tzinfos, tzname, tzoffset):\n        if callable(tzinfos):\n            tzdata = tzinfos(tzname, tzoffset)\n        else:\n            tzdata = tzinfos.get(tzname)\n        # handle case where tzinfo is paased an options that returns None\n        # eg tzinfos = {'BRST' : None}\n        if isinstance(tzdata, datetime.tzinfo) or tzdata is None:\n            tzinfo = tzdata\n        elif isinstance(tzdata, text_type):\n            tzinfo = tz.tzstr(tzdata)\n        elif isinstance(tzdata, integer_types):\n            tzinfo = tz.tzoffset(tzname, tzdata)\n        else:\n            raise TypeError(\"Offset must be tzinfo subclass, tz string, \"\n                            \"or int offset.\")\n        return tzinfo\n\n    def _build_tzaware(self, naive, res, tzinfos):\n        if (callable(tzinfos) or (tzinfos and res.tzname in tzinfos)):\n            tzinfo = self._build_tzinfo(tzinfos, res.tzname, res.tzoffset)\n            aware = naive.replace(tzinfo=tzinfo)\n            aware = self._assign_tzname(aware, res.tzname)\n\n        elif res.tzname and res.tzname in time.tzname:\n            aware = naive.replace(tzinfo=tz.tzlocal())\n\n            # Handle ambiguous local datetime\n            aware = self._assign_tzname(aware, res.tzname)\n\n            # This is mostly relevant for winter GMT zones parsed in the UK\n            if (aware.tzname() != res.tzname and\n                    res.tzname in self.info.UTCZONE):\n                aware = aware.replace(tzinfo=tz.UTC)\n\n        elif res.tzoffset == 0:\n            aware = naive.replace(tzinfo=tz.UTC)\n\n        elif res.tzoffset:\n            aware = naive.replace(tzinfo=tz.tzoffset(res.tzname, res.tzoffset))\n\n        elif not res.tzname and not res.tzoffset:\n            # i.e. no timezone information was found.\n            aware = naive\n\n        elif res.tzname:\n            # tz-like string was parsed but we don't know what to do\n            # with it\n            warnings.warn(\"tzname {tzname} identified but not understood.  \"\n                          \"Pass `tzinfos` argument in order to correctly \"\n                          \"return a timezone-aware datetime.  In a future \"\n                          \"version, this will raise an \"\n                          \"exception.\".format(tzname=res.tzname),\n                          category=UnknownTimezoneWarning)\n            aware = naive\n\n        return aware\n\n    def _build_naive(self, res, default):\n        repl = {}\n        for attr in (\"year\", \"month\", \"day\", \"hour\",\n                     \"minute\", \"second\", \"microsecond\"):\n            value = getattr(res, attr)\n            if value is not None:\n                repl[attr] = value\n\n        if 'day' not in repl:\n            # If the default day exceeds the last day of the month, fall back\n            # to the end of the month.\n            cyear = default.year if res.year is None else res.year\n            cmonth = default.month if res.month is None else res.month\n            cday = default.day if res.day is None else res.day\n\n            if cday > monthrange(cyear, cmonth)[1]:\n                repl['day'] = monthrange(cyear, cmonth)[1]\n\n        naive = default.replace(**repl)\n\n        if res.weekday is not None and not res.day:\n            naive = naive + relativedelta.relativedelta(weekday=res.weekday)\n\n        return naive\n\n    def _assign_tzname(self, dt, tzname):\n        if dt.tzname() != tzname:\n            new_dt = tz.enfold(dt, fold=1)\n            if new_dt.tzname() == tzname:\n                return new_dt\n\n        return dt\n\n    def _recombine_skipped(self, tokens, skipped_idxs):\n        \"\"\"\n        >>> tokens = [\"foo\", \" \", \"bar\", \" \", \"19June2000\", \"baz\"]\n        >>> skipped_idxs = [0, 1, 2, 5]\n        >>> _recombine_skipped(tokens, skipped_idxs)\n        [\"foo bar\", \"baz\"]\n        \"\"\"\n        skipped_tokens = []\n        for i, idx in enumerate(sorted(skipped_idxs)):\n            if i > 0 and idx - 1 == skipped_idxs[i - 1]:\n                skipped_tokens[-1] = skipped_tokens[-1] + tokens[idx]\n            else:\n                skipped_tokens.append(tokens[idx])\n\n        return skipped_tokens\n\n\nDEFAULTPARSER = parser()\n\n\ndef parse(timestr, parserinfo=None, **kwargs):\n    \"\"\"\n\n    Parse a string in one of the supported formats, using the\n    ``parserinfo`` parameters.\n\n    :param timestr:\n        A string containing a date/time stamp.\n\n    :param parserinfo:\n        A :class:`parserinfo` object containing parameters for the parser.\n        If ``None``, the default arguments to the :class:`parserinfo`\n        constructor are used.\n\n    The ``**kwargs`` parameter takes the following keyword arguments:\n\n    :param default:\n        The default datetime object, if this is a datetime object and not\n        ``None``, elements specified in ``timestr`` replace elements in the\n        default object.\n\n    :param ignoretz:\n        If set ``True``, time zones in parsed strings are ignored and a naive\n        :class:`datetime` object is returned.\n\n    :param tzinfos:\n        Additional time zone names / aliases which may be present in the\n        string. This argument maps time zone names (and optionally offsets\n        from those time zones) to time zones. This parameter can be a\n        dictionary with timezone aliases mapping time zone names to time\n        zones or a function taking two parameters (``tzname`` and\n        ``tzoffset``) and returning a time zone.\n\n        The timezones to which the names are mapped can be an integer\n        offset from UTC in seconds or a :class:`tzinfo` object.\n\n        .. doctest::\n           :options: +NORMALIZE_WHITESPACE\n\n            >>> from dateutil.parser import parse\n            >>> from dateutil.tz import gettz\n            >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n            >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n            datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n            >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n            datetime.datetime(2012, 1, 19, 17, 21,\n                              tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n        This parameter is ignored if ``ignoretz`` is set.\n\n    :param dayfirst:\n        Whether to interpret the first value in an ambiguous 3-integer date\n        (e.g. 01/05/09) as the day (``True``) or month (``False``). If\n        ``yearfirst`` is set to ``True``, this distinguishes between YDM and\n        YMD. If set to ``None``, this value is retrieved from the current\n        :class:`parserinfo` object (which itself defaults to ``False``).\n\n    :param yearfirst:\n        Whether to interpret the first value in an ambiguous 3-integer date\n        (e.g. 01/05/09) as the year. If ``True``, the first number is taken to\n        be the year, otherwise the last number is taken to be the year. If\n        this is set to ``None``, the value is retrieved from the current\n        :class:`parserinfo` object (which itself defaults to ``False``).\n\n    :param fuzzy:\n        Whether to allow fuzzy parsing, allowing for string like \"Today is\n        January 1, 2047 at 8:21:00AM\".\n\n    :param fuzzy_with_tokens:\n        If ``True``, ``fuzzy`` is automatically set to True, and the parser\n        will return a tuple where the first element is the parsed\n        :class:`datetime.datetime` datetimestamp and the second element is\n        a tuple containing the portions of the string which were ignored:\n\n        .. doctest::\n\n            >>> from dateutil.parser import parse\n            >>> parse(\"Today is January 1, 2047 at 8:21:00AM\", fuzzy_with_tokens=True)\n            (datetime.datetime(2047, 1, 1, 8, 21), (u'Today is ', u' ', u'at '))\n\n    :return:\n        Returns a :class:`datetime.datetime` object or, if the\n        ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n        first element being a :class:`datetime.datetime` object, the second\n        a tuple containing the fuzzy tokens.\n\n    :raises ValueError:\n        Raised for invalid or unknown string format, if the provided\n        :class:`tzinfo` is not in a valid format, or if an invalid date\n        would be created.\n\n    :raises OverflowError:\n        Raised if the parsed date exceeds the largest valid C integer on\n        your system.\n    \"\"\"\n    if parserinfo:\n        return parser(parserinfo).parse(timestr, **kwargs)\n    else:\n        return DEFAULTPARSER.parse(timestr, **kwargs)\n\n\nclass _tzparser(object):\n\n    class _result(_resultbase):\n\n        __slots__ = [\"stdabbr\", \"stdoffset\", \"dstabbr\", \"dstoffset\",\n                     \"start\", \"end\"]\n\n        class _attr(_resultbase):\n            __slots__ = [\"month\", \"week\", \"weekday\",\n                         \"yday\", \"jyday\", \"day\", \"time\"]\n\n        def __repr__(self):\n            return self._repr(\"\")\n\n        def __init__(self):\n            _resultbase.__init__(self)\n            self.start = self._attr()\n            self.end = self._attr()\n\n    def parse(self, tzstr):\n        res = self._result()\n        l = [x for x in re.split(r'([,:.]|[a-zA-Z]+|[0-9]+)',tzstr) if x]\n        used_idxs = list()\n        try:\n\n            len_l = len(l)\n\n            i = 0\n            while i < len_l:\n                # BRST+3[BRDT[+2]]\n                j = i\n                while j < len_l and not [x for x in l[j]\n                                         if x in \"0123456789:,-+\"]:\n                    j += 1\n                if j != i:\n                    if not res.stdabbr:\n                        offattr = \"stdoffset\"\n                        res.stdabbr = \"\".join(l[i:j])\n                    else:\n                        offattr = \"dstoffset\"\n                        res.dstabbr = \"\".join(l[i:j])\n\n                    for ii in range(j):\n                        used_idxs.append(ii)\n                    i = j\n                    if (i < len_l and (l[i] in ('+', '-') or l[i][0] in\n                                       \"0123456789\")):\n                        if l[i] in ('+', '-'):\n                            # Yes, that's right.  See the TZ variable\n                            # documentation.\n                            signal = (1, -1)[l[i] == '+']\n                            used_idxs.append(i)\n                            i += 1\n                        else:\n                            signal = -1\n                        len_li = len(l[i])\n                        if len_li == 4:\n                            # -0300\n                            setattr(res, offattr, (int(l[i][:2]) * 3600 +\n                                                   int(l[i][2:]) * 60) * signal)\n                        elif i + 1 < len_l and l[i + 1] == ':':\n                            # -03:00\n                            setattr(res, offattr,\n                                    (int(l[i]) * 3600 +\n                                     int(l[i + 2]) * 60) * signal)\n                            used_idxs.append(i)\n                            i += 2\n                        elif len_li <= 2:\n                            # -[0]3\n                            setattr(res, offattr,\n                                    int(l[i][:2]) * 3600 * signal)\n                        else:\n                            return None\n                        used_idxs.append(i)\n                        i += 1\n                    if res.dstabbr:\n                        break\n                else:\n                    break\n\n\n            if i < len_l:\n                for j in range(i, len_l):\n                    if l[j] == ';':\n                        l[j] = ','\n\n                assert l[i] == ','\n\n                i += 1\n\n            if i >= len_l:\n                pass\n            elif (8 <= l.count(',') <= 9 and\n                  not [y for x in l[i:] if x != ','\n                       for y in x if y not in \"0123456789+-\"]):\n                # GMT0BST,3,0,30,3600,10,0,26,7200[,3600]\n                for x in (res.start, res.end):\n                    x.month = int(l[i])\n                    used_idxs.append(i)\n                    i += 2\n                    if l[i] == '-':\n                        value = int(l[i + 1]) * -1\n                        used_idxs.append(i)\n                        i += 1\n                    else:\n                        value = int(l[i])\n                    used_idxs.append(i)\n                    i += 2\n                    if value:\n                        x.week = value\n                        x.weekday = (int(l[i]) - 1) % 7\n                    else:\n                        x.day = int(l[i])\n                    used_idxs.append(i)\n                    i += 2\n                    x.time = int(l[i])\n                    used_idxs.append(i)\n                    i += 2\n                if i < len_l:\n                    if l[i] in ('-', '+'):\n                        signal = (-1, 1)[l[i] == \"+\"]\n                        used_idxs.append(i)\n                        i += 1\n                    else:\n                        signal = 1\n                    used_idxs.append(i)\n                    res.dstoffset = (res.stdoffset + int(l[i]) * signal)\n\n                # This was a made-up format that is not in normal use\n                warn(('Parsed time zone \"%s\"' % tzstr) +\n                     'is in a non-standard dateutil-specific format, which ' +\n                     'is now deprecated; support for parsing this format ' +\n                     'will be removed in future versions. It is recommended ' +\n                     'that you switch to a standard format like the GNU ' +\n                     'TZ variable format.', tz.DeprecatedTzFormatWarning)\n            elif (l.count(',') == 2 and l[i:].count('/') <= 2 and\n                  not [y for x in l[i:] if x not in (',', '/', 'J', 'M',\n                                                     '.', '-', ':')\n                       for y in x if y not in \"0123456789\"]):\n                for x in (res.start, res.end):\n                    if l[i] == 'J':\n                        # non-leap year day (1 based)\n                        used_idxs.append(i)\n                        i += 1\n                        x.jyday = int(l[i])\n                    elif l[i] == 'M':\n                        # month[-.]week[-.]weekday\n                        used_idxs.append(i)\n                        i += 1\n                        x.month = int(l[i])\n                        used_idxs.append(i)\n                        i += 1\n                        assert l[i] in ('-', '.')\n                        used_idxs.append(i)\n                        i += 1\n                        x.week = int(l[i])\n                        if x.week == 5:\n                            x.week = -1\n                        used_idxs.append(i)\n                        i += 1\n                        assert l[i] in ('-', '.')\n                        used_idxs.append(i)\n                        i += 1\n                        x.weekday = (int(l[i]) - 1) % 7\n                    else:\n                        # year day (zero based)\n                        x.yday = int(l[i]) + 1\n\n                    used_idxs.append(i)\n                    i += 1\n\n                    if i < len_l and l[i] == '/':\n                        used_idxs.append(i)\n                        i += 1\n                        # start time\n                        len_li = len(l[i])\n                        if len_li == 4:\n                            # -0300\n                            x.time = (int(l[i][:2]) * 3600 +\n                                      int(l[i][2:]) * 60)\n                        elif i + 1 < len_l and l[i + 1] == ':':\n                            # -03:00\n                            x.time = int(l[i]) * 3600 + int(l[i + 2]) * 60\n                            used_idxs.append(i)\n                            i += 2\n                            if i + 1 < len_l and l[i + 1] == ':':\n                                used_idxs.append(i)\n                                i += 2\n                                x.time += int(l[i])\n                        elif len_li <= 2:\n                            # -[0]3\n                            x.time = (int(l[i][:2]) * 3600)\n                        else:\n                            return None\n                        used_idxs.append(i)\n                        i += 1\n\n                    assert i == len_l or l[i] == ','\n\n                    i += 1\n\n                assert i >= len_l\n\n        except (IndexError, ValueError, AssertionError):\n            return None\n\n        unused_idxs = set(range(len_l)).difference(used_idxs)\n        res.any_unused_tokens = not {l[n] for n in unused_idxs}.issubset({\",\",\":\"})\n        return res\n\n\nDEFAULTTZPARSER = _tzparser()\n\n\ndef _parsetz(tzstr):\n    return DEFAULTTZPARSER.parse(tzstr)\n\n\nclass ParserError(ValueError):\n    \"\"\"Error class for representing failure to parse a datetime string.\"\"\"\n    def __str__(self):\n        try:\n            return self.args[0] % self.args[1:]\n        except (TypeError, IndexError):\n            return super(ParserError, self).__str__()\n\n        def __repr__(self):\n            return \"%s(%s)\" % (self.__class__.__name__, str(self))\n\n\nclass UnknownTimezoneWarning(RuntimeWarning):\n    \"\"\"Raised when the parser finds a timezone it cannot parse into a tzinfo\"\"\"\n# vim:ts=4:sw=4:et\n",
            "file_path": "dateutil/parser/_parser.py",
            "human_label": "Parse the date/time string into a :class:`datetime.datetime` object.\n\n:param timestr:\n    Any date/time string using the supported formats.\n\n:param default:\n    The default datetime object, if this is a datetime object and not\n    ``None``, elements specified in ``timestr`` replace elements in the\n    default object.\n\n:param ignoretz:\n    If set ``True``, time zones in parsed strings are ignored and a\n    naive :class:`datetime.datetime` object is returned.\n\n:param tzinfos:\n    Additional time zone names / aliases which may be present in the\n    string. This argument maps time zone names (and optionally offsets\n    from those time zones) to time zones. This parameter can be a\n    dictionary with timezone aliases mapping time zone names to time\n    zones or a function taking two parameters (``tzname`` and\n    ``tzoffset``) and returning a time zone.\n\n    The timezones to which the names are mapped can be an integer\n    offset from UTC in seconds or a :class:`tzinfo` object.\n\n    .. doctest::\n       :options: +NORMALIZE_WHITESPACE\n\n        >>> from dateutil.parser import parse\n        >>> from dateutil.tz import gettz\n        >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n        >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n        >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n        datetime.datetime(2012, 1, 19, 17, 21,\n                          tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\n\n    This parameter is ignored if ``ignoretz`` is set.\n\n:param \\*\\*kwargs:\n    Keyword arguments as passed to ``_parse()``.\n\n:return:\n    Returns a :class:`datetime.datetime` object or, if the\n    ``fuzzy_with_tokens`` option is ``True``, returns a tuple, the\n    first element being a :class:`datetime.datetime` object, the second\n    a tuple containing the fuzzy tokens.\n\n:raises ParserError:\n    Raised for invalid or unknown string format, if the provided\n    :class:`tzinfo` is not in a valid format, or if an invalid date\n    would be created.\n\n:raises TypeError:\n    Raised for non-string or character stream input.\n\n:raises OverflowError:\n    Raised if the parsed date exceeds the largest valid C integer on\n    your system.",
            "level": "project_runnable",
            "lineno": "578",
            "name": "parse",
            "oracle_context": "{ \"apis\" : \"['get', '_build_tzaware', 'now', '_build_naive', '_parse', 'len', 'raise_from', 'replace']\", \"classes\" : \"['datetime', 'six', 'ParserError']\", \"vars\" : \"['kwargs', 'args', 'datetime']\" }",
            "package": "_parser",
            "project": "pexip/os-python-dateutil",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "62b8a4a4755ee91dce50a3d3",
            "all_context": "{ \"import\" : \"functools datetime functools datetime six \", \"file\" : \"\", \"class\" : \"self._fold(self,dt) ; self.is_ambiguous(self,dt) ; self._fold_status ; self.is_ambiguous ; self.fromutc(self,dt) ; self._fromutc(self,dt) ; self._fromutc ; self._fold_status(self,dt_utc,dt_wall) ; \" }",
            "code": "    @_validate_fromutc_inputs\n    def fromutc(self, dt):\n        \"\"\"\n        Given a timezone-aware datetime in a given timezone, calculates a\n        timezone-aware datetime in a new timezone.\n\n        Since this is the one time that we *know* we have an unambiguous\n        datetime object, we take this opportunity to determine whether the\n        datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n        occurrence, chronologically, of the ambiguous datetime).\n\n        :param dt:\n            A timezone-aware :class:`datetime.datetime` object.\n        \"\"\"\n        dt_wall = self._fromutc(dt)\n\n        # Calculate the fold status given the two datetimes.\n        _fold = self._fold_status(dt, dt_wall)\n\n        # Set the default fold value for ambiguous dates\n        return enfold(dt_wall, fold=_fold)\n",
            "dependency": "",
            "docstring": "Given a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.",
            "end_lineno": "264",
            "file_content": "from six import PY2\n\nfrom functools import wraps\n\nfrom datetime import datetime, timedelta, tzinfo\n\n\nZERO = timedelta(0)\n\n__all__ = ['tzname_in_python2', 'enfold']\n\n\ndef tzname_in_python2(namefunc):\n    \"\"\"Change unicode output into bytestrings in Python 2\n\n    tzname() API changed in Python 3. It used to return bytes, but was changed\n    to unicode strings\n    \"\"\"\n    if PY2:\n        @wraps(namefunc)\n        def adjust_encoding(*args, **kwargs):\n            name = namefunc(*args, **kwargs)\n            if name is not None:\n                name = name.encode()\n\n            return name\n\n        return adjust_encoding\n    else:\n        return namefunc\n\n\n# The following is adapted from Alexander Belopolsky's tz library\n# https://github.com/abalkin/tz\nif hasattr(datetime, 'fold'):\n    # This is the pre-python 3.6 fold situation\n    def enfold(dt, fold=1):\n        \"\"\"\n        Provides a unified interface for assigning the ``fold`` attribute to\n        datetimes both before and after the implementation of PEP-495.\n\n        :param fold:\n            The value for the ``fold`` attribute in the returned datetime. This\n            should be either 0 or 1.\n\n        :return:\n            Returns an object for which ``getattr(dt, 'fold', 0)`` returns\n            ``fold`` for all versions of Python. In versions prior to\n            Python 3.6, this is a ``_DatetimeWithFold`` object, which is a\n            subclass of :py:class:`datetime.datetime` with the ``fold``\n            attribute added, if ``fold`` is 1.\n\n        .. versionadded:: 2.6.0\n        \"\"\"\n        return dt.replace(fold=fold)\n\nelse:\n    class _DatetimeWithFold(datetime):\n        \"\"\"\n        This is a class designed to provide a PEP 495-compliant interface for\n        Python versions before 3.6. It is used only for dates in a fold, so\n        the ``fold`` attribute is fixed at ``1``.\n\n        .. versionadded:: 2.6.0\n        \"\"\"\n        __slots__ = ()\n\n        def replace(self, *args, **kwargs):\n            \"\"\"\n            Return a datetime with the same attributes, except for those\n            attributes given new values by whichever keyword arguments are\n            specified. Note that tzinfo=None can be specified to create a naive\n            datetime from an aware datetime with no conversion of date and time\n            data.\n\n            This is reimplemented in ``_DatetimeWithFold`` because pypy3 will\n            return a ``datetime.datetime`` even if ``fold`` is unchanged.\n            \"\"\"\n            argnames = (\n                'year', 'month', 'day', 'hour', 'minute', 'second',\n                'microsecond', 'tzinfo'\n            )\n\n            for arg, argname in zip(args, argnames):\n                if argname in kwargs:\n                    raise TypeError('Duplicate argument: {}'.format(argname))\n\n                kwargs[argname] = arg\n\n            for argname in argnames:\n                if argname not in kwargs:\n                    kwargs[argname] = getattr(self, argname)\n\n            dt_class = self.__class__ if kwargs.get('fold', 1) else datetime\n\n            return dt_class(**kwargs)\n\n        @property\n        def fold(self):\n            return 1\n\n    def enfold(dt, fold=1):\n        \"\"\"\n        Provides a unified interface for assigning the ``fold`` attribute to\n        datetimes both before and after the implementation of PEP-495.\n\n        :param fold:\n            The value for the ``fold`` attribute in the returned datetime. This\n            should be either 0 or 1.\n\n        :return:\n            Returns an object for which ``getattr(dt, 'fold', 0)`` returns\n            ``fold`` for all versions of Python. In versions prior to\n            Python 3.6, this is a ``_DatetimeWithFold`` object, which is a\n            subclass of :py:class:`datetime.datetime` with the ``fold``\n            attribute added, if ``fold`` is 1.\n\n        .. versionadded:: 2.6.0\n        \"\"\"\n        if getattr(dt, 'fold', 0) == fold:\n            return dt\n\n        args = dt.timetuple()[:6]\n        args += (dt.microsecond, dt.tzinfo)\n\n        if fold:\n            return _DatetimeWithFold(*args)\n        else:\n            return datetime(*args)\n\n\ndef _validate_fromutc_inputs(f):\n    \"\"\"\n    The CPython version of ``fromutc`` checks that the input is a ``datetime``\n    object and that ``self`` is attached as its ``tzinfo``.\n    \"\"\"\n    @wraps(f)\n    def fromutc(self, dt):\n        if not isinstance(dt, datetime):\n            raise TypeError(\"fromutc() requires a datetime argument\")\n        if dt.tzinfo is not self:\n            raise ValueError(\"dt.tzinfo is not self\")\n\n        return f(self, dt)\n\n    return fromutc\n\n\nclass _tzinfo(tzinfo):\n    \"\"\"\n    Base class for all ``dateutil`` ``tzinfo`` objects.\n    \"\"\"\n\n    def is_ambiguous(self, dt):\n        \"\"\"\n        Whether or not the \"wall time\" of a given datetime is ambiguous in this\n        zone.\n\n        :param dt:\n            A :py:class:`datetime.datetime`, naive or time zone aware.\n\n\n        :return:\n            Returns ``True`` if ambiguous, ``False`` otherwise.\n\n        .. versionadded:: 2.6.0\n        \"\"\"\n\n        dt = dt.replace(tzinfo=self)\n\n        wall_0 = enfold(dt, fold=0)\n        wall_1 = enfold(dt, fold=1)\n\n        same_offset = wall_0.utcoffset() == wall_1.utcoffset()\n        same_dt = wall_0.replace(tzinfo=None) == wall_1.replace(tzinfo=None)\n\n        return same_dt and not same_offset\n\n    def _fold_status(self, dt_utc, dt_wall):\n        \"\"\"\n        Determine the fold status of a \"wall\" datetime, given a representation\n        of the same datetime as a (naive) UTC datetime. This is calculated based\n        on the assumption that ``dt.utcoffset() - dt.dst()`` is constant for all\n        datetimes, and that this offset is the actual number of hours separating\n        ``dt_utc`` and ``dt_wall``.\n\n        :param dt_utc:\n            Representation of the datetime as UTC\n\n        :param dt_wall:\n            Representation of the datetime as \"wall time\". This parameter must\n            either have a `fold` attribute or have a fold-naive\n            :class:`datetime.tzinfo` attached, otherwise the calculation may\n            fail.\n        \"\"\"\n        if self.is_ambiguous(dt_wall):\n            delta_wall = dt_wall - dt_utc\n            _fold = int(delta_wall == (dt_utc.utcoffset() - dt_utc.dst()))\n        else:\n            _fold = 0\n\n        return _fold\n\n    def _fold(self, dt):\n        return getattr(dt, 'fold', 0)\n\n    def _fromutc(self, dt):\n        \"\"\"\n        Given a timezone-aware datetime in a given timezone, calculates a\n        timezone-aware datetime in a new timezone.\n\n        Since this is the one time that we *know* we have an unambiguous\n        datetime object, we take this opportunity to determine whether the\n        datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n        occurrence, chronologically, of the ambiguous datetime).\n\n        :param dt:\n            A timezone-aware :class:`datetime.datetime` object.\n        \"\"\"\n\n        # Re-implement the algorithm from Python's datetime.py\n        dtoff = dt.utcoffset()\n        if dtoff is None:\n            raise ValueError(\"fromutc() requires a non-None utcoffset() \"\n                             \"result\")\n\n        # The original datetime.py code assumes that `dst()` defaults to\n        # zero during ambiguous times. PEP 495 inverts this presumption, so\n        # for pre-PEP 495 versions of python, we need to tweak the algorithm.\n        dtdst = dt.dst()\n        if dtdst is None:\n            raise ValueError(\"fromutc() requires a non-None dst() result\")\n        delta = dtoff - dtdst\n\n        dt += delta\n        # Set fold=1 so we can default to being in the fold for\n        # ambiguous dates.\n        dtdst = enfold(dt, fold=1).dst()\n        if dtdst is None:\n            raise ValueError(\"fromutc(): dt.dst gave inconsistent \"\n                             \"results; cannot convert\")\n        return dt + dtdst\n\n    @_validate_fromutc_inputs\n    def fromutc(self, dt):\n        \"\"\"\n        Given a timezone-aware datetime in a given timezone, calculates a\n        timezone-aware datetime in a new timezone.\n\n        Since this is the one time that we *know* we have an unambiguous\n        datetime object, we take this opportunity to determine whether the\n        datetime is ambiguous and in a \"fold\" state (e.g. if it's the first\n        occurrence, chronologically, of the ambiguous datetime).\n\n        :param dt:\n            A timezone-aware :class:`datetime.datetime` object.\n        \"\"\"\n        dt_wall = self._fromutc(dt)\n\n        # Calculate the fold status given the two datetimes.\n        _fold = self._fold_status(dt, dt_wall)\n\n        # Set the default fold value for ambiguous dates\n        return enfold(dt_wall, fold=_fold)\n\n\nclass tzrangebase(_tzinfo):\n    \"\"\"\n    This is an abstract base class for time zones represented by an annual\n    transition into and out of DST. Child classes should implement the following\n    methods:\n\n        * ``__init__(self, *args, **kwargs)``\n        * ``transitions(self, year)`` - this is expected to return a tuple of\n          datetimes representing the DST on and off transitions in standard\n          time.\n\n    A fully initialized ``tzrangebase`` subclass should also provide the\n    following attributes:\n        * ``hasdst``: Boolean whether or not the zone uses DST.\n        * ``_dst_offset`` / ``_std_offset``: :class:`datetime.timedelta` objects\n          representing the respective UTC offsets.\n        * ``_dst_abbr`` / ``_std_abbr``: Strings representing the timezone short\n          abbreviations in DST and STD, respectively.\n        * ``_hasdst``: Whether or not the zone has DST.\n\n    .. versionadded:: 2.6.0\n    \"\"\"\n    def __init__(self):\n        raise NotImplementedError('tzrangebase is an abstract base class')\n\n    def utcoffset(self, dt):\n        isdst = self._isdst(dt)\n\n        if isdst is None:\n            return None\n        elif isdst:\n            return self._dst_offset\n        else:\n            return self._std_offset\n\n    def dst(self, dt):\n        isdst = self._isdst(dt)\n\n        if isdst is None:\n            return None\n        elif isdst:\n            return self._dst_base_offset\n        else:\n            return ZERO\n\n    @tzname_in_python2\n    def tzname(self, dt):\n        if self._isdst(dt):\n            return self._dst_abbr\n        else:\n            return self._std_abbr\n\n    def fromutc(self, dt):\n        \"\"\" Given a datetime in UTC, return local time \"\"\"\n        if not isinstance(dt, datetime):\n            raise TypeError(\"fromutc() requires a datetime argument\")\n\n        if dt.tzinfo is not self:\n            raise ValueError(\"dt.tzinfo is not self\")\n\n        # Get transitions - if there are none, fixed offset\n        transitions = self.transitions(dt.year)\n        if transitions is None:\n            return dt + self.utcoffset(dt)\n\n        # Get the transition times in UTC\n        dston, dstoff = transitions\n\n        dston -= self._std_offset\n        dstoff -= self._std_offset\n\n        utc_transitions = (dston, dstoff)\n        dt_utc = dt.replace(tzinfo=None)\n\n        isdst = self._naive_isdst(dt_utc, utc_transitions)\n\n        if isdst:\n            dt_wall = dt + self._dst_offset\n        else:\n            dt_wall = dt + self._std_offset\n\n        _fold = int(not isdst and self.is_ambiguous(dt_wall))\n\n        return enfold(dt_wall, fold=_fold)\n\n    def is_ambiguous(self, dt):\n        \"\"\"\n        Whether or not the \"wall time\" of a given datetime is ambiguous in this\n        zone.\n\n        :param dt:\n            A :py:class:`datetime.datetime`, naive or time zone aware.\n\n\n        :return:\n            Returns ``True`` if ambiguous, ``False`` otherwise.\n\n        .. versionadded:: 2.6.0\n        \"\"\"\n        if not self.hasdst:\n            return False\n\n        start, end = self.transitions(dt.year)\n\n        dt = dt.replace(tzinfo=None)\n        return (end <= dt < end + self._dst_base_offset)\n\n    def _isdst(self, dt):\n        if not self.hasdst:\n            return False\n        elif dt is None:\n            return None\n\n        transitions = self.transitions(dt.year)\n\n        if transitions is None:\n            return False\n\n        dt = dt.replace(tzinfo=None)\n\n        isdst = self._naive_isdst(dt, transitions)\n\n        # Handle ambiguous dates\n        if not isdst and self.is_ambiguous(dt):\n            return not self._fold(dt)\n        else:\n            return isdst\n\n    def _naive_isdst(self, dt, transitions):\n        dston, dstoff = transitions\n\n        dt = dt.replace(tzinfo=None)\n\n        if dston < dstoff:\n            isdst = dston <= dt < dstoff\n        else:\n            isdst = not dstoff <= dt < dston\n\n        return isdst\n\n    @property\n    def _dst_base_offset(self):\n        return self._dst_offset - self._std_offset\n\n    __hash__ = None\n\n    def __ne__(self, other):\n        return not (self == other)\n\n    def __repr__(self):\n        return \"%s(...)\" % self.__class__.__name__\n\n    __reduce__ = object.__reduce__\n",
            "file_path": "dateutil/tz/_common.py",
            "human_label": "Given a timezone-aware datetime in a given timezone, calculates a\ntimezone-aware datetime in a new timezone.\n\nSince this is the one time that we *know* we have an unambiguous\ndatetime object, we take this opportunity to determine whether the\ndatetime is ambiguous and in a \"fold\" state (e.g. if it's the first\noccurrence, chronologically, of the ambiguous datetime).\n\n:param dt:\n    A timezone-aware :class:`datetime.datetime` object.",
            "level": "class_runnable",
            "lineno": "244",
            "name": "fromutc",
            "oracle_context": "{ \"apis\" : \"['_fold_status', 'enfold', '_fold', '_fromutc']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }",
            "package": "_common",
            "project": "pexip/os-python-dateutil",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "6306292052e177c0ba469f09",
            "all_context": "{ \"import\" : \"typing logging json base64 urllib typing federation logging lxml \", \"file\" : \"logger ; PROTOCOL_NAME ; PROTOCOL_NS ; MAGIC_ENV_TAG ; identify_id(id) ; identify_request(request) ; \", \"class\" : \"\" }",
            "code": "def identify_request(request: RequestType):\n    \"\"\"Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Private encrypted JSON payload\n    try:\n        data = json.loads(decode_if_bytes(request.body))\n        if \"encrypted_magic_envelope\" in data:\n            return True\n    except Exception:\n        pass\n    # Public XML payload\n    try:\n        xml = etree.fromstring(encode_if_text(request.body))\n        if xml.tag == MAGIC_ENV_TAG:\n            return True\n    except Exception:\n        pass\n    return False\n",
            "dependency": "",
            "docstring": "Try to identify whether this is a Diaspora request.\n\nTry first public message. Then private message. The check if this is a legacy payload.",
            "end_lineno": "52",
            "file_content": "import json\nimport logging\nfrom base64 import urlsafe_b64decode\nfrom typing import Callable, Tuple, Union, Dict\nfrom urllib.parse import unquote\n\nfrom Crypto.PublicKey.RSA import RsaKey\nfrom lxml import etree\n\nfrom federation.entities.mixins import BaseEntity\nfrom federation.exceptions import EncryptedMessageError, NoSenderKeyFoundError\nfrom federation.protocols.diaspora.encrypted import EncryptedPayload\nfrom federation.protocols.diaspora.magic_envelope import MagicEnvelope\nfrom federation.types import UserType, RequestType\nfrom federation.utils.diaspora import fetch_public_key\nfrom federation.utils.text import decode_if_bytes, encode_if_text, validate_handle\n\nlogger = logging.getLogger(\"federation\")\n\nPROTOCOL_NAME = \"diaspora\"\nPROTOCOL_NS = \"https://joindiaspora.com/protocol\"\nMAGIC_ENV_TAG = \"{http://salmon-protocol.org/ns/magic-env}env\"\n\n\ndef identify_id(id: str) -> bool:\n    \"\"\"\n    Try to identify if this ID is a Diaspora ID.\n    \"\"\"\n    return validate_handle(id)\n\n\n# noinspection PyBroadException\ndef identify_request(request: RequestType):\n    \"\"\"Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n    # Private encrypted JSON payload\n    try:\n        data = json.loads(decode_if_bytes(request.body))\n        if \"encrypted_magic_envelope\" in data:\n            return True\n    except Exception:\n        pass\n    # Public XML payload\n    try:\n        xml = etree.fromstring(encode_if_text(request.body))\n        if xml.tag == MAGIC_ENV_TAG:\n            return True\n    except Exception:\n        pass\n    return False\n\n\nclass Protocol:\n    \"\"\"Diaspora protocol parts\n\n    Original legacy implementation mostly taken from Pyaspora (https://github.com/lukeross/pyaspora).\n    \"\"\"\n    content = None\n    doc = None\n    get_contact_key = None\n    user = None\n    sender_handle = None\n\n    def get_json_payload_magic_envelope(self, payload):\n        \"\"\"Encrypted JSON payload\"\"\"\n        private_key = self._get_user_key()\n        return EncryptedPayload.decrypt(payload=payload, private_key=private_key)\n\n    def store_magic_envelope_doc(self, payload):\n        \"\"\"Get the Magic Envelope, trying JSON first.\"\"\"\n        try:\n            json_payload = json.loads(decode_if_bytes(payload))\n        except ValueError:\n            # XML payload\n            xml = unquote(decode_if_bytes(payload))\n            xml = xml.lstrip().encode(\"utf-8\")\n            logger.debug(\"diaspora.protocol.store_magic_envelope_doc: xml payload: %s\", xml)\n            self.doc = etree.fromstring(xml)\n        else:\n            logger.debug(\"diaspora.protocol.store_magic_envelope_doc: json payload: %s\", json_payload)\n            self.doc = self.get_json_payload_magic_envelope(json_payload)\n\n    def receive(\n            self,\n            request: RequestType,\n            user: UserType = None,\n            sender_key_fetcher: Callable[[str], str] = None,\n            skip_author_verification: bool = False) -> Tuple[str, str]:\n        \"\"\"Receive a payload.\n\n        For testing purposes, `skip_author_verification` can be passed. Authorship will not be verified.\"\"\"\n        self.user = user\n        self.get_contact_key = sender_key_fetcher\n        self.store_magic_envelope_doc(request.body)\n        # Open payload and get actual message\n        self.content = self.get_message_content()\n        # Get sender handle\n        self.sender_handle = self.get_sender()\n        # Verify the message is from who it claims to be\n        if not skip_author_verification:\n            self.verify_signature()\n        return self.sender_handle, self.content\n\n    def _get_user_key(self):\n        if not getattr(self.user, \"private_key\", None):\n            raise EncryptedMessageError(\"Cannot decrypt private message without user key\")\n        return self.user.rsa_private_key\n\n    def get_sender(self):\n        return MagicEnvelope.get_sender(self.doc)\n\n    def get_message_content(self):\n        \"\"\"\n        Given the Slap XML, extract out the payload.\n        \"\"\"\n        body = self.doc.find(\n            \".//{http://salmon-protocol.org/ns/magic-env}data\").text\n\n        body = urlsafe_b64decode(body.encode(\"ascii\"))\n\n        logger.debug(\"diaspora.protocol.get_message_content: %s\", body)\n        return body\n\n    def verify_signature(self):\n        \"\"\"\n        Verify the signed XML elements to have confidence that the claimed\n        author did actually generate this message.\n        \"\"\"\n        if self.get_contact_key:\n            sender_key = self.get_contact_key(self.sender_handle)\n        else:\n            sender_key = fetch_public_key(self.sender_handle)\n        if not sender_key:\n            raise NoSenderKeyFoundError(\"Could not find a sender contact to retrieve key\")\n        MagicEnvelope(doc=self.doc, public_key=sender_key, verify=True)\n\n    def build_send(self, entity: BaseEntity, from_user: UserType, to_user_key: RsaKey = None) -> Union[str, Dict]:\n        \"\"\"\n        Build POST data for sending out to remotes.\n\n        :param entity: The outbound ready entity for this protocol.\n        :param from_user: The user sending this payload. Must have ``private_key`` and ``id`` properties.\n        :param to_user_key: (Optional) Public key of user we're sending a private payload to.\n        :returns: dict or string depending on if private or public payload.\n        \"\"\"\n        if entity.outbound_doc is not None:\n            # Use pregenerated outbound document\n            xml = entity.outbound_doc\n        else:\n            xml = entity.to_xml()\n        me = MagicEnvelope(etree.tostring(xml), private_key=from_user.rsa_private_key, author_handle=from_user.handle)\n        rendered = me.render()\n        if to_user_key:\n            return EncryptedPayload.encrypt(rendered, to_user_key)\n        return rendered\n",
            "file_path": "federation/protocols/diaspora/protocol.py",
            "human_label": "Check whether the request body loaded using JSON contains events. If yes, True is returned, otherwise, check whether the tag of the XML loaded from the request body is Magic_ENV_TAG, if yes, return True. If neither of the preceding conditions is met, return False.",
            "level": "file_runnable",
            "lineno": "33",
            "name": "identify_request",
            "oracle_context": "{ \"apis\" : \"['decode_if_bytes', 'loads', 'fromstring', 'encode_if_text']\", \"classes\" : \"['encode_if_text', 'json', 'etree', 'RequestType', 'decode_if_bytes']\", \"vars\" : \"['body', 'tag', 'MAGIC_ENV_TAG']\" }",
            "package": "protocol",
            "project": "jaywink/federation",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "6306292152e177c0ba469f0d",
            "all_context": "{ \"import\" : \"typing re logging json typing federation logging \", \"file\" : \"\", \"class\" : \"\" }",
            "code": "def identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # noinspection PyBroadException\n    try:\n        data = json.loads(decode_if_bytes(request.body))\n        if \"events\" in data:\n            return True\n    except Exception:\n        pass\n    return False\n",
            "dependency": "",
            "docstring": "Try to identify whether this is a Matrix request",
            "end_lineno": "35",
            "file_content": "import json\nimport logging\nimport re\nfrom typing import Callable, Tuple, List, Dict\n\nfrom federation.entities.matrix.entities import MatrixEntityMixin\nfrom federation.types import UserType, RequestType\nfrom federation.utils.text import decode_if_bytes\n\nlogger = logging.getLogger('federation')\n\nPROTOCOL_NAME = \"activitypub\"\n\n\ndef identify_id(identifier: str) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix identifier.\n\n    TODO fix, not entirely correct..\n    \"\"\"\n    return re.match(r'^[@#!].*:.*$', identifier, flags=re.IGNORECASE) is not None\n\n\ndef identify_request(request: RequestType) -> bool:\n    \"\"\"\n    Try to identify whether this is a Matrix request\n    \"\"\"\n    # noinspection PyBroadException\n    try:\n        data = json.loads(decode_if_bytes(request.body))\n        if \"events\" in data:\n            return True\n    except Exception:\n        pass\n    return False\n\n\nclass Protocol:\n    actor = None\n    get_contact_key = None\n    payload = None\n    request = None\n    user = None\n\n    # noinspection PyUnusedLocal\n    @staticmethod\n    def build_send(entity: MatrixEntityMixin, *args, **kwargs) -> List[Dict]:\n        \"\"\"\n        Build POST data for sending out to the homeserver.\n\n        :param entity: The outbound ready entity for this protocol.\n        :returns: list of payloads\n        \"\"\"\n        return entity.payloads()\n\n    def extract_actor(self):\n        # TODO TBD\n        pass\n\n    def receive(\n            self,\n            request: RequestType,\n            user: UserType = None,\n            sender_key_fetcher: Callable[[str], str] = None,\n            skip_author_verification: bool = False) -> Tuple[str, dict]:\n        \"\"\"\n        Receive a request.\n\n        Matrix appservices will deliver 1+ events at a time.\n        \"\"\"\n        # TODO TBD\n        return self.actor, self.payload\n",
            "file_path": "federation/protocols/matrix/protocol.py",
            "human_label": "Check whether the request body loaded using JSON contains events. If yes, True is returned. Otherwise, False is returned.",
            "level": "plib_runnable",
            "lineno": "24",
            "name": "identify_request",
            "oracle_context": "{ \"apis\" : \"['decode_if_bytes', 'loads']\", \"classes\" : \"['decode_if_bytes', 'RequestType', 'json']\", \"vars\" : \"['body']\" }",
            "package": "protocol",
            "project": "jaywink/federation",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "6306292252e177c0ba469f11",
            "all_context": "{ \"import\" : \"\", \"file\" : \"\", \"class\" : \"\" }",
            "code": "def format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    return ensure_timezone(dt).astimezone(tzutc()).strftime(\n        '%Y-%m-%dT%H:%M:%SZ'\n    )\n",
            "dependency": "",
            "docstring": "Format a datetime in the way that D* nodes expect.",
            "end_lineno": "22",
            "file_content": "from dateutil.tz import tzlocal, tzutc\nfrom lxml import etree\n\n\ndef ensure_timezone(dt, tz=None):\n    \"\"\"\n    Make sure the datetime <dt> has a timezone set, using timezone <tz> if it\n    doesn't. <tz> defaults to the local timezone.\n    \"\"\"\n    if dt.tzinfo is None:\n        return dt.replace(tzinfo=tz or tzlocal())\n    else:\n        return dt\n\n\ndef format_dt(dt):\n    \"\"\"\n    Format a datetime in the way that D* nodes expect.\n    \"\"\"\n    return ensure_timezone(dt).astimezone(tzutc()).strftime(\n        '%Y-%m-%dT%H:%M:%SZ'\n    )\n\n\ndef struct_to_xml(node, struct):\n    \"\"\"\n    Turn a list of dicts into XML nodes with tag names taken from the dict\n    keys and element text taken from dict values. This is a list of dicts\n    so that the XML nodes can be ordered in the XML output.\n    \"\"\"\n    for obj in struct:\n        for k, v in obj.items():\n            etree.SubElement(node, k).text = v\n\n\ndef get_full_xml_representation(entity, private_key):\n    \"\"\"Get full XML representation of an entity.\n\n    This contains the <XML><post>..</post></XML> wrapper.\n\n    Accepts either a Base entity or a Diaspora entity.\n\n    Author `private_key` must be given so that certain entities can be signed.\n    \"\"\"\n    from federation.entities.diaspora.mappers import get_outbound_entity\n    diaspora_entity = get_outbound_entity(entity, private_key)\n    xml = diaspora_entity.to_xml()\n    return \"<XML><post>%s</post></XML>\" % etree.tostring(xml).decode(\"utf-8\")\n\n\ndef add_element_to_doc(doc, tag, value):\n    \"\"\"Set text value of an etree.Element of tag, appending a new element with given tag if it doesn't exist.\"\"\"\n    element = doc.find(\".//%s\" % tag)\n    if element is None:\n        element = etree.SubElement(doc, tag)\n    element.text = value\n",
            "file_path": "federation/entities/diaspora/utils.py",
            "human_label": "Use the ensure_timezone function to format the time of dt and return the time.",
            "level": "project_runnable",
            "lineno": "16",
            "name": "format_dt",
            "oracle_context": "{ \"apis\" : \"['astimezone', 'strftime', 'ensure_timezone', 'tzutc']\", \"classes\" : \"['tzutc']\", \"vars\" : \"[]\" }",
            "package": "utils",
            "project": "jaywink/federation",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "6306292352e177c0ba469f1d",
            "all_context": "{ \"import\" : \"typing urllib re typing bleach \", \"file\" : \"ILLEGAL_TAG_CHARS ; decode_if_bytes(text) ; encode_if_text(text) ; find_tags(text,replacer) ; get_path_from_url(url) ; process_text_links(text) ; test_tag(tag) ; validate_handle(handle) ; with_slash(url) ; \", \"class\" : \"\" }",
            "code": "def find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    found_tags = set()\n    # <br> and <p> tags cause issues in us finding words - add some spacing around them\n    new_text = text.replace(\"<br>\", \" <br> \").replace(\"<p>\", \" <p> \").replace(\"</p>\", \" </p> \")\n    lines = new_text.splitlines(keepends=True)\n    final_lines = []\n    code_block = False\n    final_text = None\n    # Check each line separately\n    for line in lines:\n        final_words = []\n        if line[0:3] == \"```\":\n            code_block = not code_block\n        if line.find(\"#\") == -1 or line[0:4] == \"    \" or code_block:\n            # Just add the whole line\n            final_lines.append(line)\n            continue\n        # Check each word separately\n        words = line.split(\" \")\n        for word in words:\n            if word.find('#') > -1:\n                candidate = word.strip().strip(\"([]),.!?:*_%/\")\n                if candidate.find('<') > -1 or candidate.find('>') > -1:\n                    # Strip html\n                    candidate = bleach.clean(word, strip=True)\n                # Now split with slashes\n                candidates = candidate.split(\"/\")\n                to_replace = []\n                for candidate in candidates:\n                    if candidate.startswith(\"#\"):\n                        candidate = candidate.strip(\"#\")\n                        if test_tag(candidate.lower()):\n                            found_tags.add(candidate.lower())\n                            to_replace.append(candidate)\n                if replacer:\n                    tag_word = word\n                    try:\n                        for counter, replacee in enumerate(to_replace, 1):\n                            tag_word = tag_word.replace(\"#%s\" % replacee, replacer(replacee))\n                    except Exception:\n                        pass\n                    final_words.append(tag_word)\n                else:\n                    final_words.append(word)\n            else:\n                final_words.append(word)\n        final_lines.append(\" \".join(final_words))\n    if replacer:\n        final_text = \"\".join(final_lines)\n    if final_text:\n        final_text = final_text.replace(\" <br> \", \"<br>\").replace(\" <p> \", \"<p>\").replace(\" </p> \", \"</p>\")\n    return found_tags, final_text or text\n",
            "dependency": "",
            "docstring": "Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.",
            "end_lineno": "85",
            "file_content": "import re\nfrom typing import Set, Tuple\nfrom urllib.parse import urlparse\n\nimport bleach\nfrom bleach import callbacks\n\nILLEGAL_TAG_CHARS = \"!#$%^&*+.,@£/()=?`'\\\\{[]}~;:\\\"’”—\\xa0\"\n\n\ndef decode_if_bytes(text):\n    try:\n        return text.decode(\"utf-8\")\n    except AttributeError:\n        return text\n\n\ndef encode_if_text(text):\n    try:\n        return bytes(text, encoding=\"utf-8\")\n    except TypeError:\n        return text\n\n\ndef find_tags(text: str, replacer: callable = None) -> Tuple[Set, str]:\n    \"\"\"Find tags in text.\n\n    Tries to ignore tags inside code blocks.\n\n    Optionally, if passed a \"replacer\", will also replace the tag word with the result\n    of the replacer function called with the tag word.\n\n    Returns a set of tags and the original or replaced text.\n    \"\"\"\n    found_tags = set()\n    # <br> and <p> tags cause issues in us finding words - add some spacing around them\n    new_text = text.replace(\"<br>\", \" <br> \").replace(\"<p>\", \" <p> \").replace(\"</p>\", \" </p> \")\n    lines = new_text.splitlines(keepends=True)\n    final_lines = []\n    code_block = False\n    final_text = None\n    # Check each line separately\n    for line in lines:\n        final_words = []\n        if line[0:3] == \"```\":\n            code_block = not code_block\n        if line.find(\"#\") == -1 or line[0:4] == \"    \" or code_block:\n            # Just add the whole line\n            final_lines.append(line)\n            continue\n        # Check each word separately\n        words = line.split(\" \")\n        for word in words:\n            if word.find('#') > -1:\n                candidate = word.strip().strip(\"([]),.!?:*_%/\")\n                if candidate.find('<') > -1 or candidate.find('>') > -1:\n                    # Strip html\n                    candidate = bleach.clean(word, strip=True)\n                # Now split with slashes\n                candidates = candidate.split(\"/\")\n                to_replace = []\n                for candidate in candidates:\n                    if candidate.startswith(\"#\"):\n                        candidate = candidate.strip(\"#\")\n                        if test_tag(candidate.lower()):\n                            found_tags.add(candidate.lower())\n                            to_replace.append(candidate)\n                if replacer:\n                    tag_word = word\n                    try:\n                        for counter, replacee in enumerate(to_replace, 1):\n                            tag_word = tag_word.replace(\"#%s\" % replacee, replacer(replacee))\n                    except Exception:\n                        pass\n                    final_words.append(tag_word)\n                else:\n                    final_words.append(word)\n            else:\n                final_words.append(word)\n        final_lines.append(\" \".join(final_words))\n    if replacer:\n        final_text = \"\".join(final_lines)\n    if final_text:\n        final_text = final_text.replace(\" <br> \", \"<br>\").replace(\" <p> \", \"<p>\").replace(\" </p> \", \"</p>\")\n    return found_tags, final_text or text\n\n\ndef get_path_from_url(url: str) -> str:\n    \"\"\"\n    Return only the path part of an URL.\n    \"\"\"\n    parsed = urlparse(url)\n    return parsed.path\n\n\ndef process_text_links(text):\n    \"\"\"Process links in text, adding some attributes and linkifying textual links.\"\"\"\n    link_callbacks = [callbacks.nofollow, callbacks.target_blank]\n\n    def link_attributes(attrs, new=False):\n        \"\"\"Run standard callbacks except for internal links.\"\"\"\n        href_key = (None, \"href\")\n        if attrs.get(href_key).startswith(\"/\"):\n            return attrs\n\n        # Run the standard callbacks\n        for callback in link_callbacks:\n            attrs = callback(attrs, new)\n        return attrs\n\n    return bleach.linkify(\n        text,\n        callbacks=[link_attributes],\n        parse_email=False,\n        skip_tags=[\"code\"],\n    )\n\n\ndef test_tag(tag: str) -> bool:\n    \"\"\"Test a word whether it could be accepted as a tag.\"\"\"\n    if not tag:\n        return False\n    for char in ILLEGAL_TAG_CHARS:\n        if char in tag:\n            return False\n    return True\n\n\ndef validate_handle(handle):\n    \"\"\"\n    Very basic handle validation as per\n    https://diaspora.github.io/diaspora_federation/federation/types.html#diaspora-id\n    \"\"\"\n    return re.match(r\"[a-z0-9\\-_.]+@[^@/]+\\.[^@/]+\", handle, flags=re.IGNORECASE) is not None\n\n\ndef with_slash(url):\n    if url.endswith('/'):\n        return url\n    return f\"{url}/\"\n",
            "file_path": "federation/utils/text.py",
            "human_label": "Find tags in text.\n\nTries to ignore tags inside code blocks.\n\nOptionally, if passed a \"replacer\", will also replace the tag word with the result\nof the replacer function called with the tag word.\n\nReturns a set of tags and the original or replaced text.",
            "level": "file_runnable",
            "lineno": "25",
            "name": "find_tags",
            "oracle_context": "{ \"apis\" : \"['set', 'splitlines', 'find', 'append', 'join', 'test_tag', 'add', 'startswith', 'strip', 'split', 'lower', 'enumerate', 'clean', 'replace', 'replacer']\", \"classes\" : \"['Tuple', 'bleach', 'Set']\", \"vars\" : \"['Str']\" }",
            "package": "text",
            "project": "jaywink/federation",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "6306292652e177c0ba469f34",
            "all_context": "{ \"import\" : \"typing re logging datetime uuid calendar urllib socket typing requests federation logging datetime uuid \", \"file\" : \"logger ; USER_AGENT ; fetch_content_type(url) ; fetch_document(url,host,path,timeout,raise_ssl_errors,extra_headers) ; fetch_host_ip(host) ; fetch_file(url,timeout,extra_headers) ; parse_http_date(date) ; send_document(url,data,timeout,method) ; try_retrieve_webfinger_document(handle) ; \", \"class\" : \"\" }",
            "code": "def fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, headers={'user-agent': USER_AGENT}, timeout=10)\n    except RequestException as ex:\n        logger.warning(\"fetch_content_type - %s when fetching url %s\", ex, url)\n    else:\n        return response.headers.get('Content-Type')\n",
            "dependency": "",
            "docstring": "Fetch the HEAD of the remote url to determine the content type.",
            "end_lineno": "31",
            "file_content": "import calendar\nimport datetime\nimport logging\nimport re\nimport socket\nfrom typing import Optional, Dict\nfrom urllib.parse import quote\nfrom uuid import uuid4\n\nimport requests\nfrom requests.exceptions import RequestException, HTTPError, SSLError\nfrom requests.exceptions import ConnectionError\nfrom requests.structures import CaseInsensitiveDict\n\nfrom federation import __version__\n\nlogger = logging.getLogger(\"federation\")\n\nUSER_AGENT = \"python/federation/%s\" % __version__\n\n\ndef fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, headers={'user-agent': USER_AGENT}, timeout=10)\n    except RequestException as ex:\n        logger.warning(\"fetch_content_type - %s when fetching url %s\", ex, url)\n    else:\n        return response.headers.get('Content-Type')\n\n\ndef fetch_document(url=None, host=None, path=\"/\", timeout=10, raise_ssl_errors=True, extra_headers=None, **kwargs):\n    \"\"\"Helper method to fetch remote document.\n\n    Must be given either the ``url`` or ``host``.\n    If ``url`` is given, only that will be tried without falling back to http from https.\n    If ``host`` given, `path` will be added to it. Will fall back to http on non-success status code.\n\n    :arg url: Full url to fetch, including protocol\n    :arg host: Domain part only without path or protocol\n    :arg path: Path without domain (defaults to \"/\")\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg raise_ssl_errors: Pass False if you want to try HTTP even for sites with SSL errors (default True)\n    :arg extra_headers: Optional extra headers dictionary to add to requests\n    :arg kwargs holds extra args passed to requests.get\n    :returns: Tuple of document (str or None), status code (int or None) and error (an exception class instance or None)\n    :raises ValueError: If neither url nor host are given as parameters\n    \"\"\"\n    if not url and not host:\n        raise ValueError(\"Need url or host.\")\n\n    logger.debug(\"fetch_document: url=%s, host=%s, path=%s, timeout=%s, raise_ssl_errors=%s\",\n                 url, host, path, timeout, raise_ssl_errors)\n    headers = {'user-agent': USER_AGENT}\n    if extra_headers:\n        headers.update(extra_headers)\n    if url:\n        # Use url since it was given\n        logger.debug(\"fetch_document: trying %s\", url)\n        try:\n            response = requests.get(url, timeout=timeout, headers=headers, **kwargs)\n            logger.debug(\"fetch_document: found document, code %s\", response.status_code)\n            response.raise_for_status()\n            return response.text, response.status_code, None\n        except RequestException as ex:\n            logger.debug(\"fetch_document: exception %s\", ex)\n            return None, None, ex\n    # Build url with some little sanitizing\n    host_string = host.replace(\"http://\", \"\").replace(\"https://\", \"\").strip(\"/\")\n    path_string = path if path.startswith(\"/\") else \"/%s\" % path\n    url = \"https://%s%s\" % (host_string, path_string)\n    logger.debug(\"fetch_document: trying %s\", url)\n    try:\n        response = requests.get(url, timeout=timeout, headers=headers)\n        logger.debug(\"fetch_document: found document, code %s\", response.status_code)\n        response.raise_for_status()\n        return response.text, response.status_code, None\n    except (HTTPError, SSLError, ConnectionError) as ex:\n        if isinstance(ex, SSLError) and raise_ssl_errors:\n            logger.debug(\"fetch_document: exception %s\", ex)\n            return None, None, ex\n        # Try http then\n        url = url.replace(\"https://\", \"http://\")\n        logger.debug(\"fetch_document: trying %s\", url)\n        try:\n            response = requests.get(url, timeout=timeout, headers=headers)\n            logger.debug(\"fetch_document: found document, code %s\", response.status_code)\n            response.raise_for_status()\n            return response.text, response.status_code, None\n        except RequestException as ex:\n            logger.debug(\"fetch_document: exception %s\", ex)\n            return None, None, ex\n    except RequestException as ex:\n        logger.debug(\"fetch_document: exception %s\", ex)\n        return None, None, ex\n\n\ndef fetch_host_ip(host: str) -> str:\n    \"\"\"\n    Fetch ip by host\n    \"\"\"\n    try:\n        ip = socket.gethostbyname(host)\n    except socket.gaierror:\n        return ''\n\n    return ip\n\n\ndef fetch_file(url: str, timeout: int = 30, extra_headers: Dict = None) -> str:\n    \"\"\"\n    Download a file with a temporary name and return the name.\n    \"\"\"\n    headers = {'user-agent': USER_AGENT}\n    if extra_headers:\n        headers.update(extra_headers)\n    response = requests.get(url, timeout=timeout, headers=headers, stream=True)\n    response.raise_for_status()\n    name = f\"/tmp/{str(uuid4())}\"\n    with open(name, \"wb\") as f:\n        for chunk in response.iter_content(chunk_size=8192):\n            f.write(chunk)\n    return name\n\n\ndef parse_http_date(date):\n    \"\"\"\n    Parse a date format as specified by HTTP RFC7231 section 7.1.1.1.\n\n    The three formats allowed by the RFC are accepted, even if only the first\n    one is still in widespread use.\n\n    Return an integer expressed in seconds since the epoch, in UTC.\n\n    Implementation copied from Django.\n    https://github.com/django/django/blob/master/django/utils/http.py#L157\n    License: BSD 3-clause\n    \"\"\"\n    MONTHS = 'jan feb mar apr may jun jul aug sep oct nov dec'.split()\n    __D = r'(?P<day>\\d{2})'\n    __D2 = r'(?P<day>[ \\d]\\d)'\n    __M = r'(?P<mon>\\w{3})'\n    __Y = r'(?P<year>\\d{4})'\n    __Y2 = r'(?P<year>\\d{2})'\n    __T = r'(?P<hour>\\d{2}):(?P<min>\\d{2}):(?P<sec>\\d{2})'\n    RFC1123_DATE = re.compile(r'^\\w{3}, %s %s %s %s GMT$' % (__D, __M, __Y, __T))\n    RFC850_DATE = re.compile(r'^\\w{6,9}, %s-%s-%s %s GMT$' % (__D, __M, __Y2, __T))\n    ASCTIME_DATE = re.compile(r'^\\w{3} %s %s %s %s$' % (__M, __D2, __T, __Y))\n    # email.utils.parsedate() does the job for RFC1123 dates; unfortunately\n    # RFC7231 makes it mandatory to support RFC850 dates too. So we roll\n    # our own RFC-compliant parsing.\n    for regex in RFC1123_DATE, RFC850_DATE, ASCTIME_DATE:\n        m = regex.match(date)\n        if m is not None:\n            break\n    else:\n        raise ValueError(\"%r is not in a valid HTTP date format\" % date)\n    try:\n        year = int(m.group('year'))\n        if year < 100:\n            if year < 70:\n                year += 2000\n            else:\n                year += 1900\n        month = MONTHS.index(m.group('mon').lower()) + 1\n        day = int(m.group('day'))\n        hour = int(m.group('hour'))\n        min = int(m.group('min'))\n        sec = int(m.group('sec'))\n        result = datetime.datetime(year, month, day, hour, min, sec)\n        return calendar.timegm(result.utctimetuple())\n    except Exception as exc:\n        raise ValueError(\"%r is not a valid date\" % date) from exc\n\n\ndef send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    logger.debug(\"send_document: url=%s, data=%s, timeout=%s, method=%s\", url, data, timeout, method)\n    if not method:\n        method = \"post\"\n    headers = CaseInsensitiveDict({\n        'User-Agent': USER_AGENT,\n    })\n    if \"headers\" in kwargs:\n        # Update from kwargs\n        headers.update(kwargs.get(\"headers\"))\n    kwargs.update({\n        \"data\": data, \"timeout\": timeout, \"headers\": headers\n    })\n    request_func = getattr(requests, method)\n    try:\n        response = request_func(url, *args, **kwargs)\n        logger.debug(\"send_document: response status code %s\", response.status_code)\n        return response.status_code, None\n    # TODO support rate limit 429 code\n    except RequestException as ex:\n        logger.debug(\"send_document: exception %s\", ex)\n        return None, ex\n\n\ndef try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        host = handle.split(\"@\")[1]\n    except AttributeError:\n        logger.warning(\"retrieve_webfinger_document: invalid handle given: %s\", handle)\n        return None\n    document, code, exception = fetch_document(\n        host=host, path=\"/.well-known/webfinger?resource=acct:%s\" % quote(handle),\n    )\n    if exception:\n        logger.debug(\"retrieve_webfinger_document: failed to fetch webfinger document: %s, %s\", code, exception)\n    return document\n",
            "file_path": "federation/utils/network.py",
            "human_label": "Set the head of the request through the URL and USER_AGENT.",
            "level": "file_runnable",
            "lineno": "22",
            "name": "fetch_content_type",
            "oracle_context": "{ \"apis\" : \"['warning', 'get', 'head']\", \"classes\" : \"['Optional', 'RequestException', 'requests']\", \"vars\" : \"['logger', 'headers', 'USER_AGENT']\" }",
            "package": "network",
            "project": "jaywink/federation",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "630629d052e177c0ba46a0a1",
            "all_context": "{ \"import\" : \"\", \"file\" : \"\", \"class\" : \"\" }",
            "code": "def verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    sig_hash = _create_signature_hash(doc)\n    cipher = PKCS1_v1_5.new(RSA.importKey(public_key))\n    return cipher.verify(sig_hash, b64decode(signature))\n",
            "dependency": "",
            "docstring": "Verify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.",
            "end_lineno": "37",
            "file_content": "from base64 import b64decode, b64encode\n\nfrom Crypto.Hash import SHA256\nfrom Crypto.PublicKey import RSA\nfrom Crypto.PublicKey.RSA import RsaKey\nfrom Crypto.Signature import PKCS1_v1_5\n\n\ndef get_element_child_info(doc, attr):\n    \"\"\"Get information from child elements of this elementas a list since order is important.\n\n    Don't include signature tags.\n\n    :param doc: XML element\n    :param attr: Attribute to get from the elements, for example \"tag\" or \"text\".\n    \"\"\"\n    props = []\n    for child in doc:\n        if child.tag not in [\"author_signature\", \"parent_author_signature\"]:\n            props.append(getattr(child, attr))\n    return props\n\n\ndef _create_signature_hash(doc):\n    props = get_element_child_info(doc, \"text\")\n    content = \";\".join(props)\n    return SHA256.new(content.encode(\"utf-8\"))\n\n\ndef verify_relayable_signature(public_key, doc, signature):\n    \"\"\"\n    Verify the signed XML elements to have confidence that the claimed\n    author did actually generate this message.\n    \"\"\"\n    sig_hash = _create_signature_hash(doc)\n    cipher = PKCS1_v1_5.new(RSA.importKey(public_key))\n    return cipher.verify(sig_hash, b64decode(signature))\n\n\ndef create_relayable_signature(private_key: RsaKey, doc):\n    sig_hash = _create_signature_hash(doc)\n    cipher = PKCS1_v1_5.new(private_key)\n    return b64encode(cipher.sign(sig_hash)).decode(\"ascii\")\n",
            "file_path": "federation/protocols/diaspora/signatures.py",
            "human_label": "Verify the signed XML elements to have confidence that the claimed\nauthor did actually generate this message.",
            "level": "project_runnable",
            "lineno": "30",
            "name": "verify_relayable_signature",
            "oracle_context": "{ \"apis\" : \"['new', 'b64decode', '_create_signature_hash', 'verify', 'importKey']\", \"classes\" : \"['RSA', 'b64decode', 'PKCS1_v1_5']\", \"vars\" : \"[]\" }",
            "package": "signatures",
            "project": "jaywink/federation",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "630629e052e177c0ba46a0c4",
            "all_context": "{ \"import\" : \"typing logging xml json urllib typing xrd federation logging lxml \", \"file\" : \"logger ; fetch_public_key(handle) ; parse_diaspora_webfinger(document) ; retrieve_diaspora_hcard(handle) ; retrieve_and_parse_diaspora_webfinger(handle) ; retrieve_diaspora_host_meta(host) ; _get_element_text_or_none(document,selector) ; _get_element_attr_or_none(document,selector,attribute) ; parse_profile_from_hcard(hcard,handle) ; retrieve_and_parse_content(id,guid,handle,entity_type,sender_key_fetcher) ; retrieve_and_parse_profile(handle) ; get_fetch_content_endpoint(domain,entity_type,guid) ; get_public_endpoint(id) ; get_private_endpoint(id,guid) ; \", \"class\" : \"\" }",
            "code": "def parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    webfinger = {\n        \"hcard_url\": None,\n    }\n    # noinspection PyBroadException\n    try:\n        doc = json.loads(document)\n        for link in doc[\"links\"]:\n            if link[\"rel\"] == \"http://microformats.org/profile/hcard\":\n                webfinger[\"hcard_url\"] = link[\"href\"]\n                break\n        else:\n            logger.warning(\"parse_diaspora_webfinger: found JSON webfinger but it has no hcard href\")\n            raise ValueError\n    except Exception:\n        try:\n            xrd = XRD.parse_xrd(document)\n            webfinger[\"hcard_url\"] = xrd.find_link(rels=\"http://microformats.org/profile/hcard\").href\n        except (xml.parsers.expat.ExpatError, TypeError):\n            logger.warning(\"parse_diaspora_webfinger: found XML webfinger but it fails to parse\")\n            pass\n    return webfinger\n",
            "dependency": "",
            "docstring": "Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\nhttps://diaspora.github.io/diaspora_federation/discovery/webfinger.html",
            "end_lineno": "54",
            "file_content": "import json\nimport logging\nimport xml\nfrom typing import Callable, Dict\nfrom urllib.parse import quote\n\nfrom lxml import html\nfrom xrd import XRD\n\nfrom federation.inbound import handle_receive\nfrom federation.types import RequestType\nfrom federation.utils.network import fetch_document, try_retrieve_webfinger_document\nfrom federation.utils.text import validate_handle\n\nlogger = logging.getLogger(\"federation\")\n\n\ndef fetch_public_key(handle):\n    \"\"\"Fetch public key over the network.\n\n    :param handle: Remote handle to retrieve public key for.\n    :return: Public key in str format from parsed profile.\n    \"\"\"\n    profile = retrieve_and_parse_profile(handle)\n    return profile.public_key\n\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    webfinger = {\n        \"hcard_url\": None,\n    }\n    # noinspection PyBroadException\n    try:\n        doc = json.loads(document)\n        for link in doc[\"links\"]:\n            if link[\"rel\"] == \"http://microformats.org/profile/hcard\":\n                webfinger[\"hcard_url\"] = link[\"href\"]\n                break\n        else:\n            logger.warning(\"parse_diaspora_webfinger: found JSON webfinger but it has no hcard href\")\n            raise ValueError\n    except Exception:\n        try:\n            xrd = XRD.parse_xrd(document)\n            webfinger[\"hcard_url\"] = xrd.find_link(rels=\"http://microformats.org/profile/hcard\").href\n        except (xml.parsers.expat.ExpatError, TypeError):\n            logger.warning(\"parse_diaspora_webfinger: found XML webfinger but it fails to parse\")\n            pass\n    return webfinger\n\n\ndef retrieve_diaspora_hcard(handle):\n    \"\"\"\n    Retrieve a remote Diaspora hCard document.\n\n    :arg handle: Remote handle to retrieve\n    :return: str (HTML document)\n    \"\"\"\n    webfinger = retrieve_and_parse_diaspora_webfinger(handle)\n    document, code, exception = fetch_document(webfinger.get(\"hcard_url\"))\n    if exception:\n        return None\n    return document\n\n\ndef retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    document = try_retrieve_webfinger_document(handle)\n    if document:\n        return parse_diaspora_webfinger(document)\n    host = handle.split(\"@\")[1]\n    hostmeta = retrieve_diaspora_host_meta(host)\n    if not hostmeta:\n        return None\n    url = hostmeta.find_link(rels=\"lrdd\").template.replace(\"{uri}\", quote(handle))\n    document, code, exception = fetch_document(url)\n    if exception:\n        return None\n    return parse_diaspora_webfinger(document)\n\n\ndef retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: ``XRD`` instance\n    \"\"\"\n    document, code, exception = fetch_document(host=host, path=\"/.well-known/host-meta\")\n    if exception:\n        return None\n    xrd = XRD.parse_xrd(document)\n    return xrd\n\n\ndef _get_element_text_or_none(document, selector):\n    \"\"\"\n    Using a CSS selector, get the element and return the text, or None if no element.\n\n    :arg document: ``HTMLElement`` document\n    :arg selector: CSS selector\n    :returns: str or None\n    \"\"\"\n    element = document.cssselect(selector)\n    if element:\n        return element[0].text\n    return None\n\n\ndef _get_element_attr_or_none(document, selector, attribute):\n    \"\"\"\n    Using a CSS selector, get the element and return the given attribute value, or None if no element.\n\n    Args:\n        document (HTMLElement) - HTMLElement document\n        selector (str) - CSS selector\n        attribute (str) - The attribute to get from the element\n    \"\"\"\n    element = document.cssselect(selector)\n    if element:\n        return element[0].get(attribute)\n    return None\n\n\ndef parse_profile_from_hcard(hcard: str, handle: str):\n    \"\"\"\n    Parse all the fields we can from a hCard document to get a Profile.\n\n    :arg hcard: HTML hcard document (str)\n    :arg handle: User handle in username@domain.tld format\n    :returns: ``federation.entities.diaspora.entities.DiasporaProfile`` instance\n    \"\"\"\n    from federation.entities.diaspora.entities import DiasporaProfile  # Circulars\n    doc = html.fromstring(hcard)\n    profile = DiasporaProfile(\n        name=_get_element_text_or_none(doc, \".fn\"),\n        image_urls={\n            \"small\": _get_element_attr_or_none(doc, \".entity_photo_small .photo\", \"src\"),\n            \"medium\": _get_element_attr_or_none(doc, \".entity_photo_medium .photo\", \"src\"),\n            \"large\": _get_element_attr_or_none(doc, \".entity_photo .photo\", \"src\"),\n        },\n        public=True,\n        id=handle,\n        handle=handle,\n        guid=_get_element_text_or_none(doc, \".uid\"),\n        public_key=_get_element_text_or_none(doc, \".key\"),\n        username=handle.split('@')[0],\n        _source_protocol=\"diaspora\",\n    )\n    return profile\n\n\ndef retrieve_and_parse_content(\n        id: str, guid: str, handle: str, entity_type: str, sender_key_fetcher: Callable[[str], str]=None):\n    \"\"\"Retrieve remote content and return an Entity class instance.\n\n    This is basically the inverse of receiving an entity. Instead, we fetch it, then call \"handle_receive\".\n\n    :param sender_key_fetcher: Function to use to fetch sender public key. If not given, network will be used\n        to fetch the profile and the key. Function must take handle as only parameter and return a public key.\n    :returns: Entity object instance or ``None``\n    \"\"\"\n    if not validate_handle(handle):\n        return\n    _username, domain = handle.split(\"@\")\n    url = get_fetch_content_endpoint(domain, entity_type.lower(), guid)\n    document, status_code, error = fetch_document(url)\n    if status_code == 200:\n        request = RequestType(body=document)\n        _sender, _protocol, entities = handle_receive(request, sender_key_fetcher=sender_key_fetcher)\n        if len(entities) > 1:\n            logger.warning(\"retrieve_and_parse_content - more than one entity parsed from remote even though we\"\n                           \"expected only one! ID %s\", guid)\n        if entities:\n            return entities[0]\n        return\n    elif status_code == 404:\n        logger.warning(\"retrieve_and_parse_content - remote content %s not found\", guid)\n        return\n    if error:\n        raise error\n    raise Exception(\"retrieve_and_parse_content - unknown problem when fetching document: %s, %s, %s\" % (\n        document, status_code, error,\n    ))\n\n\ndef retrieve_and_parse_profile(handle):\n    \"\"\"\n    Retrieve the remote user and return a Profile object.\n\n    :arg handle: User handle in username@domain.tld format\n    :returns: ``federation.entities.Profile`` instance or None\n    \"\"\"\n    hcard = retrieve_diaspora_hcard(handle)\n    if not hcard:\n        return None\n    profile = parse_profile_from_hcard(hcard, handle)\n    try:\n        profile.validate()\n    except ValueError as ex:\n        logger.warning(\"retrieve_and_parse_profile - found profile %s but it didn't validate: %s\",\n                       profile, ex)\n        return None\n    return profile\n\n\ndef get_fetch_content_endpoint(domain, entity_type, guid):\n    \"\"\"Get remote fetch content endpoint.\n\n    See: https://diaspora.github.io/diaspora_federation/federation/fetching.html\n    \"\"\"\n    return \"https://%s/fetch/%s/%s\" % (domain, entity_type, guid)\n\n\ndef get_public_endpoint(id: str) -> str:\n    \"\"\"Get remote endpoint for delivering public payloads.\"\"\"\n    _username, domain = id.split(\"@\")\n    return \"https://%s/receive/public\" % domain\n\n\ndef get_private_endpoint(id: str, guid: str) -> str:\n    \"\"\"Get remote endpoint for delivering private payloads.\"\"\"\n    _username, domain = id.split(\"@\")\n    return \"https://%s/receive/users/%s\" % (domain, guid)\n",
            "file_path": "federation/utils/diaspora.py",
            "human_label": "The webfinger is obtained by reading the document in JSON format, the value of hcard_url in the webfinger is the value of href in links in the document.",
            "level": "file_runnable",
            "lineno": "28",
            "name": "parse_diaspora_webfinger",
            "oracle_context": "{ \"apis\" : \"['warning', 'loads', 'find_link', 'parse_xrd']\", \"classes\" : \"['XRD', 'json', 'Dict', 'xml']\", \"vars\" : \"['logger', 'parsers', 'href', 'ExpatError', 'expat']\" }",
            "package": "diaspora",
            "project": "jaywink/federation",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "630629e152e177c0ba46a0d1",
            "all_context": "{ \"import\" : \"typing re logging datetime uuid calendar urllib socket typing requests federation logging datetime uuid \", \"file\" : \"logger ; USER_AGENT ; fetch_content_type(url) ; fetch_document(url,host,path,timeout,raise_ssl_errors,extra_headers) ; fetch_host_ip(host) ; fetch_file(url,timeout,extra_headers) ; parse_http_date(date) ; send_document(url,data,timeout,method) ; try_retrieve_webfinger_document(handle) ; \", \"class\" : \"\" }",
            "code": "def try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        host = handle.split(\"@\")[1]\n    except AttributeError:\n        logger.warning(\"retrieve_webfinger_document: invalid handle given: %s\", handle)\n        return None\n    document, code, exception = fetch_document(\n        host=host, path=\"/.well-known/webfinger?resource=acct:%s\" % quote(handle),\n    )\n    if exception:\n        logger.debug(\"retrieve_webfinger_document: failed to fetch webfinger document: %s, %s\", code, exception)\n    return document\n",
            "dependency": "",
            "docstring": "Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.",
            "end_lineno": "226",
            "file_content": "import calendar\nimport datetime\nimport logging\nimport re\nimport socket\nfrom typing import Optional, Dict\nfrom urllib.parse import quote\nfrom uuid import uuid4\n\nimport requests\nfrom requests.exceptions import RequestException, HTTPError, SSLError\nfrom requests.exceptions import ConnectionError\nfrom requests.structures import CaseInsensitiveDict\n\nfrom federation import __version__\n\nlogger = logging.getLogger(\"federation\")\n\nUSER_AGENT = \"python/federation/%s\" % __version__\n\n\ndef fetch_content_type(url: str) -> Optional[str]:\n    \"\"\"\n    Fetch the HEAD of the remote url to determine the content type.\n    \"\"\"\n    try:\n        response = requests.head(url, headers={'user-agent': USER_AGENT}, timeout=10)\n    except RequestException as ex:\n        logger.warning(\"fetch_content_type - %s when fetching url %s\", ex, url)\n    else:\n        return response.headers.get('Content-Type')\n\n\ndef fetch_document(url=None, host=None, path=\"/\", timeout=10, raise_ssl_errors=True, extra_headers=None, **kwargs):\n    \"\"\"Helper method to fetch remote document.\n\n    Must be given either the ``url`` or ``host``.\n    If ``url`` is given, only that will be tried without falling back to http from https.\n    If ``host`` given, `path` will be added to it. Will fall back to http on non-success status code.\n\n    :arg url: Full url to fetch, including protocol\n    :arg host: Domain part only without path or protocol\n    :arg path: Path without domain (defaults to \"/\")\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg raise_ssl_errors: Pass False if you want to try HTTP even for sites with SSL errors (default True)\n    :arg extra_headers: Optional extra headers dictionary to add to requests\n    :arg kwargs holds extra args passed to requests.get\n    :returns: Tuple of document (str or None), status code (int or None) and error (an exception class instance or None)\n    :raises ValueError: If neither url nor host are given as parameters\n    \"\"\"\n    if not url and not host:\n        raise ValueError(\"Need url or host.\")\n\n    logger.debug(\"fetch_document: url=%s, host=%s, path=%s, timeout=%s, raise_ssl_errors=%s\",\n                 url, host, path, timeout, raise_ssl_errors)\n    headers = {'user-agent': USER_AGENT}\n    if extra_headers:\n        headers.update(extra_headers)\n    if url:\n        # Use url since it was given\n        logger.debug(\"fetch_document: trying %s\", url)\n        try:\n            response = requests.get(url, timeout=timeout, headers=headers, **kwargs)\n            logger.debug(\"fetch_document: found document, code %s\", response.status_code)\n            response.raise_for_status()\n            return response.text, response.status_code, None\n        except RequestException as ex:\n            logger.debug(\"fetch_document: exception %s\", ex)\n            return None, None, ex\n    # Build url with some little sanitizing\n    host_string = host.replace(\"http://\", \"\").replace(\"https://\", \"\").strip(\"/\")\n    path_string = path if path.startswith(\"/\") else \"/%s\" % path\n    url = \"https://%s%s\" % (host_string, path_string)\n    logger.debug(\"fetch_document: trying %s\", url)\n    try:\n        response = requests.get(url, timeout=timeout, headers=headers)\n        logger.debug(\"fetch_document: found document, code %s\", response.status_code)\n        response.raise_for_status()\n        return response.text, response.status_code, None\n    except (HTTPError, SSLError, ConnectionError) as ex:\n        if isinstance(ex, SSLError) and raise_ssl_errors:\n            logger.debug(\"fetch_document: exception %s\", ex)\n            return None, None, ex\n        # Try http then\n        url = url.replace(\"https://\", \"http://\")\n        logger.debug(\"fetch_document: trying %s\", url)\n        try:\n            response = requests.get(url, timeout=timeout, headers=headers)\n            logger.debug(\"fetch_document: found document, code %s\", response.status_code)\n            response.raise_for_status()\n            return response.text, response.status_code, None\n        except RequestException as ex:\n            logger.debug(\"fetch_document: exception %s\", ex)\n            return None, None, ex\n    except RequestException as ex:\n        logger.debug(\"fetch_document: exception %s\", ex)\n        return None, None, ex\n\n\ndef fetch_host_ip(host: str) -> str:\n    \"\"\"\n    Fetch ip by host\n    \"\"\"\n    try:\n        ip = socket.gethostbyname(host)\n    except socket.gaierror:\n        return ''\n\n    return ip\n\n\ndef fetch_file(url: str, timeout: int = 30, extra_headers: Dict = None) -> str:\n    \"\"\"\n    Download a file with a temporary name and return the name.\n    \"\"\"\n    headers = {'user-agent': USER_AGENT}\n    if extra_headers:\n        headers.update(extra_headers)\n    response = requests.get(url, timeout=timeout, headers=headers, stream=True)\n    response.raise_for_status()\n    name = f\"/tmp/{str(uuid4())}\"\n    with open(name, \"wb\") as f:\n        for chunk in response.iter_content(chunk_size=8192):\n            f.write(chunk)\n    return name\n\n\ndef parse_http_date(date):\n    \"\"\"\n    Parse a date format as specified by HTTP RFC7231 section 7.1.1.1.\n\n    The three formats allowed by the RFC are accepted, even if only the first\n    one is still in widespread use.\n\n    Return an integer expressed in seconds since the epoch, in UTC.\n\n    Implementation copied from Django.\n    https://github.com/django/django/blob/master/django/utils/http.py#L157\n    License: BSD 3-clause\n    \"\"\"\n    MONTHS = 'jan feb mar apr may jun jul aug sep oct nov dec'.split()\n    __D = r'(?P<day>\\d{2})'\n    __D2 = r'(?P<day>[ \\d]\\d)'\n    __M = r'(?P<mon>\\w{3})'\n    __Y = r'(?P<year>\\d{4})'\n    __Y2 = r'(?P<year>\\d{2})'\n    __T = r'(?P<hour>\\d{2}):(?P<min>\\d{2}):(?P<sec>\\d{2})'\n    RFC1123_DATE = re.compile(r'^\\w{3}, %s %s %s %s GMT$' % (__D, __M, __Y, __T))\n    RFC850_DATE = re.compile(r'^\\w{6,9}, %s-%s-%s %s GMT$' % (__D, __M, __Y2, __T))\n    ASCTIME_DATE = re.compile(r'^\\w{3} %s %s %s %s$' % (__M, __D2, __T, __Y))\n    # email.utils.parsedate() does the job for RFC1123 dates; unfortunately\n    # RFC7231 makes it mandatory to support RFC850 dates too. So we roll\n    # our own RFC-compliant parsing.\n    for regex in RFC1123_DATE, RFC850_DATE, ASCTIME_DATE:\n        m = regex.match(date)\n        if m is not None:\n            break\n    else:\n        raise ValueError(\"%r is not in a valid HTTP date format\" % date)\n    try:\n        year = int(m.group('year'))\n        if year < 100:\n            if year < 70:\n                year += 2000\n            else:\n                year += 1900\n        month = MONTHS.index(m.group('mon').lower()) + 1\n        day = int(m.group('day'))\n        hour = int(m.group('hour'))\n        min = int(m.group('min'))\n        sec = int(m.group('sec'))\n        result = datetime.datetime(year, month, day, hour, min, sec)\n        return calendar.timegm(result.utctimetuple())\n    except Exception as exc:\n        raise ValueError(\"%r is not a valid date\" % date) from exc\n\n\ndef send_document(url, data, timeout=10, method=\"post\", *args, **kwargs):\n    \"\"\"Helper method to send a document via POST.\n\n    Additional ``*args`` and ``**kwargs`` will be passed on to ``requests.post``.\n\n    :arg url: Full url to send to, including protocol\n    :arg data: Dictionary (will be form-encoded), bytes, or file-like object to send in the body\n    :arg timeout: Seconds to wait for response (defaults to 10)\n    :arg method: Method to use, defaults to post\n    :returns: Tuple of status code (int or None) and error (exception class instance or None)\n    \"\"\"\n    logger.debug(\"send_document: url=%s, data=%s, timeout=%s, method=%s\", url, data, timeout, method)\n    if not method:\n        method = \"post\"\n    headers = CaseInsensitiveDict({\n        'User-Agent': USER_AGENT,\n    })\n    if \"headers\" in kwargs:\n        # Update from kwargs\n        headers.update(kwargs.get(\"headers\"))\n    kwargs.update({\n        \"data\": data, \"timeout\": timeout, \"headers\": headers\n    })\n    request_func = getattr(requests, method)\n    try:\n        response = request_func(url, *args, **kwargs)\n        logger.debug(\"send_document: response status code %s\", response.status_code)\n        return response.status_code, None\n    # TODO support rate limit 429 code\n    except RequestException as ex:\n        logger.debug(\"send_document: exception %s\", ex)\n        return None, ex\n\n\ndef try_retrieve_webfinger_document(handle: str) -> Optional[str]:\n    \"\"\"\n    Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.\n    \"\"\"\n    try:\n        host = handle.split(\"@\")[1]\n    except AttributeError:\n        logger.warning(\"retrieve_webfinger_document: invalid handle given: %s\", handle)\n        return None\n    document, code, exception = fetch_document(\n        host=host, path=\"/.well-known/webfinger?resource=acct:%s\" % quote(handle),\n    )\n    if exception:\n        logger.debug(\"retrieve_webfinger_document: failed to fetch webfinger document: %s, %s\", code, exception)\n    return document\n",
            "file_path": "federation/utils/network.py",
            "human_label": "Try to retrieve an RFC7033 webfinger document. Does not raise if it fails.",
            "level": "file_runnable",
            "lineno": "212",
            "name": "try_retrieve_webfinger_document",
            "oracle_context": "{ \"apis\" : \"['warning', 'debug', 'fetch_document', 'quote', 'split']\", \"classes\" : \"['Optional', 'quote']\", \"vars\" : \"['logger']\" }",
            "package": "network",
            "project": "jaywink/federation",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "630629e152e177c0ba46a0d2",
            "all_context": "{ \"import\" : \"typing logging xml json urllib typing xrd federation logging lxml \", \"file\" : \"logger ; fetch_public_key(handle) ; parse_diaspora_webfinger(document) ; retrieve_diaspora_hcard(handle) ; retrieve_and_parse_diaspora_webfinger(handle) ; retrieve_diaspora_host_meta(host) ; _get_element_text_or_none(document,selector) ; _get_element_attr_or_none(document,selector,attribute) ; parse_profile_from_hcard(hcard,handle) ; retrieve_and_parse_content(id,guid,handle,entity_type,sender_key_fetcher) ; retrieve_and_parse_profile(handle) ; get_fetch_content_endpoint(domain,entity_type,guid) ; get_public_endpoint(id) ; get_private_endpoint(id,guid) ; \", \"class\" : \"\" }",
            "code": "def retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    document = try_retrieve_webfinger_document(handle)\n    if document:\n        return parse_diaspora_webfinger(document)\n    host = handle.split(\"@\")[1]\n    hostmeta = retrieve_diaspora_host_meta(host)\n    if not hostmeta:\n        return None\n    url = hostmeta.find_link(rels=\"lrdd\").template.replace(\"{uri}\", quote(handle))\n    document, code, exception = fetch_document(url)\n    if exception:\n        return None\n    return parse_diaspora_webfinger(document)\n",
            "dependency": "",
            "docstring": "Retrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict",
            "end_lineno": "89",
            "file_content": "import json\nimport logging\nimport xml\nfrom typing import Callable, Dict\nfrom urllib.parse import quote\n\nfrom lxml import html\nfrom xrd import XRD\n\nfrom federation.inbound import handle_receive\nfrom federation.types import RequestType\nfrom federation.utils.network import fetch_document, try_retrieve_webfinger_document\nfrom federation.utils.text import validate_handle\n\nlogger = logging.getLogger(\"federation\")\n\n\ndef fetch_public_key(handle):\n    \"\"\"Fetch public key over the network.\n\n    :param handle: Remote handle to retrieve public key for.\n    :return: Public key in str format from parsed profile.\n    \"\"\"\n    profile = retrieve_and_parse_profile(handle)\n    return profile.public_key\n\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    webfinger = {\n        \"hcard_url\": None,\n    }\n    # noinspection PyBroadException\n    try:\n        doc = json.loads(document)\n        for link in doc[\"links\"]:\n            if link[\"rel\"] == \"http://microformats.org/profile/hcard\":\n                webfinger[\"hcard_url\"] = link[\"href\"]\n                break\n        else:\n            logger.warning(\"parse_diaspora_webfinger: found JSON webfinger but it has no hcard href\")\n            raise ValueError\n    except Exception:\n        try:\n            xrd = XRD.parse_xrd(document)\n            webfinger[\"hcard_url\"] = xrd.find_link(rels=\"http://microformats.org/profile/hcard\").href\n        except (xml.parsers.expat.ExpatError, TypeError):\n            logger.warning(\"parse_diaspora_webfinger: found XML webfinger but it fails to parse\")\n            pass\n    return webfinger\n\n\ndef retrieve_diaspora_hcard(handle):\n    \"\"\"\n    Retrieve a remote Diaspora hCard document.\n\n    :arg handle: Remote handle to retrieve\n    :return: str (HTML document)\n    \"\"\"\n    webfinger = retrieve_and_parse_diaspora_webfinger(handle)\n    document, code, exception = fetch_document(webfinger.get(\"hcard_url\"))\n    if exception:\n        return None\n    return document\n\n\ndef retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    document = try_retrieve_webfinger_document(handle)\n    if document:\n        return parse_diaspora_webfinger(document)\n    host = handle.split(\"@\")[1]\n    hostmeta = retrieve_diaspora_host_meta(host)\n    if not hostmeta:\n        return None\n    url = hostmeta.find_link(rels=\"lrdd\").template.replace(\"{uri}\", quote(handle))\n    document, code, exception = fetch_document(url)\n    if exception:\n        return None\n    return parse_diaspora_webfinger(document)\n\n\ndef retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: ``XRD`` instance\n    \"\"\"\n    document, code, exception = fetch_document(host=host, path=\"/.well-known/host-meta\")\n    if exception:\n        return None\n    xrd = XRD.parse_xrd(document)\n    return xrd\n\n\ndef _get_element_text_or_none(document, selector):\n    \"\"\"\n    Using a CSS selector, get the element and return the text, or None if no element.\n\n    :arg document: ``HTMLElement`` document\n    :arg selector: CSS selector\n    :returns: str or None\n    \"\"\"\n    element = document.cssselect(selector)\n    if element:\n        return element[0].text\n    return None\n\n\ndef _get_element_attr_or_none(document, selector, attribute):\n    \"\"\"\n    Using a CSS selector, get the element and return the given attribute value, or None if no element.\n\n    Args:\n        document (HTMLElement) - HTMLElement document\n        selector (str) - CSS selector\n        attribute (str) - The attribute to get from the element\n    \"\"\"\n    element = document.cssselect(selector)\n    if element:\n        return element[0].get(attribute)\n    return None\n\n\ndef parse_profile_from_hcard(hcard: str, handle: str):\n    \"\"\"\n    Parse all the fields we can from a hCard document to get a Profile.\n\n    :arg hcard: HTML hcard document (str)\n    :arg handle: User handle in username@domain.tld format\n    :returns: ``federation.entities.diaspora.entities.DiasporaProfile`` instance\n    \"\"\"\n    from federation.entities.diaspora.entities import DiasporaProfile  # Circulars\n    doc = html.fromstring(hcard)\n    profile = DiasporaProfile(\n        name=_get_element_text_or_none(doc, \".fn\"),\n        image_urls={\n            \"small\": _get_element_attr_or_none(doc, \".entity_photo_small .photo\", \"src\"),\n            \"medium\": _get_element_attr_or_none(doc, \".entity_photo_medium .photo\", \"src\"),\n            \"large\": _get_element_attr_or_none(doc, \".entity_photo .photo\", \"src\"),\n        },\n        public=True,\n        id=handle,\n        handle=handle,\n        guid=_get_element_text_or_none(doc, \".uid\"),\n        public_key=_get_element_text_or_none(doc, \".key\"),\n        username=handle.split('@')[0],\n        _source_protocol=\"diaspora\",\n    )\n    return profile\n\n\ndef retrieve_and_parse_content(\n        id: str, guid: str, handle: str, entity_type: str, sender_key_fetcher: Callable[[str], str]=None):\n    \"\"\"Retrieve remote content and return an Entity class instance.\n\n    This is basically the inverse of receiving an entity. Instead, we fetch it, then call \"handle_receive\".\n\n    :param sender_key_fetcher: Function to use to fetch sender public key. If not given, network will be used\n        to fetch the profile and the key. Function must take handle as only parameter and return a public key.\n    :returns: Entity object instance or ``None``\n    \"\"\"\n    if not validate_handle(handle):\n        return\n    _username, domain = handle.split(\"@\")\n    url = get_fetch_content_endpoint(domain, entity_type.lower(), guid)\n    document, status_code, error = fetch_document(url)\n    if status_code == 200:\n        request = RequestType(body=document)\n        _sender, _protocol, entities = handle_receive(request, sender_key_fetcher=sender_key_fetcher)\n        if len(entities) > 1:\n            logger.warning(\"retrieve_and_parse_content - more than one entity parsed from remote even though we\"\n                           \"expected only one! ID %s\", guid)\n        if entities:\n            return entities[0]\n        return\n    elif status_code == 404:\n        logger.warning(\"retrieve_and_parse_content - remote content %s not found\", guid)\n        return\n    if error:\n        raise error\n    raise Exception(\"retrieve_and_parse_content - unknown problem when fetching document: %s, %s, %s\" % (\n        document, status_code, error,\n    ))\n\n\ndef retrieve_and_parse_profile(handle):\n    \"\"\"\n    Retrieve the remote user and return a Profile object.\n\n    :arg handle: User handle in username@domain.tld format\n    :returns: ``federation.entities.Profile`` instance or None\n    \"\"\"\n    hcard = retrieve_diaspora_hcard(handle)\n    if not hcard:\n        return None\n    profile = parse_profile_from_hcard(hcard, handle)\n    try:\n        profile.validate()\n    except ValueError as ex:\n        logger.warning(\"retrieve_and_parse_profile - found profile %s but it didn't validate: %s\",\n                       profile, ex)\n        return None\n    return profile\n\n\ndef get_fetch_content_endpoint(domain, entity_type, guid):\n    \"\"\"Get remote fetch content endpoint.\n\n    See: https://diaspora.github.io/diaspora_federation/federation/fetching.html\n    \"\"\"\n    return \"https://%s/fetch/%s/%s\" % (domain, entity_type, guid)\n\n\ndef get_public_endpoint(id: str) -> str:\n    \"\"\"Get remote endpoint for delivering public payloads.\"\"\"\n    _username, domain = id.split(\"@\")\n    return \"https://%s/receive/public\" % domain\n\n\ndef get_private_endpoint(id: str, guid: str) -> str:\n    \"\"\"Get remote endpoint for delivering private payloads.\"\"\"\n    _username, domain = id.split(\"@\")\n    return \"https://%s/receive/users/%s\" % (domain, guid)\n",
            "file_path": "federation/utils/diaspora.py",
            "human_label": "Retrieve a and parse a remote Diaspora webfinger document.\n\n:arg handle: Remote handle to retrieve\n:returns: dict",
            "level": "file_runnable",
            "lineno": "71",
            "name": "retrieve_and_parse_diaspora_webfinger",
            "oracle_context": "{ \"apis\" : \"['retrieve_diaspora_host_meta', 'fetch_document', 'split', 'quote', 'find_link', 'try_retrieve_webfinger_document', 'replace', 'parse_diaspora_webfinger']\", \"classes\" : \"['fetch_document', 'quote', 'try_retrieve_webfinger_document']\", \"vars\" : \"['template']\" }",
            "package": "diaspora",
            "project": "jaywink/federation",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "630629e252e177c0ba46a0d6",
            "all_context": "{ \"import\" : \"typing logging xml json urllib typing xrd federation logging lxml \", \"file\" : \"\", \"class\" : \"\" }",
            "code": "def retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: ``XRD`` instance\n    \"\"\"\n    document, code, exception = fetch_document(host=host, path=\"/.well-known/host-meta\")\n    if exception:\n        return None\n    xrd = XRD.parse_xrd(document)\n    return xrd\n",
            "dependency": "",
            "docstring": "Retrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance",
            "end_lineno": "103",
            "file_content": "import json\nimport logging\nimport xml\nfrom typing import Callable, Dict\nfrom urllib.parse import quote\n\nfrom lxml import html\nfrom xrd import XRD\n\nfrom federation.inbound import handle_receive\nfrom federation.types import RequestType\nfrom federation.utils.network import fetch_document, try_retrieve_webfinger_document\nfrom federation.utils.text import validate_handle\n\nlogger = logging.getLogger(\"federation\")\n\n\ndef fetch_public_key(handle):\n    \"\"\"Fetch public key over the network.\n\n    :param handle: Remote handle to retrieve public key for.\n    :return: Public key in str format from parsed profile.\n    \"\"\"\n    profile = retrieve_and_parse_profile(handle)\n    return profile.public_key\n\n\ndef parse_diaspora_webfinger(document: str) -> Dict:\n    \"\"\"\n    Parse Diaspora webfinger which is either in JSON format (new) or XRD (old).\n\n    https://diaspora.github.io/diaspora_federation/discovery/webfinger.html\n    \"\"\"\n    webfinger = {\n        \"hcard_url\": None,\n    }\n    # noinspection PyBroadException\n    try:\n        doc = json.loads(document)\n        for link in doc[\"links\"]:\n            if link[\"rel\"] == \"http://microformats.org/profile/hcard\":\n                webfinger[\"hcard_url\"] = link[\"href\"]\n                break\n        else:\n            logger.warning(\"parse_diaspora_webfinger: found JSON webfinger but it has no hcard href\")\n            raise ValueError\n    except Exception:\n        try:\n            xrd = XRD.parse_xrd(document)\n            webfinger[\"hcard_url\"] = xrd.find_link(rels=\"http://microformats.org/profile/hcard\").href\n        except (xml.parsers.expat.ExpatError, TypeError):\n            logger.warning(\"parse_diaspora_webfinger: found XML webfinger but it fails to parse\")\n            pass\n    return webfinger\n\n\ndef retrieve_diaspora_hcard(handle):\n    \"\"\"\n    Retrieve a remote Diaspora hCard document.\n\n    :arg handle: Remote handle to retrieve\n    :return: str (HTML document)\n    \"\"\"\n    webfinger = retrieve_and_parse_diaspora_webfinger(handle)\n    document, code, exception = fetch_document(webfinger.get(\"hcard_url\"))\n    if exception:\n        return None\n    return document\n\n\ndef retrieve_and_parse_diaspora_webfinger(handle):\n    \"\"\"\n    Retrieve a and parse a remote Diaspora webfinger document.\n\n    :arg handle: Remote handle to retrieve\n    :returns: dict\n    \"\"\"\n    document = try_retrieve_webfinger_document(handle)\n    if document:\n        return parse_diaspora_webfinger(document)\n    host = handle.split(\"@\")[1]\n    hostmeta = retrieve_diaspora_host_meta(host)\n    if not hostmeta:\n        return None\n    url = hostmeta.find_link(rels=\"lrdd\").template.replace(\"{uri}\", quote(handle))\n    document, code, exception = fetch_document(url)\n    if exception:\n        return None\n    return parse_diaspora_webfinger(document)\n\n\ndef retrieve_diaspora_host_meta(host):\n    \"\"\"\n    Retrieve a remote Diaspora host-meta document.\n\n    :arg host: Host to retrieve from\n    :returns: ``XRD`` instance\n    \"\"\"\n    document, code, exception = fetch_document(host=host, path=\"/.well-known/host-meta\")\n    if exception:\n        return None\n    xrd = XRD.parse_xrd(document)\n    return xrd\n\n\ndef _get_element_text_or_none(document, selector):\n    \"\"\"\n    Using a CSS selector, get the element and return the text, or None if no element.\n\n    :arg document: ``HTMLElement`` document\n    :arg selector: CSS selector\n    :returns: str or None\n    \"\"\"\n    element = document.cssselect(selector)\n    if element:\n        return element[0].text\n    return None\n\n\ndef _get_element_attr_or_none(document, selector, attribute):\n    \"\"\"\n    Using a CSS selector, get the element and return the given attribute value, or None if no element.\n\n    Args:\n        document (HTMLElement) - HTMLElement document\n        selector (str) - CSS selector\n        attribute (str) - The attribute to get from the element\n    \"\"\"\n    element = document.cssselect(selector)\n    if element:\n        return element[0].get(attribute)\n    return None\n\n\ndef parse_profile_from_hcard(hcard: str, handle: str):\n    \"\"\"\n    Parse all the fields we can from a hCard document to get a Profile.\n\n    :arg hcard: HTML hcard document (str)\n    :arg handle: User handle in username@domain.tld format\n    :returns: ``federation.entities.diaspora.entities.DiasporaProfile`` instance\n    \"\"\"\n    from federation.entities.diaspora.entities import DiasporaProfile  # Circulars\n    doc = html.fromstring(hcard)\n    profile = DiasporaProfile(\n        name=_get_element_text_or_none(doc, \".fn\"),\n        image_urls={\n            \"small\": _get_element_attr_or_none(doc, \".entity_photo_small .photo\", \"src\"),\n            \"medium\": _get_element_attr_or_none(doc, \".entity_photo_medium .photo\", \"src\"),\n            \"large\": _get_element_attr_or_none(doc, \".entity_photo .photo\", \"src\"),\n        },\n        public=True,\n        id=handle,\n        handle=handle,\n        guid=_get_element_text_or_none(doc, \".uid\"),\n        public_key=_get_element_text_or_none(doc, \".key\"),\n        username=handle.split('@')[0],\n        _source_protocol=\"diaspora\",\n    )\n    return profile\n\n\ndef retrieve_and_parse_content(\n        id: str, guid: str, handle: str, entity_type: str, sender_key_fetcher: Callable[[str], str]=None):\n    \"\"\"Retrieve remote content and return an Entity class instance.\n\n    This is basically the inverse of receiving an entity. Instead, we fetch it, then call \"handle_receive\".\n\n    :param sender_key_fetcher: Function to use to fetch sender public key. If not given, network will be used\n        to fetch the profile and the key. Function must take handle as only parameter and return a public key.\n    :returns: Entity object instance or ``None``\n    \"\"\"\n    if not validate_handle(handle):\n        return\n    _username, domain = handle.split(\"@\")\n    url = get_fetch_content_endpoint(domain, entity_type.lower(), guid)\n    document, status_code, error = fetch_document(url)\n    if status_code == 200:\n        request = RequestType(body=document)\n        _sender, _protocol, entities = handle_receive(request, sender_key_fetcher=sender_key_fetcher)\n        if len(entities) > 1:\n            logger.warning(\"retrieve_and_parse_content - more than one entity parsed from remote even though we\"\n                           \"expected only one! ID %s\", guid)\n        if entities:\n            return entities[0]\n        return\n    elif status_code == 404:\n        logger.warning(\"retrieve_and_parse_content - remote content %s not found\", guid)\n        return\n    if error:\n        raise error\n    raise Exception(\"retrieve_and_parse_content - unknown problem when fetching document: %s, %s, %s\" % (\n        document, status_code, error,\n    ))\n\n\ndef retrieve_and_parse_profile(handle):\n    \"\"\"\n    Retrieve the remote user and return a Profile object.\n\n    :arg handle: User handle in username@domain.tld format\n    :returns: ``federation.entities.Profile`` instance or None\n    \"\"\"\n    hcard = retrieve_diaspora_hcard(handle)\n    if not hcard:\n        return None\n    profile = parse_profile_from_hcard(hcard, handle)\n    try:\n        profile.validate()\n    except ValueError as ex:\n        logger.warning(\"retrieve_and_parse_profile - found profile %s but it didn't validate: %s\",\n                       profile, ex)\n        return None\n    return profile\n\n\ndef get_fetch_content_endpoint(domain, entity_type, guid):\n    \"\"\"Get remote fetch content endpoint.\n\n    See: https://diaspora.github.io/diaspora_federation/federation/fetching.html\n    \"\"\"\n    return \"https://%s/fetch/%s/%s\" % (domain, entity_type, guid)\n\n\ndef get_public_endpoint(id: str) -> str:\n    \"\"\"Get remote endpoint for delivering public payloads.\"\"\"\n    _username, domain = id.split(\"@\")\n    return \"https://%s/receive/public\" % domain\n\n\ndef get_private_endpoint(id: str, guid: str) -> str:\n    \"\"\"Get remote endpoint for delivering private payloads.\"\"\"\n    _username, domain = id.split(\"@\")\n    return \"https://%s/receive/users/%s\" % (domain, guid)\n",
            "file_path": "federation/utils/diaspora.py",
            "human_label": "Retrieve a remote Diaspora host-meta document.\n\n:arg host: Host to retrieve from\n:returns: ``XRD`` instance",
            "level": "plib_runnable",
            "lineno": "92",
            "name": "retrieve_diaspora_host_meta",
            "oracle_context": "{ \"apis\" : \"['parse_xrd', 'fetch_document']\", \"classes\" : \"['XRD', 'fetch_document']\", \"vars\" : \"[]\" }",
            "package": "diaspora",
            "project": "jaywink/federation",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "6306091073426c38ae68acac",
            "all_context": "{ \"import\" : \"infrared \", \"file\" : \"LOG ; dict_insert(dic,val,key) ; dict_merge(first,second,conflict_resolver) ; \", \"class\" : \"\" }",
            "code": "def dict_insert(dic, val, key, *keys):\n    \"\"\"insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    if dic is None:\n        return\n\n    if not keys:\n        if isinstance(dic.get(key, None), dict) and isinstance(val, dict):\n            dict_merge(dic[key], val)\n        else:\n            dic[key] = val\n        return\n\n    dict_insert(dic.setdefault(key, {}), val, *keys)\n",
            "dependency": "",
            "docstring": "insert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain",
            "end_lineno": "32",
            "file_content": "\"\"\"This module provides helper methods for dict merging and dict insertion. \"\"\"\n\nfrom infrared.core.utils import logger\n\nLOG = logger.LOG\n\n\ndef dict_insert(dic, val, key, *keys):\n    \"\"\"insert a value of a nested key into a dictionary\n\n    to insert value for a nested key, all ancestor keys should be given as\n    method's arguments\n\n    example:\n      dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n    :param dic: a dictionary object to insert the nested key value into\n    :param val: a value to insert to the given dictionary\n    :param key: first key in a chain of key that will store the value\n    :param keys: sub keys in the keys chain\n    \"\"\"\n    if dic is None:\n        return\n\n    if not keys:\n        if isinstance(dic.get(key, None), dict) and isinstance(val, dict):\n            dict_merge(dic[key], val)\n        else:\n            dic[key] = val\n        return\n\n    dict_insert(dic.setdefault(key, {}), val, *keys)\n\n\nclass ConflictResolver(object):\n    \"\"\"Resolves conflicts while merging dicts. \"\"\"\n\n    @staticmethod\n    def none_resolver(first, second, key):\n        \"\"\"Replaces value in first dict only if it is None.\n\n        Appends second value into the first in type is list.\n        \"\"\"\n\n        # tyr to merge lists first\n        if isinstance(first[key], list):\n            if isinstance(second[key], list):\n                first[key].extend(second[key])\n            elif second[key] is not None:\n                first[key].append(second[key])\n\n        if key not in first or first[key] is None:\n            first[key] = second[key]\n\n    @staticmethod\n    def greedy_resolver(first, second, key):\n        \"\"\"Replace always first with the value from second \"\"\"\n        first[key] = second[key]\n\n    @staticmethod\n    def unique_append_list_resolver(first, second, key):\n        \"\"\"Merges first and second lists \"\"\"\n        if isinstance(first[key], list) and isinstance(second[key], list):\n            for item in second[key]:\n                if item not in first[key]:\n                    first[key].append(item)\n        else:\n            return ConflictResolver.greedy_resolver(first, second, key)\n\n\ndef dict_merge(first, second,\n               conflict_resolver=ConflictResolver.greedy_resolver):\n    \"\"\"Merge `second` dict into `first`.\n\n    :param first: Modified dict\n    :param second: Modifier dict\n    :param conflict_resolver: Function that resolves a merge between 2 values\n        when one of them isn't a dict\n    \"\"\"\n    for key in second:\n        if key in first:\n            if isinstance(first[key], dict) and isinstance(second[key], dict):\n                dict_merge(first[key], second[key],\n                           conflict_resolver=conflict_resolver)\n            else:\n                # replace first value with the value from second\n                conflict_resolver(first, second, key)\n        else:\n            try:\n                first[key] = second[key]\n            except TypeError as e:\n                LOG.error(\"dict_merge(%s, %s) failed on: %s\" % (first, second, key))\n                raise e\n",
            "file_path": "infrared/core/utils/dict_utils.py",
            "human_label": "insert a value of a nested key into a dictionary\n\nto insert value for a nested key, all ancestor keys should be given as\nmethod's arguments\n\nexample:\n  dict_insert({}, 'val', 'key1.key2'.split('.'))\n\n:param dic: a dictionary object to insert the nested key value into\n:param val: a value to insert to the given dictionary\n:param key: first key in a chain of key that will store the value\n:param keys: sub keys in the keys chain",
            "level": "file_runnable",
            "lineno": "8",
            "name": "dict_insert",
            "oracle_context": "{ \"apis\" : \"['setdefault', 'get', 'isinstance', 'dict_merge']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }",
            "package": "dict_utils",
            "project": "redhat-openstack/infrared",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "6306091b73426c38ae68acd7",
            "all_context": "{ \"import\" : \"os sys infrared \", \"file\" : \"\", \"class\" : \"self._get_service(cls,name) ; self.setup(cls,core_settings) ; self.plugins_manager(cls) ; self.ansible_config_manager(cls) ; self.execution_logger_manager(cls) ; self.register_service(cls,service_name,service) ; self.workspace_manager(cls) ; \" }",
            "code": "    @classmethod\n    def ansible_config_manager(cls):\n        \"\"\"Gets the ansible config manager. \"\"\"\n        return cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n",
            "dependency": "",
            "docstring": "Gets the ansible config manager.",
            "end_lineno": "136",
            "file_content": "\"\"\"Service locator for the IR services\n\nStores and resolves all the dependencies for the services.\n\"\"\"\nimport os\nimport sys\n\nfrom infrared.core.services import ansible_config\nfrom infrared.core.services import execution_logger\nfrom infrared.core.services import plugins\nfrom infrared.core.services import workspaces\nfrom infrared.core.utils import logger\n\nLOG = logger.LOG\n\n\nclass ServiceName(object):\n    \"\"\"Holds the supported services names. \"\"\"\n    WORKSPACE_MANAGER = \"workspace_manager\"\n    PLUGINS_MANAGER = \"plugins_manager\"\n    ANSIBLE_CONFIG_MANAGER = \"ansible_config_manager\"\n    EXECUTION_LOGGER_MANAGER = \"execution_logger_manager\"\n\n\nclass CoreSettings(object):\n    \"\"\"Holds the main settings for the infrared. \"\"\"\n\n    def __init__(self, workspaces_base_folder=None,\n                 plugins_conf_file=None,\n                 install_plugin_at_start=True,\n                 plugins_base_folder=None):\n        \"\"\"Constructor.\n\n        :param workspaces_base_folder: folder where the\n               workspace will be stored\n        :param plugins_conf_file: location of the plugins.ini file with the\n               list of all plugins and types.\n        :param install_plugin_at_start: specifies whether all the plugins\n               should be installed on ir start. Skip installation may be\n               required for unit tests, for example.\n        \"\"\"\n\n        self.infrared_home = os.path.abspath(os.environ.get(\n            \"IR_HOME\", os.path.join(os.path.expanduser(\"~\"), '.infrared')))\n\n        # todo(obaranov) replace .workspaces to workspaces and .plugins.ini to\n        # todo(obaranov) plugins.ini once IR is packaged as pip\n        self.plugins_conf_file = plugins_conf_file or os.path.join(\n            self.infrared_home, '.plugins.ini')\n        self.workspaces_base_folder = workspaces_base_folder or os.path.join(\n            self.infrared_home, '.workspaces')\n        self.install_plugin_at_start = install_plugin_at_start\n        self.plugins_base_folder = plugins_base_folder or os.path.join(\n            self.infrared_home, 'plugins')\n\n\nclass CoreServices(object):\n    \"\"\"Holds and configures all the required for core services. \"\"\"\n\n    _SERVICES = {}\n\n    @classmethod\n    def setup(cls, core_settings=None):\n        \"\"\"Creates configuration from file or from defaults.\n\n        :param core_settings: the instance of the CoreSettings class with the\n        desired settings. If None is provided then the default settings\n        will be used.\n        \"\"\"\n\n        if core_settings is None:\n            core_settings = CoreSettings()\n\n        # create workspace manager\n        if ServiceName.WORKSPACE_MANAGER not in cls._SERVICES:\n            cls.register_service(ServiceName.WORKSPACE_MANAGER,\n                                 workspaces.WorkspaceManager(\n                                     core_settings.workspaces_base_folder))\n\n        # create plugins manager\n        if ServiceName.PLUGINS_MANAGER not in cls._SERVICES:\n            # A temporary WH to skip all plugins installation on first InfraRed\n            # command if the command is 'infrared plugin add'.\n            # Should be removed together with auto plugins installation\n            # mechanism.\n            skip_plugins_install = {'plugin', 'add'}.issubset(sys.argv)\n            cls.register_service(\n                ServiceName.PLUGINS_MANAGER, plugins.InfraredPluginManager(\n                    plugins_conf=core_settings.plugins_conf_file,\n                    install_plugins=(core_settings.install_plugin_at_start and\n                                     not skip_plugins_install),\n                    plugins_dir=core_settings.plugins_base_folder))\n\n        # create ansible config manager\n        if ServiceName.ANSIBLE_CONFIG_MANAGER not in cls._SERVICES:\n            cls.register_service(ServiceName.ANSIBLE_CONFIG_MANAGER,\n                                 ansible_config.AnsibleConfigManager(\n                                     core_settings.infrared_home))\n\n        # create execution logger manager\n        if ServiceName.EXECUTION_LOGGER_MANAGER not in cls._SERVICES:\n            # get ansible manager\n            ansible_manager = CoreServices.ansible_config_manager()\n            # build log file path\n            log_file = \\\n                os.path.join(core_settings.infrared_home, 'ir-commands.log')\n            cls.register_service(ServiceName.EXECUTION_LOGGER_MANAGER,\n                                 execution_logger.ExecutionLoggerManager(\n                                     ansible_manager.ansible_config_path,\n                                     log_file=log_file))\n\n    @classmethod\n    def register_service(cls, service_name, service):\n        \"\"\"Protect the _SERVICES dict\"\"\"\n        CoreServices._SERVICES[service_name] = service\n\n    @classmethod\n    def _get_service(cls, name):\n        if name not in cls._SERVICES:\n            cls.setup()\n        return cls._SERVICES[name]\n\n    @classmethod\n    def workspace_manager(cls):\n        \"\"\"Gets the workspace manager. \"\"\"\n        return cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n    @classmethod\n    def plugins_manager(cls):\n        \"\"\"Gets the plugin manager. \"\"\"\n        return cls._get_service(ServiceName.PLUGINS_MANAGER)\n\n    @classmethod\n    def ansible_config_manager(cls):\n        \"\"\"Gets the ansible config manager. \"\"\"\n        return cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n    @classmethod\n    def execution_logger_manager(cls):\n        \"\"\"Gets the execution logger manager. \"\"\"\n        return cls._get_service(ServiceName.EXECUTION_LOGGER_MANAGER)\n",
            "file_path": "infrared/core/services/__init__.py",
            "human_label": "Gets the ansible config manager via ServiceName.ANSIBLE_CONFIG_MANAGER in cls._get_service()",
            "level": "class_runnable",
            "lineno": "133",
            "name": "ansible_config_manager",
            "oracle_context": "{ \"apis\" : \"['_get_service']\", \"classes\" : \"['ServiceName']\", \"vars\" : \"['ANSIBLE_CONFIG_MANAGER']\" }",
            "package": "__init__",
            "project": "redhat-openstack/infrared",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "6306091b73426c38ae68acd9",
            "all_context": "{ \"import\" : \"os sys infrared \", \"file\" : \"\", \"class\" : \"self._get_service(cls,name) ; self.setup(cls,core_settings) ; self.plugins_manager(cls) ; self.ansible_config_manager(cls) ; self.execution_logger_manager(cls) ; self.register_service(cls,service_name,service) ; self.workspace_manager(cls) ; \" }",
            "code": "    @classmethod\n    def workspace_manager(cls):\n        \"\"\"Gets the workspace manager. \"\"\"\n        return cls._get_service(ServiceName.WORKSPACE_MANAGER)\n",
            "dependency": "",
            "docstring": "Gets the workspace manager.",
            "end_lineno": "126",
            "file_content": "\"\"\"Service locator for the IR services\n\nStores and resolves all the dependencies for the services.\n\"\"\"\nimport os\nimport sys\n\nfrom infrared.core.services import ansible_config\nfrom infrared.core.services import execution_logger\nfrom infrared.core.services import plugins\nfrom infrared.core.services import workspaces\nfrom infrared.core.utils import logger\n\nLOG = logger.LOG\n\n\nclass ServiceName(object):\n    \"\"\"Holds the supported services names. \"\"\"\n    WORKSPACE_MANAGER = \"workspace_manager\"\n    PLUGINS_MANAGER = \"plugins_manager\"\n    ANSIBLE_CONFIG_MANAGER = \"ansible_config_manager\"\n    EXECUTION_LOGGER_MANAGER = \"execution_logger_manager\"\n\n\nclass CoreSettings(object):\n    \"\"\"Holds the main settings for the infrared. \"\"\"\n\n    def __init__(self, workspaces_base_folder=None,\n                 plugins_conf_file=None,\n                 install_plugin_at_start=True,\n                 plugins_base_folder=None):\n        \"\"\"Constructor.\n\n        :param workspaces_base_folder: folder where the\n               workspace will be stored\n        :param plugins_conf_file: location of the plugins.ini file with the\n               list of all plugins and types.\n        :param install_plugin_at_start: specifies whether all the plugins\n               should be installed on ir start. Skip installation may be\n               required for unit tests, for example.\n        \"\"\"\n\n        self.infrared_home = os.path.abspath(os.environ.get(\n            \"IR_HOME\", os.path.join(os.path.expanduser(\"~\"), '.infrared')))\n\n        # todo(obaranov) replace .workspaces to workspaces and .plugins.ini to\n        # todo(obaranov) plugins.ini once IR is packaged as pip\n        self.plugins_conf_file = plugins_conf_file or os.path.join(\n            self.infrared_home, '.plugins.ini')\n        self.workspaces_base_folder = workspaces_base_folder or os.path.join(\n            self.infrared_home, '.workspaces')\n        self.install_plugin_at_start = install_plugin_at_start\n        self.plugins_base_folder = plugins_base_folder or os.path.join(\n            self.infrared_home, 'plugins')\n\n\nclass CoreServices(object):\n    \"\"\"Holds and configures all the required for core services. \"\"\"\n\n    _SERVICES = {}\n\n    @classmethod\n    def setup(cls, core_settings=None):\n        \"\"\"Creates configuration from file or from defaults.\n\n        :param core_settings: the instance of the CoreSettings class with the\n        desired settings. If None is provided then the default settings\n        will be used.\n        \"\"\"\n\n        if core_settings is None:\n            core_settings = CoreSettings()\n\n        # create workspace manager\n        if ServiceName.WORKSPACE_MANAGER not in cls._SERVICES:\n            cls.register_service(ServiceName.WORKSPACE_MANAGER,\n                                 workspaces.WorkspaceManager(\n                                     core_settings.workspaces_base_folder))\n\n        # create plugins manager\n        if ServiceName.PLUGINS_MANAGER not in cls._SERVICES:\n            # A temporary WH to skip all plugins installation on first InfraRed\n            # command if the command is 'infrared plugin add'.\n            # Should be removed together with auto plugins installation\n            # mechanism.\n            skip_plugins_install = {'plugin', 'add'}.issubset(sys.argv)\n            cls.register_service(\n                ServiceName.PLUGINS_MANAGER, plugins.InfraredPluginManager(\n                    plugins_conf=core_settings.plugins_conf_file,\n                    install_plugins=(core_settings.install_plugin_at_start and\n                                     not skip_plugins_install),\n                    plugins_dir=core_settings.plugins_base_folder))\n\n        # create ansible config manager\n        if ServiceName.ANSIBLE_CONFIG_MANAGER not in cls._SERVICES:\n            cls.register_service(ServiceName.ANSIBLE_CONFIG_MANAGER,\n                                 ansible_config.AnsibleConfigManager(\n                                     core_settings.infrared_home))\n\n        # create execution logger manager\n        if ServiceName.EXECUTION_LOGGER_MANAGER not in cls._SERVICES:\n            # get ansible manager\n            ansible_manager = CoreServices.ansible_config_manager()\n            # build log file path\n            log_file = \\\n                os.path.join(core_settings.infrared_home, 'ir-commands.log')\n            cls.register_service(ServiceName.EXECUTION_LOGGER_MANAGER,\n                                 execution_logger.ExecutionLoggerManager(\n                                     ansible_manager.ansible_config_path,\n                                     log_file=log_file))\n\n    @classmethod\n    def register_service(cls, service_name, service):\n        \"\"\"Protect the _SERVICES dict\"\"\"\n        CoreServices._SERVICES[service_name] = service\n\n    @classmethod\n    def _get_service(cls, name):\n        if name not in cls._SERVICES:\n            cls.setup()\n        return cls._SERVICES[name]\n\n    @classmethod\n    def workspace_manager(cls):\n        \"\"\"Gets the workspace manager. \"\"\"\n        return cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n    @classmethod\n    def plugins_manager(cls):\n        \"\"\"Gets the plugin manager. \"\"\"\n        return cls._get_service(ServiceName.PLUGINS_MANAGER)\n\n    @classmethod\n    def ansible_config_manager(cls):\n        \"\"\"Gets the ansible config manager. \"\"\"\n        return cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n    @classmethod\n    def execution_logger_manager(cls):\n        \"\"\"Gets the execution logger manager. \"\"\"\n        return cls._get_service(ServiceName.EXECUTION_LOGGER_MANAGER)\n",
            "file_path": "infrared/core/services/__init__.py",
            "human_label": "Gets the workspace manager via ServiceName.WORKSPACE_MANAGER in cls._get_service()",
            "level": "class_runnable",
            "lineno": "123",
            "name": "workspace_manager",
            "oracle_context": "{ \"apis\" : \"['_get_service']\", \"classes\" : \"['ServiceName']\", \"vars\" : \"['WORKSPACE_MANAGER']\" }",
            "package": "__init__",
            "project": "redhat-openstack/infrared",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "6306091b73426c38ae68acda",
            "all_context": "{ \"import\" : \"os sys infrared \", \"file\" : \"\", \"class\" : \"self._get_service(cls,name) ; self.setup(cls,core_settings) ; self.plugins_manager(cls) ; self.ansible_config_manager(cls) ; self.execution_logger_manager(cls) ; self.register_service(cls,service_name,service) ; self.workspace_manager(cls) ; \" }",
            "code": "    @classmethod\n    def plugins_manager(cls):\n        \"\"\"Gets the plugin manager. \"\"\"\n        return cls._get_service(ServiceName.PLUGINS_MANAGER)\n",
            "dependency": "",
            "docstring": "Gets the plugin manager.",
            "end_lineno": "131",
            "file_content": "\"\"\"Service locator for the IR services\n\nStores and resolves all the dependencies for the services.\n\"\"\"\nimport os\nimport sys\n\nfrom infrared.core.services import ansible_config\nfrom infrared.core.services import execution_logger\nfrom infrared.core.services import plugins\nfrom infrared.core.services import workspaces\nfrom infrared.core.utils import logger\n\nLOG = logger.LOG\n\n\nclass ServiceName(object):\n    \"\"\"Holds the supported services names. \"\"\"\n    WORKSPACE_MANAGER = \"workspace_manager\"\n    PLUGINS_MANAGER = \"plugins_manager\"\n    ANSIBLE_CONFIG_MANAGER = \"ansible_config_manager\"\n    EXECUTION_LOGGER_MANAGER = \"execution_logger_manager\"\n\n\nclass CoreSettings(object):\n    \"\"\"Holds the main settings for the infrared. \"\"\"\n\n    def __init__(self, workspaces_base_folder=None,\n                 plugins_conf_file=None,\n                 install_plugin_at_start=True,\n                 plugins_base_folder=None):\n        \"\"\"Constructor.\n\n        :param workspaces_base_folder: folder where the\n               workspace will be stored\n        :param plugins_conf_file: location of the plugins.ini file with the\n               list of all plugins and types.\n        :param install_plugin_at_start: specifies whether all the plugins\n               should be installed on ir start. Skip installation may be\n               required for unit tests, for example.\n        \"\"\"\n\n        self.infrared_home = os.path.abspath(os.environ.get(\n            \"IR_HOME\", os.path.join(os.path.expanduser(\"~\"), '.infrared')))\n\n        # todo(obaranov) replace .workspaces to workspaces and .plugins.ini to\n        # todo(obaranov) plugins.ini once IR is packaged as pip\n        self.plugins_conf_file = plugins_conf_file or os.path.join(\n            self.infrared_home, '.plugins.ini')\n        self.workspaces_base_folder = workspaces_base_folder or os.path.join(\n            self.infrared_home, '.workspaces')\n        self.install_plugin_at_start = install_plugin_at_start\n        self.plugins_base_folder = plugins_base_folder or os.path.join(\n            self.infrared_home, 'plugins')\n\n\nclass CoreServices(object):\n    \"\"\"Holds and configures all the required for core services. \"\"\"\n\n    _SERVICES = {}\n\n    @classmethod\n    def setup(cls, core_settings=None):\n        \"\"\"Creates configuration from file or from defaults.\n\n        :param core_settings: the instance of the CoreSettings class with the\n        desired settings. If None is provided then the default settings\n        will be used.\n        \"\"\"\n\n        if core_settings is None:\n            core_settings = CoreSettings()\n\n        # create workspace manager\n        if ServiceName.WORKSPACE_MANAGER not in cls._SERVICES:\n            cls.register_service(ServiceName.WORKSPACE_MANAGER,\n                                 workspaces.WorkspaceManager(\n                                     core_settings.workspaces_base_folder))\n\n        # create plugins manager\n        if ServiceName.PLUGINS_MANAGER not in cls._SERVICES:\n            # A temporary WH to skip all plugins installation on first InfraRed\n            # command if the command is 'infrared plugin add'.\n            # Should be removed together with auto plugins installation\n            # mechanism.\n            skip_plugins_install = {'plugin', 'add'}.issubset(sys.argv)\n            cls.register_service(\n                ServiceName.PLUGINS_MANAGER, plugins.InfraredPluginManager(\n                    plugins_conf=core_settings.plugins_conf_file,\n                    install_plugins=(core_settings.install_plugin_at_start and\n                                     not skip_plugins_install),\n                    plugins_dir=core_settings.plugins_base_folder))\n\n        # create ansible config manager\n        if ServiceName.ANSIBLE_CONFIG_MANAGER not in cls._SERVICES:\n            cls.register_service(ServiceName.ANSIBLE_CONFIG_MANAGER,\n                                 ansible_config.AnsibleConfigManager(\n                                     core_settings.infrared_home))\n\n        # create execution logger manager\n        if ServiceName.EXECUTION_LOGGER_MANAGER not in cls._SERVICES:\n            # get ansible manager\n            ansible_manager = CoreServices.ansible_config_manager()\n            # build log file path\n            log_file = \\\n                os.path.join(core_settings.infrared_home, 'ir-commands.log')\n            cls.register_service(ServiceName.EXECUTION_LOGGER_MANAGER,\n                                 execution_logger.ExecutionLoggerManager(\n                                     ansible_manager.ansible_config_path,\n                                     log_file=log_file))\n\n    @classmethod\n    def register_service(cls, service_name, service):\n        \"\"\"Protect the _SERVICES dict\"\"\"\n        CoreServices._SERVICES[service_name] = service\n\n    @classmethod\n    def _get_service(cls, name):\n        if name not in cls._SERVICES:\n            cls.setup()\n        return cls._SERVICES[name]\n\n    @classmethod\n    def workspace_manager(cls):\n        \"\"\"Gets the workspace manager. \"\"\"\n        return cls._get_service(ServiceName.WORKSPACE_MANAGER)\n\n    @classmethod\n    def plugins_manager(cls):\n        \"\"\"Gets the plugin manager. \"\"\"\n        return cls._get_service(ServiceName.PLUGINS_MANAGER)\n\n    @classmethod\n    def ansible_config_manager(cls):\n        \"\"\"Gets the ansible config manager. \"\"\"\n        return cls._get_service(ServiceName.ANSIBLE_CONFIG_MANAGER)\n\n    @classmethod\n    def execution_logger_manager(cls):\n        \"\"\"Gets the execution logger manager. \"\"\"\n        return cls._get_service(ServiceName.EXECUTION_LOGGER_MANAGER)\n",
            "file_path": "infrared/core/services/__init__.py",
            "human_label": "Gets the plugin manager via ServiceName.PLUGINS_MANAGER in cls._get_service()",
            "level": "file_runnable",
            "lineno": "128",
            "name": "plugins_manager",
            "oracle_context": "{ \"apis\" : \"['_get_service']\", \"classes\" : \"['ServiceName']\", \"vars\" : \"['PLUGINS_MANAGER']\" }",
            "package": "__init__",
            "project": "redhat-openstack/infrared",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "6306091c73426c38ae68acdd",
            "all_context": "{ \"import\" : \"os jsonschema six infrared \", \"file\" : \"\", \"class\" : \"self.validate_from_file(cls,yaml_file) ; self.validate_from_content(cls,file_content) ; \" }",
            "code": "    @classmethod\n    def validate_from_file(cls, yaml_file=None):\n        \"\"\"Loads & validates that a YAML file has all required fields\n\n        :param yaml_file: Path to YAML file\n        :raise IRValidatorException: when mandatory data is missing in file\n        :return: Dictionary with data loaded from a YAML file\n        \"\"\"\n        if yaml_file is None:\n            raise IRValidatorException(\n                \"YAML file is missing\")\n\n        if not os.path.isfile(yaml_file):\n            raise IRValidatorException(\n                \"The YAML file doesn't exist: {}\".format(yaml_file))\n\n        with open(yaml_file) as fp:\n            spec_dict = cls.validate_from_content(fp.read())\n\n        return spec_dict\n",
            "dependency": "",
            "docstring": "Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file",
            "end_lineno": "31",
            "file_content": "import jsonschema\nimport os\nfrom six.moves import configparser\nimport yaml\n\nfrom infrared.core.utils.exceptions import IRValidatorException\nfrom infrared.core.utils.logger import LOG as logger\n\n\nclass Validator(object):\n\n    @classmethod\n    def validate_from_file(cls, yaml_file=None):\n        \"\"\"Loads & validates that a YAML file has all required fields\n\n        :param yaml_file: Path to YAML file\n        :raise IRValidatorException: when mandatory data is missing in file\n        :return: Dictionary with data loaded from a YAML file\n        \"\"\"\n        if yaml_file is None:\n            raise IRValidatorException(\n                \"YAML file is missing\")\n\n        if not os.path.isfile(yaml_file):\n            raise IRValidatorException(\n                \"The YAML file doesn't exist: {}\".format(yaml_file))\n\n        with open(yaml_file) as fp:\n            spec_dict = cls.validate_from_content(fp.read())\n\n        return spec_dict\n\n    @classmethod\n    def validate_from_content(cls, file_content=None):\n        \"\"\"validates that YAML content has all required fields\n\n        :param file_content: content of the YAML file\n        :raise IRValidatorException: when mandatory data is missing in file\n        :return: Dictionary with data loaded from a YAML file\n        \"\"\"\n        raise NotImplementedError\n\n\nclass SpecValidator(Validator):\n    \"\"\"Class for validating a plugin spec.\n\n    It checks that a plugin spec (YAML) has all required fields.\n    \"\"\"\n    CONFIG_PART_SCHEMA = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"plugin_type\": {\"type\": \"string\", \"minLength\": 1},\n            \"entry_point\": {\"type\": \"string\", \"minLength\": 1},\n            \"roles_path\": {\"type\": \"string\", \"minLength\": 1},\n        },\n        \"additionalProperties\": False,\n        \"required\": [\"plugin_type\"]\n    }\n\n    SUBPARSER_PART_SCHEMA = {\n        \"type\": \"object\",\n        \"minProperties\": 1,\n        \"maxProperties\": 1,\n        \"patternProperties\": {\n            \"^(?!(?:all)$).+$\": {\n                \"type\": \"object\",\n            }\n        },\n        \"additionalProperties\": False\n    }\n\n    SCHEMA_WITH_CONFIG = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"description\": {\"type\": \"string\", \"minLength\": 1},\n            \"config\": CONFIG_PART_SCHEMA,\n            \"subparsers\": SUBPARSER_PART_SCHEMA\n        },\n        \"additionalProperties\": False,\n        \"required\": [\"config\", \"subparsers\"]\n    }\n\n    SCHEMA_WITHOUT_CONFIG = {\n        \"type\": \"object\",\n        \"properties\": {\n            \"plugin_type\": {\"type\": \"string\", \"minLength\": 1},\n            \"entry_point\": {\"type\": \"string\", \"minLength\": 1},\n            \"roles_path\": {\"type\": \"string\", \"minLength\": 1},\n            \"description\": {\"type\": \"string\", \"minLength\": 1},\n            \"subparsers\": SUBPARSER_PART_SCHEMA\n        },\n        \"additionalProperties\": False,\n        \"required\": [\"plugin_type\", \"subparsers\"]\n    }\n\n    @classmethod\n    def validate_from_content(cls, spec_content=None):\n        \"\"\"validates that spec (YAML) content has all required fields\n\n        :param spec_content: content of spec file\n        :raise IRValidatorException: when mandatory data\n        is missing in spec file\n        :return: Dictionary with data loaded from a spec (YAML) file\n        \"\"\"\n        if spec_content is None:\n            raise IRValidatorException(\n                \"Plugin spec content is missing\")\n\n        spec_dict = yaml.safe_load(spec_content)\n\n        if not isinstance(spec_dict, dict):\n            raise IRValidatorException(\n                \"Spec file is empty or corrupted: {}\".format(spec_content))\n\n        # check if new spec file structure\n        try:\n            if \"config\" in spec_dict:\n                jsonschema.validate(spec_dict,\n                                    cls.SCHEMA_WITH_CONFIG)\n            else:\n                jsonschema.validate(spec_dict,\n                                    cls.SCHEMA_WITHOUT_CONFIG)\n\n        except jsonschema.exceptions.ValidationError as error:\n            raise IRValidatorException(\n                \"{} in file:\\n{}\".format(error.message, spec_content))\n\n        subparsers_key = \"subparsers\"\n        if (\"description\" not in spec_dict and \"description\"\n                not in list(spec_dict[subparsers_key].values())[0]):\n            raise IRValidatorException(\n                \"Required key 'description' is missing for supbarser '{}' in \"\n                \"spec file: {}\".format(\n                    list(spec_dict[subparsers_key].keys())[0], spec_content))\n\n        return spec_dict\n\n\nclass RegistryValidator(Validator):\n    SCHEMA_REGISTRY = {\n        \"type\": \"object\",\n        \"patternProperties\": {\n            \"^.+$\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"src\": {\"type\": \"string\", \"minLength\": 1},\n                    \"src_path\": {\"type\": \"string\", \"minLength\": 1},\n                    \"rev\": {\"type\": \"string\", \"minLength\": 1},\n                    \"desc\": {\"type\": \"string\", \"minLength\": 1},\n                    \"type\": {\"type\": \"string\", \"minLength\": 1},\n                },\n                \"additionalProperties\": False,\n                \"required\": [\"src\", \"desc\", \"type\"]\n            }\n        },\n        \"additionalProperties\": False,\n    }\n\n    @classmethod\n    def validate_from_content(cls, file_content=None):\n        \"\"\"validates that Registry YAML content has all required fields\n\n        :param file_content: content of the Registry YAML file\n        :raise IRValidatorException: when mandatory data is missing in Registry\n        :return: Dictionary with data loaded from a Registry YAML file\n        \"\"\"\n        if file_content is None:\n            raise IRValidatorException(\n                \"Registry YAML content is missing\")\n\n        registry_dict = yaml.safe_load(file_content)\n\n        if not isinstance(registry_dict, dict):\n            raise IRValidatorException(\n                \"Registry file is empty or corrupted: {}\".format(file_content))\n\n        try:\n            # validate schema\n            jsonschema.validate(registry_dict,\n                                cls.SCHEMA_REGISTRY)\n\n        except jsonschema.exceptions.ValidationError as error:\n            raise IRValidatorException(\n                \"{} in file:\\n{}\".format(error.message, file_content))\n\n        return registry_dict\n\n\nclass AnsibleConfigValidator(Validator):\n    ANSIBLE_CONFIG_OPTIONS = {\n        'defaults': {\n            'host_key_checking': {\n                'type': 'bool',\n                'comparison': 'eq',\n                'expected_value': False,\n                'critical': True\n            },\n            'forks': {\n                'type': 'int',\n                'comparison': 'gt',\n                'expected_value': 500,\n                'critical': False\n            },\n            'timeout': {\n                'type': 'int',\n                'comparison': 'gt',\n                'expected_value': 30,\n                'critical': False\n            }\n        }\n    }\n\n    @classmethod\n    def validate_from_file(cls, yaml_file=None):\n        config = configparser.RawConfigParser()\n        config.read(yaml_file)\n        config_dict = cls._convert_config_to_dict(config)\n\n        for section, option_details in cls.ANSIBLE_CONFIG_OPTIONS.items():\n            for opt_name, opt_params in option_details.items():\n                try:\n                    config_value = config_dict[section][opt_name]\n                    cls._validate_config_option(yaml_file,\n                                                opt_name,\n                                                opt_params['type'],\n                                                opt_params['comparison'],\n                                                opt_params['expected_value'],\n                                                config_value,\n                                                opt_params['critical'])\n                except KeyError:\n                    cls._handle_missing_value(yaml_file, section, opt_name,\n                                              opt_params['expected_value'],\n                                              opt_params['critical'])\n\n    @classmethod\n    def validate_from_content(cls, file_content=None):\n        pass\n\n    @classmethod\n    def _validate_config_option(cls, yaml_file, opt_name, opt_type,\n                                comparison, exp_value, cur_value, critical):\n        if opt_type == 'int':\n            cur_value = int(cur_value)\n        if opt_type == 'bool':\n            if cur_value == 'True':\n                cur_value = True\n            else:\n                cur_value = False\n\n        if comparison == 'eq':\n            if cur_value != exp_value:\n                cls._handle_wrong_value(yaml_file, opt_name, exp_value,\n                                        cur_value, critical)\n\n        if comparison == 'gt':\n            if cur_value < exp_value:\n                cls._handle_wrong_value(yaml_file, opt_name, exp_value,\n                                        cur_value, critical)\n\n    @classmethod\n    def _handle_wrong_value(cls, yaml_file, option_name, exp_value,\n                            cur_value, critical):\n        msg = \"There is an issue with Ansible configuration in \" \\\n              \"{}. Expected value for the option '{}' is '{}', \" \\\n              \"current value is '{}'\".format(yaml_file, option_name,\n                                             exp_value, cur_value)\n        if critical:\n            raise IRValidatorException(msg)\n        else:\n            logger.warn(msg)\n\n    @classmethod\n    def _handle_missing_value(cls, yaml_file, section, option_name,\n                              exp_value, critical):\n        msg = \"There is an issue with Ansible configuration in\" \\\n              \" {}. Option '{}' with value of '{}' not found in\" \\\n              \" section '{}'\".format(yaml_file, option_name,\n                                     exp_value, section)\n        if critical:\n            raise IRValidatorException(msg)\n        else:\n            logger.warn(msg)\n\n    @staticmethod\n    def _convert_config_to_dict(config):\n        config_dict = {}\n        for section in config.sections():\n            if section not in config_dict:\n                config_dict[section] = {}\n\n            for option in config.options(section):\n                option_value = config.get(section, option)\n                try:\n                    option_value = int(option_value)\n                except ValueError:\n                    pass\n\n                config_dict[section][option] = option_value\n\n        return config_dict\n",
            "file_path": "infrared/core/utils/validators.py",
            "human_label": "Loads & validates that a YAML file has all required fields\n\n:param yaml_file: Path to YAML file\n:raise IRValidatorException: when mandatory data is missing in file\n:return: Dictionary with data loaded from a YAML file",
            "level": "class_runnable",
            "lineno": "12",
            "name": "validate_from_file",
            "oracle_context": "{ \"apis\" : \"['read', 'validate_from_content', 'isfile', 'format', 'open']\", \"classes\" : \"['IRValidatorException', 'os']\", \"vars\" : \"['Str', 'path']\" }",
            "package": "validators",
            "project": "redhat-openstack/infrared",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "6306092373426c38ae68acfa",
            "all_context": "{ \"import\" : \"string os collections six infrared \", \"file\" : \"LOG ; \", \"class\" : \"self.resolve_custom_types ; self.generate_answers_file(self,cli_args,spec_defaults) ; self.validate_min_max_args(self,args) ; self._merge_duplicated_cli_args ; self.validate_arg_sources ; self.vars ; self._iterate_received_arguments(self,args) ; self.get_deprecated_args(self) ; self._merge_duplicated_cli_args(self,cli_args) ; self.validate_choices_args(self,args) ; self._get_conditionally_required_args ; self.validate_arg_deprecation ; self.defaults ; self._get_conditionally_required_args(self,command_name,options_spec,args) ; self.parse_args(self,arg_parser,args) ; self.validate_arg_deprecation(self,cli_args,answer_file_args) ; self.resolve_custom_types(self,args) ; self.__init__(self,subparser,spec_dict,vars_dir,defaults_dir,plugin_path) ; self.validate_requires_args(self,args) ; self.create_complex_argumet_type(self,subcommand,type_name,option_name,spec_option) ; self.plugin_path ; self.get_spec_defaults(self) ; self.validate_arg_sources(cli_args,answer_file_args,spec_defaults) ; self.generate_answers_file ; self.create_complex_argumet_type ; self.parser ; self.get_answers_file_args(self,cli_args) ; self._convert_non_cli_args ; self.add_shared_groups(self,list_of_groups) ; self.parse_env_variable_from_file(value) ; self._get_defaults ; self.get_deprecated_args ; self.validate_requires_args ; self.get_answers_file_args ; self._convert_non_cli_args(self,parser_name,values_dict) ; self.validate_length_args ; self.spec_helper ; self.get_env_option(name) ; self.validate_length_args(self,args) ; self.validate_choices_args ; self.get_silent_args(self,args) ; self.get_spec_defaults ; self.get_nested_custom_and_control_args(self,args) ; self.from_plugin(cls,subparser,plugin,base_groups) ; self.get_silent_args ; self._get_defaults(self,default_getter_func) ; self.validate_min_max_args ; self.get_nested_custom_and_control_args ; self._iterate_received_arguments ; \" }",
            "code": "    def get_spec_defaults(self):\n        \"\"\"Resolve arguments' values from spec and other sources. \"\"\"\n\n        def spec_default_getter(option):\n            \"\"\"Getter function to retrieve the default value from spec.\n\n            :param option: argument name\n            \"\"\"\n\n            # first try to get environment variable with IR_ prefix\n            default_value = SpecParser.get_env_option(option['name'])\n            if default_value is not None:\n                LOG.info(\n                    \"[environ] Loading '{0}' default value\"\n                    \" '{1}' from the environment variable\".format(\n                        option['name'], default_value))\n            elif option.get('default', None) is not None:\n                default_value = option['default']\n            elif option.get('action', None) in ['store_true']:\n                default_value = False\n            return default_value\n\n        return self._get_defaults(spec_default_getter)\n",
            "dependency": "",
            "docstring": "Resolve arguments' values from spec and other sources.",
            "end_lineno": "111",
            "file_content": "import collections\nimport os\nfrom six.moves import configparser\nfrom string import Template\nimport yaml\n\nfrom infrared.core.cli.cli import CliParser\nfrom infrared.core.cli.cli import COMPLEX_TYPES\nfrom infrared.core.inspector import helper\nfrom infrared.core.utils import dict_utils\nfrom infrared.core.utils import exceptions\nfrom infrared.core.utils import logger\n\nLOG = logger.LOG\n\n\nclass SpecParser(object):\n    \"\"\"Parses input arguments from different sources (cli, answers file). \"\"\"\n\n    @classmethod\n    def from_plugin(cls, subparser, plugin, base_groups):\n        \"\"\"Reads spec & vars from plugin and constructs the parser instance\n\n        :param subparser: argparse.subparser to extend\n        :param plugin: InfraredPlugin object\n        :param base_groups: dict, included groups\n        :return: SpecParser object based on given plugin spec & vars\n        \"\"\"\n\n        spec_dict = base_groups or {}\n        with open(plugin.spec) as stream:\n            spec = yaml.safe_load(stream) or {}\n            dict_utils.dict_merge(\n                base_groups,\n                spec,\n                dict_utils.ConflictResolver.unique_append_list_resolver)\n\n        # The \"try-excpet\" block here is for adding spec file path if it\n        # includes an unsupported option type\n        try:\n            return SpecParser(subparser, spec_dict, plugin.vars_dir,\n                              plugin.defaults_dir, plugin.path)\n        except exceptions.IRUnsupportedSpecOptionType as ex:\n            ex.message += ' in file: {}'.format(plugin.spec)\n            raise ex\n\n    def __init__(self, subparser, spec_dict, vars_dir, defaults_dir,\n                 plugin_path):\n        \"\"\"Constructor.\n\n        :param subparser: argparse.subparser to extend\n        :param spec_dict: dict with CLI description\n        :param vars_dir: Path to plugin's vars dir\n        :param defaults_dir: Path to plugin's defaults dir\n        \"\"\"\n        self.vars = vars_dir\n        self.defaults = defaults_dir\n        self.plugin_path = plugin_path\n        self.spec_helper = helper.SpecDictHelper(spec_dict)\n\n        # create parser\n        self.parser = CliParser.create_parser(self, subparser)\n\n    def add_shared_groups(self, list_of_groups):\n        \"\"\"Adds the user defined shared groups\n\n        :param list_of_groups: list, of group dicts\n        \"\"\"\n        shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n        shared_groups.expand(list_of_groups)\n        self.spec_helper.spec_dict['shared_groups'] = shared_groups\n\n    def _get_defaults(self, default_getter_func):\n        \"\"\"Resolve arguments' values from cli or answers file.\n\n        :param default_getter_func: callable. will be called for all the\n            available options in spec file.\n        \"\"\"\n\n        result = collections.defaultdict(dict)\n        for parser, option in self.spec_helper.iterate_option_specs():\n            default_value = default_getter_func(option)\n            if default_value is not None:\n                sub = parser['name']\n                result[sub][option['name']] = default_value\n\n        return result\n\n    def get_spec_defaults(self):\n        \"\"\"Resolve arguments' values from spec and other sources. \"\"\"\n\n        def spec_default_getter(option):\n            \"\"\"Getter function to retrieve the default value from spec.\n\n            :param option: argument name\n            \"\"\"\n\n            # first try to get environment variable with IR_ prefix\n            default_value = SpecParser.get_env_option(option['name'])\n            if default_value is not None:\n                LOG.info(\n                    \"[environ] Loading '{0}' default value\"\n                    \" '{1}' from the environment variable\".format(\n                        option['name'], default_value))\n            elif option.get('default', None) is not None:\n                default_value = option['default']\n            elif option.get('action', None) in ['store_true']:\n                default_value = False\n            return default_value\n\n        return self._get_defaults(spec_default_getter)\n\n    @staticmethod\n    def get_env_option(name):\n        \"\"\"Try get \"\"\"\n        return os.environ.get('IR_' + name.upper().replace('-', '_'))\n\n    def get_deprecated_args(self):\n        \"\"\"Returning dict with options which deprecate others. \"\"\"\n\n        result = collections.defaultdict(dict)\n        for parser, option in self.spec_helper.iterate_option_specs():\n            if option.get('deprecates') is not None:\n                result[option.get('deprecates')] = option.get('name')\n\n        return result\n\n    @staticmethod\n    def parse_env_variable_from_file(value):\n        if isinstance(value, str):\n            t = Template(value)\n            try:\n                value = t.substitute(os.environ)\n            except KeyError as undefined_var:\n                raise exceptions.IRAnswersFileEnvVarNotDefined(undefined_var)\n        return value\n\n    def get_answers_file_args(self, cli_args):\n        \"\"\"Resolve arguments' values from answers INI file. \"\"\"\n\n        file_result = {}\n        args_to_remove = []\n        for (parser_name, parser_dict, arg_name, arg_value,\n             option_spec) in self._iterate_received_arguments(cli_args):\n            file_result[parser_name] = file_result.get(parser_name, {})\n            if option_spec and option_spec.get(\n                    'action', '') == 'read-answers':\n                # Iterate over arguments supplied by file\n                for parsed_arg in parser_dict[arg_name]:\n                    # Supplied arguments' value can be a list\n                    if isinstance(parser_dict[arg_name][parsed_arg], list):\n                        i = 0\n                        # Iterrate over argument values list\n                        for parsed_value in parser_dict[arg_name][parsed_arg]:\n                            parser_dict[arg_name][parsed_arg][i] = \\\n                                SpecParser.parse_env_variable_from_file(parsed_value)\n                            i += 1\n                    else:\n                        parser_dict[arg_name][parsed_arg] = \\\n                            SpecParser.parse_env_variable_from_file(parser_dict[arg_name][parsed_arg])\n                # we have config option. saving it.\n                self._convert_non_cli_args(\n                    parser_name, parser_dict[arg_name])\n                dict_utils.dict_merge(\n                    file_result[parser_name],\n                    parser_dict[arg_name])\n                # remove from cli args\n                args_to_remove.append((parser_name, arg_name))\n\n        # remove parser dict outside loop to avoid iteration dict modification\n        for parser_name, arg_name in args_to_remove:\n            for spec_parser in self.spec_helper.iterate_parsers():\n                if spec_parser['name'] in cli_args and spec_parser['name'] == parser_name:\n                    parser_dict = cli_args[spec_parser['name']]\n                    parser_dict.pop(arg_name)\n                    break\n\n        return file_result\n\n    def generate_answers_file(self, cli_args, spec_defaults):\n        \"\"\"Generates answers INI file\n\n        :param cli_args: list, cli arguments.\n        :param spec_defaults: the default values.\n        \"\"\"\n\n        def put_option(config, parser_name, option_name, value):\n            for opt_help in option.get('help', '').split('\\n'):\n                help_opt = '# ' + opt_help\n\n                # add help comment\n                if config.has_option(parser_name, help_opt):\n                    config.remove_option(parser_name, help_opt)\n                config.set(\n                    parser_name, help_opt)\n\n            if config.has_option(parser_name, option_name):\n                value = config.get(parser_name, option_name)\n                config.remove_option(parser_name, option_name)\n\n            config.set(\n                parser_name,\n                option_name,\n                str(value))\n\n        file_generated = False\n\n        # load generate answers file for all the parsers\n        for (parser_name, parser_dict, arg_name, arg_value,\n             option_spec) in self._iterate_received_arguments(cli_args):\n            if option_spec and option_spec.get(\n                    'action', '') == 'generate-answers':\n                options_to_save = \\\n                    self.spec_helper.get_parser_option_specs(parser_name)\n                out_answers = configparser.ConfigParser(allow_no_value=True)\n\n                if not out_answers.has_section(parser_name):\n                    out_answers.add_section(parser_name)\n\n                for option in options_to_save:\n                    opt_name = option['name']\n                    if opt_name in parser_dict:\n                        put_option(\n                            out_answers,\n                            parser_name,\n                            opt_name,\n                            parser_dict[opt_name])\n                    elif opt_name in spec_defaults[parser_name]:\n                        put_option(\n                            out_answers,\n                            parser_name,\n                            opt_name,\n                            spec_defaults[parser_name][opt_name])\n                    elif option.get('required', False):\n                        put_option(\n                            out_answers,\n                            parser_name,\n                            '# ' + opt_name,\n                            \"Required argument. \"\n                            \"Edit with one of the allowed values OR \"\n                            \"override with \"\n                            \"CLI: --{}=<option>\".format(opt_name))\n\n                # write to file\n                with open(arg_value, 'w') as answers_file:\n                    out_answers.write(answers_file)\n                file_generated = True\n\n        return file_generated\n\n    def resolve_custom_types(self, args):\n        \"\"\"Transforms the arguments with custom types\n\n        :param args: the list of received arguments.\n        \"\"\"\n        for parser_name, parser_dict in args.items():\n            spec_complex_options = [opt for opt in\n                                    self.spec_helper.get_parser_option_specs(\n                                        parser_name) if\n                                    opt.get('type', None) in COMPLEX_TYPES]\n            for spec_option in spec_complex_options:\n                option_name = spec_option['name']\n                if option_name in parser_dict:\n                    # we have custom type to resolve\n                    type_name = spec_option['type']\n                    option_value = parser_dict[option_name]\n                    action = self.create_complex_argumet_type(\n                        parser_name,\n                        type_name,\n                        option_name,\n                        spec_option)\n\n                    # resolving value\n                    parser_dict[option_name] = action.resolve(option_value)\n\n    def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n        \"\"\"Build the complex argument type\n\n        :param subcommand: the command name\n        :param type_name: the complex type name\n        :param option_name: the option name\n        :param spec_option: option's specifications\n        :return: the complex type instance\n        \"\"\"\n        complex_action = COMPLEX_TYPES.get(\n            type_name, None)\n        if complex_action is None:\n            raise exceptions.SpecParserException(\n                \"Unknown complex type: {}\".format(type_name))\n        return complex_action(\n            option_name,\n            (self.vars, self.defaults, self.plugin_path),\n            subcommand,\n            spec_option)\n\n    def parse_args(self, arg_parser, args=None):\n        \"\"\"Parses all the arguments (cli, answers file)\n\n        :return: None, if ``--generate-answers-file`` in arg_arg_parser\n        :return: (dict, dict):\n            * command arguments dict (arguments to control the IR logic)\n            * nested arguments dict (arguments to pass to the playbooks)\n        \"\"\"\n\n        spec_defaults = self.get_spec_defaults()\n        cli_args = CliParser.parse_cli_input(arg_parser, args)\n\n        file_args = self.get_answers_file_args(cli_args)\n\n        # generate answers file and exit\n        if self.generate_answers_file(cli_args, spec_defaults):\n            LOG.warning(\"Answers file generated. Exiting.\")\n\n        # print warnings when something was overridden from non-cli source.\n        self.validate_arg_sources(cli_args, file_args,\n                                  spec_defaults)\n\n        # print warnings for deprecated\n        self.validate_arg_deprecation(cli_args, file_args)\n\n        # now filter defaults to have only parser defined in cli\n        defaults = dict((key, spec_defaults[key])\n                        for key in cli_args.keys() if\n                        key in spec_defaults)\n\n        # copy cli args with the same name to all parser groups\n        self._merge_duplicated_cli_args(cli_args)\n        self._merge_duplicated_cli_args(file_args)\n\n        dict_utils.dict_merge(defaults, file_args)\n        dict_utils.dict_merge(defaults, cli_args)\n        self.validate_requires_args(defaults)\n        self.validate_length_args(defaults)\n        self.validate_choices_args(defaults)\n        self.validate_min_max_args(defaults)\n\n        # now resolve complex types.\n        self.resolve_custom_types(defaults)\n        nested, control, custom = \\\n            self.get_nested_custom_and_control_args(defaults)\n        return nested, control, custom\n\n    def validate_arg_deprecation(self, cli_args, answer_file_args):\n        \"\"\"Validates and prints the deprecated arguments.\n\n        :param cli_args: the dict of arguments from cli\n        :param answer_file_args:  the dict of arguments from files\n        \"\"\"\n\n        for deprecated, deprecates in self.get_deprecated_args().items():\n            for input_args in (answer_file_args.items(), cli_args.items()):\n                for command, command_dict in input_args:\n                    if deprecated in command_dict:\n                        if deprecates in command_dict:\n                            raise exceptions.IRDeprecationException(\n                                \"[{}] Argument '{}' deprecates '{}',\"\n                                \" please use only the new one.\".format(\n                                    command, deprecated, deprecates))\n\n                        if deprecated in answer_file_args[command]:\n                            answer_file_args[command][deprecates] = \\\n                                answer_file_args[command][deprecated]\n\n                        if deprecated in cli_args[command]:\n                            cli_args[command][deprecates] = \\\n                                cli_args[command][deprecated]\n\n                        LOG.warning(\n                            \"[{}] Argument '{}' was deprecated,\"\n                            \" please use '{}'.\".format(\n                                command, deprecated, deprecates))\n\n    @staticmethod\n    def validate_arg_sources(cli_args, answer_file_args, spec_defaults):\n        \"\"\"Validates and prints the arguments' source.\n\n        :param cli_args: the dict of arguments from cli\n        :param answer_file_args:  the dict of arguments from files\n        :param spec_defaults:  the default values from spec files\n        \"\"\"\n\n        def show_diff(diff, command_name, cmd_dict, source_name):\n            if diff:\n                for arg_name in diff:\n                    value = cmd_dict[arg_name]\n                    LOG.info(\n                        \"[{}] Argument '{}' was set to\"\n                        \" '{}' from the {} source.\".format(\n                            command_name, arg_name, value, source_name))\n\n        for command, command_dict in cli_args.items():\n            file_dict = answer_file_args.get(command, {})\n            file_diff = set(file_dict.keys()) - set(command_dict.keys())\n            show_diff(file_diff, command, file_dict, 'answers file')\n\n            def_dict = spec_defaults.get(command, {})\n            default_diff = set(def_dict.keys()) - set(\n                command_dict.keys()) - file_diff\n            show_diff(default_diff, command, def_dict, 'spec defaults')\n\n    def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n        \"\"\"List arguments with ``required_when`` condition matched.\n\n        :param command_name: the command name.\n        :param options_spec:  the list of command spec options.\n        :param args: the received input arguments\n        :return: list, list of argument names with matched ``required_when``\n            condition\n        \"\"\"\n        opts_names = [option_spec['name'] for option_spec in options_spec]\n        missing_args = []\n        for option_spec in options_spec:\n            option_results = []\n            if option_spec and 'required_when' in option_spec:\n                req_when_args = [option_spec['required_when']] \\\n                    if not type(option_spec['required_when']) is list \\\n                    else option_spec['required_when']\n\n                # validate conditions\n                for req_when_arg in req_when_args:\n                    splited_args_list = req_when_arg.split()\n                    for idx, req_arg in enumerate(splited_args_list):\n                        if req_arg in opts_names:\n                            splited_args_list[idx] = \\\n                                args.get(command_name, {}).get(req_arg.strip())\n                        if splited_args_list[idx] is None:\n                            option_results.append(False)\n                            break\n                        splited_args_list[idx] = str(splited_args_list[idx])\n                        if (splited_args_list[idx] not in ['and', 'or'] and\n                            not any(\n                                (c in '<>=') for c in splited_args_list[idx])):\n                            splited_args_list[idx] = \"'{0}'\".format(\n                                yaml.safe_load(splited_args_list[idx]))\n                    else:\n                        option_results.append(\n                            eval(' '.join(splited_args_list)))\n                if all(option_results) and \\\n                        self.spec_helper.get_option_state(\n                            command_name,\n                            option_spec['name'],\n                            args) == helper.OptionState['NOT_SET']:\n                    missing_args.append(option_spec['name'])\n        return missing_args\n\n    def validate_requires_args(self, args):\n        \"\"\"Check if all the required arguments have been provided. \"\"\"\n\n        silent_args = self.get_silent_args(args)\n\n        def validate_parser(parser_name, expected_options, parser_args):\n            \"\"\"Helper method to resolve dict_merge. \"\"\"\n\n            result = collections.defaultdict(list)\n            condition_req_args = self._get_conditionally_required_args(\n                parser_name, expected_options, args)\n\n            for option in expected_options:\n                name = option['name']\n\n                # check required options.\n                if (option.get('required', False) and\n                    name not in parser_args or\n                    option['name'] in condition_req_args) and \\\n                        name not in silent_args:\n                    result[parser_name].append(name)\n\n            return result\n\n        res = {}\n        for command_data in self.spec_helper.iterate_parsers():\n            cmd_name = command_data['name']\n            if cmd_name in args:\n                dict_utils.dict_merge(\n                    res,\n                    validate_parser(\n                        cmd_name,\n                        self.spec_helper.get_parser_option_specs(cmd_name),\n                        args[cmd_name]))\n\n        missing_args = dict((cmd_name, args)\n                            for cmd_name, args in res.items() if len(args) > 0)\n        if missing_args:\n            raise exceptions.IRRequiredArgsMissingException(missing_args)\n\n    def validate_length_args(self, args):\n        \"\"\"Check if value of arguments is not longer than length specified.\n\n        :param args: The received arguments.\n        \"\"\"\n        invalid_options = []\n        for parser_name, parser_dict in args.items():\n            for spec_option in \\\n                    self.spec_helper.get_parser_option_specs(parser_name):\n                if 'length' not in spec_option:\n                    # skip options that does not contain length\n                    continue\n                option_name = spec_option['name']\n                if option_name in parser_dict:\n                    # resolve length\n                    length = spec_option['length']\n                    option_value = parser_dict[option_name]\n                    if len(option_value) > int(length):\n                        # found invalid option, append to list of invalid opts\n                        invalid_options.append((\n                            option_name,\n                            option_value,\n                            length\n                        ))\n        if invalid_options:\n            # raise exception with all arguments that exceed length\n            raise exceptions.IRInvalidLengthException(invalid_options)\n\n    def validate_choices_args(self, args):\n        \"\"\"Check if value of choice arguments is one of the available choices.\n\n        :param args: The received arguments.\n        \"\"\"\n        invalid_options = []\n        for parser_name, parser_dict in args.items():\n            for spec_option in \\\n                    self.spec_helper.get_parser_option_specs(parser_name):\n                if 'choices' not in spec_option:\n                    # skip options that does not contain choices\n                    continue\n                option_name = spec_option['name']\n                if option_name in parser_dict:\n                    # resolve choices\n                    choices = spec_option['choices']\n                    option_value = parser_dict[option_name]\n                    if option_value not in choices:\n                        # found invalid option, append to list of invalid opts\n                        invalid_options.append((\n                            option_name,\n                            option_value,\n                            choices\n                        ))\n        if invalid_options:\n            # raise exception with all arguments that contains invalid choices\n            raise exceptions.IRInvalidChoiceException(invalid_options)\n\n    def validate_min_max_args(self, args):\n        \"\"\"Check if value of arguments is between minimum and maximum values.\n\n        :param args: The received arguments.\n        \"\"\"\n        invalid_options = []\n        for parser_name, parser_dict in args.items():\n            for spec_option in \\\n                    self.spec_helper.get_parser_option_specs(parser_name):\n                if all([key not in spec_option\n                        for key in ('maximum', 'minimum')]):\n                    # skip options that does not contain minimum or maximum\n                    continue\n                option_name = spec_option['name']\n\n                if option_name in parser_dict:\n                    option_value = parser_dict[option_name]\n                    min_value = spec_option.get('minimum')\n                    max_value = spec_option.get('maximum')\n                    # handle empty values in spec files which load as None\n                    min_value = '' if 'minimum' in spec_option \\\n                                      and min_value is None else min_value\n                    max_value = '' if 'maximum' in spec_option \\\n                                      and max_value is None else max_value\n\n                    values = {\n                        \"value\": option_value,\n                        \"maximum\": max_value,\n                        \"minimum\": min_value\n                    }\n\n                    # make sure that values are numbers\n                    is_all_values_numbers = True\n                    for name, num in values.items():\n                        if num is not None \\\n                                and (isinstance(num, bool) or\n                                     not isinstance(num, (int, float))):\n                            invalid_options.append((\n                                option_name,\n                                name,\n                                \"number\",\n                                type(num).__name__\n                            ))\n                            is_all_values_numbers = False\n\n                    if not is_all_values_numbers:\n                        # don't continue to min max checks since some of the\n                        # values are not numbers\n                        continue\n\n                    # check bigger than minimum\n                    if min_value is not None and option_value < min_value:\n                        invalid_options.append((\n                            option_name,\n                            \"minimum\",\n                            min_value,\n                            option_value\n                        ))\n                    # check smaller than maximum\n                    if max_value is not None and option_value > max_value:\n                        invalid_options.append((\n                            option_name,\n                            \"maximum\",\n                            max_value,\n                            option_value\n                        ))\n\n        if invalid_options:\n            # raise exception with all arguments that contains invalid choices\n            raise exceptions.IRInvalidMinMaxRangeException(invalid_options)\n\n    def get_silent_args(self, args):\n        \"\"\"list of silenced argument\n\n        :param args: The received arguments.\n        :return: list, slienced argument names\n        \"\"\"\n        silent_args_names = []\n        for (parser_name, parser_dict, arg_name, arg_value,\n             arg_spec) in self._iterate_received_arguments(args):\n            if arg_spec and 'silent' in arg_spec and \\\n                    self.spec_helper.get_option_state(\n                        parser_name,\n                        arg_name,\n                        args) == helper.OptionState['IS_SET']:\n                silent_args_names.extend(arg_spec['silent'])\n\n        return list(set(silent_args_names))\n\n    def get_nested_custom_and_control_args(self, args):\n        \"\"\"Split input arguments to control nested and custom.\n\n        Controls arguments: control the IR behavior. These arguments\n            will not be put into the spec yml file\n        Nested arguments: are used by the Ansible playbooks and will be put\n            into the spec yml file.\n        Custom arguments: Custom ansible variables to be used instead of the\n            normal nested usage.\n\n        :param args: the collected list of args.\n        :return: (dict, dict): flat dicts (control_args, nested_args)\n        \"\"\"\n        # returns flat dicts\n        nested = {}\n        control_args = {}\n        custom_args = {}\n        for (parser_name, parser_dict, arg_name, arg_value,\n             arg_spec) in self._iterate_received_arguments(args):\n            if all([arg_spec, arg_spec.get('type', None),\n                    arg_spec.get('type', None) in\n                    [ctype_name for ctype_name, klass in\n                     COMPLEX_TYPES.items() if klass.is_nested]\n                    ]) or ('is_shared_group_option' not in arg_spec):\n                if arg_name in nested:\n                    LOG.warning(\n                        \"Duplicated nested argument found:'{}'. \"\n                        \"Using old value: '{}'\".format(\n                            arg_name, nested[arg_name]))\n                elif arg_name in custom_args:\n                    LOG.warning(\n                        \"Duplicated custom argument found:'{}'. \"\n                        \"Using old value: '{}'\".format(\n                            arg_name, custom_args[arg_name]))\n                else:\n                    if \"ansible_variable\" in arg_spec:\n                        custom_args[arg_spec[\"ansible_variable\"]] = arg_value\n                    else:\n                        nested[arg_name] = arg_value\n            else:\n                if arg_name in control_args:\n                    LOG.warning(\n                        \"Duplicated control argument found: '{}'. Using \"\n                        \"old value: '{}'\".format(\n                            arg_name, control_args[arg_name]))\n                else:\n                    control_args[arg_name] = arg_value\n\n        return nested, control_args, custom_args\n\n    def _iterate_received_arguments(self, args):\n        \"\"\"Iterator helper method over all the received arguments\n\n        :return: yields tuple:\n            (spec name, spec dict,\n             argument name, argument value, argument spec)\n        \"\"\"\n        for spec_parser in self.spec_helper.iterate_parsers():\n            if spec_parser['name'] in args:\n                parser_dict = args[spec_parser['name']]\n                for arg_name, arg_val in parser_dict.items():\n                    arg_spec = self.spec_helper.get_option_spec(\n                        spec_parser['name'], arg_name)\n                    yield (spec_parser['name'], parser_dict,\n                           arg_name, arg_val, arg_spec)\n\n    def _convert_non_cli_args(self, parser_name, values_dict):\n        \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n        By default all the values are strings.\n\n        :param parser_name: The command name, e.g. main, virsh, ospd, etc\n        :param values_dict: The dict of with arguments\n       \"\"\"\n        for opt_name, opt_value in values_dict.items():\n            file_option_spec = self.spec_helper.get_option_spec(\n                parser_name, opt_name)\n            if file_option_spec.get('type', None) in ['int', ] or \\\n                    file_option_spec.get('action', None) in ['count', ]:\n                values_dict[opt_name] = int(opt_value)\n\n    def _merge_duplicated_cli_args(self, cli_args):\n        \"\"\"Merge duplicated arguments to all the parsers\n\n        This is need to handle control args, shared among several parsers.\n        for example, verbose, inventory\n        \"\"\"\n        for (parser_name, parser_dict, arg_name, arg_value,\n             arg_spec) in self._iterate_received_arguments(cli_args):\n            for parser_name2, parser_dict2 in cli_args.items():\n                if all([parser_name2, parser_name != parser_name2,\n                        arg_name not in parser_dict2]):\n                    if self.spec_helper.get_option_spec(parser_name2,\n                                                        arg_name):\n                        parser_dict2[arg_name] = arg_value\n",
            "file_path": "infrared/core/inspector/inspector.py",
            "human_label": "Resolve arguments' values from spec and other sources with self._get_defaults()",
            "level": "file_runnable",
            "lineno": "89",
            "name": "get_spec_defaults",
            "oracle_context": "{ \"apis\" : \"['format', 'info', 'get', 'get_env_option', '_get_defaults']\", \"classes\" : \"[]\", \"vars\" : \"['LOG', 'Str', 'SpecParser', 'option']\" }",
            "package": "inspector",
            "project": "redhat-openstack/infrared",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "6306092c73426c38ae68ad02",
            "all_context": "{ \"import\" : \"string os collections six infrared \", \"file\" : \"LOG ; \", \"class\" : \"self.resolve_custom_types ; self.generate_answers_file(self,cli_args,spec_defaults) ; self.validate_min_max_args(self,args) ; self._merge_duplicated_cli_args ; self.validate_arg_sources ; self.vars ; self._iterate_received_arguments(self,args) ; self.get_deprecated_args(self) ; self._merge_duplicated_cli_args(self,cli_args) ; self.validate_choices_args(self,args) ; self._get_conditionally_required_args ; self.validate_arg_deprecation ; self.defaults ; self._get_conditionally_required_args(self,command_name,options_spec,args) ; self.parse_args(self,arg_parser,args) ; self.validate_arg_deprecation(self,cli_args,answer_file_args) ; self.resolve_custom_types(self,args) ; self.__init__(self,subparser,spec_dict,vars_dir,defaults_dir,plugin_path) ; self.validate_requires_args(self,args) ; self.create_complex_argumet_type(self,subcommand,type_name,option_name,spec_option) ; self.plugin_path ; self.get_spec_defaults(self) ; self.validate_arg_sources(cli_args,answer_file_args,spec_defaults) ; self.generate_answers_file ; self.create_complex_argumet_type ; self.parser ; self.get_answers_file_args(self,cli_args) ; self._convert_non_cli_args ; self.add_shared_groups(self,list_of_groups) ; self.parse_env_variable_from_file(value) ; self._get_defaults ; self.get_deprecated_args ; self.validate_requires_args ; self.get_answers_file_args ; self._convert_non_cli_args(self,parser_name,values_dict) ; self.validate_length_args ; self.spec_helper ; self.get_env_option(name) ; self.validate_length_args(self,args) ; self.validate_choices_args ; self.get_silent_args(self,args) ; self.get_spec_defaults ; self.get_nested_custom_and_control_args(self,args) ; self.from_plugin(cls,subparser,plugin,base_groups) ; self.get_silent_args ; self._get_defaults(self,default_getter_func) ; self.validate_min_max_args ; self.get_nested_custom_and_control_args ; self._iterate_received_arguments ; \" }",
            "code": "    def validate_arg_deprecation(self, cli_args, answer_file_args):\n        \"\"\"Validates and prints the deprecated arguments.\n\n        :param cli_args: the dict of arguments from cli\n        :param answer_file_args:  the dict of arguments from files\n        \"\"\"\n\n        for deprecated, deprecates in self.get_deprecated_args().items():\n            for input_args in (answer_file_args.items(), cli_args.items()):\n                for command, command_dict in input_args:\n                    if deprecated in command_dict:\n                        if deprecates in command_dict:\n                            raise exceptions.IRDeprecationException(\n                                \"[{}] Argument '{}' deprecates '{}',\"\n                                \" please use only the new one.\".format(\n                                    command, deprecated, deprecates))\n\n                        if deprecated in answer_file_args[command]:\n                            answer_file_args[command][deprecates] = \\\n                                answer_file_args[command][deprecated]\n\n                        if deprecated in cli_args[command]:\n                            cli_args[command][deprecates] = \\\n                                cli_args[command][deprecated]\n\n                        LOG.warning(\n                            \"[{}] Argument '{}' was deprecated,\"\n                            \" please use '{}'.\".format(\n                                command, deprecated, deprecates))\n        return cli_args            \n",
            "dependency": "",
            "docstring": "Validates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files",
            "end_lineno": "372",
            "file_content": "import collections\nimport os\nfrom six.moves import configparser\nfrom string import Template\nimport yaml\n\nfrom infrared.core.cli.cli import CliParser\nfrom infrared.core.cli.cli import COMPLEX_TYPES\nfrom infrared.core.inspector import helper\nfrom infrared.core.utils import dict_utils\nfrom infrared.core.utils import exceptions\nfrom infrared.core.utils import logger\n\nLOG = logger.LOG\n\n\nclass SpecParser(object):\n    \"\"\"Parses input arguments from different sources (cli, answers file). \"\"\"\n\n    @classmethod\n    def from_plugin(cls, subparser, plugin, base_groups):\n        \"\"\"Reads spec & vars from plugin and constructs the parser instance\n\n        :param subparser: argparse.subparser to extend\n        :param plugin: InfraredPlugin object\n        :param base_groups: dict, included groups\n        :return: SpecParser object based on given plugin spec & vars\n        \"\"\"\n\n        spec_dict = base_groups or {}\n        with open(plugin.spec) as stream:\n            spec = yaml.safe_load(stream) or {}\n            dict_utils.dict_merge(\n                base_groups,\n                spec,\n                dict_utils.ConflictResolver.unique_append_list_resolver)\n\n        # The \"try-excpet\" block here is for adding spec file path if it\n        # includes an unsupported option type\n        try:\n            return SpecParser(subparser, spec_dict, plugin.vars_dir,\n                              plugin.defaults_dir, plugin.path)\n        except exceptions.IRUnsupportedSpecOptionType as ex:\n            ex.message += ' in file: {}'.format(plugin.spec)\n            raise ex\n\n    def __init__(self, subparser, spec_dict, vars_dir, defaults_dir,\n                 plugin_path):\n        \"\"\"Constructor.\n\n        :param subparser: argparse.subparser to extend\n        :param spec_dict: dict with CLI description\n        :param vars_dir: Path to plugin's vars dir\n        :param defaults_dir: Path to plugin's defaults dir\n        \"\"\"\n        self.vars = vars_dir\n        self.defaults = defaults_dir\n        self.plugin_path = plugin_path\n        self.spec_helper = helper.SpecDictHelper(spec_dict)\n\n        # create parser\n        self.parser = CliParser.create_parser(self, subparser)\n\n    def add_shared_groups(self, list_of_groups):\n        \"\"\"Adds the user defined shared groups\n\n        :param list_of_groups: list, of group dicts\n        \"\"\"\n        shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n        shared_groups.expand(list_of_groups)\n        self.spec_helper.spec_dict['shared_groups'] = shared_groups\n\n    def _get_defaults(self, default_getter_func):\n        \"\"\"Resolve arguments' values from cli or answers file.\n\n        :param default_getter_func: callable. will be called for all the\n            available options in spec file.\n        \"\"\"\n\n        result = collections.defaultdict(dict)\n        for parser, option in self.spec_helper.iterate_option_specs():\n            default_value = default_getter_func(option)\n            if default_value is not None:\n                sub = parser['name']\n                result[sub][option['name']] = default_value\n\n        return result\n\n    def get_spec_defaults(self):\n        \"\"\"Resolve arguments' values from spec and other sources. \"\"\"\n\n        def spec_default_getter(option):\n            \"\"\"Getter function to retrieve the default value from spec.\n\n            :param option: argument name\n            \"\"\"\n\n            # first try to get environment variable with IR_ prefix\n            default_value = SpecParser.get_env_option(option['name'])\n            if default_value is not None:\n                LOG.info(\n                    \"[environ] Loading '{0}' default value\"\n                    \" '{1}' from the environment variable\".format(\n                        option['name'], default_value))\n            elif option.get('default', None) is not None:\n                default_value = option['default']\n            elif option.get('action', None) in ['store_true']:\n                default_value = False\n            return default_value\n\n        return self._get_defaults(spec_default_getter)\n\n    @staticmethod\n    def get_env_option(name):\n        \"\"\"Try get \"\"\"\n        return os.environ.get('IR_' + name.upper().replace('-', '_'))\n\n    def get_deprecated_args(self):\n        \"\"\"Returning dict with options which deprecate others. \"\"\"\n\n        result = collections.defaultdict(dict)\n        for parser, option in self.spec_helper.iterate_option_specs():\n            if option.get('deprecates') is not None:\n                result[option.get('deprecates')] = option.get('name')\n\n        return result\n\n    @staticmethod\n    def parse_env_variable_from_file(value):\n        if isinstance(value, str):\n            t = Template(value)\n            try:\n                value = t.substitute(os.environ)\n            except KeyError as undefined_var:\n                raise exceptions.IRAnswersFileEnvVarNotDefined(undefined_var)\n        return value\n\n    def get_answers_file_args(self, cli_args):\n        \"\"\"Resolve arguments' values from answers INI file. \"\"\"\n\n        file_result = {}\n        args_to_remove = []\n        for (parser_name, parser_dict, arg_name, arg_value,\n             option_spec) in self._iterate_received_arguments(cli_args):\n            file_result[parser_name] = file_result.get(parser_name, {})\n            if option_spec and option_spec.get(\n                    'action', '') == 'read-answers':\n                # Iterate over arguments supplied by file\n                for parsed_arg in parser_dict[arg_name]:\n                    # Supplied arguments' value can be a list\n                    if isinstance(parser_dict[arg_name][parsed_arg], list):\n                        i = 0\n                        # Iterrate over argument values list\n                        for parsed_value in parser_dict[arg_name][parsed_arg]:\n                            parser_dict[arg_name][parsed_arg][i] = \\\n                                SpecParser.parse_env_variable_from_file(parsed_value)\n                            i += 1\n                    else:\n                        parser_dict[arg_name][parsed_arg] = \\\n                            SpecParser.parse_env_variable_from_file(parser_dict[arg_name][parsed_arg])\n                # we have config option. saving it.\n                self._convert_non_cli_args(\n                    parser_name, parser_dict[arg_name])\n                dict_utils.dict_merge(\n                    file_result[parser_name],\n                    parser_dict[arg_name])\n                # remove from cli args\n                args_to_remove.append((parser_name, arg_name))\n\n        # remove parser dict outside loop to avoid iteration dict modification\n        for parser_name, arg_name in args_to_remove:\n            for spec_parser in self.spec_helper.iterate_parsers():\n                if spec_parser['name'] in cli_args and spec_parser['name'] == parser_name:\n                    parser_dict = cli_args[spec_parser['name']]\n                    parser_dict.pop(arg_name)\n                    break\n\n        return file_result\n\n    def generate_answers_file(self, cli_args, spec_defaults):\n        \"\"\"Generates answers INI file\n\n        :param cli_args: list, cli arguments.\n        :param spec_defaults: the default values.\n        \"\"\"\n\n        def put_option(config, parser_name, option_name, value):\n            for opt_help in option.get('help', '').split('\\n'):\n                help_opt = '# ' + opt_help\n\n                # add help comment\n                if config.has_option(parser_name, help_opt):\n                    config.remove_option(parser_name, help_opt)\n                config.set(\n                    parser_name, help_opt)\n\n            if config.has_option(parser_name, option_name):\n                value = config.get(parser_name, option_name)\n                config.remove_option(parser_name, option_name)\n\n            config.set(\n                parser_name,\n                option_name,\n                str(value))\n\n        file_generated = False\n\n        # load generate answers file for all the parsers\n        for (parser_name, parser_dict, arg_name, arg_value,\n             option_spec) in self._iterate_received_arguments(cli_args):\n            if option_spec and option_spec.get(\n                    'action', '') == 'generate-answers':\n                options_to_save = \\\n                    self.spec_helper.get_parser_option_specs(parser_name)\n                out_answers = configparser.ConfigParser(allow_no_value=True)\n\n                if not out_answers.has_section(parser_name):\n                    out_answers.add_section(parser_name)\n\n                for option in options_to_save:\n                    opt_name = option['name']\n                    if opt_name in parser_dict:\n                        put_option(\n                            out_answers,\n                            parser_name,\n                            opt_name,\n                            parser_dict[opt_name])\n                    elif opt_name in spec_defaults[parser_name]:\n                        put_option(\n                            out_answers,\n                            parser_name,\n                            opt_name,\n                            spec_defaults[parser_name][opt_name])\n                    elif option.get('required', False):\n                        put_option(\n                            out_answers,\n                            parser_name,\n                            '# ' + opt_name,\n                            \"Required argument. \"\n                            \"Edit with one of the allowed values OR \"\n                            \"override with \"\n                            \"CLI: --{}=<option>\".format(opt_name))\n\n                # write to file\n                with open(arg_value, 'w') as answers_file:\n                    out_answers.write(answers_file)\n                file_generated = True\n\n        return file_generated\n\n    def resolve_custom_types(self, args):\n        \"\"\"Transforms the arguments with custom types\n\n        :param args: the list of received arguments.\n        \"\"\"\n        for parser_name, parser_dict in args.items():\n            spec_complex_options = [opt for opt in\n                                    self.spec_helper.get_parser_option_specs(\n                                        parser_name) if\n                                    opt.get('type', None) in COMPLEX_TYPES]\n            for spec_option in spec_complex_options:\n                option_name = spec_option['name']\n                if option_name in parser_dict:\n                    # we have custom type to resolve\n                    type_name = spec_option['type']\n                    option_value = parser_dict[option_name]\n                    action = self.create_complex_argumet_type(\n                        parser_name,\n                        type_name,\n                        option_name,\n                        spec_option)\n\n                    # resolving value\n                    parser_dict[option_name] = action.resolve(option_value)\n\n    def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n        \"\"\"Build the complex argument type\n\n        :param subcommand: the command name\n        :param type_name: the complex type name\n        :param option_name: the option name\n        :param spec_option: option's specifications\n        :return: the complex type instance\n        \"\"\"\n        complex_action = COMPLEX_TYPES.get(\n            type_name, None)\n        if complex_action is None:\n            raise exceptions.SpecParserException(\n                \"Unknown complex type: {}\".format(type_name))\n        return complex_action(\n            option_name,\n            (self.vars, self.defaults, self.plugin_path),\n            subcommand,\n            spec_option)\n\n    def parse_args(self, arg_parser, args=None):\n        \"\"\"Parses all the arguments (cli, answers file)\n\n        :return: None, if ``--generate-answers-file`` in arg_arg_parser\n        :return: (dict, dict):\n            * command arguments dict (arguments to control the IR logic)\n            * nested arguments dict (arguments to pass to the playbooks)\n        \"\"\"\n\n        spec_defaults = self.get_spec_defaults()\n        cli_args = CliParser.parse_cli_input(arg_parser, args)\n\n        file_args = self.get_answers_file_args(cli_args)\n\n        # generate answers file and exit\n        if self.generate_answers_file(cli_args, spec_defaults):\n            LOG.warning(\"Answers file generated. Exiting.\")\n\n        # print warnings when something was overridden from non-cli source.\n        self.validate_arg_sources(cli_args, file_args,\n                                  spec_defaults)\n\n        # print warnings for deprecated\n        self.validate_arg_deprecation(cli_args, file_args)\n\n        # now filter defaults to have only parser defined in cli\n        defaults = dict((key, spec_defaults[key])\n                        for key in cli_args.keys() if\n                        key in spec_defaults)\n\n        # copy cli args with the same name to all parser groups\n        self._merge_duplicated_cli_args(cli_args)\n        self._merge_duplicated_cli_args(file_args)\n\n        dict_utils.dict_merge(defaults, file_args)\n        dict_utils.dict_merge(defaults, cli_args)\n        self.validate_requires_args(defaults)\n        self.validate_length_args(defaults)\n        self.validate_choices_args(defaults)\n        self.validate_min_max_args(defaults)\n\n        # now resolve complex types.\n        self.resolve_custom_types(defaults)\n        nested, control, custom = \\\n            self.get_nested_custom_and_control_args(defaults)\n        return nested, control, custom\n\n    def validate_arg_deprecation(self, cli_args, answer_file_args):\n        \"\"\"Validates and prints the deprecated arguments.\n\n        :param cli_args: the dict of arguments from cli\n        :param answer_file_args:  the dict of arguments from files\n        \"\"\"\n\n        for deprecated, deprecates in self.get_deprecated_args().items():\n            for input_args in (answer_file_args.items(), cli_args.items()):\n                for command, command_dict in input_args:\n                    if deprecated in command_dict:\n                        if deprecates in command_dict:\n                            raise exceptions.IRDeprecationException(\n                                \"[{}] Argument '{}' deprecates '{}',\"\n                                \" please use only the new one.\".format(\n                                    command, deprecated, deprecates))\n\n                        if deprecated in answer_file_args[command]:\n                            answer_file_args[command][deprecates] = \\\n                                answer_file_args[command][deprecated]\n\n                        if deprecated in cli_args[command]:\n                            cli_args[command][deprecates] = \\\n                                cli_args[command][deprecated]\n\n                        LOG.warning(\n                            \"[{}] Argument '{}' was deprecated,\"\n                            \" please use '{}'.\".format(\n                                command, deprecated, deprecates))\n\n    @staticmethod\n    def validate_arg_sources(cli_args, answer_file_args, spec_defaults):\n        \"\"\"Validates and prints the arguments' source.\n\n        :param cli_args: the dict of arguments from cli\n        :param answer_file_args:  the dict of arguments from files\n        :param spec_defaults:  the default values from spec files\n        \"\"\"\n\n        def show_diff(diff, command_name, cmd_dict, source_name):\n            if diff:\n                for arg_name in diff:\n                    value = cmd_dict[arg_name]\n                    LOG.info(\n                        \"[{}] Argument '{}' was set to\"\n                        \" '{}' from the {} source.\".format(\n                            command_name, arg_name, value, source_name))\n\n        for command, command_dict in cli_args.items():\n            file_dict = answer_file_args.get(command, {})\n            file_diff = set(file_dict.keys()) - set(command_dict.keys())\n            show_diff(file_diff, command, file_dict, 'answers file')\n\n            def_dict = spec_defaults.get(command, {})\n            default_diff = set(def_dict.keys()) - set(\n                command_dict.keys()) - file_diff\n            show_diff(default_diff, command, def_dict, 'spec defaults')\n\n    def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n        \"\"\"List arguments with ``required_when`` condition matched.\n\n        :param command_name: the command name.\n        :param options_spec:  the list of command spec options.\n        :param args: the received input arguments\n        :return: list, list of argument names with matched ``required_when``\n            condition\n        \"\"\"\n        opts_names = [option_spec['name'] for option_spec in options_spec]\n        missing_args = []\n        for option_spec in options_spec:\n            option_results = []\n            if option_spec and 'required_when' in option_spec:\n                req_when_args = [option_spec['required_when']] \\\n                    if not type(option_spec['required_when']) is list \\\n                    else option_spec['required_when']\n\n                # validate conditions\n                for req_when_arg in req_when_args:\n                    splited_args_list = req_when_arg.split()\n                    for idx, req_arg in enumerate(splited_args_list):\n                        if req_arg in opts_names:\n                            splited_args_list[idx] = \\\n                                args.get(command_name, {}).get(req_arg.strip())\n                        if splited_args_list[idx] is None:\n                            option_results.append(False)\n                            break\n                        splited_args_list[idx] = str(splited_args_list[idx])\n                        if (splited_args_list[idx] not in ['and', 'or'] and\n                            not any(\n                                (c in '<>=') for c in splited_args_list[idx])):\n                            splited_args_list[idx] = \"'{0}'\".format(\n                                yaml.safe_load(splited_args_list[idx]))\n                    else:\n                        option_results.append(\n                            eval(' '.join(splited_args_list)))\n                if all(option_results) and \\\n                        self.spec_helper.get_option_state(\n                            command_name,\n                            option_spec['name'],\n                            args) == helper.OptionState['NOT_SET']:\n                    missing_args.append(option_spec['name'])\n        return missing_args\n\n    def validate_requires_args(self, args):\n        \"\"\"Check if all the required arguments have been provided. \"\"\"\n\n        silent_args = self.get_silent_args(args)\n\n        def validate_parser(parser_name, expected_options, parser_args):\n            \"\"\"Helper method to resolve dict_merge. \"\"\"\n\n            result = collections.defaultdict(list)\n            condition_req_args = self._get_conditionally_required_args(\n                parser_name, expected_options, args)\n\n            for option in expected_options:\n                name = option['name']\n\n                # check required options.\n                if (option.get('required', False) and\n                    name not in parser_args or\n                    option['name'] in condition_req_args) and \\\n                        name not in silent_args:\n                    result[parser_name].append(name)\n\n            return result\n\n        res = {}\n        for command_data in self.spec_helper.iterate_parsers():\n            cmd_name = command_data['name']\n            if cmd_name in args:\n                dict_utils.dict_merge(\n                    res,\n                    validate_parser(\n                        cmd_name,\n                        self.spec_helper.get_parser_option_specs(cmd_name),\n                        args[cmd_name]))\n\n        missing_args = dict((cmd_name, args)\n                            for cmd_name, args in res.items() if len(args) > 0)\n        if missing_args:\n            raise exceptions.IRRequiredArgsMissingException(missing_args)\n\n    def validate_length_args(self, args):\n        \"\"\"Check if value of arguments is not longer than length specified.\n\n        :param args: The received arguments.\n        \"\"\"\n        invalid_options = []\n        for parser_name, parser_dict in args.items():\n            for spec_option in \\\n                    self.spec_helper.get_parser_option_specs(parser_name):\n                if 'length' not in spec_option:\n                    # skip options that does not contain length\n                    continue\n                option_name = spec_option['name']\n                if option_name in parser_dict:\n                    # resolve length\n                    length = spec_option['length']\n                    option_value = parser_dict[option_name]\n                    if len(option_value) > int(length):\n                        # found invalid option, append to list of invalid opts\n                        invalid_options.append((\n                            option_name,\n                            option_value,\n                            length\n                        ))\n        if invalid_options:\n            # raise exception with all arguments that exceed length\n            raise exceptions.IRInvalidLengthException(invalid_options)\n\n    def validate_choices_args(self, args):\n        \"\"\"Check if value of choice arguments is one of the available choices.\n\n        :param args: The received arguments.\n        \"\"\"\n        invalid_options = []\n        for parser_name, parser_dict in args.items():\n            for spec_option in \\\n                    self.spec_helper.get_parser_option_specs(parser_name):\n                if 'choices' not in spec_option:\n                    # skip options that does not contain choices\n                    continue\n                option_name = spec_option['name']\n                if option_name in parser_dict:\n                    # resolve choices\n                    choices = spec_option['choices']\n                    option_value = parser_dict[option_name]\n                    if option_value not in choices:\n                        # found invalid option, append to list of invalid opts\n                        invalid_options.append((\n                            option_name,\n                            option_value,\n                            choices\n                        ))\n        if invalid_options:\n            # raise exception with all arguments that contains invalid choices\n            raise exceptions.IRInvalidChoiceException(invalid_options)\n\n    def validate_min_max_args(self, args):\n        \"\"\"Check if value of arguments is between minimum and maximum values.\n\n        :param args: The received arguments.\n        \"\"\"\n        invalid_options = []\n        for parser_name, parser_dict in args.items():\n            for spec_option in \\\n                    self.spec_helper.get_parser_option_specs(parser_name):\n                if all([key not in spec_option\n                        for key in ('maximum', 'minimum')]):\n                    # skip options that does not contain minimum or maximum\n                    continue\n                option_name = spec_option['name']\n\n                if option_name in parser_dict:\n                    option_value = parser_dict[option_name]\n                    min_value = spec_option.get('minimum')\n                    max_value = spec_option.get('maximum')\n                    # handle empty values in spec files which load as None\n                    min_value = '' if 'minimum' in spec_option \\\n                                      and min_value is None else min_value\n                    max_value = '' if 'maximum' in spec_option \\\n                                      and max_value is None else max_value\n\n                    values = {\n                        \"value\": option_value,\n                        \"maximum\": max_value,\n                        \"minimum\": min_value\n                    }\n\n                    # make sure that values are numbers\n                    is_all_values_numbers = True\n                    for name, num in values.items():\n                        if num is not None \\\n                                and (isinstance(num, bool) or\n                                     not isinstance(num, (int, float))):\n                            invalid_options.append((\n                                option_name,\n                                name,\n                                \"number\",\n                                type(num).__name__\n                            ))\n                            is_all_values_numbers = False\n\n                    if not is_all_values_numbers:\n                        # don't continue to min max checks since some of the\n                        # values are not numbers\n                        continue\n\n                    # check bigger than minimum\n                    if min_value is not None and option_value < min_value:\n                        invalid_options.append((\n                            option_name,\n                            \"minimum\",\n                            min_value,\n                            option_value\n                        ))\n                    # check smaller than maximum\n                    if max_value is not None and option_value > max_value:\n                        invalid_options.append((\n                            option_name,\n                            \"maximum\",\n                            max_value,\n                            option_value\n                        ))\n\n        if invalid_options:\n            # raise exception with all arguments that contains invalid choices\n            raise exceptions.IRInvalidMinMaxRangeException(invalid_options)\n\n    def get_silent_args(self, args):\n        \"\"\"list of silenced argument\n\n        :param args: The received arguments.\n        :return: list, slienced argument names\n        \"\"\"\n        silent_args_names = []\n        for (parser_name, parser_dict, arg_name, arg_value,\n             arg_spec) in self._iterate_received_arguments(args):\n            if arg_spec and 'silent' in arg_spec and \\\n                    self.spec_helper.get_option_state(\n                        parser_name,\n                        arg_name,\n                        args) == helper.OptionState['IS_SET']:\n                silent_args_names.extend(arg_spec['silent'])\n\n        return list(set(silent_args_names))\n\n    def get_nested_custom_and_control_args(self, args):\n        \"\"\"Split input arguments to control nested and custom.\n\n        Controls arguments: control the IR behavior. These arguments\n            will not be put into the spec yml file\n        Nested arguments: are used by the Ansible playbooks and will be put\n            into the spec yml file.\n        Custom arguments: Custom ansible variables to be used instead of the\n            normal nested usage.\n\n        :param args: the collected list of args.\n        :return: (dict, dict): flat dicts (control_args, nested_args)\n        \"\"\"\n        # returns flat dicts\n        nested = {}\n        control_args = {}\n        custom_args = {}\n        for (parser_name, parser_dict, arg_name, arg_value,\n             arg_spec) in self._iterate_received_arguments(args):\n            if all([arg_spec, arg_spec.get('type', None),\n                    arg_spec.get('type', None) in\n                    [ctype_name for ctype_name, klass in\n                     COMPLEX_TYPES.items() if klass.is_nested]\n                    ]) or ('is_shared_group_option' not in arg_spec):\n                if arg_name in nested:\n                    LOG.warning(\n                        \"Duplicated nested argument found:'{}'. \"\n                        \"Using old value: '{}'\".format(\n                            arg_name, nested[arg_name]))\n                elif arg_name in custom_args:\n                    LOG.warning(\n                        \"Duplicated custom argument found:'{}'. \"\n                        \"Using old value: '{}'\".format(\n                            arg_name, custom_args[arg_name]))\n                else:\n                    if \"ansible_variable\" in arg_spec:\n                        custom_args[arg_spec[\"ansible_variable\"]] = arg_value\n                    else:\n                        nested[arg_name] = arg_value\n            else:\n                if arg_name in control_args:\n                    LOG.warning(\n                        \"Duplicated control argument found: '{}'. Using \"\n                        \"old value: '{}'\".format(\n                            arg_name, control_args[arg_name]))\n                else:\n                    control_args[arg_name] = arg_value\n\n        return nested, control_args, custom_args\n\n    def _iterate_received_arguments(self, args):\n        \"\"\"Iterator helper method over all the received arguments\n\n        :return: yields tuple:\n            (spec name, spec dict,\n             argument name, argument value, argument spec)\n        \"\"\"\n        for spec_parser in self.spec_helper.iterate_parsers():\n            if spec_parser['name'] in args:\n                parser_dict = args[spec_parser['name']]\n                for arg_name, arg_val in parser_dict.items():\n                    arg_spec = self.spec_helper.get_option_spec(\n                        spec_parser['name'], arg_name)\n                    yield (spec_parser['name'], parser_dict,\n                           arg_name, arg_val, arg_spec)\n\n    def _convert_non_cli_args(self, parser_name, values_dict):\n        \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n        By default all the values are strings.\n\n        :param parser_name: The command name, e.g. main, virsh, ospd, etc\n        :param values_dict: The dict of with arguments\n       \"\"\"\n        for opt_name, opt_value in values_dict.items():\n            file_option_spec = self.spec_helper.get_option_spec(\n                parser_name, opt_name)\n            if file_option_spec.get('type', None) in ['int', ] or \\\n                    file_option_spec.get('action', None) in ['count', ]:\n                values_dict[opt_name] = int(opt_value)\n\n    def _merge_duplicated_cli_args(self, cli_args):\n        \"\"\"Merge duplicated arguments to all the parsers\n\n        This is need to handle control args, shared among several parsers.\n        for example, verbose, inventory\n        \"\"\"\n        for (parser_name, parser_dict, arg_name, arg_value,\n             arg_spec) in self._iterate_received_arguments(cli_args):\n            for parser_name2, parser_dict2 in cli_args.items():\n                if all([parser_name2, parser_name != parser_name2,\n                        arg_name not in parser_dict2]):\n                    if self.spec_helper.get_option_spec(parser_name2,\n                                                        arg_name):\n                        parser_dict2[arg_name] = arg_value\n",
            "file_path": "infrared/core/inspector/inspector.py",
            "human_label": "Validates and prints the deprecated arguments.\n\n:param cli_args: the dict of arguments from cli\n:param answer_file_args:  the dict of arguments from files",
            "level": "file_runnable",
            "lineno": "344",
            "name": "validate_arg_deprecation",
            "oracle_context": "{ \"apis\" : \"['warning', 'items', 'format', 'get_deprecated_args', 'IRDeprecationException']\", \"classes\" : \"['exceptions']\", \"vars\" : \"['LOG', 'Str']\" }",
            "package": "inspector",
            "project": "redhat-openstack/infrared",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "6306092d73426c38ae68ad04",
            "all_context": "{ \"import\" : \"copy infrared \", \"file\" : \"\", \"class\" : \"self.get_option_spec(self,command_name,argument_name) ; self.get_parser_option_specs ; self._include_groups(self,parser_dict) ; self.iterate_parsers ; self._include_groups ; self._get_all_options_spec ; self.__init__(self,spec_dict) ; self.get_option_state(self,command_name,option_name,args) ; self.get_parser_option_specs(self,command_name) ; self._get_all_options_spec(parser_dict) ; self.spec_dict ; self.iterate_parsers(self) ; self.iterate_option_specs(self) ; self.get_option_spec ; \" }",
            "code": "    def get_parser_option_specs(self, command_name):\n        \"\"\"Gets all the options for the specified command\n\n        :param command_name: the command name (main, virsh, ospd, etc...)\n        :return: the list of all command options\n        \"\"\"\n        options = []\n        for parser in self.iterate_parsers():\n            if parser['name'] == command_name:\n                options = self._get_all_options_spec(parser)\n                break\n        return options\n",
            "dependency": "",
            "docstring": "Gets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options",
            "end_lineno": "65",
            "file_content": "from copy import deepcopy\nfrom infrared.core.utils.exceptions import SpecParserException\n\nOptionState = dict(\n    UNRECOGNIZED='unrecognized',\n    IS_SET='is set',\n    NOT_SET='is no set'\n)\n\n\nclass SpecDictHelper(object):\n    \"\"\"Controls the spec dicts and provides useful methods to get spec info.\"\"\"\n\n    def __init__(self, spec_dict):\n        self.spec_dict = spec_dict\n        # make structure of the dict flat\n        # 1. handle include_groups directive in main parser\n        parser_dict = self.spec_dict\n        self._include_groups(parser_dict)\n        # 2. Include groups for all subparsers\n        for subparser_name, subparser_dict in parser_dict.get(\n                'subparsers', {}).items():\n            self._include_groups(subparser_dict)\n\n    def iterate_parsers(self):\n        \"\"\"Iterates over the main parsers and subparsers. \"\"\"\n\n        for subparser_name, subparser_dict in self.spec_dict.get(\n                'subparsers', {}).items():\n            yield dict(name=subparser_name, **subparser_dict)\n\n    def iterate_option_specs(self):\n        \"\"\"Iterates over all the option specs.\n\n        Returns pair of parser and option on every iteration.\n        \"\"\"\n        for parser in self.iterate_parsers():\n            for spec_option in self._get_all_options_spec(parser):\n                yield parser, spec_option\n\n    @staticmethod\n    def _get_all_options_spec(parser_dict):\n        \"\"\"Gets all the options specification as the list of dicts. \"\"\"\n        result = []\n        for group in parser_dict.get('groups', []):\n            for option_name, option_dict in group.get('options', {}).items():\n                result.append(dict(name=option_name, **option_dict))\n\n        for option_name, option_dict in parser_dict.get('options', {}).items():\n            result.append(dict(name=option_name, **option_dict))\n\n        return result\n\n    def get_parser_option_specs(self, command_name):\n        \"\"\"Gets all the options for the specified command\n\n        :param command_name: the command name (main, virsh, ospd, etc...)\n        :return: the list of all command options\n        \"\"\"\n        options = []\n        for parser in self.iterate_parsers():\n            if parser['name'] == command_name:\n                options = self._get_all_options_spec(parser)\n                break\n        return options\n\n    def get_option_spec(self, command_name, argument_name):\n        \"\"\"Gets the specification for the specified option name. \"\"\"\n\n        options = self.get_parser_option_specs(command_name)\n        return next((opt for opt in options\n                     if opt['name'] == argument_name), {})\n\n    def get_option_state(self, command_name, option_name, args):\n        \"\"\"Gets the option state.\n\n        :param command_name: The command name\n        :param option_name: The option name to analyze\n        :param args: The received arguments.\n        \"\"\"\n        option_spec = self.get_option_spec(command_name, option_name)\n\n        if not option_spec:\n            res = OptionState['UNRECOGNIZED']\n\n        elif option_name not in args.get(command_name, {}):\n            res = OptionState['NOT_SET']\n        else:\n            option_value = args[command_name][option_name]\n            if option_spec.get('action', '') in ['store_true'] \\\n                    and option_value is False:\n                res = OptionState['NOT_SET']\n            else:\n                res = OptionState['IS_SET']\n\n        return res\n\n    def _include_groups(self, parser_dict):\n        \"\"\"Resolves the include dict directive in the spec files. \"\"\"\n        for group in parser_dict.get('include_groups', []):\n            # ensure we have that group\n            grp_dict = next(\n                (grp for grp in self.spec_dict.get('shared_groups', [])\n                 if grp['title'] == group),\n                None)\n            if grp_dict is None:\n                raise SpecParserException(\n                    \"Unable to include group '{}' in '{}' parser. \"\n                    \"Group was not found!\".format(\n                        group,\n                        parser_dict['name']))\n\n            for option in grp_dict.get('options', {}).values():\n                option['is_shared_group_option'] = True\n\n            parser_groups_list = parser_dict.get('groups', [])\n            parser_groups_list.append(deepcopy(grp_dict))\n            parser_dict['groups'] = parser_groups_list\n",
            "file_path": "infrared/core/inspector/helper.py",
            "human_label": "Gets all the options for the specified command\n\n:param command_name: the command name (main, virsh, ospd, etc...)\n:return: the list of all command options",
            "level": "class_runnable",
            "lineno": "54",
            "name": "get_parser_option_specs",
            "oracle_context": "{ \"apis\" : \"['iterate_parsers', '_get_all_options_spec']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }",
            "package": "helper",
            "project": "redhat-openstack/infrared",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "6306092d73426c38ae68ad05",
            "all_context": "{ \"import\" : \"copy infrared \", \"file\" : \"\", \"class\" : \"self.get_option_spec(self,command_name,argument_name) ; self.get_parser_option_specs ; self._include_groups(self,parser_dict) ; self.iterate_parsers ; self._include_groups ; self._get_all_options_spec ; self.__init__(self,spec_dict) ; self.get_option_state(self,command_name,option_name,args) ; self.get_parser_option_specs(self,command_name) ; self._get_all_options_spec(parser_dict) ; self.spec_dict ; self.iterate_parsers(self) ; self.iterate_option_specs(self) ; self.get_option_spec ; \" }",
            "code": "    def get_option_spec(self, command_name, argument_name):\n        \"\"\"Gets the specification for the specified option name. \"\"\"\n\n        options = self.get_parser_option_specs(command_name)\n        return next((opt for opt in options\n                     if opt['name'] == argument_name), {})\n",
            "dependency": "",
            "docstring": "Gets the specification for the specified option name.",
            "end_lineno": "72",
            "file_content": "from copy import deepcopy\nfrom infrared.core.utils.exceptions import SpecParserException\n\nOptionState = dict(\n    UNRECOGNIZED='unrecognized',\n    IS_SET='is set',\n    NOT_SET='is no set'\n)\n\n\nclass SpecDictHelper(object):\n    \"\"\"Controls the spec dicts and provides useful methods to get spec info.\"\"\"\n\n    def __init__(self, spec_dict):\n        self.spec_dict = spec_dict\n        # make structure of the dict flat\n        # 1. handle include_groups directive in main parser\n        parser_dict = self.spec_dict\n        self._include_groups(parser_dict)\n        # 2. Include groups for all subparsers\n        for subparser_name, subparser_dict in parser_dict.get(\n                'subparsers', {}).items():\n            self._include_groups(subparser_dict)\n\n    def iterate_parsers(self):\n        \"\"\"Iterates over the main parsers and subparsers. \"\"\"\n\n        for subparser_name, subparser_dict in self.spec_dict.get(\n                'subparsers', {}).items():\n            yield dict(name=subparser_name, **subparser_dict)\n\n    def iterate_option_specs(self):\n        \"\"\"Iterates over all the option specs.\n\n        Returns pair of parser and option on every iteration.\n        \"\"\"\n        for parser in self.iterate_parsers():\n            for spec_option in self._get_all_options_spec(parser):\n                yield parser, spec_option\n\n    @staticmethod\n    def _get_all_options_spec(parser_dict):\n        \"\"\"Gets all the options specification as the list of dicts. \"\"\"\n        result = []\n        for group in parser_dict.get('groups', []):\n            for option_name, option_dict in group.get('options', {}).items():\n                result.append(dict(name=option_name, **option_dict))\n\n        for option_name, option_dict in parser_dict.get('options', {}).items():\n            result.append(dict(name=option_name, **option_dict))\n\n        return result\n\n    def get_parser_option_specs(self, command_name):\n        \"\"\"Gets all the options for the specified command\n\n        :param command_name: the command name (main, virsh, ospd, etc...)\n        :return: the list of all command options\n        \"\"\"\n        options = []\n        for parser in self.iterate_parsers():\n            if parser['name'] == command_name:\n                options = self._get_all_options_spec(parser)\n                break\n        return options\n\n    def get_option_spec(self, command_name, argument_name):\n        \"\"\"Gets the specification for the specified option name. \"\"\"\n\n        options = self.get_parser_option_specs(command_name)\n        return next((opt for opt in options\n                     if opt['name'] == argument_name), {})\n\n    def get_option_state(self, command_name, option_name, args):\n        \"\"\"Gets the option state.\n\n        :param command_name: The command name\n        :param option_name: The option name to analyze\n        :param args: The received arguments.\n        \"\"\"\n        option_spec = self.get_option_spec(command_name, option_name)\n\n        if not option_spec:\n            res = OptionState['UNRECOGNIZED']\n\n        elif option_name not in args.get(command_name, {}):\n            res = OptionState['NOT_SET']\n        else:\n            option_value = args[command_name][option_name]\n            if option_spec.get('action', '') in ['store_true'] \\\n                    and option_value is False:\n                res = OptionState['NOT_SET']\n            else:\n                res = OptionState['IS_SET']\n\n        return res\n\n    def _include_groups(self, parser_dict):\n        \"\"\"Resolves the include dict directive in the spec files. \"\"\"\n        for group in parser_dict.get('include_groups', []):\n            # ensure we have that group\n            grp_dict = next(\n                (grp for grp in self.spec_dict.get('shared_groups', [])\n                 if grp['title'] == group),\n                None)\n            if grp_dict is None:\n                raise SpecParserException(\n                    \"Unable to include group '{}' in '{}' parser. \"\n                    \"Group was not found!\".format(\n                        group,\n                        parser_dict['name']))\n\n            for option in grp_dict.get('options', {}).values():\n                option['is_shared_group_option'] = True\n\n            parser_groups_list = parser_dict.get('groups', [])\n            parser_groups_list.append(deepcopy(grp_dict))\n            parser_dict['groups'] = parser_groups_list\n",
            "file_path": "infrared/core/inspector/helper.py",
            "human_label": "Gets the specification for the specified option name. Get options of the given command_name with self.get_parser_option_specs()",
            "level": "class_runnable",
            "lineno": "67",
            "name": "get_option_spec",
            "oracle_context": "{ \"apis\" : \"['get_parser_option_specs', 'next']\", \"classes\" : \"[]\", \"vars\" : \"[]\" }",
            "package": "helper",
            "project": "redhat-openstack/infrared",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "6306092d73426c38ae68ad06",
            "all_context": "{ \"import\" : \"string os collections six infrared \", \"file\" : \"\", \"class\" : \"self.resolve_custom_types ; self.generate_answers_file(self,cli_args,spec_defaults) ; self.validate_min_max_args(self,args) ; self._merge_duplicated_cli_args ; self.validate_arg_sources ; self.vars ; self._iterate_received_arguments(self,args) ; self.get_deprecated_args(self) ; self._merge_duplicated_cli_args(self,cli_args) ; self.validate_choices_args(self,args) ; self._get_conditionally_required_args ; self.validate_arg_deprecation ; self.defaults ; self._get_conditionally_required_args(self,command_name,options_spec,args) ; self.parse_args(self,arg_parser,args) ; self.validate_arg_deprecation(self,cli_args,answer_file_args) ; self.resolve_custom_types(self,args) ; self.__init__(self,subparser,spec_dict,vars_dir,defaults_dir,plugin_path) ; self.validate_requires_args(self,args) ; self.create_complex_argumet_type(self,subcommand,type_name,option_name,spec_option) ; self.plugin_path ; self.get_spec_defaults(self) ; self.validate_arg_sources(cli_args,answer_file_args,spec_defaults) ; self.generate_answers_file ; self.create_complex_argumet_type ; self.parser ; self.get_answers_file_args(self,cli_args) ; self._convert_non_cli_args ; self.add_shared_groups(self,list_of_groups) ; self.parse_env_variable_from_file(value) ; self._get_defaults ; self.get_deprecated_args ; self.validate_requires_args ; self.get_answers_file_args ; self._convert_non_cli_args(self,parser_name,values_dict) ; self.validate_length_args ; self.spec_helper ; self.get_env_option(name) ; self.validate_length_args(self,args) ; self.validate_choices_args ; self.get_silent_args(self,args) ; self.get_spec_defaults ; self.get_nested_custom_and_control_args(self,args) ; self.from_plugin(cls,subparser,plugin,base_groups) ; self.get_silent_args ; self._get_defaults(self,default_getter_func) ; self.validate_min_max_args ; self.get_nested_custom_and_control_args ; self._iterate_received_arguments ; \" }",
            "code": "    def get_silent_args(self, args):\n        \"\"\"list of silenced argument\n\n        :param args: The received arguments.\n        :return: list, slienced argument names\n        \"\"\"\n        silent_args_names = []\n        for (parser_name, parser_dict, arg_name, arg_value,\n             arg_spec) in self._iterate_received_arguments(args):\n            if arg_spec and 'silent' in arg_spec and \\\n                    self.spec_helper.get_option_state(\n                        parser_name,\n                        arg_name,\n                        args) == helper.OptionState['IS_SET']:\n                silent_args_names.extend(arg_spec['silent'])\n\n        return list(set(silent_args_names))\n",
            "dependency": "",
            "docstring": "list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names",
            "end_lineno": "631",
            "file_content": "import collections\nimport os\nfrom six.moves import configparser\nfrom string import Template\nimport yaml\n\nfrom infrared.core.cli.cli import CliParser\nfrom infrared.core.cli.cli import COMPLEX_TYPES\nfrom infrared.core.inspector import helper\nfrom infrared.core.utils import dict_utils\nfrom infrared.core.utils import exceptions\nfrom infrared.core.utils import logger\n\nLOG = logger.LOG\n\n\nclass SpecParser(object):\n    \"\"\"Parses input arguments from different sources (cli, answers file). \"\"\"\n\n    @classmethod\n    def from_plugin(cls, subparser, plugin, base_groups):\n        \"\"\"Reads spec & vars from plugin and constructs the parser instance\n\n        :param subparser: argparse.subparser to extend\n        :param plugin: InfraredPlugin object\n        :param base_groups: dict, included groups\n        :return: SpecParser object based on given plugin spec & vars\n        \"\"\"\n\n        spec_dict = base_groups or {}\n        with open(plugin.spec) as stream:\n            spec = yaml.safe_load(stream) or {}\n            dict_utils.dict_merge(\n                base_groups,\n                spec,\n                dict_utils.ConflictResolver.unique_append_list_resolver)\n\n        # The \"try-excpet\" block here is for adding spec file path if it\n        # includes an unsupported option type\n        try:\n            return SpecParser(subparser, spec_dict, plugin.vars_dir,\n                              plugin.defaults_dir, plugin.path)\n        except exceptions.IRUnsupportedSpecOptionType as ex:\n            ex.message += ' in file: {}'.format(plugin.spec)\n            raise ex\n\n    def __init__(self, subparser, spec_dict, vars_dir, defaults_dir,\n                 plugin_path):\n        \"\"\"Constructor.\n\n        :param subparser: argparse.subparser to extend\n        :param spec_dict: dict with CLI description\n        :param vars_dir: Path to plugin's vars dir\n        :param defaults_dir: Path to plugin's defaults dir\n        \"\"\"\n        self.vars = vars_dir\n        self.defaults = defaults_dir\n        self.plugin_path = plugin_path\n        self.spec_helper = helper.SpecDictHelper(spec_dict)\n\n        # create parser\n        self.parser = CliParser.create_parser(self, subparser)\n\n    def add_shared_groups(self, list_of_groups):\n        \"\"\"Adds the user defined shared groups\n\n        :param list_of_groups: list, of group dicts\n        \"\"\"\n        shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n        shared_groups.expand(list_of_groups)\n        self.spec_helper.spec_dict['shared_groups'] = shared_groups\n\n    def _get_defaults(self, default_getter_func):\n        \"\"\"Resolve arguments' values from cli or answers file.\n\n        :param default_getter_func: callable. will be called for all the\n            available options in spec file.\n        \"\"\"\n\n        result = collections.defaultdict(dict)\n        for parser, option in self.spec_helper.iterate_option_specs():\n            default_value = default_getter_func(option)\n            if default_value is not None:\n                sub = parser['name']\n                result[sub][option['name']] = default_value\n\n        return result\n\n    def get_spec_defaults(self):\n        \"\"\"Resolve arguments' values from spec and other sources. \"\"\"\n\n        def spec_default_getter(option):\n            \"\"\"Getter function to retrieve the default value from spec.\n\n            :param option: argument name\n            \"\"\"\n\n            # first try to get environment variable with IR_ prefix\n            default_value = SpecParser.get_env_option(option['name'])\n            if default_value is not None:\n                LOG.info(\n                    \"[environ] Loading '{0}' default value\"\n                    \" '{1}' from the environment variable\".format(\n                        option['name'], default_value))\n            elif option.get('default', None) is not None:\n                default_value = option['default']\n            elif option.get('action', None) in ['store_true']:\n                default_value = False\n            return default_value\n\n        return self._get_defaults(spec_default_getter)\n\n    @staticmethod\n    def get_env_option(name):\n        \"\"\"Try get \"\"\"\n        return os.environ.get('IR_' + name.upper().replace('-', '_'))\n\n    def get_deprecated_args(self):\n        \"\"\"Returning dict with options which deprecate others. \"\"\"\n\n        result = collections.defaultdict(dict)\n        for parser, option in self.spec_helper.iterate_option_specs():\n            if option.get('deprecates') is not None:\n                result[option.get('deprecates')] = option.get('name')\n\n        return result\n\n    @staticmethod\n    def parse_env_variable_from_file(value):\n        if isinstance(value, str):\n            t = Template(value)\n            try:\n                value = t.substitute(os.environ)\n            except KeyError as undefined_var:\n                raise exceptions.IRAnswersFileEnvVarNotDefined(undefined_var)\n        return value\n\n    def get_answers_file_args(self, cli_args):\n        \"\"\"Resolve arguments' values from answers INI file. \"\"\"\n\n        file_result = {}\n        args_to_remove = []\n        for (parser_name, parser_dict, arg_name, arg_value,\n             option_spec) in self._iterate_received_arguments(cli_args):\n            file_result[parser_name] = file_result.get(parser_name, {})\n            if option_spec and option_spec.get(\n                    'action', '') == 'read-answers':\n                # Iterate over arguments supplied by file\n                for parsed_arg in parser_dict[arg_name]:\n                    # Supplied arguments' value can be a list\n                    if isinstance(parser_dict[arg_name][parsed_arg], list):\n                        i = 0\n                        # Iterrate over argument values list\n                        for parsed_value in parser_dict[arg_name][parsed_arg]:\n                            parser_dict[arg_name][parsed_arg][i] = \\\n                                SpecParser.parse_env_variable_from_file(parsed_value)\n                            i += 1\n                    else:\n                        parser_dict[arg_name][parsed_arg] = \\\n                            SpecParser.parse_env_variable_from_file(parser_dict[arg_name][parsed_arg])\n                # we have config option. saving it.\n                self._convert_non_cli_args(\n                    parser_name, parser_dict[arg_name])\n                dict_utils.dict_merge(\n                    file_result[parser_name],\n                    parser_dict[arg_name])\n                # remove from cli args\n                args_to_remove.append((parser_name, arg_name))\n\n        # remove parser dict outside loop to avoid iteration dict modification\n        for parser_name, arg_name in args_to_remove:\n            for spec_parser in self.spec_helper.iterate_parsers():\n                if spec_parser['name'] in cli_args and spec_parser['name'] == parser_name:\n                    parser_dict = cli_args[spec_parser['name']]\n                    parser_dict.pop(arg_name)\n                    break\n\n        return file_result\n\n    def generate_answers_file(self, cli_args, spec_defaults):\n        \"\"\"Generates answers INI file\n\n        :param cli_args: list, cli arguments.\n        :param spec_defaults: the default values.\n        \"\"\"\n\n        def put_option(config, parser_name, option_name, value):\n            for opt_help in option.get('help', '').split('\\n'):\n                help_opt = '# ' + opt_help\n\n                # add help comment\n                if config.has_option(parser_name, help_opt):\n                    config.remove_option(parser_name, help_opt)\n                config.set(\n                    parser_name, help_opt)\n\n            if config.has_option(parser_name, option_name):\n                value = config.get(parser_name, option_name)\n                config.remove_option(parser_name, option_name)\n\n            config.set(\n                parser_name,\n                option_name,\n                str(value))\n\n        file_generated = False\n\n        # load generate answers file for all the parsers\n        for (parser_name, parser_dict, arg_name, arg_value,\n             option_spec) in self._iterate_received_arguments(cli_args):\n            if option_spec and option_spec.get(\n                    'action', '') == 'generate-answers':\n                options_to_save = \\\n                    self.spec_helper.get_parser_option_specs(parser_name)\n                out_answers = configparser.ConfigParser(allow_no_value=True)\n\n                if not out_answers.has_section(parser_name):\n                    out_answers.add_section(parser_name)\n\n                for option in options_to_save:\n                    opt_name = option['name']\n                    if opt_name in parser_dict:\n                        put_option(\n                            out_answers,\n                            parser_name,\n                            opt_name,\n                            parser_dict[opt_name])\n                    elif opt_name in spec_defaults[parser_name]:\n                        put_option(\n                            out_answers,\n                            parser_name,\n                            opt_name,\n                            spec_defaults[parser_name][opt_name])\n                    elif option.get('required', False):\n                        put_option(\n                            out_answers,\n                            parser_name,\n                            '# ' + opt_name,\n                            \"Required argument. \"\n                            \"Edit with one of the allowed values OR \"\n                            \"override with \"\n                            \"CLI: --{}=<option>\".format(opt_name))\n\n                # write to file\n                with open(arg_value, 'w') as answers_file:\n                    out_answers.write(answers_file)\n                file_generated = True\n\n        return file_generated\n\n    def resolve_custom_types(self, args):\n        \"\"\"Transforms the arguments with custom types\n\n        :param args: the list of received arguments.\n        \"\"\"\n        for parser_name, parser_dict in args.items():\n            spec_complex_options = [opt for opt in\n                                    self.spec_helper.get_parser_option_specs(\n                                        parser_name) if\n                                    opt.get('type', None) in COMPLEX_TYPES]\n            for spec_option in spec_complex_options:\n                option_name = spec_option['name']\n                if option_name in parser_dict:\n                    # we have custom type to resolve\n                    type_name = spec_option['type']\n                    option_value = parser_dict[option_name]\n                    action = self.create_complex_argumet_type(\n                        parser_name,\n                        type_name,\n                        option_name,\n                        spec_option)\n\n                    # resolving value\n                    parser_dict[option_name] = action.resolve(option_value)\n\n    def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n        \"\"\"Build the complex argument type\n\n        :param subcommand: the command name\n        :param type_name: the complex type name\n        :param option_name: the option name\n        :param spec_option: option's specifications\n        :return: the complex type instance\n        \"\"\"\n        complex_action = COMPLEX_TYPES.get(\n            type_name, None)\n        if complex_action is None:\n            raise exceptions.SpecParserException(\n                \"Unknown complex type: {}\".format(type_name))\n        return complex_action(\n            option_name,\n            (self.vars, self.defaults, self.plugin_path),\n            subcommand,\n            spec_option)\n\n    def parse_args(self, arg_parser, args=None):\n        \"\"\"Parses all the arguments (cli, answers file)\n\n        :return: None, if ``--generate-answers-file`` in arg_arg_parser\n        :return: (dict, dict):\n            * command arguments dict (arguments to control the IR logic)\n            * nested arguments dict (arguments to pass to the playbooks)\n        \"\"\"\n\n        spec_defaults = self.get_spec_defaults()\n        cli_args = CliParser.parse_cli_input(arg_parser, args)\n\n        file_args = self.get_answers_file_args(cli_args)\n\n        # generate answers file and exit\n        if self.generate_answers_file(cli_args, spec_defaults):\n            LOG.warning(\"Answers file generated. Exiting.\")\n\n        # print warnings when something was overridden from non-cli source.\n        self.validate_arg_sources(cli_args, file_args,\n                                  spec_defaults)\n\n        # print warnings for deprecated\n        self.validate_arg_deprecation(cli_args, file_args)\n\n        # now filter defaults to have only parser defined in cli\n        defaults = dict((key, spec_defaults[key])\n                        for key in cli_args.keys() if\n                        key in spec_defaults)\n\n        # copy cli args with the same name to all parser groups\n        self._merge_duplicated_cli_args(cli_args)\n        self._merge_duplicated_cli_args(file_args)\n\n        dict_utils.dict_merge(defaults, file_args)\n        dict_utils.dict_merge(defaults, cli_args)\n        self.validate_requires_args(defaults)\n        self.validate_length_args(defaults)\n        self.validate_choices_args(defaults)\n        self.validate_min_max_args(defaults)\n\n        # now resolve complex types.\n        self.resolve_custom_types(defaults)\n        nested, control, custom = \\\n            self.get_nested_custom_and_control_args(defaults)\n        return nested, control, custom\n\n    def validate_arg_deprecation(self, cli_args, answer_file_args):\n        \"\"\"Validates and prints the deprecated arguments.\n\n        :param cli_args: the dict of arguments from cli\n        :param answer_file_args:  the dict of arguments from files\n        \"\"\"\n\n        for deprecated, deprecates in self.get_deprecated_args().items():\n            for input_args in (answer_file_args.items(), cli_args.items()):\n                for command, command_dict in input_args:\n                    if deprecated in command_dict:\n                        if deprecates in command_dict:\n                            raise exceptions.IRDeprecationException(\n                                \"[{}] Argument '{}' deprecates '{}',\"\n                                \" please use only the new one.\".format(\n                                    command, deprecated, deprecates))\n\n                        if deprecated in answer_file_args[command]:\n                            answer_file_args[command][deprecates] = \\\n                                answer_file_args[command][deprecated]\n\n                        if deprecated in cli_args[command]:\n                            cli_args[command][deprecates] = \\\n                                cli_args[command][deprecated]\n\n                        LOG.warning(\n                            \"[{}] Argument '{}' was deprecated,\"\n                            \" please use '{}'.\".format(\n                                command, deprecated, deprecates))\n\n    @staticmethod\n    def validate_arg_sources(cli_args, answer_file_args, spec_defaults):\n        \"\"\"Validates and prints the arguments' source.\n\n        :param cli_args: the dict of arguments from cli\n        :param answer_file_args:  the dict of arguments from files\n        :param spec_defaults:  the default values from spec files\n        \"\"\"\n\n        def show_diff(diff, command_name, cmd_dict, source_name):\n            if diff:\n                for arg_name in diff:\n                    value = cmd_dict[arg_name]\n                    LOG.info(\n                        \"[{}] Argument '{}' was set to\"\n                        \" '{}' from the {} source.\".format(\n                            command_name, arg_name, value, source_name))\n\n        for command, command_dict in cli_args.items():\n            file_dict = answer_file_args.get(command, {})\n            file_diff = set(file_dict.keys()) - set(command_dict.keys())\n            show_diff(file_diff, command, file_dict, 'answers file')\n\n            def_dict = spec_defaults.get(command, {})\n            default_diff = set(def_dict.keys()) - set(\n                command_dict.keys()) - file_diff\n            show_diff(default_diff, command, def_dict, 'spec defaults')\n\n    def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n        \"\"\"List arguments with ``required_when`` condition matched.\n\n        :param command_name: the command name.\n        :param options_spec:  the list of command spec options.\n        :param args: the received input arguments\n        :return: list, list of argument names with matched ``required_when``\n            condition\n        \"\"\"\n        opts_names = [option_spec['name'] for option_spec in options_spec]\n        missing_args = []\n        for option_spec in options_spec:\n            option_results = []\n            if option_spec and 'required_when' in option_spec:\n                req_when_args = [option_spec['required_when']] \\\n                    if not type(option_spec['required_when']) is list \\\n                    else option_spec['required_when']\n\n                # validate conditions\n                for req_when_arg in req_when_args:\n                    splited_args_list = req_when_arg.split()\n                    for idx, req_arg in enumerate(splited_args_list):\n                        if req_arg in opts_names:\n                            splited_args_list[idx] = \\\n                                args.get(command_name, {}).get(req_arg.strip())\n                        if splited_args_list[idx] is None:\n                            option_results.append(False)\n                            break\n                        splited_args_list[idx] = str(splited_args_list[idx])\n                        if (splited_args_list[idx] not in ['and', 'or'] and\n                            not any(\n                                (c in '<>=') for c in splited_args_list[idx])):\n                            splited_args_list[idx] = \"'{0}'\".format(\n                                yaml.safe_load(splited_args_list[idx]))\n                    else:\n                        option_results.append(\n                            eval(' '.join(splited_args_list)))\n                if all(option_results) and \\\n                        self.spec_helper.get_option_state(\n                            command_name,\n                            option_spec['name'],\n                            args) == helper.OptionState['NOT_SET']:\n                    missing_args.append(option_spec['name'])\n        return missing_args\n\n    def validate_requires_args(self, args):\n        \"\"\"Check if all the required arguments have been provided. \"\"\"\n\n        silent_args = self.get_silent_args(args)\n\n        def validate_parser(parser_name, expected_options, parser_args):\n            \"\"\"Helper method to resolve dict_merge. \"\"\"\n\n            result = collections.defaultdict(list)\n            condition_req_args = self._get_conditionally_required_args(\n                parser_name, expected_options, args)\n\n            for option in expected_options:\n                name = option['name']\n\n                # check required options.\n                if (option.get('required', False) and\n                    name not in parser_args or\n                    option['name'] in condition_req_args) and \\\n                        name not in silent_args:\n                    result[parser_name].append(name)\n\n            return result\n\n        res = {}\n        for command_data in self.spec_helper.iterate_parsers():\n            cmd_name = command_data['name']\n            if cmd_name in args:\n                dict_utils.dict_merge(\n                    res,\n                    validate_parser(\n                        cmd_name,\n                        self.spec_helper.get_parser_option_specs(cmd_name),\n                        args[cmd_name]))\n\n        missing_args = dict((cmd_name, args)\n                            for cmd_name, args in res.items() if len(args) > 0)\n        if missing_args:\n            raise exceptions.IRRequiredArgsMissingException(missing_args)\n\n    def validate_length_args(self, args):\n        \"\"\"Check if value of arguments is not longer than length specified.\n\n        :param args: The received arguments.\n        \"\"\"\n        invalid_options = []\n        for parser_name, parser_dict in args.items():\n            for spec_option in \\\n                    self.spec_helper.get_parser_option_specs(parser_name):\n                if 'length' not in spec_option:\n                    # skip options that does not contain length\n                    continue\n                option_name = spec_option['name']\n                if option_name in parser_dict:\n                    # resolve length\n                    length = spec_option['length']\n                    option_value = parser_dict[option_name]\n                    if len(option_value) > int(length):\n                        # found invalid option, append to list of invalid opts\n                        invalid_options.append((\n                            option_name,\n                            option_value,\n                            length\n                        ))\n        if invalid_options:\n            # raise exception with all arguments that exceed length\n            raise exceptions.IRInvalidLengthException(invalid_options)\n\n    def validate_choices_args(self, args):\n        \"\"\"Check if value of choice arguments is one of the available choices.\n\n        :param args: The received arguments.\n        \"\"\"\n        invalid_options = []\n        for parser_name, parser_dict in args.items():\n            for spec_option in \\\n                    self.spec_helper.get_parser_option_specs(parser_name):\n                if 'choices' not in spec_option:\n                    # skip options that does not contain choices\n                    continue\n                option_name = spec_option['name']\n                if option_name in parser_dict:\n                    # resolve choices\n                    choices = spec_option['choices']\n                    option_value = parser_dict[option_name]\n                    if option_value not in choices:\n                        # found invalid option, append to list of invalid opts\n                        invalid_options.append((\n                            option_name,\n                            option_value,\n                            choices\n                        ))\n        if invalid_options:\n            # raise exception with all arguments that contains invalid choices\n            raise exceptions.IRInvalidChoiceException(invalid_options)\n\n    def validate_min_max_args(self, args):\n        \"\"\"Check if value of arguments is between minimum and maximum values.\n\n        :param args: The received arguments.\n        \"\"\"\n        invalid_options = []\n        for parser_name, parser_dict in args.items():\n            for spec_option in \\\n                    self.spec_helper.get_parser_option_specs(parser_name):\n                if all([key not in spec_option\n                        for key in ('maximum', 'minimum')]):\n                    # skip options that does not contain minimum or maximum\n                    continue\n                option_name = spec_option['name']\n\n                if option_name in parser_dict:\n                    option_value = parser_dict[option_name]\n                    min_value = spec_option.get('minimum')\n                    max_value = spec_option.get('maximum')\n                    # handle empty values in spec files which load as None\n                    min_value = '' if 'minimum' in spec_option \\\n                                      and min_value is None else min_value\n                    max_value = '' if 'maximum' in spec_option \\\n                                      and max_value is None else max_value\n\n                    values = {\n                        \"value\": option_value,\n                        \"maximum\": max_value,\n                        \"minimum\": min_value\n                    }\n\n                    # make sure that values are numbers\n                    is_all_values_numbers = True\n                    for name, num in values.items():\n                        if num is not None \\\n                                and (isinstance(num, bool) or\n                                     not isinstance(num, (int, float))):\n                            invalid_options.append((\n                                option_name,\n                                name,\n                                \"number\",\n                                type(num).__name__\n                            ))\n                            is_all_values_numbers = False\n\n                    if not is_all_values_numbers:\n                        # don't continue to min max checks since some of the\n                        # values are not numbers\n                        continue\n\n                    # check bigger than minimum\n                    if min_value is not None and option_value < min_value:\n                        invalid_options.append((\n                            option_name,\n                            \"minimum\",\n                            min_value,\n                            option_value\n                        ))\n                    # check smaller than maximum\n                    if max_value is not None and option_value > max_value:\n                        invalid_options.append((\n                            option_name,\n                            \"maximum\",\n                            max_value,\n                            option_value\n                        ))\n\n        if invalid_options:\n            # raise exception with all arguments that contains invalid choices\n            raise exceptions.IRInvalidMinMaxRangeException(invalid_options)\n\n    def get_silent_args(self, args):\n        \"\"\"list of silenced argument\n\n        :param args: The received arguments.\n        :return: list, slienced argument names\n        \"\"\"\n        silent_args_names = []\n        for (parser_name, parser_dict, arg_name, arg_value,\n             arg_spec) in self._iterate_received_arguments(args):\n            if arg_spec and 'silent' in arg_spec and \\\n                    self.spec_helper.get_option_state(\n                        parser_name,\n                        arg_name,\n                        args) == helper.OptionState['IS_SET']:\n                silent_args_names.extend(arg_spec['silent'])\n\n        return list(set(silent_args_names))\n\n    def get_nested_custom_and_control_args(self, args):\n        \"\"\"Split input arguments to control nested and custom.\n\n        Controls arguments: control the IR behavior. These arguments\n            will not be put into the spec yml file\n        Nested arguments: are used by the Ansible playbooks and will be put\n            into the spec yml file.\n        Custom arguments: Custom ansible variables to be used instead of the\n            normal nested usage.\n\n        :param args: the collected list of args.\n        :return: (dict, dict): flat dicts (control_args, nested_args)\n        \"\"\"\n        # returns flat dicts\n        nested = {}\n        control_args = {}\n        custom_args = {}\n        for (parser_name, parser_dict, arg_name, arg_value,\n             arg_spec) in self._iterate_received_arguments(args):\n            if all([arg_spec, arg_spec.get('type', None),\n                    arg_spec.get('type', None) in\n                    [ctype_name for ctype_name, klass in\n                     COMPLEX_TYPES.items() if klass.is_nested]\n                    ]) or ('is_shared_group_option' not in arg_spec):\n                if arg_name in nested:\n                    LOG.warning(\n                        \"Duplicated nested argument found:'{}'. \"\n                        \"Using old value: '{}'\".format(\n                            arg_name, nested[arg_name]))\n                elif arg_name in custom_args:\n                    LOG.warning(\n                        \"Duplicated custom argument found:'{}'. \"\n                        \"Using old value: '{}'\".format(\n                            arg_name, custom_args[arg_name]))\n                else:\n                    if \"ansible_variable\" in arg_spec:\n                        custom_args[arg_spec[\"ansible_variable\"]] = arg_value\n                    else:\n                        nested[arg_name] = arg_value\n            else:\n                if arg_name in control_args:\n                    LOG.warning(\n                        \"Duplicated control argument found: '{}'. Using \"\n                        \"old value: '{}'\".format(\n                            arg_name, control_args[arg_name]))\n                else:\n                    control_args[arg_name] = arg_value\n\n        return nested, control_args, custom_args\n\n    def _iterate_received_arguments(self, args):\n        \"\"\"Iterator helper method over all the received arguments\n\n        :return: yields tuple:\n            (spec name, spec dict,\n             argument name, argument value, argument spec)\n        \"\"\"\n        for spec_parser in self.spec_helper.iterate_parsers():\n            if spec_parser['name'] in args:\n                parser_dict = args[spec_parser['name']]\n                for arg_name, arg_val in parser_dict.items():\n                    arg_spec = self.spec_helper.get_option_spec(\n                        spec_parser['name'], arg_name)\n                    yield (spec_parser['name'], parser_dict,\n                           arg_name, arg_val, arg_spec)\n\n    def _convert_non_cli_args(self, parser_name, values_dict):\n        \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n        By default all the values are strings.\n\n        :param parser_name: The command name, e.g. main, virsh, ospd, etc\n        :param values_dict: The dict of with arguments\n       \"\"\"\n        for opt_name, opt_value in values_dict.items():\n            file_option_spec = self.spec_helper.get_option_spec(\n                parser_name, opt_name)\n            if file_option_spec.get('type', None) in ['int', ] or \\\n                    file_option_spec.get('action', None) in ['count', ]:\n                values_dict[opt_name] = int(opt_value)\n\n    def _merge_duplicated_cli_args(self, cli_args):\n        \"\"\"Merge duplicated arguments to all the parsers\n\n        This is need to handle control args, shared among several parsers.\n        for example, verbose, inventory\n        \"\"\"\n        for (parser_name, parser_dict, arg_name, arg_value,\n             arg_spec) in self._iterate_received_arguments(cli_args):\n            for parser_name2, parser_dict2 in cli_args.items():\n                if all([parser_name2, parser_name != parser_name2,\n                        arg_name not in parser_dict2]):\n                    if self.spec_helper.get_option_spec(parser_name2,\n                                                        arg_name):\n                        parser_dict2[arg_name] = arg_value\n",
            "file_path": "infrared/core/inspector/inspector.py",
            "human_label": "list of silenced argument\n\n:param args: The received arguments.\n:return: list, slienced argument names",
            "level": "class_runnable",
            "lineno": "615",
            "name": "get_silent_args",
            "oracle_context": "{ \"apis\" : \"['set', 'list', 'get_option_state', 'extend', '_iterate_received_arguments']\", \"classes\" : \"['helper']\", \"vars\" : \"['OptionState', 'spec_helper']\" }",
            "package": "inspector",
            "project": "redhat-openstack/infrared",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "6306092d73426c38ae68ad07",
            "all_context": "{ \"import\" : \"string os collections six infrared \", \"file\" : \"\", \"class\" : \"self.resolve_custom_types ; self.generate_answers_file(self,cli_args,spec_defaults) ; self.validate_min_max_args(self,args) ; self._merge_duplicated_cli_args ; self.validate_arg_sources ; self.vars ; self._iterate_received_arguments(self,args) ; self.get_deprecated_args(self) ; self._merge_duplicated_cli_args(self,cli_args) ; self.validate_choices_args(self,args) ; self._get_conditionally_required_args ; self.validate_arg_deprecation ; self.defaults ; self._get_conditionally_required_args(self,command_name,options_spec,args) ; self.parse_args(self,arg_parser,args) ; self.validate_arg_deprecation(self,cli_args,answer_file_args) ; self.resolve_custom_types(self,args) ; self.__init__(self,subparser,spec_dict,vars_dir,defaults_dir,plugin_path) ; self.validate_requires_args(self,args) ; self.create_complex_argumet_type(self,subcommand,type_name,option_name,spec_option) ; self.plugin_path ; self.get_spec_defaults(self) ; self.validate_arg_sources(cli_args,answer_file_args,spec_defaults) ; self.generate_answers_file ; self.create_complex_argumet_type ; self.parser ; self.get_answers_file_args(self,cli_args) ; self._convert_non_cli_args ; self.add_shared_groups(self,list_of_groups) ; self.parse_env_variable_from_file(value) ; self._get_defaults ; self.get_deprecated_args ; self.validate_requires_args ; self.get_answers_file_args ; self._convert_non_cli_args(self,parser_name,values_dict) ; self.validate_length_args ; self.spec_helper ; self.get_env_option(name) ; self.validate_length_args(self,args) ; self.validate_choices_args ; self.get_silent_args(self,args) ; self.get_spec_defaults ; self.get_nested_custom_and_control_args(self,args) ; self.from_plugin(cls,subparser,plugin,base_groups) ; self.get_silent_args ; self._get_defaults(self,default_getter_func) ; self.validate_min_max_args ; self.get_nested_custom_and_control_args ; self._iterate_received_arguments ; \" }",
            "code": "    def validate_requires_args(self, args):\n        \"\"\"Check if all the required arguments have been provided. \"\"\"\n\n        silent_args = self.get_silent_args(args)\n\n        def validate_parser(parser_name, expected_options, parser_args):\n            \"\"\"Helper method to resolve dict_merge. \"\"\"\n\n            result = collections.defaultdict(list)\n            condition_req_args = self._get_conditionally_required_args(\n                parser_name, expected_options, args)\n\n            for option in expected_options:\n                name = option['name']\n\n                # check required options.\n                if (option.get('required', False) and\n                    name not in parser_args or\n                    option['name'] in condition_req_args) and \\\n                        name not in silent_args:\n                    result[parser_name].append(name)\n\n            return result\n\n        res = {}\n        for command_data in self.spec_helper.iterate_parsers():\n            cmd_name = command_data['name']\n            if cmd_name in args:\n                dict_utils.dict_merge(\n                    res,\n                    validate_parser(\n                        cmd_name,\n                        self.spec_helper.get_parser_option_specs(cmd_name),\n                        args[cmd_name]))\n\n        missing_args = dict((cmd_name, args)\n                            for cmd_name, args in res.items() if len(args) > 0)\n        return missing_args\n",
            "dependency": "",
            "docstring": "Check if all the required arguments have been provided.",
            "end_lineno": "486",
            "file_content": "import collections\nimport os\nfrom six.moves import configparser\nfrom string import Template\nimport yaml\n\nfrom infrared.core.cli.cli import CliParser\nfrom infrared.core.cli.cli import COMPLEX_TYPES\nfrom infrared.core.inspector import helper\nfrom infrared.core.utils import dict_utils\nfrom infrared.core.utils import exceptions\nfrom infrared.core.utils import logger\n\nLOG = logger.LOG\n\n\nclass SpecParser(object):\n    \"\"\"Parses input arguments from different sources (cli, answers file). \"\"\"\n\n    @classmethod\n    def from_plugin(cls, subparser, plugin, base_groups):\n        \"\"\"Reads spec & vars from plugin and constructs the parser instance\n\n        :param subparser: argparse.subparser to extend\n        :param plugin: InfraredPlugin object\n        :param base_groups: dict, included groups\n        :return: SpecParser object based on given plugin spec & vars\n        \"\"\"\n\n        spec_dict = base_groups or {}\n        with open(plugin.spec) as stream:\n            spec = yaml.safe_load(stream) or {}\n            dict_utils.dict_merge(\n                base_groups,\n                spec,\n                dict_utils.ConflictResolver.unique_append_list_resolver)\n\n        # The \"try-excpet\" block here is for adding spec file path if it\n        # includes an unsupported option type\n        try:\n            return SpecParser(subparser, spec_dict, plugin.vars_dir,\n                              plugin.defaults_dir, plugin.path)\n        except exceptions.IRUnsupportedSpecOptionType as ex:\n            ex.message += ' in file: {}'.format(plugin.spec)\n            raise ex\n\n    def __init__(self, subparser, spec_dict, vars_dir, defaults_dir,\n                 plugin_path):\n        \"\"\"Constructor.\n\n        :param subparser: argparse.subparser to extend\n        :param spec_dict: dict with CLI description\n        :param vars_dir: Path to plugin's vars dir\n        :param defaults_dir: Path to plugin's defaults dir\n        \"\"\"\n        self.vars = vars_dir\n        self.defaults = defaults_dir\n        self.plugin_path = plugin_path\n        self.spec_helper = helper.SpecDictHelper(spec_dict)\n\n        # create parser\n        self.parser = CliParser.create_parser(self, subparser)\n\n    def add_shared_groups(self, list_of_groups):\n        \"\"\"Adds the user defined shared groups\n\n        :param list_of_groups: list, of group dicts\n        \"\"\"\n        shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n        shared_groups.expand(list_of_groups)\n        self.spec_helper.spec_dict['shared_groups'] = shared_groups\n\n    def _get_defaults(self, default_getter_func):\n        \"\"\"Resolve arguments' values from cli or answers file.\n\n        :param default_getter_func: callable. will be called for all the\n            available options in spec file.\n        \"\"\"\n\n        result = collections.defaultdict(dict)\n        for parser, option in self.spec_helper.iterate_option_specs():\n            default_value = default_getter_func(option)\n            if default_value is not None:\n                sub = parser['name']\n                result[sub][option['name']] = default_value\n\n        return result\n\n    def get_spec_defaults(self):\n        \"\"\"Resolve arguments' values from spec and other sources. \"\"\"\n\n        def spec_default_getter(option):\n            \"\"\"Getter function to retrieve the default value from spec.\n\n            :param option: argument name\n            \"\"\"\n\n            # first try to get environment variable with IR_ prefix\n            default_value = SpecParser.get_env_option(option['name'])\n            if default_value is not None:\n                LOG.info(\n                    \"[environ] Loading '{0}' default value\"\n                    \" '{1}' from the environment variable\".format(\n                        option['name'], default_value))\n            elif option.get('default', None) is not None:\n                default_value = option['default']\n            elif option.get('action', None) in ['store_true']:\n                default_value = False\n            return default_value\n\n        return self._get_defaults(spec_default_getter)\n\n    @staticmethod\n    def get_env_option(name):\n        \"\"\"Try get \"\"\"\n        return os.environ.get('IR_' + name.upper().replace('-', '_'))\n\n    def get_deprecated_args(self):\n        \"\"\"Returning dict with options which deprecate others. \"\"\"\n\n        result = collections.defaultdict(dict)\n        for parser, option in self.spec_helper.iterate_option_specs():\n            if option.get('deprecates') is not None:\n                result[option.get('deprecates')] = option.get('name')\n\n        return result\n\n    @staticmethod\n    def parse_env_variable_from_file(value):\n        if isinstance(value, str):\n            t = Template(value)\n            try:\n                value = t.substitute(os.environ)\n            except KeyError as undefined_var:\n                raise exceptions.IRAnswersFileEnvVarNotDefined(undefined_var)\n        return value\n\n    def get_answers_file_args(self, cli_args):\n        \"\"\"Resolve arguments' values from answers INI file. \"\"\"\n\n        file_result = {}\n        args_to_remove = []\n        for (parser_name, parser_dict, arg_name, arg_value,\n             option_spec) in self._iterate_received_arguments(cli_args):\n            file_result[parser_name] = file_result.get(parser_name, {})\n            if option_spec and option_spec.get(\n                    'action', '') == 'read-answers':\n                # Iterate over arguments supplied by file\n                for parsed_arg in parser_dict[arg_name]:\n                    # Supplied arguments' value can be a list\n                    if isinstance(parser_dict[arg_name][parsed_arg], list):\n                        i = 0\n                        # Iterrate over argument values list\n                        for parsed_value in parser_dict[arg_name][parsed_arg]:\n                            parser_dict[arg_name][parsed_arg][i] = \\\n                                SpecParser.parse_env_variable_from_file(parsed_value)\n                            i += 1\n                    else:\n                        parser_dict[arg_name][parsed_arg] = \\\n                            SpecParser.parse_env_variable_from_file(parser_dict[arg_name][parsed_arg])\n                # we have config option. saving it.\n                self._convert_non_cli_args(\n                    parser_name, parser_dict[arg_name])\n                dict_utils.dict_merge(\n                    file_result[parser_name],\n                    parser_dict[arg_name])\n                # remove from cli args\n                args_to_remove.append((parser_name, arg_name))\n\n        # remove parser dict outside loop to avoid iteration dict modification\n        for parser_name, arg_name in args_to_remove:\n            for spec_parser in self.spec_helper.iterate_parsers():\n                if spec_parser['name'] in cli_args and spec_parser['name'] == parser_name:\n                    parser_dict = cli_args[spec_parser['name']]\n                    parser_dict.pop(arg_name)\n                    break\n\n        return file_result\n\n    def generate_answers_file(self, cli_args, spec_defaults):\n        \"\"\"Generates answers INI file\n\n        :param cli_args: list, cli arguments.\n        :param spec_defaults: the default values.\n        \"\"\"\n\n        def put_option(config, parser_name, option_name, value):\n            for opt_help in option.get('help', '').split('\\n'):\n                help_opt = '# ' + opt_help\n\n                # add help comment\n                if config.has_option(parser_name, help_opt):\n                    config.remove_option(parser_name, help_opt)\n                config.set(\n                    parser_name, help_opt)\n\n            if config.has_option(parser_name, option_name):\n                value = config.get(parser_name, option_name)\n                config.remove_option(parser_name, option_name)\n\n            config.set(\n                parser_name,\n                option_name,\n                str(value))\n\n        file_generated = False\n\n        # load generate answers file for all the parsers\n        for (parser_name, parser_dict, arg_name, arg_value,\n             option_spec) in self._iterate_received_arguments(cli_args):\n            if option_spec and option_spec.get(\n                    'action', '') == 'generate-answers':\n                options_to_save = \\\n                    self.spec_helper.get_parser_option_specs(parser_name)\n                out_answers = configparser.ConfigParser(allow_no_value=True)\n\n                if not out_answers.has_section(parser_name):\n                    out_answers.add_section(parser_name)\n\n                for option in options_to_save:\n                    opt_name = option['name']\n                    if opt_name in parser_dict:\n                        put_option(\n                            out_answers,\n                            parser_name,\n                            opt_name,\n                            parser_dict[opt_name])\n                    elif opt_name in spec_defaults[parser_name]:\n                        put_option(\n                            out_answers,\n                            parser_name,\n                            opt_name,\n                            spec_defaults[parser_name][opt_name])\n                    elif option.get('required', False):\n                        put_option(\n                            out_answers,\n                            parser_name,\n                            '# ' + opt_name,\n                            \"Required argument. \"\n                            \"Edit with one of the allowed values OR \"\n                            \"override with \"\n                            \"CLI: --{}=<option>\".format(opt_name))\n\n                # write to file\n                with open(arg_value, 'w') as answers_file:\n                    out_answers.write(answers_file)\n                file_generated = True\n\n        return file_generated\n\n    def resolve_custom_types(self, args):\n        \"\"\"Transforms the arguments with custom types\n\n        :param args: the list of received arguments.\n        \"\"\"\n        for parser_name, parser_dict in args.items():\n            spec_complex_options = [opt for opt in\n                                    self.spec_helper.get_parser_option_specs(\n                                        parser_name) if\n                                    opt.get('type', None) in COMPLEX_TYPES]\n            for spec_option in spec_complex_options:\n                option_name = spec_option['name']\n                if option_name in parser_dict:\n                    # we have custom type to resolve\n                    type_name = spec_option['type']\n                    option_value = parser_dict[option_name]\n                    action = self.create_complex_argumet_type(\n                        parser_name,\n                        type_name,\n                        option_name,\n                        spec_option)\n\n                    # resolving value\n                    parser_dict[option_name] = action.resolve(option_value)\n\n    def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n        \"\"\"Build the complex argument type\n\n        :param subcommand: the command name\n        :param type_name: the complex type name\n        :param option_name: the option name\n        :param spec_option: option's specifications\n        :return: the complex type instance\n        \"\"\"\n        complex_action = COMPLEX_TYPES.get(\n            type_name, None)\n        if complex_action is None:\n            raise exceptions.SpecParserException(\n                \"Unknown complex type: {}\".format(type_name))\n        return complex_action(\n            option_name,\n            (self.vars, self.defaults, self.plugin_path),\n            subcommand,\n            spec_option)\n\n    def parse_args(self, arg_parser, args=None):\n        \"\"\"Parses all the arguments (cli, answers file)\n\n        :return: None, if ``--generate-answers-file`` in arg_arg_parser\n        :return: (dict, dict):\n            * command arguments dict (arguments to control the IR logic)\n            * nested arguments dict (arguments to pass to the playbooks)\n        \"\"\"\n\n        spec_defaults = self.get_spec_defaults()\n        cli_args = CliParser.parse_cli_input(arg_parser, args)\n\n        file_args = self.get_answers_file_args(cli_args)\n\n        # generate answers file and exit\n        if self.generate_answers_file(cli_args, spec_defaults):\n            LOG.warning(\"Answers file generated. Exiting.\")\n\n        # print warnings when something was overridden from non-cli source.\n        self.validate_arg_sources(cli_args, file_args,\n                                  spec_defaults)\n\n        # print warnings for deprecated\n        self.validate_arg_deprecation(cli_args, file_args)\n\n        # now filter defaults to have only parser defined in cli\n        defaults = dict((key, spec_defaults[key])\n                        for key in cli_args.keys() if\n                        key in spec_defaults)\n\n        # copy cli args with the same name to all parser groups\n        self._merge_duplicated_cli_args(cli_args)\n        self._merge_duplicated_cli_args(file_args)\n\n        dict_utils.dict_merge(defaults, file_args)\n        dict_utils.dict_merge(defaults, cli_args)\n        self.validate_requires_args(defaults)\n        self.validate_length_args(defaults)\n        self.validate_choices_args(defaults)\n        self.validate_min_max_args(defaults)\n\n        # now resolve complex types.\n        self.resolve_custom_types(defaults)\n        nested, control, custom = \\\n            self.get_nested_custom_and_control_args(defaults)\n        return nested, control, custom\n\n    def validate_arg_deprecation(self, cli_args, answer_file_args):\n        \"\"\"Validates and prints the deprecated arguments.\n\n        :param cli_args: the dict of arguments from cli\n        :param answer_file_args:  the dict of arguments from files\n        \"\"\"\n\n        for deprecated, deprecates in self.get_deprecated_args().items():\n            for input_args in (answer_file_args.items(), cli_args.items()):\n                for command, command_dict in input_args:\n                    if deprecated in command_dict:\n                        if deprecates in command_dict:\n                            raise exceptions.IRDeprecationException(\n                                \"[{}] Argument '{}' deprecates '{}',\"\n                                \" please use only the new one.\".format(\n                                    command, deprecated, deprecates))\n\n                        if deprecated in answer_file_args[command]:\n                            answer_file_args[command][deprecates] = \\\n                                answer_file_args[command][deprecated]\n\n                        if deprecated in cli_args[command]:\n                            cli_args[command][deprecates] = \\\n                                cli_args[command][deprecated]\n\n                        LOG.warning(\n                            \"[{}] Argument '{}' was deprecated,\"\n                            \" please use '{}'.\".format(\n                                command, deprecated, deprecates))\n\n    @staticmethod\n    def validate_arg_sources(cli_args, answer_file_args, spec_defaults):\n        \"\"\"Validates and prints the arguments' source.\n\n        :param cli_args: the dict of arguments from cli\n        :param answer_file_args:  the dict of arguments from files\n        :param spec_defaults:  the default values from spec files\n        \"\"\"\n\n        def show_diff(diff, command_name, cmd_dict, source_name):\n            if diff:\n                for arg_name in diff:\n                    value = cmd_dict[arg_name]\n                    LOG.info(\n                        \"[{}] Argument '{}' was set to\"\n                        \" '{}' from the {} source.\".format(\n                            command_name, arg_name, value, source_name))\n\n        for command, command_dict in cli_args.items():\n            file_dict = answer_file_args.get(command, {})\n            file_diff = set(file_dict.keys()) - set(command_dict.keys())\n            show_diff(file_diff, command, file_dict, 'answers file')\n\n            def_dict = spec_defaults.get(command, {})\n            default_diff = set(def_dict.keys()) - set(\n                command_dict.keys()) - file_diff\n            show_diff(default_diff, command, def_dict, 'spec defaults')\n\n    def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n        \"\"\"List arguments with ``required_when`` condition matched.\n\n        :param command_name: the command name.\n        :param options_spec:  the list of command spec options.\n        :param args: the received input arguments\n        :return: list, list of argument names with matched ``required_when``\n            condition\n        \"\"\"\n        opts_names = [option_spec['name'] for option_spec in options_spec]\n        missing_args = []\n        for option_spec in options_spec:\n            option_results = []\n            if option_spec and 'required_when' in option_spec:\n                req_when_args = [option_spec['required_when']] \\\n                    if not type(option_spec['required_when']) is list \\\n                    else option_spec['required_when']\n\n                # validate conditions\n                for req_when_arg in req_when_args:\n                    splited_args_list = req_when_arg.split()\n                    for idx, req_arg in enumerate(splited_args_list):\n                        if req_arg in opts_names:\n                            splited_args_list[idx] = \\\n                                args.get(command_name, {}).get(req_arg.strip())\n                        if splited_args_list[idx] is None:\n                            option_results.append(False)\n                            break\n                        splited_args_list[idx] = str(splited_args_list[idx])\n                        if (splited_args_list[idx] not in ['and', 'or'] and\n                            not any(\n                                (c in '<>=') for c in splited_args_list[idx])):\n                            splited_args_list[idx] = \"'{0}'\".format(\n                                yaml.safe_load(splited_args_list[idx]))\n                    else:\n                        option_results.append(\n                            eval(' '.join(splited_args_list)))\n                if all(option_results) and \\\n                        self.spec_helper.get_option_state(\n                            command_name,\n                            option_spec['name'],\n                            args) == helper.OptionState['NOT_SET']:\n                    missing_args.append(option_spec['name'])\n        return missing_args\n\n    def validate_requires_args(self, args):\n        \"\"\"Check if all the required arguments have been provided. \"\"\"\n\n        silent_args = self.get_silent_args(args)\n\n        def validate_parser(parser_name, expected_options, parser_args):\n            \"\"\"Helper method to resolve dict_merge. \"\"\"\n\n            result = collections.defaultdict(list)\n            condition_req_args = self._get_conditionally_required_args(\n                parser_name, expected_options, args)\n\n            for option in expected_options:\n                name = option['name']\n\n                # check required options.\n                if (option.get('required', False) and\n                    name not in parser_args or\n                    option['name'] in condition_req_args) and \\\n                        name not in silent_args:\n                    result[parser_name].append(name)\n\n            return result\n\n        res = {}\n        for command_data in self.spec_helper.iterate_parsers():\n            cmd_name = command_data['name']\n            if cmd_name in args:\n                dict_utils.dict_merge(\n                    res,\n                    validate_parser(\n                        cmd_name,\n                        self.spec_helper.get_parser_option_specs(cmd_name),\n                        args[cmd_name]))\n\n        missing_args = dict((cmd_name, args)\n                            for cmd_name, args in res.items() if len(args) > 0)\n        if missing_args:\n            raise exceptions.IRRequiredArgsMissingException(missing_args)\n\n    def validate_length_args(self, args):\n        \"\"\"Check if value of arguments is not longer than length specified.\n\n        :param args: The received arguments.\n        \"\"\"\n        invalid_options = []\n        for parser_name, parser_dict in args.items():\n            for spec_option in \\\n                    self.spec_helper.get_parser_option_specs(parser_name):\n                if 'length' not in spec_option:\n                    # skip options that does not contain length\n                    continue\n                option_name = spec_option['name']\n                if option_name in parser_dict:\n                    # resolve length\n                    length = spec_option['length']\n                    option_value = parser_dict[option_name]\n                    if len(option_value) > int(length):\n                        # found invalid option, append to list of invalid opts\n                        invalid_options.append((\n                            option_name,\n                            option_value,\n                            length\n                        ))\n        if invalid_options:\n            # raise exception with all arguments that exceed length\n            raise exceptions.IRInvalidLengthException(invalid_options)\n\n    def validate_choices_args(self, args):\n        \"\"\"Check if value of choice arguments is one of the available choices.\n\n        :param args: The received arguments.\n        \"\"\"\n        invalid_options = []\n        for parser_name, parser_dict in args.items():\n            for spec_option in \\\n                    self.spec_helper.get_parser_option_specs(parser_name):\n                if 'choices' not in spec_option:\n                    # skip options that does not contain choices\n                    continue\n                option_name = spec_option['name']\n                if option_name in parser_dict:\n                    # resolve choices\n                    choices = spec_option['choices']\n                    option_value = parser_dict[option_name]\n                    if option_value not in choices:\n                        # found invalid option, append to list of invalid opts\n                        invalid_options.append((\n                            option_name,\n                            option_value,\n                            choices\n                        ))\n        if invalid_options:\n            # raise exception with all arguments that contains invalid choices\n            raise exceptions.IRInvalidChoiceException(invalid_options)\n\n    def validate_min_max_args(self, args):\n        \"\"\"Check if value of arguments is between minimum and maximum values.\n\n        :param args: The received arguments.\n        \"\"\"\n        invalid_options = []\n        for parser_name, parser_dict in args.items():\n            for spec_option in \\\n                    self.spec_helper.get_parser_option_specs(parser_name):\n                if all([key not in spec_option\n                        for key in ('maximum', 'minimum')]):\n                    # skip options that does not contain minimum or maximum\n                    continue\n                option_name = spec_option['name']\n\n                if option_name in parser_dict:\n                    option_value = parser_dict[option_name]\n                    min_value = spec_option.get('minimum')\n                    max_value = spec_option.get('maximum')\n                    # handle empty values in spec files which load as None\n                    min_value = '' if 'minimum' in spec_option \\\n                                      and min_value is None else min_value\n                    max_value = '' if 'maximum' in spec_option \\\n                                      and max_value is None else max_value\n\n                    values = {\n                        \"value\": option_value,\n                        \"maximum\": max_value,\n                        \"minimum\": min_value\n                    }\n\n                    # make sure that values are numbers\n                    is_all_values_numbers = True\n                    for name, num in values.items():\n                        if num is not None \\\n                                and (isinstance(num, bool) or\n                                     not isinstance(num, (int, float))):\n                            invalid_options.append((\n                                option_name,\n                                name,\n                                \"number\",\n                                type(num).__name__\n                            ))\n                            is_all_values_numbers = False\n\n                    if not is_all_values_numbers:\n                        # don't continue to min max checks since some of the\n                        # values are not numbers\n                        continue\n\n                    # check bigger than minimum\n                    if min_value is not None and option_value < min_value:\n                        invalid_options.append((\n                            option_name,\n                            \"minimum\",\n                            min_value,\n                            option_value\n                        ))\n                    # check smaller than maximum\n                    if max_value is not None and option_value > max_value:\n                        invalid_options.append((\n                            option_name,\n                            \"maximum\",\n                            max_value,\n                            option_value\n                        ))\n\n        if invalid_options:\n            # raise exception with all arguments that contains invalid choices\n            raise exceptions.IRInvalidMinMaxRangeException(invalid_options)\n\n    def get_silent_args(self, args):\n        \"\"\"list of silenced argument\n\n        :param args: The received arguments.\n        :return: list, slienced argument names\n        \"\"\"\n        silent_args_names = []\n        for (parser_name, parser_dict, arg_name, arg_value,\n             arg_spec) in self._iterate_received_arguments(args):\n            if arg_spec and 'silent' in arg_spec and \\\n                    self.spec_helper.get_option_state(\n                        parser_name,\n                        arg_name,\n                        args) == helper.OptionState['IS_SET']:\n                silent_args_names.extend(arg_spec['silent'])\n\n        return list(set(silent_args_names))\n\n    def get_nested_custom_and_control_args(self, args):\n        \"\"\"Split input arguments to control nested and custom.\n\n        Controls arguments: control the IR behavior. These arguments\n            will not be put into the spec yml file\n        Nested arguments: are used by the Ansible playbooks and will be put\n            into the spec yml file.\n        Custom arguments: Custom ansible variables to be used instead of the\n            normal nested usage.\n\n        :param args: the collected list of args.\n        :return: (dict, dict): flat dicts (control_args, nested_args)\n        \"\"\"\n        # returns flat dicts\n        nested = {}\n        control_args = {}\n        custom_args = {}\n        for (parser_name, parser_dict, arg_name, arg_value,\n             arg_spec) in self._iterate_received_arguments(args):\n            if all([arg_spec, arg_spec.get('type', None),\n                    arg_spec.get('type', None) in\n                    [ctype_name for ctype_name, klass in\n                     COMPLEX_TYPES.items() if klass.is_nested]\n                    ]) or ('is_shared_group_option' not in arg_spec):\n                if arg_name in nested:\n                    LOG.warning(\n                        \"Duplicated nested argument found:'{}'. \"\n                        \"Using old value: '{}'\".format(\n                            arg_name, nested[arg_name]))\n                elif arg_name in custom_args:\n                    LOG.warning(\n                        \"Duplicated custom argument found:'{}'. \"\n                        \"Using old value: '{}'\".format(\n                            arg_name, custom_args[arg_name]))\n                else:\n                    if \"ansible_variable\" in arg_spec:\n                        custom_args[arg_spec[\"ansible_variable\"]] = arg_value\n                    else:\n                        nested[arg_name] = arg_value\n            else:\n                if arg_name in control_args:\n                    LOG.warning(\n                        \"Duplicated control argument found: '{}'. Using \"\n                        \"old value: '{}'\".format(\n                            arg_name, control_args[arg_name]))\n                else:\n                    control_args[arg_name] = arg_value\n\n        return nested, control_args, custom_args\n\n    def _iterate_received_arguments(self, args):\n        \"\"\"Iterator helper method over all the received arguments\n\n        :return: yields tuple:\n            (spec name, spec dict,\n             argument name, argument value, argument spec)\n        \"\"\"\n        for spec_parser in self.spec_helper.iterate_parsers():\n            if spec_parser['name'] in args:\n                parser_dict = args[spec_parser['name']]\n                for arg_name, arg_val in parser_dict.items():\n                    arg_spec = self.spec_helper.get_option_spec(\n                        spec_parser['name'], arg_name)\n                    yield (spec_parser['name'], parser_dict,\n                           arg_name, arg_val, arg_spec)\n\n    def _convert_non_cli_args(self, parser_name, values_dict):\n        \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n        By default all the values are strings.\n\n        :param parser_name: The command name, e.g. main, virsh, ospd, etc\n        :param values_dict: The dict of with arguments\n       \"\"\"\n        for opt_name, opt_value in values_dict.items():\n            file_option_spec = self.spec_helper.get_option_spec(\n                parser_name, opt_name)\n            if file_option_spec.get('type', None) in ['int', ] or \\\n                    file_option_spec.get('action', None) in ['count', ]:\n                values_dict[opt_name] = int(opt_value)\n\n    def _merge_duplicated_cli_args(self, cli_args):\n        \"\"\"Merge duplicated arguments to all the parsers\n\n        This is need to handle control args, shared among several parsers.\n        for example, verbose, inventory\n        \"\"\"\n        for (parser_name, parser_dict, arg_name, arg_value,\n             arg_spec) in self._iterate_received_arguments(cli_args):\n            for parser_name2, parser_dict2 in cli_args.items():\n                if all([parser_name2, parser_name != parser_name2,\n                        arg_name not in parser_dict2]):\n                    if self.spec_helper.get_option_spec(parser_name2,\n                                                        arg_name):\n                        parser_dict2[arg_name] = arg_value\n",
            "file_path": "infrared/core/inspector/inspector.py",
            "human_label": "Check if all the required arguments have been provided.",
            "level": "class_runnable",
            "lineno": "448",
            "name": "validate_requires_args",
            "oracle_context": "{ \"apis\" : \"['dict', 'IRRequiredArgsMissingException', 'items', 'append', 'get', 'get_silent_args', 'dict_merge', 'len', 'defaultdict', 'get_parser_option_specs', '_get_conditionally_required_args', 'iterate_parsers']\", \"classes\" : \"['dict_utils', 'collections', 'exceptions']\", \"vars\" : \"['parser_name', 'spec_helper']\" }",
            "package": "inspector",
            "project": "redhat-openstack/infrared",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "6306092e73426c38ae68ad0f",
            "all_context": "{ \"import\" : \"string os collections six infrared \", \"file\" : \"LOG ; \", \"class\" : \"self.resolve_custom_types ; self.generate_answers_file(self,cli_args,spec_defaults) ; self.validate_min_max_args(self,args) ; self._merge_duplicated_cli_args ; self.validate_arg_sources ; self.vars ; self._iterate_received_arguments(self,args) ; self.get_deprecated_args(self) ; self._merge_duplicated_cli_args(self,cli_args) ; self.validate_choices_args(self,args) ; self._get_conditionally_required_args ; self.validate_arg_deprecation ; self.defaults ; self._get_conditionally_required_args(self,command_name,options_spec,args) ; self.parse_args(self,arg_parser,args) ; self.validate_arg_deprecation(self,cli_args,answer_file_args) ; self.resolve_custom_types(self,args) ; self.__init__(self,subparser,spec_dict,vars_dir,defaults_dir,plugin_path) ; self.validate_requires_args(self,args) ; self.create_complex_argumet_type(self,subcommand,type_name,option_name,spec_option) ; self.plugin_path ; self.get_spec_defaults(self) ; self.validate_arg_sources(cli_args,answer_file_args,spec_defaults) ; self.generate_answers_file ; self.create_complex_argumet_type ; self.parser ; self.get_answers_file_args(self,cli_args) ; self._convert_non_cli_args ; self.add_shared_groups(self,list_of_groups) ; self.parse_env_variable_from_file(value) ; self._get_defaults ; self.get_deprecated_args ; self.validate_requires_args ; self.get_answers_file_args ; self._convert_non_cli_args(self,parser_name,values_dict) ; self.validate_length_args ; self.spec_helper ; self.get_env_option(name) ; self.validate_length_args(self,args) ; self.validate_choices_args ; self.get_silent_args(self,args) ; self.get_spec_defaults ; self.get_nested_custom_and_control_args(self,args) ; self.from_plugin(cls,subparser,plugin,base_groups) ; self.get_silent_args ; self._get_defaults(self,default_getter_func) ; self.validate_min_max_args ; self.get_nested_custom_and_control_args ; self._iterate_received_arguments ; \" }",
            "code": "    def get_nested_custom_and_control_args(self, args):\n        \"\"\"Split input arguments to control nested and custom.\n\n        Controls arguments: control the IR behavior. These arguments\n            will not be put into the spec yml file\n        Nested arguments: are used by the Ansible playbooks and will be put\n            into the spec yml file.\n        Custom arguments: Custom ansible variables to be used instead of the\n            normal nested usage.\n\n        :param args: the collected list of args.\n        :return: (dict, dict): flat dicts (control_args, nested_args)\n        \"\"\"\n        # returns flat dicts\n        nested = {}\n        control_args = {}\n        custom_args = {}\n        for (parser_name, parser_dict, arg_name, arg_value,\n             arg_spec) in self._iterate_received_arguments(args):\n            if all([arg_spec, arg_spec.get('type', None),\n                    arg_spec.get('type', None) in\n                    [ctype_name for ctype_name, klass in\n                     COMPLEX_TYPES.items() if klass.is_nested]\n                    ]) or ('is_shared_group_option' not in arg_spec):\n                if arg_name in nested:\n                    LOG.warning(\n                        \"Duplicated nested argument found:'{}'. \"\n                        \"Using old value: '{}'\".format(\n                            arg_name, nested[arg_name]))\n                elif arg_name in custom_args:\n                    LOG.warning(\n                        \"Duplicated custom argument found:'{}'. \"\n                        \"Using old value: '{}'\".format(\n                            arg_name, custom_args[arg_name]))\n                else:\n                    if \"ansible_variable\" in arg_spec:\n                        custom_args[arg_spec[\"ansible_variable\"]] = arg_value\n                    else:\n                        nested[arg_name] = arg_value\n            else:\n                if arg_name in control_args:\n                    LOG.warning(\n                        \"Duplicated control argument found: '{}'. Using \"\n                        \"old value: '{}'\".format(\n                            arg_name, control_args[arg_name]))\n                else:\n                    control_args[arg_name] = arg_value\n\n        return nested, control_args, custom_args\n",
            "dependency": "",
            "docstring": "Split input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)",
            "end_lineno": "681",
            "file_content": "import collections\nimport os\nfrom six.moves import configparser\nfrom string import Template\nimport yaml\n\nfrom infrared.core.cli.cli import CliParser\nfrom infrared.core.cli.cli import COMPLEX_TYPES\nfrom infrared.core.inspector import helper\nfrom infrared.core.utils import dict_utils\nfrom infrared.core.utils import exceptions\nfrom infrared.core.utils import logger\n\nLOG = logger.LOG\n\n\nclass SpecParser(object):\n    \"\"\"Parses input arguments from different sources (cli, answers file). \"\"\"\n\n    @classmethod\n    def from_plugin(cls, subparser, plugin, base_groups):\n        \"\"\"Reads spec & vars from plugin and constructs the parser instance\n\n        :param subparser: argparse.subparser to extend\n        :param plugin: InfraredPlugin object\n        :param base_groups: dict, included groups\n        :return: SpecParser object based on given plugin spec & vars\n        \"\"\"\n\n        spec_dict = base_groups or {}\n        with open(plugin.spec) as stream:\n            spec = yaml.safe_load(stream) or {}\n            dict_utils.dict_merge(\n                base_groups,\n                spec,\n                dict_utils.ConflictResolver.unique_append_list_resolver)\n\n        # The \"try-excpet\" block here is for adding spec file path if it\n        # includes an unsupported option type\n        try:\n            return SpecParser(subparser, spec_dict, plugin.vars_dir,\n                              plugin.defaults_dir, plugin.path)\n        except exceptions.IRUnsupportedSpecOptionType as ex:\n            ex.message += ' in file: {}'.format(plugin.spec)\n            raise ex\n\n    def __init__(self, subparser, spec_dict, vars_dir, defaults_dir,\n                 plugin_path):\n        \"\"\"Constructor.\n\n        :param subparser: argparse.subparser to extend\n        :param spec_dict: dict with CLI description\n        :param vars_dir: Path to plugin's vars dir\n        :param defaults_dir: Path to plugin's defaults dir\n        \"\"\"\n        self.vars = vars_dir\n        self.defaults = defaults_dir\n        self.plugin_path = plugin_path\n        self.spec_helper = helper.SpecDictHelper(spec_dict)\n\n        # create parser\n        self.parser = CliParser.create_parser(self, subparser)\n\n    def add_shared_groups(self, list_of_groups):\n        \"\"\"Adds the user defined shared groups\n\n        :param list_of_groups: list, of group dicts\n        \"\"\"\n        shared_groups = self.spec_helper.spec_dict.get('shared_groups', [])\n        shared_groups.expand(list_of_groups)\n        self.spec_helper.spec_dict['shared_groups'] = shared_groups\n\n    def _get_defaults(self, default_getter_func):\n        \"\"\"Resolve arguments' values from cli or answers file.\n\n        :param default_getter_func: callable. will be called for all the\n            available options in spec file.\n        \"\"\"\n\n        result = collections.defaultdict(dict)\n        for parser, option in self.spec_helper.iterate_option_specs():\n            default_value = default_getter_func(option)\n            if default_value is not None:\n                sub = parser['name']\n                result[sub][option['name']] = default_value\n\n        return result\n\n    def get_spec_defaults(self):\n        \"\"\"Resolve arguments' values from spec and other sources. \"\"\"\n\n        def spec_default_getter(option):\n            \"\"\"Getter function to retrieve the default value from spec.\n\n            :param option: argument name\n            \"\"\"\n\n            # first try to get environment variable with IR_ prefix\n            default_value = SpecParser.get_env_option(option['name'])\n            if default_value is not None:\n                LOG.info(\n                    \"[environ] Loading '{0}' default value\"\n                    \" '{1}' from the environment variable\".format(\n                        option['name'], default_value))\n            elif option.get('default', None) is not None:\n                default_value = option['default']\n            elif option.get('action', None) in ['store_true']:\n                default_value = False\n            return default_value\n\n        return self._get_defaults(spec_default_getter)\n\n    @staticmethod\n    def get_env_option(name):\n        \"\"\"Try get \"\"\"\n        return os.environ.get('IR_' + name.upper().replace('-', '_'))\n\n    def get_deprecated_args(self):\n        \"\"\"Returning dict with options which deprecate others. \"\"\"\n\n        result = collections.defaultdict(dict)\n        for parser, option in self.spec_helper.iterate_option_specs():\n            if option.get('deprecates') is not None:\n                result[option.get('deprecates')] = option.get('name')\n\n        return result\n\n    @staticmethod\n    def parse_env_variable_from_file(value):\n        if isinstance(value, str):\n            t = Template(value)\n            try:\n                value = t.substitute(os.environ)\n            except KeyError as undefined_var:\n                raise exceptions.IRAnswersFileEnvVarNotDefined(undefined_var)\n        return value\n\n    def get_answers_file_args(self, cli_args):\n        \"\"\"Resolve arguments' values from answers INI file. \"\"\"\n\n        file_result = {}\n        args_to_remove = []\n        for (parser_name, parser_dict, arg_name, arg_value,\n             option_spec) in self._iterate_received_arguments(cli_args):\n            file_result[parser_name] = file_result.get(parser_name, {})\n            if option_spec and option_spec.get(\n                    'action', '') == 'read-answers':\n                # Iterate over arguments supplied by file\n                for parsed_arg in parser_dict[arg_name]:\n                    # Supplied arguments' value can be a list\n                    if isinstance(parser_dict[arg_name][parsed_arg], list):\n                        i = 0\n                        # Iterrate over argument values list\n                        for parsed_value in parser_dict[arg_name][parsed_arg]:\n                            parser_dict[arg_name][parsed_arg][i] = \\\n                                SpecParser.parse_env_variable_from_file(parsed_value)\n                            i += 1\n                    else:\n                        parser_dict[arg_name][parsed_arg] = \\\n                            SpecParser.parse_env_variable_from_file(parser_dict[arg_name][parsed_arg])\n                # we have config option. saving it.\n                self._convert_non_cli_args(\n                    parser_name, parser_dict[arg_name])\n                dict_utils.dict_merge(\n                    file_result[parser_name],\n                    parser_dict[arg_name])\n                # remove from cli args\n                args_to_remove.append((parser_name, arg_name))\n\n        # remove parser dict outside loop to avoid iteration dict modification\n        for parser_name, arg_name in args_to_remove:\n            for spec_parser in self.spec_helper.iterate_parsers():\n                if spec_parser['name'] in cli_args and spec_parser['name'] == parser_name:\n                    parser_dict = cli_args[spec_parser['name']]\n                    parser_dict.pop(arg_name)\n                    break\n\n        return file_result\n\n    def generate_answers_file(self, cli_args, spec_defaults):\n        \"\"\"Generates answers INI file\n\n        :param cli_args: list, cli arguments.\n        :param spec_defaults: the default values.\n        \"\"\"\n\n        def put_option(config, parser_name, option_name, value):\n            for opt_help in option.get('help', '').split('\\n'):\n                help_opt = '# ' + opt_help\n\n                # add help comment\n                if config.has_option(parser_name, help_opt):\n                    config.remove_option(parser_name, help_opt)\n                config.set(\n                    parser_name, help_opt)\n\n            if config.has_option(parser_name, option_name):\n                value = config.get(parser_name, option_name)\n                config.remove_option(parser_name, option_name)\n\n            config.set(\n                parser_name,\n                option_name,\n                str(value))\n\n        file_generated = False\n\n        # load generate answers file for all the parsers\n        for (parser_name, parser_dict, arg_name, arg_value,\n             option_spec) in self._iterate_received_arguments(cli_args):\n            if option_spec and option_spec.get(\n                    'action', '') == 'generate-answers':\n                options_to_save = \\\n                    self.spec_helper.get_parser_option_specs(parser_name)\n                out_answers = configparser.ConfigParser(allow_no_value=True)\n\n                if not out_answers.has_section(parser_name):\n                    out_answers.add_section(parser_name)\n\n                for option in options_to_save:\n                    opt_name = option['name']\n                    if opt_name in parser_dict:\n                        put_option(\n                            out_answers,\n                            parser_name,\n                            opt_name,\n                            parser_dict[opt_name])\n                    elif opt_name in spec_defaults[parser_name]:\n                        put_option(\n                            out_answers,\n                            parser_name,\n                            opt_name,\n                            spec_defaults[parser_name][opt_name])\n                    elif option.get('required', False):\n                        put_option(\n                            out_answers,\n                            parser_name,\n                            '# ' + opt_name,\n                            \"Required argument. \"\n                            \"Edit with one of the allowed values OR \"\n                            \"override with \"\n                            \"CLI: --{}=<option>\".format(opt_name))\n\n                # write to file\n                with open(arg_value, 'w') as answers_file:\n                    out_answers.write(answers_file)\n                file_generated = True\n\n        return file_generated\n\n    def resolve_custom_types(self, args):\n        \"\"\"Transforms the arguments with custom types\n\n        :param args: the list of received arguments.\n        \"\"\"\n        for parser_name, parser_dict in args.items():\n            spec_complex_options = [opt for opt in\n                                    self.spec_helper.get_parser_option_specs(\n                                        parser_name) if\n                                    opt.get('type', None) in COMPLEX_TYPES]\n            for spec_option in spec_complex_options:\n                option_name = spec_option['name']\n                if option_name in parser_dict:\n                    # we have custom type to resolve\n                    type_name = spec_option['type']\n                    option_value = parser_dict[option_name]\n                    action = self.create_complex_argumet_type(\n                        parser_name,\n                        type_name,\n                        option_name,\n                        spec_option)\n\n                    # resolving value\n                    parser_dict[option_name] = action.resolve(option_value)\n\n    def create_complex_argumet_type(self, subcommand, type_name, option_name,\n                                    spec_option):\n        \"\"\"Build the complex argument type\n\n        :param subcommand: the command name\n        :param type_name: the complex type name\n        :param option_name: the option name\n        :param spec_option: option's specifications\n        :return: the complex type instance\n        \"\"\"\n        complex_action = COMPLEX_TYPES.get(\n            type_name, None)\n        if complex_action is None:\n            raise exceptions.SpecParserException(\n                \"Unknown complex type: {}\".format(type_name))\n        return complex_action(\n            option_name,\n            (self.vars, self.defaults, self.plugin_path),\n            subcommand,\n            spec_option)\n\n    def parse_args(self, arg_parser, args=None):\n        \"\"\"Parses all the arguments (cli, answers file)\n\n        :return: None, if ``--generate-answers-file`` in arg_arg_parser\n        :return: (dict, dict):\n            * command arguments dict (arguments to control the IR logic)\n            * nested arguments dict (arguments to pass to the playbooks)\n        \"\"\"\n\n        spec_defaults = self.get_spec_defaults()\n        cli_args = CliParser.parse_cli_input(arg_parser, args)\n\n        file_args = self.get_answers_file_args(cli_args)\n\n        # generate answers file and exit\n        if self.generate_answers_file(cli_args, spec_defaults):\n            LOG.warning(\"Answers file generated. Exiting.\")\n\n        # print warnings when something was overridden from non-cli source.\n        self.validate_arg_sources(cli_args, file_args,\n                                  spec_defaults)\n\n        # print warnings for deprecated\n        self.validate_arg_deprecation(cli_args, file_args)\n\n        # now filter defaults to have only parser defined in cli\n        defaults = dict((key, spec_defaults[key])\n                        for key in cli_args.keys() if\n                        key in spec_defaults)\n\n        # copy cli args with the same name to all parser groups\n        self._merge_duplicated_cli_args(cli_args)\n        self._merge_duplicated_cli_args(file_args)\n\n        dict_utils.dict_merge(defaults, file_args)\n        dict_utils.dict_merge(defaults, cli_args)\n        self.validate_requires_args(defaults)\n        self.validate_length_args(defaults)\n        self.validate_choices_args(defaults)\n        self.validate_min_max_args(defaults)\n\n        # now resolve complex types.\n        self.resolve_custom_types(defaults)\n        nested, control, custom = \\\n            self.get_nested_custom_and_control_args(defaults)\n        return nested, control, custom\n\n    def validate_arg_deprecation(self, cli_args, answer_file_args):\n        \"\"\"Validates and prints the deprecated arguments.\n\n        :param cli_args: the dict of arguments from cli\n        :param answer_file_args:  the dict of arguments from files\n        \"\"\"\n\n        for deprecated, deprecates in self.get_deprecated_args().items():\n            for input_args in (answer_file_args.items(), cli_args.items()):\n                for command, command_dict in input_args:\n                    if deprecated in command_dict:\n                        if deprecates in command_dict:\n                            raise exceptions.IRDeprecationException(\n                                \"[{}] Argument '{}' deprecates '{}',\"\n                                \" please use only the new one.\".format(\n                                    command, deprecated, deprecates))\n\n                        if deprecated in answer_file_args[command]:\n                            answer_file_args[command][deprecates] = \\\n                                answer_file_args[command][deprecated]\n\n                        if deprecated in cli_args[command]:\n                            cli_args[command][deprecates] = \\\n                                cli_args[command][deprecated]\n\n                        LOG.warning(\n                            \"[{}] Argument '{}' was deprecated,\"\n                            \" please use '{}'.\".format(\n                                command, deprecated, deprecates))\n\n    @staticmethod\n    def validate_arg_sources(cli_args, answer_file_args, spec_defaults):\n        \"\"\"Validates and prints the arguments' source.\n\n        :param cli_args: the dict of arguments from cli\n        :param answer_file_args:  the dict of arguments from files\n        :param spec_defaults:  the default values from spec files\n        \"\"\"\n\n        def show_diff(diff, command_name, cmd_dict, source_name):\n            if diff:\n                for arg_name in diff:\n                    value = cmd_dict[arg_name]\n                    LOG.info(\n                        \"[{}] Argument '{}' was set to\"\n                        \" '{}' from the {} source.\".format(\n                            command_name, arg_name, value, source_name))\n\n        for command, command_dict in cli_args.items():\n            file_dict = answer_file_args.get(command, {})\n            file_diff = set(file_dict.keys()) - set(command_dict.keys())\n            show_diff(file_diff, command, file_dict, 'answers file')\n\n            def_dict = spec_defaults.get(command, {})\n            default_diff = set(def_dict.keys()) - set(\n                command_dict.keys()) - file_diff\n            show_diff(default_diff, command, def_dict, 'spec defaults')\n\n    def _get_conditionally_required_args(self, command_name, options_spec,\n                                         args):\n        \"\"\"List arguments with ``required_when`` condition matched.\n\n        :param command_name: the command name.\n        :param options_spec:  the list of command spec options.\n        :param args: the received input arguments\n        :return: list, list of argument names with matched ``required_when``\n            condition\n        \"\"\"\n        opts_names = [option_spec['name'] for option_spec in options_spec]\n        missing_args = []\n        for option_spec in options_spec:\n            option_results = []\n            if option_spec and 'required_when' in option_spec:\n                req_when_args = [option_spec['required_when']] \\\n                    if not type(option_spec['required_when']) is list \\\n                    else option_spec['required_when']\n\n                # validate conditions\n                for req_when_arg in req_when_args:\n                    splited_args_list = req_when_arg.split()\n                    for idx, req_arg in enumerate(splited_args_list):\n                        if req_arg in opts_names:\n                            splited_args_list[idx] = \\\n                                args.get(command_name, {}).get(req_arg.strip())\n                        if splited_args_list[idx] is None:\n                            option_results.append(False)\n                            break\n                        splited_args_list[idx] = str(splited_args_list[idx])\n                        if (splited_args_list[idx] not in ['and', 'or'] and\n                            not any(\n                                (c in '<>=') for c in splited_args_list[idx])):\n                            splited_args_list[idx] = \"'{0}'\".format(\n                                yaml.safe_load(splited_args_list[idx]))\n                    else:\n                        option_results.append(\n                            eval(' '.join(splited_args_list)))\n                if all(option_results) and \\\n                        self.spec_helper.get_option_state(\n                            command_name,\n                            option_spec['name'],\n                            args) == helper.OptionState['NOT_SET']:\n                    missing_args.append(option_spec['name'])\n        return missing_args\n\n    def validate_requires_args(self, args):\n        \"\"\"Check if all the required arguments have been provided. \"\"\"\n\n        silent_args = self.get_silent_args(args)\n\n        def validate_parser(parser_name, expected_options, parser_args):\n            \"\"\"Helper method to resolve dict_merge. \"\"\"\n\n            result = collections.defaultdict(list)\n            condition_req_args = self._get_conditionally_required_args(\n                parser_name, expected_options, args)\n\n            for option in expected_options:\n                name = option['name']\n\n                # check required options.\n                if (option.get('required', False) and\n                    name not in parser_args or\n                    option['name'] in condition_req_args) and \\\n                        name not in silent_args:\n                    result[parser_name].append(name)\n\n            return result\n\n        res = {}\n        for command_data in self.spec_helper.iterate_parsers():\n            cmd_name = command_data['name']\n            if cmd_name in args:\n                dict_utils.dict_merge(\n                    res,\n                    validate_parser(\n                        cmd_name,\n                        self.spec_helper.get_parser_option_specs(cmd_name),\n                        args[cmd_name]))\n\n        missing_args = dict((cmd_name, args)\n                            for cmd_name, args in res.items() if len(args) > 0)\n        if missing_args:\n            raise exceptions.IRRequiredArgsMissingException(missing_args)\n\n    def validate_length_args(self, args):\n        \"\"\"Check if value of arguments is not longer than length specified.\n\n        :param args: The received arguments.\n        \"\"\"\n        invalid_options = []\n        for parser_name, parser_dict in args.items():\n            for spec_option in \\\n                    self.spec_helper.get_parser_option_specs(parser_name):\n                if 'length' not in spec_option:\n                    # skip options that does not contain length\n                    continue\n                option_name = spec_option['name']\n                if option_name in parser_dict:\n                    # resolve length\n                    length = spec_option['length']\n                    option_value = parser_dict[option_name]\n                    if len(option_value) > int(length):\n                        # found invalid option, append to list of invalid opts\n                        invalid_options.append((\n                            option_name,\n                            option_value,\n                            length\n                        ))\n        if invalid_options:\n            # raise exception with all arguments that exceed length\n            raise exceptions.IRInvalidLengthException(invalid_options)\n\n    def validate_choices_args(self, args):\n        \"\"\"Check if value of choice arguments is one of the available choices.\n\n        :param args: The received arguments.\n        \"\"\"\n        invalid_options = []\n        for parser_name, parser_dict in args.items():\n            for spec_option in \\\n                    self.spec_helper.get_parser_option_specs(parser_name):\n                if 'choices' not in spec_option:\n                    # skip options that does not contain choices\n                    continue\n                option_name = spec_option['name']\n                if option_name in parser_dict:\n                    # resolve choices\n                    choices = spec_option['choices']\n                    option_value = parser_dict[option_name]\n                    if option_value not in choices:\n                        # found invalid option, append to list of invalid opts\n                        invalid_options.append((\n                            option_name,\n                            option_value,\n                            choices\n                        ))\n        if invalid_options:\n            # raise exception with all arguments that contains invalid choices\n            raise exceptions.IRInvalidChoiceException(invalid_options)\n\n    def validate_min_max_args(self, args):\n        \"\"\"Check if value of arguments is between minimum and maximum values.\n\n        :param args: The received arguments.\n        \"\"\"\n        invalid_options = []\n        for parser_name, parser_dict in args.items():\n            for spec_option in \\\n                    self.spec_helper.get_parser_option_specs(parser_name):\n                if all([key not in spec_option\n                        for key in ('maximum', 'minimum')]):\n                    # skip options that does not contain minimum or maximum\n                    continue\n                option_name = spec_option['name']\n\n                if option_name in parser_dict:\n                    option_value = parser_dict[option_name]\n                    min_value = spec_option.get('minimum')\n                    max_value = spec_option.get('maximum')\n                    # handle empty values in spec files which load as None\n                    min_value = '' if 'minimum' in spec_option \\\n                                      and min_value is None else min_value\n                    max_value = '' if 'maximum' in spec_option \\\n                                      and max_value is None else max_value\n\n                    values = {\n                        \"value\": option_value,\n                        \"maximum\": max_value,\n                        \"minimum\": min_value\n                    }\n\n                    # make sure that values are numbers\n                    is_all_values_numbers = True\n                    for name, num in values.items():\n                        if num is not None \\\n                                and (isinstance(num, bool) or\n                                     not isinstance(num, (int, float))):\n                            invalid_options.append((\n                                option_name,\n                                name,\n                                \"number\",\n                                type(num).__name__\n                            ))\n                            is_all_values_numbers = False\n\n                    if not is_all_values_numbers:\n                        # don't continue to min max checks since some of the\n                        # values are not numbers\n                        continue\n\n                    # check bigger than minimum\n                    if min_value is not None and option_value < min_value:\n                        invalid_options.append((\n                            option_name,\n                            \"minimum\",\n                            min_value,\n                            option_value\n                        ))\n                    # check smaller than maximum\n                    if max_value is not None and option_value > max_value:\n                        invalid_options.append((\n                            option_name,\n                            \"maximum\",\n                            max_value,\n                            option_value\n                        ))\n\n        if invalid_options:\n            # raise exception with all arguments that contains invalid choices\n            raise exceptions.IRInvalidMinMaxRangeException(invalid_options)\n\n    def get_silent_args(self, args):\n        \"\"\"list of silenced argument\n\n        :param args: The received arguments.\n        :return: list, slienced argument names\n        \"\"\"\n        silent_args_names = []\n        for (parser_name, parser_dict, arg_name, arg_value,\n             arg_spec) in self._iterate_received_arguments(args):\n            if arg_spec and 'silent' in arg_spec and \\\n                    self.spec_helper.get_option_state(\n                        parser_name,\n                        arg_name,\n                        args) == helper.OptionState['IS_SET']:\n                silent_args_names.extend(arg_spec['silent'])\n\n        return list(set(silent_args_names))\n\n    def get_nested_custom_and_control_args(self, args):\n        \"\"\"Split input arguments to control nested and custom.\n\n        Controls arguments: control the IR behavior. These arguments\n            will not be put into the spec yml file\n        Nested arguments: are used by the Ansible playbooks and will be put\n            into the spec yml file.\n        Custom arguments: Custom ansible variables to be used instead of the\n            normal nested usage.\n\n        :param args: the collected list of args.\n        :return: (dict, dict): flat dicts (control_args, nested_args)\n        \"\"\"\n        # returns flat dicts\n        nested = {}\n        control_args = {}\n        custom_args = {}\n        for (parser_name, parser_dict, arg_name, arg_value,\n             arg_spec) in self._iterate_received_arguments(args):\n            if all([arg_spec, arg_spec.get('type', None),\n                    arg_spec.get('type', None) in\n                    [ctype_name for ctype_name, klass in\n                     COMPLEX_TYPES.items() if klass.is_nested]\n                    ]) or ('is_shared_group_option' not in arg_spec):\n                if arg_name in nested:\n                    LOG.warning(\n                        \"Duplicated nested argument found:'{}'. \"\n                        \"Using old value: '{}'\".format(\n                            arg_name, nested[arg_name]))\n                elif arg_name in custom_args:\n                    LOG.warning(\n                        \"Duplicated custom argument found:'{}'. \"\n                        \"Using old value: '{}'\".format(\n                            arg_name, custom_args[arg_name]))\n                else:\n                    if \"ansible_variable\" in arg_spec:\n                        custom_args[arg_spec[\"ansible_variable\"]] = arg_value\n                    else:\n                        nested[arg_name] = arg_value\n            else:\n                if arg_name in control_args:\n                    LOG.warning(\n                        \"Duplicated control argument found: '{}'. Using \"\n                        \"old value: '{}'\".format(\n                            arg_name, control_args[arg_name]))\n                else:\n                    control_args[arg_name] = arg_value\n\n        return nested, control_args, custom_args\n\n    def _iterate_received_arguments(self, args):\n        \"\"\"Iterator helper method over all the received arguments\n\n        :return: yields tuple:\n            (spec name, spec dict,\n             argument name, argument value, argument spec)\n        \"\"\"\n        for spec_parser in self.spec_helper.iterate_parsers():\n            if spec_parser['name'] in args:\n                parser_dict = args[spec_parser['name']]\n                for arg_name, arg_val in parser_dict.items():\n                    arg_spec = self.spec_helper.get_option_spec(\n                        spec_parser['name'], arg_name)\n                    yield (spec_parser['name'], parser_dict,\n                           arg_name, arg_val, arg_spec)\n\n    def _convert_non_cli_args(self, parser_name, values_dict):\n        \"\"\"Casts arguments to correct types by modifying values_dict param.\n\n        By default all the values are strings.\n\n        :param parser_name: The command name, e.g. main, virsh, ospd, etc\n        :param values_dict: The dict of with arguments\n       \"\"\"\n        for opt_name, opt_value in values_dict.items():\n            file_option_spec = self.spec_helper.get_option_spec(\n                parser_name, opt_name)\n            if file_option_spec.get('type', None) in ['int', ] or \\\n                    file_option_spec.get('action', None) in ['count', ]:\n                values_dict[opt_name] = int(opt_value)\n\n    def _merge_duplicated_cli_args(self, cli_args):\n        \"\"\"Merge duplicated arguments to all the parsers\n\n        This is need to handle control args, shared among several parsers.\n        for example, verbose, inventory\n        \"\"\"\n        for (parser_name, parser_dict, arg_name, arg_value,\n             arg_spec) in self._iterate_received_arguments(cli_args):\n            for parser_name2, parser_dict2 in cli_args.items():\n                if all([parser_name2, parser_name != parser_name2,\n                        arg_name not in parser_dict2]):\n                    if self.spec_helper.get_option_spec(parser_name2,\n                                                        arg_name):\n                        parser_dict2[arg_name] = arg_value\n",
            "file_path": "infrared/core/inspector/inspector.py",
            "human_label": "Split input arguments to control nested and custom.\n\nControls arguments: control the IR behavior. These arguments\n    will not be put into the spec yml file\nNested arguments: are used by the Ansible playbooks and will be put\n    into the spec yml file.\nCustom arguments: Custom ansible variables to be used instead of the\n    normal nested usage.\n\n:param args: the collected list of args.\n:return: (dict, dict): flat dicts (control_args, nested_args)",
            "level": "file_runnable",
            "lineno": "633",
            "name": "get_nested_custom_and_control_args",
            "oracle_context": "{ \"apis\" : \"['warning', 'items', 'format', 'get', 'all', '_iterate_received_arguments']\", \"classes\" : \"['COMPLEX_TYPES']\", \"vars\" : \"['LOG', 'Str', 'arg_spec', 'is_nested']\" }",
            "package": "inspector",
            "project": "redhat-openstack/infrared",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "6306092e73426c38ae68ad11",
            "all_context": "{ \"import\" : \"infrared \", \"file\" : \"\", \"class\" : \"self.generate_settings(entry_point,nested_args,delimiter) ; self.merge_extra_vars(vars_dict,extra_vars) ; \" }",
            "code": "    @staticmethod\n    def merge_extra_vars(vars_dict, extra_vars=None):\n        \"\"\"Extend ``vars_dict`` with ``extra-vars``\n\n        :param vars_dict: Dictionary to merge extra-vars into\n        :param extra_vars: List of extra-vars\n        \"\"\"\n        for extra_var in extra_vars or []:\n            if extra_var.startswith('@'):\n                with open(extra_var[1:]) as f_obj:\n                    loaded_yml = yaml.safe_load(f_obj)\n\n                dict_utils.dict_merge(\n                    vars_dict,\n                    loaded_yml,\n                    conflict_resolver=dict_utils.ConflictResolver.\n                    unique_append_list_resolver)\n\n            else:\n                if '=' not in extra_var:\n                    raise exceptions.IRExtraVarsException(extra_var)\n                key, value = extra_var.split(\"=\", 1)\n                if value.startswith('@'):\n                    with open(value[1:]) as f_obj:\n                        loaded_yml = yaml.safe_load(f_obj)\n\n                    tmp_dict = {}\n                    dict_utils.dict_insert(tmp_dict, loaded_yml, *key.split(\".\"))\n\n                    dict_utils.dict_merge(\n                        vars_dict,\n                        tmp_dict,\n                        conflict_resolver=dict_utils.ConflictResolver.\n                        unique_append_list_resolver)\n\n                else:\n                    dict_utils.dict_insert(vars_dict, value, *key.split(\".\"))\n        return vars_dict\n",
            "dependency": "",
            "docstring": "Extend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars",
            "end_lineno": "85",
            "file_content": "import yaml\n\nfrom infrared.core.utils import dict_utils\nfrom infrared.core.utils import exceptions\n\n\nclass VarsDictManager(object):\n\n    @staticmethod\n    def generate_settings(entry_point,\n                          nested_args,\n                          delimiter='-'):\n        \"\"\"Unifies all input into a single dict of Ansible extra-vars\n\n        :param entry_point: All input will be nested under this key\n        :param nested_args: dict. these values will be nested\n            example:\n                {\n                    foo-bar: value1,\n                    foo2: value2\n                    foo-another-bar: value3\n                }\n        :param delimiter: character to split keys by.\n        :return: dict. nest input with keys splitted by delimiter\n\n        >>> VarsDictManager.generate_settings(\n        ... 'entry_point', {'foo-bar': 'value1',\n        ...                 'foo2': 'value2',\n        ...                 'foo-another-bar': 'value3'})\n        {'entry_point': {'foo': {'bar': 'value1', 'another':\\\n {'bar': 'value3'}}, 'foo2': 'value2'}}\n        \"\"\"\n        vars_dict = {entry_point: {}}\n        try:\n            for _name, argument in nested_args.items():\n                dict_utils.dict_insert(vars_dict[entry_point],\n                                       argument,\n                                       *_name.split(delimiter))\n\n        # handle errors here and provide more output for user if required\n        except exceptions.IRKeyNotFoundException as key_exception:\n            if key_exception and key_exception.key.startswith(\"private.\"):\n                raise exceptions.IRPrivateSettingsMissingException(\n                    key_exception.key)\n            else:\n                raise\n        return vars_dict\n\n    @staticmethod\n    def merge_extra_vars(vars_dict, extra_vars=None):\n        \"\"\"Extend ``vars_dict`` with ``extra-vars``\n\n        :param vars_dict: Dictionary to merge extra-vars into\n        :param extra_vars: List of extra-vars\n        \"\"\"\n        for extra_var in extra_vars or []:\n            if extra_var.startswith('@'):\n                with open(extra_var[1:]) as f_obj:\n                    loaded_yml = yaml.safe_load(f_obj)\n\n                dict_utils.dict_merge(\n                    vars_dict,\n                    loaded_yml,\n                    conflict_resolver=dict_utils.ConflictResolver.\n                    unique_append_list_resolver)\n\n            else:\n                if '=' not in extra_var:\n                    raise exceptions.IRExtraVarsException(extra_var)\n                key, value = extra_var.split(\"=\", 1)\n                if value.startswith('@'):\n                    with open(value[1:]) as f_obj:\n                        loaded_yml = yaml.safe_load(f_obj)\n\n                    tmp_dict = {}\n                    dict_utils.dict_insert(tmp_dict, loaded_yml, *key.split(\".\"))\n\n                    dict_utils.dict_merge(\n                        vars_dict,\n                        tmp_dict,\n                        conflict_resolver=dict_utils.ConflictResolver.\n                        unique_append_list_resolver)\n\n                else:\n                    dict_utils.dict_insert(vars_dict, value, *key.split(\".\"))\n",
            "file_path": "infrared/core/settings.py",
            "human_label": "Extend ``vars_dict`` with ``extra-vars``\n\n:param vars_dict: Dictionary to merge extra-vars into\n:param extra_vars: List of extra-vars",
            "level": "project_runnable",
            "lineno": "49",
            "name": "merge_extra_vars",
            "oracle_context": "{ \"apis\" : \"['safe_load', 'dict_insert', 'IRExtraVarsException', 'startswith', 'dict_merge', 'split', 'open']\", \"classes\" : \"['yaml', 'exceptions', 'dict_utils']\", \"vars\" : \"['ConflictResolver', 'unique_append_list_resolver']\" }",
            "package": "settings",
            "project": "redhat-openstack/infrared",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "6306092f73426c38ae68ad13",
            "all_context": "{ \"import\" : \"tempfile re datetime sys json distutils os errno main datetime infrared ansible \", \"file\" : \"LOG ; ansible_playbook(ir_workspace,ir_plugin,playbook_path,verbose,extra_vars,ansible_args) ; _run_playbook(cli_args,vars_dict,ir_workspace,ir_plugin) ; \", \"class\" : \"\" }",
            "code": "def ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"Wraps the 'ansible-playbook' CLI.\n\n     :param ir_workspace: An Infrared Workspace object represents the active\n     workspace\n     :param ir_plugin: An InfraredPlugin object of the current plugin\n     :param playbook_path: the playbook to invoke\n     :param verbose: Ansible verbosity level\n     :param extra_vars: dict. Passed to Ansible as extra-vars\n     :param ansible_args: dict of ansible-playbook arguments to plumb down\n         directly to Ansible.\n    \"\"\"\n    ansible_args = ansible_args or []\n    LOG.debug(\"Additional ansible args: {}\".format(ansible_args))\n\n    # hack for verbosity\n    from ansible.utils.display import Display\n    display = Display(verbosity=verbose)\n    import __main__ as main\n    setattr(main, \"display\", display)\n\n    # TODO(yfried): Use proper ansible API instead of emulating CLI\n    cli_args = ['execute',\n                playbook_path,\n                '--inventory', ir_workspace.inventory]\n\n    # infrared should not change ansible verbosity unless user specifies that\n    if verbose:\n        cli_args.append('-' + 'v' * int(verbose))\n\n    cli_args.extend(ansible_args)\n\n    results = _run_playbook(cli_args, vars_dict=extra_vars or {},\n                            ir_workspace=ir_workspace, ir_plugin=ir_plugin)\n\n    if results:\n        LOG.error('Playbook \"%s\" failed!' % playbook_path)\n    return results\n",
            "dependency": "",
            "docstring": "Wraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.",
            "end_lineno": "168",
            "file_content": "from datetime import datetime\nfrom distutils.util import strtobool\nimport errno\nimport json\nimport os\nimport re\nimport sys\nimport tempfile\n\nfrom infrared.core.utils import logger\nimport yaml\n\nLOG = logger.LOG\n\n\nclass NoAnsiFile(object):\n\n    re_ansi = re.compile(r'\\x1b[^m]*m')\n\n    def __init__(self, fd):\n        self.fd = fd\n\n    def write(self, data):\n        no_ansi_data = self.re_ansi.sub('', data)\n        self.fd.write(no_ansi_data)\n\n    def close(self):\n        self.fd.close()\n\n    def flush(self):\n        self.fd.flush()\n\n\nclass IRStdFd(object):\n    pass\n\n\nclass IRStdoutFd(IRStdFd):\n\n    def __init__(self, print_stdout=True):\n        self.print_stdout = print_stdout\n        self.org_stdout = sys.stdout\n        sys.stdout = self\n\n    def write(self, data):\n        if self.print_stdout:\n            sys.__stdout__.write(data)\n            sys.__stdout__.flush()\n        for fd in IRSTDFDManager.fds:\n            if not isinstance(fd, IRStdFd):\n                fd.write(data)\n                fd.flush()\n\n    @staticmethod\n    def flush():\n        sys.__stdout__.flush()\n\n    @staticmethod\n    def close():\n        sys.stdout = sys.__stdout__\n\n    @staticmethod\n    def fileno():\n        return sys.__stdout__.fileno()\n\n\nclass IRStderrFd(IRStdFd):\n\n    def __init__(self, print_stderr=True):\n        self.print_stderr = print_stderr\n        self.org_stderr = sys.stderr\n        sys.stderr = self\n\n    def write(self, data):\n        if self.print_stderr:\n            sys.__stderr__.write(data)\n            sys.__stderr__.flush()\n        for fd in IRSTDFDManager.fds:\n            if not isinstance(fd, IRStdFd):\n                fd.write(data)\n                fd.flush()\n\n    @staticmethod\n    def flush():\n        sys.__stderr__.flush()\n\n    @staticmethod\n    def close():\n        sys.stderr = sys.__stderr__\n\n\nclass IRSTDFDManager(object):\n\n    fds = set()\n\n    def __init__(self, stdout=True, stderr=True, *fds):\n\n        self.stdout = stdout\n        self.stderr = stderr\n\n        for fd in fds:\n            self.add(fd)\n\n        self.add(IRStdoutFd(print_stdout=self.stdout))\n        self.add(IRStderrFd(print_stderr=self.stderr))\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n\n    def write(self, data):\n        for fd in self.fds:\n            fd.write(data)\n            fd.flush()\n\n    def flush(self):\n        for fd in self.fds:\n            fd.flush()\n\n    def close(self):\n        for fd in self.fds:\n            fd.close()\n\n    def add(self, fd):\n        self.fds.add(fd)\n\n\ndef ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"Wraps the 'ansible-playbook' CLI.\n\n     :param ir_workspace: An Infrared Workspace object represents the active\n     workspace\n     :param ir_plugin: An InfraredPlugin object of the current plugin\n     :param playbook_path: the playbook to invoke\n     :param verbose: Ansible verbosity level\n     :param extra_vars: dict. Passed to Ansible as extra-vars\n     :param ansible_args: dict of ansible-playbook arguments to plumb down\n         directly to Ansible.\n    \"\"\"\n    ansible_args = ansible_args or []\n    LOG.debug(\"Additional ansible args: {}\".format(ansible_args))\n\n    # hack for verbosity\n    from ansible.utils.display import Display\n    display = Display(verbosity=verbose)\n    import __main__ as main\n    setattr(main, \"display\", display)\n\n    # TODO(yfried): Use proper ansible API instead of emulating CLI\n    cli_args = ['execute',\n                playbook_path,\n                '--inventory', ir_workspace.inventory]\n\n    # infrared should not change ansible verbosity unless user specifies that\n    if verbose:\n        cli_args.append('-' + 'v' * int(verbose))\n\n    cli_args.extend(ansible_args)\n\n    results = _run_playbook(cli_args, vars_dict=extra_vars or {},\n                            ir_workspace=ir_workspace, ir_plugin=ir_plugin)\n\n    if results:\n        LOG.error('Playbook \"%s\" failed!' % playbook_path)\n    return results\n\n\ndef _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list  of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active\n     workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n\n    # TODO(yfried): use ansible vars object instead of tmpfile\n    # NOTE(oanufrii): !!!this import should be exactly here!!!\n    #                 Ansible uses 'display' singleton from '__main__' and\n    #                 gets it on module level. While we monkeypatching our\n    #                 '__main__' in 'ansible_playbook' function import of\n    #                 PlaybookCLI shoul be after that, to get patched\n    #                 '__main__'. Otherwise ansible gets unpatched '__main__'\n    #                 and creates new 'display' object with default (0)\n    #                 verbosity.\n    # NOTE(afazekas): GlobalCLIArgs gets value only once per invocation, but\n    # since it has singleton decorator, so it would remember to old arguments in different tests\n    # removing the singleton decorator\n    try:\n        from ansible.utils import context_objects\n        context_objects.GlobalCLIArgs = context_objects.CLIArgs\n    except ImportError:\n        # older version\n        pass\n\n    from ansible.cli.playbook import PlaybookCLI\n    from ansible.errors import AnsibleOptionsError\n    from ansible.errors import AnsibleParserError\n\n    with tempfile.NamedTemporaryFile(\n            mode='w+', prefix=\"ir-settings-\", delete=True) as tmp:\n        tmp.write(yaml.safe_dump(vars_dict, default_flow_style=False))\n        # make sure created file is readable.\n        tmp.flush()\n        cli_args.extend(['--extra-vars', \"@\" + tmp.name])\n\n        if not bool(strtobool(os.environ.get('IR_NO_EXTRAS', 'no'))):\n            ir_extras = {\n                'infrared': {\n                    'python': {\n                        'executable': sys.executable,\n                        'version': {\n                            'full': sys.version.split()[0],\n                            'major': sys.version_info.major,\n                            'minor': sys.version_info.minor,\n                            'micro': sys.version_info.micro,\n                        }\n                    }\n                }\n            }\n            cli_args.extend(['--extra-vars', str(ir_extras)])\n\n        cli = PlaybookCLI(cli_args)\n        LOG.debug('Starting ansible cli with args: {}'.format(cli_args[1:]))\n        try:\n            cli.parse()\n\n            stdout = not bool(\n                strtobool(os.environ.get('IR_ANSIBLE_NO_STDOUT', 'no')))\n            stderr = not bool(\n                strtobool(os.environ.get('IR_ANSIBLE_NO_STDERR', 'no')))\n\n            ansible_outputs_dir = \\\n                os.path.join(ir_workspace.path, 'ansible_outputs')\n            ansible_vars_dir = \\\n                os.path.join(ir_workspace.path, 'ansible_vars')\n\n            timestamp = datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S.%f\")\n            filename_template = \\\n                \"ir_{timestamp}_{plugin_name}{postfix}.{file_ext}\"\n\n            for _dir in (ansible_outputs_dir, ansible_vars_dir):\n                try:\n                    os.makedirs(_dir)\n                except OSError as e:\n                    if e.errno != errno.EEXIST:\n                        raise\n\n            if bool(strtobool(os.environ.get('IR_GEN_VARS_JSON', 'no'))):\n                filename = filename_template.format(\n                    timestamp=timestamp,\n                    plugin_name=ir_plugin.name,\n                    postfix='',\n                    file_ext='json'\n                )\n                vars_file = os.path.join(ansible_vars_dir, filename)\n                with open(vars_file, 'w') as fp:\n                    json.dump(vars_dict, fp, indent=4, sort_keys=True)\n\n            with IRSTDFDManager(stdout=stdout, stderr=stderr) as fd_manager:\n\n                if bool(strtobool(os.environ.get(\n                        'IR_ANSIBLE_LOG_OUTPUT', 'no'))):\n                    filename = filename_template.format(\n                        timestamp=timestamp,\n                        plugin_name=ir_plugin.name,\n                        postfix='',\n                        file_ext='log'\n                    )\n                    log_file = os.path.join(ansible_outputs_dir, filename)\n                    fd_manager.add(open(log_file, 'w'))\n\n                if bool(strtobool(os.environ.get(\n                        'IR_ANSIBLE_LOG_OUTPUT_NO_ANSI', 'no'))):\n                    filename = filename_template.format(\n                        timestamp=timestamp,\n                        plugin_name=ir_plugin.name,\n                        postfix='_no_ansi',\n                        file_ext='log'\n                    )\n                    log_file = os.path.join(ansible_outputs_dir, filename)\n                    fd_manager.add(NoAnsiFile(open(log_file, 'w')))\n\n                # Return the result:\n                # 0: Success\n                # 1: \"Error\"\n                # 2: Host failed\n                # 3: Unreachable\n                # 4: Parser Error\n                # 5: Options error\n\n                return cli.run()\n\n        except (AnsibleParserError, AnsibleOptionsError) as error:\n            LOG.error('{}: {}'.format(type(error), error))\n            raise error\n",
            "file_path": "infrared/core/execute.py",
            "human_label": "Wraps the 'ansible-playbook' CLI.\n\n:param ir_workspace: An Infrared Workspace object represents the active\nworkspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:param playbook_path: the playbook to invoke\n:param verbose: Ansible verbosity level\n:param extra_vars: dict. Passed to Ansible as extra-vars\n:param ansible_args: dict of ansible-playbook arguments to plumb down\n    directly to Ansible.",
            "level": "file_runnable",
            "lineno": "130",
            "name": "ansible_playbook",
            "oracle_context": "{ \"apis\" : \"['debug', '_run_playbook', 'append', 'format', 'error', 'setattr', 'int', 'extend']\", \"classes\" : \"['Display', 'main']\", \"vars\" : \"['LOG', 'Str', 'inventory']\" }",
            "package": "execute",
            "project": "redhat-openstack/infrared",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "6306093273426c38ae68ad15",
            "all_context": "{ \"import\" : \"\", \"file\" : \"\", \"class\" : \"\" }",
            "code": "def _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list  of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active\n     workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n\n    # TODO(yfried): use ansible vars object instead of tmpfile\n    # NOTE(oanufrii): !!!this import should be exactly here!!!\n    #                 Ansible uses 'display' singleton from '__main__' and\n    #                 gets it on module level. While we monkeypatching our\n    #                 '__main__' in 'ansible_playbook' function import of\n    #                 PlaybookCLI shoul be after that, to get patched\n    #                 '__main__'. Otherwise ansible gets unpatched '__main__'\n    #                 and creates new 'display' object with default (0)\n    #                 verbosity.\n    # NOTE(afazekas): GlobalCLIArgs gets value only once per invocation, but\n    # since it has singleton decorator, so it would remember to old arguments in different tests\n    # removing the singleton decorator\n    try:\n        from ansible.utils import context_objects\n        context_objects.GlobalCLIArgs = context_objects.CLIArgs\n    except ImportError:\n        # older version\n        pass\n\n    from ansible.cli.playbook import PlaybookCLI\n    from ansible.errors import AnsibleOptionsError\n    from ansible.errors import AnsibleParserError\n\n    with tempfile.NamedTemporaryFile(\n            mode='w+', prefix=\"ir-settings-\", delete=True) as tmp:\n        tmp.write(yaml.safe_dump(vars_dict, default_flow_style=False))\n        # make sure created file is readable.\n        tmp.flush()\n        cli_args.extend(['--extra-vars', \"@\" + tmp.name])\n\n        if not bool(strtobool(os.environ.get('IR_NO_EXTRAS', 'no'))):\n            ir_extras = {\n                'infrared': {\n                    'python': {\n                        'executable': sys.executable,\n                        'version': {\n                            'full': sys.version.split()[0],\n                            'major': sys.version_info.major,\n                            'minor': sys.version_info.minor,\n                            'micro': sys.version_info.micro,\n                        }\n                    }\n                }\n            }\n            cli_args.extend(['--extra-vars', str(ir_extras)])\n\n        cli = PlaybookCLI(cli_args)\n        LOG.debug('Starting ansible cli with args: {}'.format(cli_args[1:]))\n        try:\n            cli.parse()\n\n            stdout = not bool(\n                strtobool(os.environ.get('IR_ANSIBLE_NO_STDOUT', 'no')))\n            stderr = not bool(\n                strtobool(os.environ.get('IR_ANSIBLE_NO_STDERR', 'no')))\n\n            ansible_outputs_dir = \\\n                os.path.join(ir_workspace.path, 'ansible_outputs')\n            ansible_vars_dir = \\\n                os.path.join(ir_workspace.path, 'ansible_vars')\n\n            timestamp = datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S.%f\")\n            filename_template = \\\n                \"ir_{timestamp}_{plugin_name}{postfix}.{file_ext}\"\n\n            for _dir in (ansible_outputs_dir, ansible_vars_dir):\n                try:\n                    os.makedirs(_dir)\n                except OSError as e:\n                    if e.errno != errno.EEXIST:\n                        raise\n\n            if bool(strtobool(os.environ.get('IR_GEN_VARS_JSON', 'no'))):\n                filename = filename_template.format(\n                    timestamp=timestamp,\n                    plugin_name=ir_plugin.name,\n                    postfix='',\n                    file_ext='json'\n                )\n                vars_file = os.path.join(ansible_vars_dir, filename)\n                with open(vars_file, 'w') as fp:\n                    json.dump(vars_dict, fp, indent=4, sort_keys=True)\n\n            with IRSTDFDManager(stdout=stdout, stderr=stderr) as fd_manager:\n\n                if bool(strtobool(os.environ.get(\n                        'IR_ANSIBLE_LOG_OUTPUT', 'no'))):\n                    filename = filename_template.format(\n                        timestamp=timestamp,\n                        plugin_name=ir_plugin.name,\n                        postfix='',\n                        file_ext='log'\n                    )\n                    log_file = os.path.join(ansible_outputs_dir, filename)\n                    fd_manager.add(open(log_file, 'w'))\n\n                if bool(strtobool(os.environ.get(\n                        'IR_ANSIBLE_LOG_OUTPUT_NO_ANSI', 'no'))):\n                    filename = filename_template.format(\n                        timestamp=timestamp,\n                        plugin_name=ir_plugin.name,\n                        postfix='_no_ansi',\n                        file_ext='log'\n                    )\n                    log_file = os.path.join(ansible_outputs_dir, filename)\n                    fd_manager.add(NoAnsiFile(open(log_file, 'w')))\n\n                # Return the result:\n                # 0: Success\n                # 1: \"Error\"\n                # 2: Host failed\n                # 3: Unreachable\n                # 4: Parser Error\n                # 5: Options error\n\n                return cli.run()\n\n        except (AnsibleParserError, AnsibleOptionsError) as error:\n            LOG.error('{}: {}'.format(type(error), error))\n            raise error\n",
            "dependency": "",
            "docstring": "Runs ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results",
            "end_lineno": "301",
            "file_content": "from datetime import datetime\nfrom distutils.util import strtobool\nimport errno\nimport json\nimport os\nimport re\nimport sys\nimport tempfile\n\nfrom infrared.core.utils import logger\nimport yaml\n\nLOG = logger.LOG\n\n\nclass NoAnsiFile(object):\n\n    re_ansi = re.compile(r'\\x1b[^m]*m')\n\n    def __init__(self, fd):\n        self.fd = fd\n\n    def write(self, data):\n        no_ansi_data = self.re_ansi.sub('', data)\n        self.fd.write(no_ansi_data)\n\n    def close(self):\n        self.fd.close()\n\n    def flush(self):\n        self.fd.flush()\n\n\nclass IRStdFd(object):\n    pass\n\n\nclass IRStdoutFd(IRStdFd):\n\n    def __init__(self, print_stdout=True):\n        self.print_stdout = print_stdout\n        self.org_stdout = sys.stdout\n        sys.stdout = self\n\n    def write(self, data):\n        if self.print_stdout:\n            sys.__stdout__.write(data)\n            sys.__stdout__.flush()\n        for fd in IRSTDFDManager.fds:\n            if not isinstance(fd, IRStdFd):\n                fd.write(data)\n                fd.flush()\n\n    @staticmethod\n    def flush():\n        sys.__stdout__.flush()\n\n    @staticmethod\n    def close():\n        sys.stdout = sys.__stdout__\n\n    @staticmethod\n    def fileno():\n        return sys.__stdout__.fileno()\n\n\nclass IRStderrFd(IRStdFd):\n\n    def __init__(self, print_stderr=True):\n        self.print_stderr = print_stderr\n        self.org_stderr = sys.stderr\n        sys.stderr = self\n\n    def write(self, data):\n        if self.print_stderr:\n            sys.__stderr__.write(data)\n            sys.__stderr__.flush()\n        for fd in IRSTDFDManager.fds:\n            if not isinstance(fd, IRStdFd):\n                fd.write(data)\n                fd.flush()\n\n    @staticmethod\n    def flush():\n        sys.__stderr__.flush()\n\n    @staticmethod\n    def close():\n        sys.stderr = sys.__stderr__\n\n\nclass IRSTDFDManager(object):\n\n    fds = set()\n\n    def __init__(self, stdout=True, stderr=True, *fds):\n\n        self.stdout = stdout\n        self.stderr = stderr\n\n        for fd in fds:\n            self.add(fd)\n\n        self.add(IRStdoutFd(print_stdout=self.stdout))\n        self.add(IRStderrFd(print_stderr=self.stderr))\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.close()\n\n    def write(self, data):\n        for fd in self.fds:\n            fd.write(data)\n            fd.flush()\n\n    def flush(self):\n        for fd in self.fds:\n            fd.flush()\n\n    def close(self):\n        for fd in self.fds:\n            fd.close()\n\n    def add(self, fd):\n        self.fds.add(fd)\n\n\ndef ansible_playbook(ir_workspace, ir_plugin, playbook_path, verbose=None,\n                     extra_vars=None, ansible_args=None):\n    \"\"\"Wraps the 'ansible-playbook' CLI.\n\n     :param ir_workspace: An Infrared Workspace object represents the active\n     workspace\n     :param ir_plugin: An InfraredPlugin object of the current plugin\n     :param playbook_path: the playbook to invoke\n     :param verbose: Ansible verbosity level\n     :param extra_vars: dict. Passed to Ansible as extra-vars\n     :param ansible_args: dict of ansible-playbook arguments to plumb down\n         directly to Ansible.\n    \"\"\"\n    ansible_args = ansible_args or []\n    LOG.debug(\"Additional ansible args: {}\".format(ansible_args))\n\n    # hack for verbosity\n    from ansible.utils.display import Display\n    display = Display(verbosity=verbose)\n    import __main__ as main\n    setattr(main, \"display\", display)\n\n    # TODO(yfried): Use proper ansible API instead of emulating CLI\n    cli_args = ['execute',\n                playbook_path,\n                '--inventory', ir_workspace.inventory]\n\n    # infrared should not change ansible verbosity unless user specifies that\n    if verbose:\n        cli_args.append('-' + 'v' * int(verbose))\n\n    cli_args.extend(ansible_args)\n\n    results = _run_playbook(cli_args, vars_dict=extra_vars or {},\n                            ir_workspace=ir_workspace, ir_plugin=ir_plugin)\n\n    if results:\n        LOG.error('Playbook \"%s\" failed!' % playbook_path)\n    return results\n\n\ndef _run_playbook(cli_args, vars_dict, ir_workspace, ir_plugin):\n    \"\"\"Runs ansible cli with vars dict\n\n    :param vars_dict: dict, Will be passed as Ansible extra-vars\n    :param cli_args: the list  of command line arguments\n    :param ir_workspace: An Infrared Workspace object represents the active\n     workspace\n    :param ir_plugin: An InfraredPlugin object of the current plugin\n    :return: ansible results\n    \"\"\"\n\n    # TODO(yfried): use ansible vars object instead of tmpfile\n    # NOTE(oanufrii): !!!this import should be exactly here!!!\n    #                 Ansible uses 'display' singleton from '__main__' and\n    #                 gets it on module level. While we monkeypatching our\n    #                 '__main__' in 'ansible_playbook' function import of\n    #                 PlaybookCLI shoul be after that, to get patched\n    #                 '__main__'. Otherwise ansible gets unpatched '__main__'\n    #                 and creates new 'display' object with default (0)\n    #                 verbosity.\n    # NOTE(afazekas): GlobalCLIArgs gets value only once per invocation, but\n    # since it has singleton decorator, so it would remember to old arguments in different tests\n    # removing the singleton decorator\n    try:\n        from ansible.utils import context_objects\n        context_objects.GlobalCLIArgs = context_objects.CLIArgs\n    except ImportError:\n        # older version\n        pass\n\n    from ansible.cli.playbook import PlaybookCLI\n    from ansible.errors import AnsibleOptionsError\n    from ansible.errors import AnsibleParserError\n\n    with tempfile.NamedTemporaryFile(\n            mode='w+', prefix=\"ir-settings-\", delete=True) as tmp:\n        tmp.write(yaml.safe_dump(vars_dict, default_flow_style=False))\n        # make sure created file is readable.\n        tmp.flush()\n        cli_args.extend(['--extra-vars', \"@\" + tmp.name])\n\n        if not bool(strtobool(os.environ.get('IR_NO_EXTRAS', 'no'))):\n            ir_extras = {\n                'infrared': {\n                    'python': {\n                        'executable': sys.executable,\n                        'version': {\n                            'full': sys.version.split()[0],\n                            'major': sys.version_info.major,\n                            'minor': sys.version_info.minor,\n                            'micro': sys.version_info.micro,\n                        }\n                    }\n                }\n            }\n            cli_args.extend(['--extra-vars', str(ir_extras)])\n\n        cli = PlaybookCLI(cli_args)\n        LOG.debug('Starting ansible cli with args: {}'.format(cli_args[1:]))\n        try:\n            cli.parse()\n\n            stdout = not bool(\n                strtobool(os.environ.get('IR_ANSIBLE_NO_STDOUT', 'no')))\n            stderr = not bool(\n                strtobool(os.environ.get('IR_ANSIBLE_NO_STDERR', 'no')))\n\n            ansible_outputs_dir = \\\n                os.path.join(ir_workspace.path, 'ansible_outputs')\n            ansible_vars_dir = \\\n                os.path.join(ir_workspace.path, 'ansible_vars')\n\n            timestamp = datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S.%f\")\n            filename_template = \\\n                \"ir_{timestamp}_{plugin_name}{postfix}.{file_ext}\"\n\n            for _dir in (ansible_outputs_dir, ansible_vars_dir):\n                try:\n                    os.makedirs(_dir)\n                except OSError as e:\n                    if e.errno != errno.EEXIST:\n                        raise\n\n            if bool(strtobool(os.environ.get('IR_GEN_VARS_JSON', 'no'))):\n                filename = filename_template.format(\n                    timestamp=timestamp,\n                    plugin_name=ir_plugin.name,\n                    postfix='',\n                    file_ext='json'\n                )\n                vars_file = os.path.join(ansible_vars_dir, filename)\n                with open(vars_file, 'w') as fp:\n                    json.dump(vars_dict, fp, indent=4, sort_keys=True)\n\n            with IRSTDFDManager(stdout=stdout, stderr=stderr) as fd_manager:\n\n                if bool(strtobool(os.environ.get(\n                        'IR_ANSIBLE_LOG_OUTPUT', 'no'))):\n                    filename = filename_template.format(\n                        timestamp=timestamp,\n                        plugin_name=ir_plugin.name,\n                        postfix='',\n                        file_ext='log'\n                    )\n                    log_file = os.path.join(ansible_outputs_dir, filename)\n                    fd_manager.add(open(log_file, 'w'))\n\n                if bool(strtobool(os.environ.get(\n                        'IR_ANSIBLE_LOG_OUTPUT_NO_ANSI', 'no'))):\n                    filename = filename_template.format(\n                        timestamp=timestamp,\n                        plugin_name=ir_plugin.name,\n                        postfix='_no_ansi',\n                        file_ext='log'\n                    )\n                    log_file = os.path.join(ansible_outputs_dir, filename)\n                    fd_manager.add(NoAnsiFile(open(log_file, 'w')))\n\n                # Return the result:\n                # 0: Success\n                # 1: \"Error\"\n                # 2: Host failed\n                # 3: Unreachable\n                # 4: Parser Error\n                # 5: Options error\n\n                return cli.run()\n\n        except (AnsibleParserError, AnsibleOptionsError) as error:\n            LOG.error('{}: {}'.format(type(error), error))\n            raise error\n",
            "file_path": "infrared/core/execute.py",
            "human_label": "Runs ansible cli with vars dict\n\n:param vars_dict: dict, Will be passed as Ansible extra-vars\n:param cli_args: the list  of command line arguments\n:param ir_workspace: An Infrared Workspace object represents the active\n workspace\n:param ir_plugin: An InfraredPlugin object of the current plugin\n:return: ansible results",
            "level": "project_runnable",
            "lineno": "171",
            "name": "_run_playbook",
            "oracle_context": "{ \"apis\" : \"['write', 'error', 'open', 'dump', 'debug', 'NamedTemporaryFile', 'join', 'safe_dump', 'run', 'strftime', 'parse', 'extend', 'strtobool', 'format', 'get', 'utcnow', 'flush', 'str', 'makedirs', 'bool', 'type', 'add', 'split']\", \"classes\" : \"['errno', 'strtobool', 'AnsibleOptionsError', 'tempfile', 'IRSTDFDManager', 'json', 'PlaybookCLI', 'AnsibleParserError', 'yaml', 'datetime', 'os', 'NoAnsiFile', 'context_objects', 'sys']\", \"vars\" : \"['Str', 'CLIArgs', 'path', 'environ', 'major', 'minor', 'micro', 'version_info', 'EEXIST', 'executable', 'errno', 'LOG', 'name', 'GlobalCLIArgs', 'version']\" }",
            "package": "execute",
            "project": "redhat-openstack/infrared",
            "test_lineno": "",
            "test_name": ""
        },
        {
            "_id": "63060b1b73426c38ae68ad43",
            "all_context": "{ \"import\" : \"os sys json __future__ pbr pkg infrared argcomplete \", \"file\" : \"\", \"class\" : \"self.spec_handler(self,parser,args) ; self.__init__(self,name) ; self._checkout_workspace(self,name,create) ; self._checkout_workspace ; self._fetch_inventory ; self._create_workspace(self,name) ; self.name ; self.workspace_manager ; self.kwargs ; self._create_workspace ; self.extend_cli(self,root_subparsers) ; self._fetch_inventory(self,name) ; \" }",
            "code": "    def extend_cli(self, root_subparsers):\n        workspace_plugin = root_subparsers.add_parser(\n            self.name,\n            help=self.kwargs[\"description\"],\n            **self.kwargs)\n        workspace_subparsers = workspace_plugin.add_subparsers(dest=\"command0\")\n\n        # create\n        create_parser = workspace_subparsers.add_parser(\n            'create', help='Creates a new workspace')\n        create_parser.add_argument(\"name\", help=\"Workspace name\")\n\n        # checkout\n        checkout_parser = workspace_subparsers.add_parser(\n            'checkout',\n            help='Switches workspace to the specified workspace')\n        checkout_parser.add_argument(\n            \"name\",\n            help=\"Workspace name\").completer = completers.workspace_list\n        checkout_parser.add_argument(\n            \"-c\", \"--create\", action='store_true', dest=\"checkout_create\",\n            help=\"Creates a workspace if not exists and \"\n                 \"switches to it\")\n\n        # inventory\n        inventory_parser = workspace_subparsers.add_parser(\n            'inventory',\n            help=\"prints workspace's inventory file\")\n        inventory_parser.add_argument(\n            \"name\", help=\"Workspace name\",\n            nargs=\"?\").completer = completers.workspace_list\n\n        # list\n        wrkspc_list_parser = workspace_subparsers.add_parser(\n            'list', help='Lists all the workspaces')\n        wrkspc_list_parser.add_argument(\n            \"--active\", action='store_true', dest='print_active',\n            help=\"Prints the active workspace only\")\n\n        # delete\n        delete_parser = workspace_subparsers.add_parser(\n            'delete', help='Deletes workspaces')\n        delete_parser.add_argument(\n            'name', nargs='+',\n            help=\"Workspace names\").completer = completers.workspace_list\n\n        # cleanup\n        cleanup_parser = workspace_subparsers.add_parser(\n            'cleanup', help='Removes all the files from workspace')\n        cleanup_parser.add_argument(\n            \"name\",\n            help=\"Workspace name\").completer = completers.workspace_list\n\n        # import settings\n        importer_parser = workspace_subparsers.add_parser(\n            'import', help='Import deployment configs.')\n        importer_parser.add_argument(\"filename\", help=\"Archive file name or URL.\")\n        importer_parser.add_argument(\n            \"-n\", \"--name\", dest=\"workspacename\",\n            help=\"Workspace name to import with. \"\n            \"If not specified - file name will be used.\")\n\n        # export settings\n        exporter_parser = workspace_subparsers.add_parser(\n            'export', help='Export deployment configurations.')\n        exporter_parser.add_argument(\n            \"-n\", \"--name\", dest=\"workspacename\",\n            help=\"Workspace name. If not sepecified - active \"\n            \"workspace will be used.\").completer = completers.workspace_list\n        exporter_parser.add_argument(\"-f\", \"--filename\", dest=\"filename\",\n                                     help=\"Archive file name.\")\n\n        exporter_parser.add_argument(\"-K\", \"--copy-keys\", dest=\"copykeys\",\n                                     action=\"store_true\",\n                                     help=\"Silently copy ssh keys \"\n                                     \"to workspace.\")\n        # node list\n        nodelist_parser = workspace_subparsers.add_parser(\n            'node-list',\n            help='List nodes, managed by workspace')\n        nodelist_parser.add_argument(\n            \"-n\", \"--name\",\n            help=\"Workspace name\").completer = completers.workspace_list\n        nodelist_parser.add_argument(\n            \"-g\", \"--group\",\n            help=\"List nodes in specific group\"\n        ).completer = completers.group_list\n        nodelist_parser.add_argument(\n            \"-f\", \"--format\", choices=['fancy', 'json'], default='fancy',\n            help=\"Output format\")\n\n        # group list\n        grouplist_parser = workspace_subparsers.add_parser(\n            'group-list',\n            help='List groups, managed by workspace')\n        grouplist_parser.add_argument(\n            \"-n\", \"--name\",\n            help=\"Workspace name\").completer = completers.workspace_list\n",
            "dependency": "",
            "docstring": "Adds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.",
            "end_lineno": "156",
            "file_content": "from __future__ import print_function\n\nimport argcomplete\nimport json\nimport os\nfrom pbr import version\nimport pkg_resources as pkg\nimport sys\n\n\ndef inject_common_paths():\n    \"\"\"Discover the path to the common/ directory provided by infrared core.\"\"\"\n    def override_conf_path(common_path, envvar, specific_dir):\n        conf_path = os.environ.get(envvar, '')\n        additional_conf_path = os.path.join(common_path, specific_dir)\n        if conf_path:\n            full_conf_path = ':'.join([additional_conf_path, conf_path])\n        else:\n            full_conf_path = additional_conf_path\n        os.environ[envvar] = full_conf_path\n\n    version_info = version.VersionInfo('infrared')\n\n    common_path = pkg.resource_filename(version_info.package,\n                                        'common')\n    override_conf_path(common_path, 'ANSIBLE_ROLES_PATH', 'roles')\n    override_conf_path(common_path, 'ANSIBLE_FILTER_PLUGINS', 'filter_plugins')\n    override_conf_path(common_path, 'ANSIBLE_CALLBACK_PLUGINS',\n                       'callback_plugins')\n    override_conf_path(common_path, 'ANSIBLE_LIBRARY', 'library')\n\n\n# This needs to be called here because as soon as an ansible class is loaded\n# the code in constants.py is triggered. That code reads the configuration\n# settings from all sources (ansible.cfg, environment variables, etc).\n# If the first include to ansible modules is moved deeper in the InfraRed\n# code (or on demand), then this call can be moved as well in that place.\ninject_common_paths()\n\nfrom infrared import api  # noqa\nimport infrared.bash_completers as completers  # noqa\nfrom infrared.core.services import CoreServices  # noqa\nfrom infrared.core.services.plugins import PLUGINS_REGISTRY  # noqa\nfrom infrared.core.utils import exceptions  # noqa\nfrom infrared.core.utils import interactive_ssh  # noqa\nfrom infrared.core.utils import logger  # noqa\nfrom infrared.core.utils.print_formats import fancy_table  # noqa\n\nLOG = logger.LOG\n\n\nclass WorkspaceManagerSpec(api.SpecObject):\n    \"\"\"The workspace manager CLI. \"\"\"\n\n    def __init__(self, name, *args, **kwargs):\n        super(WorkspaceManagerSpec, self).__init__(name, **kwargs)\n        self.workspace_manager = CoreServices.workspace_manager()\n\n    def extend_cli(self, root_subparsers):\n        workspace_plugin = root_subparsers.add_parser(\n            self.name,\n            help=self.kwargs[\"description\"],\n            **self.kwargs)\n        workspace_subparsers = workspace_plugin.add_subparsers(dest=\"command0\")\n\n        # create\n        create_parser = workspace_subparsers.add_parser(\n            'create', help='Creates a new workspace')\n        create_parser.add_argument(\"name\", help=\"Workspace name\")\n\n        # checkout\n        checkout_parser = workspace_subparsers.add_parser(\n            'checkout',\n            help='Switches workspace to the specified workspace')\n        checkout_parser.add_argument(\n            \"name\",\n            help=\"Workspace name\").completer = completers.workspace_list\n        checkout_parser.add_argument(\n            \"-c\", \"--create\", action='store_true', dest=\"checkout_create\",\n            help=\"Creates a workspace if not exists and \"\n                 \"switches to it\")\n\n        # inventory\n        inventory_parser = workspace_subparsers.add_parser(\n            'inventory',\n            help=\"prints workspace's inventory file\")\n        inventory_parser.add_argument(\n            \"name\", help=\"Workspace name\",\n            nargs=\"?\").completer = completers.workspace_list\n\n        # list\n        wrkspc_list_parser = workspace_subparsers.add_parser(\n            'list', help='Lists all the workspaces')\n        wrkspc_list_parser.add_argument(\n            \"--active\", action='store_true', dest='print_active',\n            help=\"Prints the active workspace only\")\n\n        # delete\n        delete_parser = workspace_subparsers.add_parser(\n            'delete', help='Deletes workspaces')\n        delete_parser.add_argument(\n            'name', nargs='+',\n            help=\"Workspace names\").completer = completers.workspace_list\n\n        # cleanup\n        cleanup_parser = workspace_subparsers.add_parser(\n            'cleanup', help='Removes all the files from workspace')\n        cleanup_parser.add_argument(\n            \"name\",\n            help=\"Workspace name\").completer = completers.workspace_list\n\n        # import settings\n        importer_parser = workspace_subparsers.add_parser(\n            'import', help='Import deployment configs.')\n        importer_parser.add_argument(\"filename\", help=\"Archive file name or URL.\")\n        importer_parser.add_argument(\n            \"-n\", \"--name\", dest=\"workspacename\",\n            help=\"Workspace name to import with. \"\n            \"If not specified - file name will be used.\")\n\n        # export settings\n        exporter_parser = workspace_subparsers.add_parser(\n            'export', help='Export deployment configurations.')\n        exporter_parser.add_argument(\n            \"-n\", \"--name\", dest=\"workspacename\",\n            help=\"Workspace name. If not sepecified - active \"\n            \"workspace will be used.\").completer = completers.workspace_list\n        exporter_parser.add_argument(\"-f\", \"--filename\", dest=\"filename\",\n                                     help=\"Archive file name.\")\n\n        exporter_parser.add_argument(\"-K\", \"--copy-keys\", dest=\"copykeys\",\n                                     action=\"store_true\",\n                                     help=\"Silently copy ssh keys \"\n                                     \"to workspace.\")\n        # node list\n        nodelist_parser = workspace_subparsers.add_parser(\n            'node-list',\n            help='List nodes, managed by workspace')\n        nodelist_parser.add_argument(\n            \"-n\", \"--name\",\n            help=\"Workspace name\").completer = completers.workspace_list\n        nodelist_parser.add_argument(\n            \"-g\", \"--group\",\n            help=\"List nodes in specific group\"\n        ).completer = completers.group_list\n        nodelist_parser.add_argument(\n            \"-f\", \"--format\", choices=['fancy', 'json'], default='fancy',\n            help=\"Output format\")\n\n        # group list\n        grouplist_parser = workspace_subparsers.add_parser(\n            'group-list',\n            help='List groups, managed by workspace')\n        grouplist_parser.add_argument(\n            \"-n\", \"--name\",\n            help=\"Workspace name\").completer = completers.workspace_list\n\n    def spec_handler(self, parser, args):\n        \"\"\"Handles all the plugin manager commands\n\n        :param parser: the infrared parser object.\n        :param args: the list of arguments received from cli.\n        \"\"\"\n        pargs = parser.parse_args(args)\n        subcommand = pargs.command0\n\n        if subcommand == 'create':\n            self._create_workspace(pargs.name)\n        elif subcommand == 'checkout':\n            self._checkout_workspace(pargs.name, pargs.checkout_create)\n        elif subcommand == 'inventory':\n            self._fetch_inventory(pargs.name)\n        elif subcommand == 'list':\n            if pargs.print_active:\n                print(self.workspace_manager.get_active_workspace().name)\n            else:\n                workspaces = self.workspace_manager.list()\n                headers = (\"Name\", \"Active\")\n                workspaces = sorted([workspace.name for workspace in\n                                     self.workspace_manager.list()])\n                print(fancy_table(\n                    headers,\n                    *[(workspace, ' ' * (len(headers[-1]) // 2) + \"*\" if\n                        self.workspace_manager.is_active(workspace) else \"\")\n                      for workspace in workspaces]))\n        elif subcommand == 'delete':\n            for workspace_name in pargs.name:\n                self.workspace_manager.delete(workspace_name)\n                print(\"Workspace '{}' deleted\".format(workspace_name))\n        elif subcommand == 'cleanup':\n            self.workspace_manager.cleanup(pargs.name)\n        elif subcommand == 'export':\n            self.workspace_manager.export_workspace(\n                pargs.workspacename, pargs.filename, pargs.copykeys)\n        elif subcommand == 'import':\n            self.workspace_manager.import_workspace(\n                pargs.filename, pargs.workspacename)\n        elif subcommand == 'node-list':\n            nodes = self.workspace_manager.node_list(pargs.name, pargs.group)\n            if pargs.format == 'json':\n                nodes_dict = [\n                    {'name': name, 'address': address, 'groups': groups}\n                    for name, address, groups in nodes]\n                print(json.dumps({'nodes': nodes_dict}))\n            else:\n                print(fancy_table(\n                    (\"Name\", \"Address\", \"Groups\"),\n                    *[node_name for node_name in nodes]))\n        elif subcommand == \"group-list\":\n            groups = self.workspace_manager.group_list(pargs.name)\n            print(fancy_table(\n                (\"Name\", \"Nodes\"), *[group_name for group_name in groups]))\n\n    def _create_workspace(self, name):\n        \"\"\"Creates a workspace\n\n        :param name: Name of the workspace to create\n        \"\"\"\n        self.workspace_manager.create(name)\n        print(\"Workspace '{}' has been added\".format(name))\n\n    def _checkout_workspace(self, name, create=False):\n        \"\"\"Checkouts (activate) a workspace\n\n        :param name: The name of the workspace to checkout\n        :param create: if set to true will create a new workspace\n        before checking out to it\n        \"\"\"\n        if create:\n            self._create_workspace(name)\n        self.workspace_manager.activate(name)\n        print(\"Now using workspace: '{}'\".format(name))\n\n    def _fetch_inventory(self, name):\n        \"\"\"fetch inventory file for workspace.\n\n        if no active workspace found - create a new workspace\n        \"\"\"\n        if name:\n            wkspc = self.workspace_manager.get(name)\n        else:\n            wkspc = self.workspace_manager.get_active_workspace()\n        if not wkspc:\n            raise exceptions.IRNoActiveWorkspaceFound()\n        print(wkspc.inventory)\n\n\nclass PluginManagerSpec(api.SpecObject):\n\n    def __init__(self, name, *args, **kwargs):\n        super(PluginManagerSpec, self).__init__(name, *args, **kwargs)\n        self.plugin_manager = CoreServices.plugins_manager()\n\n    def extend_cli(self, root_subparsers):\n        plugin_parser = root_subparsers.add_parser(\n            self.name,\n            help=self.kwargs[\"description\"],\n            **self.kwargs)\n        plugin_subparsers = plugin_parser.add_subparsers(dest=\"command0\")\n\n        # Add plugin\n        add_parser = plugin_subparsers.add_parser(\n            'add', help='Add a plugin')\n        add_parser.add_argument(\"src\", nargs='+',\n                                help=\"Plugin Source (name/path/git URL)\\n'all'\"\n                                     \" will install all available plugins\")\n        add_parser.add_argument(\"--revision\", help=\"git branch/tag/revision\"\n                                \" sourced plugins. Ignored for\"\n                                \"'plugin add all' command.\")\n\n        add_parser.add_argument(\"--src-path\",\n                                help=\"Relative path within the repository \"\n                                     \"where infrared plugin can be found.\\n\"\n                                     \"(Required with --link-roles\")\n\n        add_parser.add_argument(\"--link-roles\", action='store_true',\n                                help=\"Auto creates symbolic 'roles' directory \"\n                                     \"in the path provided with '--src-path' \"\n                                     \"which points to the 'roles' directory \"\n                                     \"inside the project's root dir if exists,\"\n                                     \" otherwise to the project's root dir \"\n                                     \"itself.\")\n\n        add_parser.add_argument(\"--skip-roles\", action='store_true',\n                                help=\"Skip the from file roles installation. \"\n                                     \"(Don't install Ansible roles from \"\n                                     \"'requirements.yml' or \"\n                                     \"'requirements.yaml' file)\")\n\n        # Remove plugin\n        remove_parser = plugin_subparsers.add_parser(\n            \"remove\",\n            help=\"Remove a plugin, 'all' will remove all installed plugins\")\n        remove_parser.add_argument(\n            \"name\", nargs='+',\n            help=\"Plugin name\").completer = completers.plugin_list\n\n        # List command\n        list_parser = plugin_subparsers.add_parser(\n            'list', help='List all the available plugins')\n        list_parser.add_argument(\n            \"--available\", action='store_true',\n            help=\"Prints all available plugins in addition \"\n                 \"to installed plugins\")\n        list_parser.add_argument(\n            \"--versions\", action='store_true',\n            help=\"Prints version of each installed plugins\")\n\n        # Update plugin\n        update_parser = plugin_subparsers.add_parser(\n            \"update\",\n            help=\"Update a Git-based plugin\")\n        update_parser.add_argument(\n            \"name\",\n            help=\"Name of the plugin to update\")\n        update_parser.add_argument(\n            \"revision\", nargs='?', default='latest',\n            help=\"Revision number to checkout (if not given, will only pull \"\n                 \"changes from the remote)\")\n        update_parser.add_argument(\n            '--skip_reqs', '-s', action='store_true',\n            help=\"Skips plugin's requirements installation\")\n        update_parser.add_argument(\n            '--hard-reset', action='store_true',\n            help=\"Drop all local changes using hard \"\n                 \"reset (changes will be stashed\")\n\n        plugin_subparsers.add_parser(\n            \"freeze\", help=\"Run through installed plugins. For git sourced \"\n            \"one writes its current revision to plugins registry.\")\n\n        # search all plugins from github organization\n        plugin_subparsers.add_parser(\n            'search', help='Search and list all the available plugins from '\n            \"rhos-infra organization on GitHub\")\n\n        # import plugins from registry yml file\n        plugin_subparsers.add_parser(\n            'import', help='Install plugins from a YAML file')\n\n        # Add plugin\n        import_parser = plugin_subparsers.add_parser(\n            'import', help='Install plugins from a registry YML file')\n        import_parser.add_argument(\"src\",\n                                   help=\"The registry YML file Source\")\n\n    def spec_handler(self, parser, args):\n        \"\"\"Handles all the plugin manager commands\n\n        :param parser: the infrared parser object.\n        :param args: the list of arguments received from cli.\n        \"\"\"\n        pargs = parser.parse_args(args)\n        subcommand = pargs.command0\n\n        if subcommand == 'list':\n            self._list_plugins(pargs.available, pargs.versions)\n        elif subcommand == 'add':\n            if 'all' in pargs.src:\n                self.plugin_manager.add_all_available()\n                self._list_plugins(print_available=False, print_version=False)\n            else:\n                if len(pargs.src) > 1 and (pargs.revision or pargs.src_path):\n                    raise exceptions.IRFailedToAddPlugin(\n                        \"'--revision' works with one plugin source only.\")\n                for _plugin in pargs.src:\n                    self.plugin_manager.add_plugin(\n                        _plugin, rev=pargs.revision,\n                        plugin_src_path=pargs.src_path,\n                        skip_roles=pargs.skip_roles,\n                        link_roles=pargs.link_roles)\n        elif subcommand == 'remove':\n            if 'all' in pargs.name:\n                self.plugin_manager.remove_all()\n                self._list_plugins(print_available=False, print_version=False)\n            else:\n                for _plugin in pargs.name:\n                    self.plugin_manager.remove_plugin(_plugin)\n        elif subcommand == 'freeze':\n            self.plugin_manager.freeze()\n        elif subcommand == 'update':\n            self.plugin_manager.update_plugin(\n                pargs.name, pargs.revision, pargs.skip_reqs, pargs.hard_reset)\n        elif subcommand == 'search':\n            self._search_plugins()\n        elif subcommand == 'import':\n            self.plugin_manager.import_plugins(pargs.src)\n\n    def _list_plugins(self, print_available=False, print_version=False):\n        \"\"\"Print a list of installed & available plugins\"\"\"\n        table_rows = []\n        table_headers = [\"Type\", \"Name\"]\n        installed_mark = ' ' * (len('Installed') // 2) + '*'\n\n        plugins_dict = \\\n            self.plugin_manager.get_all_plugins() \\\n            if print_available \\\n            else self.plugin_manager.get_installed_plugins()\n\n        for plugins_type, plugins in plugins_dict.items():\n            installed_plugins_list = \\\n                self.plugin_manager.get_installed_plugins(plugins_type).keys()\n            plugins_names = list(plugins.keys())\n            plugins_names.sort()\n\n            if print_available:\n                all_plugins_list = []\n                for plugin_name in plugins_names:\n                    all_plugins_list.append(plugin_name)\n                installed_plugins_mark_list = \\\n                    [installed_mark if plugin_name in installed_plugins_list\n                     else '' for plugin_name in all_plugins_list]\n\n                plugins_descs = \\\n                    [PLUGINS_REGISTRY.get(plugin, {}).get('desc', '')\n                     for plugin in all_plugins_list]\n\n                row = [plugins_type, '\\n'.join(all_plugins_list),\n                       '\\n'.join(installed_plugins_mark_list),\n                       '\\n'.join(plugins_descs)]\n\n                if print_version:\n                    plugins_version = [\n                        self.plugin_manager.get_plugin_version(plugin_name)\n                        if plugin_name in installed_plugins_list else ''\n                        for plugin_name in all_plugins_list]\n\n                    row.append('\\n'.join(plugins_version))\n\n            else:\n                row = [\n                    plugins_type,\n                    '\\n'.join(installed_plugins_list)]\n\n                if print_version:\n                    plugins_version = [self.plugin_manager.get_plugin_version(\n                        plugin_name) for plugin_name in installed_plugins_list]\n                    row.append('\\n'.join(plugins_version))\n\n            table_rows.append(row)\n\n        if print_available:\n            table_headers.append(\"Installed\")\n            table_headers.append(\"Description\")\n\n        if print_version:\n            table_headers.append(\"Version\")\n\n        print(fancy_table(table_headers, *table_rows))\n\n    def _search_plugins(self):\n        \"\"\"Search git organizations and print a list of available plugins \"\"\"\n\n        table_rows = []\n        table_headers = [\"Type\", \"Name\", \"Description\", \"Source\"]\n\n        plugins_dict = \\\n            self.plugin_manager.get_all_git_plugins()\n\n        for plugins_type, plugins in plugins_dict.items():\n            # prepare empty lists\n            all_plugins_list = []\n            plugins_descs = []\n            plugins_sources = []\n\n            for plugin_name in sorted(plugins.iterkeys()):\n                # get all plugin names\n                all_plugins_list.append(plugin_name)\n                # get all plugin descriptions\n                plugins_descs.append(plugins[plugin_name][\"desc\"])\n                # get all plugins sources\n                plugins_sources.append(plugins[plugin_name][\"src\"])\n\n            table_rows.append([\n                plugins_type,\n                '\\n'.join(all_plugins_list),\n                '\\n'.join(plugins_descs),\n                '\\n'.join(plugins_sources)])\n\n        print(fancy_table(table_headers, *table_rows))\n\n\nclass SSHSpec(api.SpecObject):\n\n    def __init__(self, name, *args, **kwargs):\n        super(SSHSpec, self).__init__(name, *args, **kwargs)\n\n    def extend_cli(self, root_subparsers):\n        issh_parser = root_subparsers.add_parser(\n            self.name,\n            help=self.kwargs[\"description\"],\n            **self.kwargs)\n\n        issh_parser.add_argument(\"node_name\", help=\"Node name. \"\n                                 \"Ex.: controller-0\"\n                                 ).completer = completers.node_list\n        issh_parser.add_argument(\"remote_command\", nargs=\"?\", help=\"Run \"\n                                 \"provided command line on remote host and \"\n                                 \"return its output.\")\n\n    def spec_handler(self, parser, args):\n        \"\"\"Handles the ssh command\n\n        :param parser: the infrared parser object.\n        :param args: the list of arguments received from cli.\n        \"\"\"\n        pargs = parser.parse_args(args)\n        return interactive_ssh.ssh_to_host(\n            pargs.node_name, remote_command=pargs.remote_command)\n\n\ndef main(args=None):\n    CoreServices.setup()\n\n    # inject ansible config file\n    CoreServices.ansible_config_manager().inject_config()\n\n    specs_manager = api.SpecManager()\n\n    # Init Managers\n    specs_manager.register_spec(\n        WorkspaceManagerSpec('workspace',\n                             description=\"Workspace manager. \"\n                                         \"Allows to create and use an \"\n                                         \"isolated environment for plugins \"\n                                         \"execution.\"))\n    specs_manager.register_spec(\n        PluginManagerSpec('plugin',\n                          description=\"Plugin management\"))\n\n    specs_manager.register_spec(\n        SSHSpec(\n            'ssh',\n            description=\"Interactive ssh session to node from inventory.\"))\n\n    # register all plugins\n    for plugin in CoreServices.plugins_manager().PLUGINS_DICT.values():\n        specs_manager.register_spec(api.InfraredPluginsSpec(plugin))\n\n    argcomplete.autocomplete(specs_manager.parser)\n    return specs_manager.run_specs(args) or 0\n\n\nif __name__ == '__main__':\n    sys.exit(int(main() or 0))\n",
            "file_path": "infrared/main.py",
            "human_label": "Adds the spec cli options to to the main entry point.\n\n:param subparser: the subparser object to extend.",
            "level": "project_runnable",
            "lineno": "59",
            "name": "extend_cli",
            "oracle_context": "{ \"apis\" : \"['add_parser', 'add_argument', 'add_subparsers']\", \"classes\" : \"['completers']\", \"vars\" : \"['completer', 'workspace_list', 'group_list', 'name', 'kwargs']\" }",
            "package": "main",
            "project": "redhat-openstack/infrared",
            "test_lineno": "",
            "test_name": ""
        }
    ]
}